{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble,tree,linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier,GradientBoostingRegressor)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "# from sklearn.cross_validation import KFold\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "py.init_notebook_mode(connected=True)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Models\n",
    "\n",
    "from sklearn import ensemble,tree,linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Print R2 and RMSE scores\n",
    "def get_score(prediction, labels):\n",
    "    print('R2: {}'.format(r2_score(prediction, labels)))\n",
    "    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction,labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "# Show scores for train and validation sets\n",
    "def train_test(estimator, x_train, x_test, y_train, y_test):\n",
    "    prediction_train = estimator.predict(x_train)\n",
    "    print(estimator)\n",
    "    get_score(prediction_train,y_train)\n",
    "    \n",
    "    prediction_test = estimator.predict(x_test)\n",
    "    print('Test')\n",
    "    get_score(prediction_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Elastic Net\n",
    "def eNet(x_train,y_train,x_val,y_val):\n",
    "    ENSTest = linear_model.ElasticNetCV(alphas=[0.0001,0.0005,0.001,0.01,0.1,1,10],l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(x_train,y_train)\n",
    "    train_test(ENSTest, x_train,x_train,y_train,y_train)\n",
    "\n",
    "    # Average R2 score and standard deviation of 5-fold cross-validation\n",
    "    scores = cross_val_score(ENSTest, x_train, y_train, cv=5)\n",
    "    print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "def gBoost(x_train,y_train,x_val,y_val):\n",
    "    GBest = ensemble.GradientBoostingRegressor(n_estimators=3000,learning_rate=0.05,max_depth=3,max_features='sqrt',\n",
    "                                              min_samples_leaf=15,min_samples_split=10,loss='huber').fit(x_train,y_train)\n",
    "    train_test(GBest,x_train,x_train,y_train,y_train)\n",
    "    # Average R2 score and standard deviation of 5-fold cross-validation\n",
    "    scores = cross_val_score(GBest, x_train, y_train, cv=5)\n",
    "    print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))\n",
    "\n",
    "    x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Neural networks\n",
    "\n",
    "# # X_train = X_train_adas13[features].values\n",
    "# y_train = y_train.values.reshape(-1,1)\n",
    "# y_train.shape\n",
    "\n",
    "def random_batch(x_train, y_train, batch_size):\n",
    "    \n",
    "    num = x_train.shape[0]\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num,\n",
    "                           size=batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = x_train[idx]\n",
    "    y_batch = y_train[idx]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN(x_train,y_train,x_val,y_val):\n",
    "    # Convert y_train shape to ?x1\n",
    "    y_train = y_train.values.reshape(-1,1)\n",
    "\n",
    "    n_input = x_train.shape[1]\n",
    "    n_hidden1 = 128\n",
    "    n_hidden2 = 512\n",
    "    n_hidden3 = 1024\n",
    "    n_output = 1\n",
    "    learning_rate = 0.001\n",
    "    epochs = 10000\n",
    "    batch_size = 100\n",
    "    REGULARIZATION_RATE = 0.0001\n",
    "\n",
    "    X = tf.placeholder(tf.float32,[None,n_input])\n",
    "    y_gt = tf.placeholder(tf.float32,[None,n_output])\n",
    "    # regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False)\n",
    "    W1 = tf.Variable(initializer([n_input,n_hidden1]))\n",
    "    # tf.add_to_collection('losses',regularizer(W1))\n",
    "    b1 = tf.Variable(tf.constant(0.1,shape=[n_hidden1]))\n",
    "    H1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "    W2 = tf.Variable(initializer([n_hidden1,n_hidden2]))\n",
    "    # tf.add_to_collection('losses',regularizer(W2))\n",
    "    b2 = tf.Variable(tf.constant(0.1,shape=[n_hidden2]))\n",
    "    H2 = tf.nn.relu(tf.matmul(H1,W2)+b2)\n",
    "\n",
    "    W3 = tf.Variable(initializer([n_hidden2,n_hidden3]))\n",
    "    # tf.add_to_collection('losses',regularizer(W3))\n",
    "    b3 = tf.Variable(tf.constant(0.1,shape=[n_hidden3]))\n",
    "    H3 = tf.nn.relu(tf.matmul(H2,W3)+b3)\n",
    "\n",
    "    W_out = tf.Variable(initializer([n_hidden3,n_output]))\n",
    "    # tf.add_to_collection('losses',regularizer(W_out))\n",
    "    b_out = tf.Variable(tf.constant(0.1,shape=[n_output]))\n",
    "    y_pred = tf.matmul(H3,W_out)+b_out\n",
    "\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_gt,predictions=y_pred)) \n",
    "    # + tf.add_n(tf.get_collection('losses'))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for iter in range(epochs):\n",
    "\n",
    "        sess.run(train_step, feed_dict={X:x_train, y_gt:y_train})\n",
    "        if iter%1000 == 0:\n",
    "            train_loss = sess.run(loss, feed_dict={X:x_train, y_gt:y_train})\n",
    "            validation_loss = sess.run(loss, feed_dict={X:x_train, y_gt:y_train})\n",
    "            print(\"Iter %d, training loss %f, validation loss %f\" % (iter, train_loss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_adas13=sess.run(y_pred,feed_dict={X:x_train})\n",
    "\n",
    "y_train\n",
    "\n",
    "X_test = X_test_adas13[features].values\n",
    "\n",
    "adas13_pred = sess.run(y_pred,feed_dict={X:X_test})*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
