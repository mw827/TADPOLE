{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble,tree,linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier,GradientBoostingRegressor)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "# from sklearn.cross_validation import KFold\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "py.init_notebook_mode(connected=True)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>...</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>PTID_Key</th>\n",
       "      <th>EXAMDATE</th>\n",
       "      <th>DX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.914682</td>\n",
       "      <td>-0.601231</td>\n",
       "      <td>-1.025777</td>\n",
       "      <td>0.646176</td>\n",
       "      <td>1.011424</td>\n",
       "      <td>0.545620</td>\n",
       "      <td>0.413133</td>\n",
       "      <td>0.157729</td>\n",
       "      <td>0.240342</td>\n",
       "      <td>1.240933</td>\n",
       "      <td>-0.614915</td>\n",
       "      <td>1.991610</td>\n",
       "      <td>0.506536</td>\n",
       "      <td>0.278690</td>\n",
       "      <td>0.153647</td>\n",
       "      <td>-0.377177</td>\n",
       "      <td>1.514802</td>\n",
       "      <td>-0.019620</td>\n",
       "      <td>0.283497</td>\n",
       "      <td>-0.656795</td>\n",
       "      <td>0.221551</td>\n",
       "      <td>0.350008</td>\n",
       "      <td>0.196467</td>\n",
       "      <td>-0.604921</td>\n",
       "      <td>-1.521657</td>\n",
       "      <td>-0.185724</td>\n",
       "      <td>-1.876779</td>\n",
       "      <td>-0.208486</td>\n",
       "      <td>0.673291</td>\n",
       "      <td>-0.596495</td>\n",
       "      <td>0.631191</td>\n",
       "      <td>0.029072</td>\n",
       "      <td>0.877182</td>\n",
       "      <td>-0.778522</td>\n",
       "      <td>0.761286</td>\n",
       "      <td>-1.076368</td>\n",
       "      <td>-0.941719</td>\n",
       "      <td>0.219719</td>\n",
       "      <td>2.491332</td>\n",
       "      <td>-1.257230</td>\n",
       "      <td>-0.303478</td>\n",
       "      <td>-1.353673</td>\n",
       "      <td>-0.387802</td>\n",
       "      <td>0.508279</td>\n",
       "      <td>-0.721791</td>\n",
       "      <td>-0.805647</td>\n",
       "      <td>-0.692598</td>\n",
       "      <td>0.739660</td>\n",
       "      <td>0.774142</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.111548</td>\n",
       "      <td>-1.504085</td>\n",
       "      <td>0.280236</td>\n",
       "      <td>-1.026748</td>\n",
       "      <td>-0.935085</td>\n",
       "      <td>-0.300665</td>\n",
       "      <td>-1.065586</td>\n",
       "      <td>-0.794485</td>\n",
       "      <td>0.810877</td>\n",
       "      <td>0.240799</td>\n",
       "      <td>-0.403455</td>\n",
       "      <td>-0.313444</td>\n",
       "      <td>-0.471123</td>\n",
       "      <td>1.063699</td>\n",
       "      <td>-0.801149</td>\n",
       "      <td>-0.386264</td>\n",
       "      <td>0.054835</td>\n",
       "      <td>0.332211</td>\n",
       "      <td>-2.399556</td>\n",
       "      <td>1.094873</td>\n",
       "      <td>2.336983</td>\n",
       "      <td>-2.151991</td>\n",
       "      <td>0.262187</td>\n",
       "      <td>0.434412</td>\n",
       "      <td>1.312646</td>\n",
       "      <td>-1.291214</td>\n",
       "      <td>0.415570</td>\n",
       "      <td>-1.990970</td>\n",
       "      <td>-1.362115</td>\n",
       "      <td>-0.359844</td>\n",
       "      <td>-0.691137</td>\n",
       "      <td>0.531001</td>\n",
       "      <td>0.796599</td>\n",
       "      <td>0.538899</td>\n",
       "      <td>0.668736</td>\n",
       "      <td>-0.362717</td>\n",
       "      <td>0.526381</td>\n",
       "      <td>-1.574725</td>\n",
       "      <td>1.199216</td>\n",
       "      <td>0.881033</td>\n",
       "      <td>-0.162544</td>\n",
       "      <td>1.078744</td>\n",
       "      <td>1.362077</td>\n",
       "      <td>1.059333</td>\n",
       "      <td>0.267904</td>\n",
       "      <td>-0.928759</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-12-10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.551814</td>\n",
       "      <td>-0.979159</td>\n",
       "      <td>-1.117663</td>\n",
       "      <td>1.076771</td>\n",
       "      <td>0.625453</td>\n",
       "      <td>0.594147</td>\n",
       "      <td>0.200665</td>\n",
       "      <td>-0.109799</td>\n",
       "      <td>0.167253</td>\n",
       "      <td>1.085187</td>\n",
       "      <td>-0.908410</td>\n",
       "      <td>0.744116</td>\n",
       "      <td>0.728125</td>\n",
       "      <td>-0.159216</td>\n",
       "      <td>0.144665</td>\n",
       "      <td>0.513853</td>\n",
       "      <td>1.799283</td>\n",
       "      <td>-0.349975</td>\n",
       "      <td>0.227961</td>\n",
       "      <td>-0.650050</td>\n",
       "      <td>0.099265</td>\n",
       "      <td>0.732234</td>\n",
       "      <td>0.033733</td>\n",
       "      <td>-0.569960</td>\n",
       "      <td>-1.853381</td>\n",
       "      <td>0.309617</td>\n",
       "      <td>-1.390592</td>\n",
       "      <td>-0.524115</td>\n",
       "      <td>0.653087</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>1.020155</td>\n",
       "      <td>-0.361017</td>\n",
       "      <td>1.143764</td>\n",
       "      <td>-0.811648</td>\n",
       "      <td>0.879657</td>\n",
       "      <td>-0.934078</td>\n",
       "      <td>-0.719017</td>\n",
       "      <td>-0.174933</td>\n",
       "      <td>1.807723</td>\n",
       "      <td>-0.246274</td>\n",
       "      <td>0.197087</td>\n",
       "      <td>-0.588264</td>\n",
       "      <td>-0.028376</td>\n",
       "      <td>1.524079</td>\n",
       "      <td>-0.491840</td>\n",
       "      <td>-0.773806</td>\n",
       "      <td>-0.905032</td>\n",
       "      <td>0.587289</td>\n",
       "      <td>0.482128</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.362359</td>\n",
       "      <td>-0.384883</td>\n",
       "      <td>-0.396091</td>\n",
       "      <td>-1.114439</td>\n",
       "      <td>-1.497867</td>\n",
       "      <td>0.073814</td>\n",
       "      <td>-1.047930</td>\n",
       "      <td>-0.472296</td>\n",
       "      <td>0.541844</td>\n",
       "      <td>-0.597084</td>\n",
       "      <td>-0.788613</td>\n",
       "      <td>-0.759914</td>\n",
       "      <td>-0.559378</td>\n",
       "      <td>0.918252</td>\n",
       "      <td>-0.153937</td>\n",
       "      <td>-0.072157</td>\n",
       "      <td>0.764843</td>\n",
       "      <td>0.574582</td>\n",
       "      <td>-1.299033</td>\n",
       "      <td>1.859001</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>-0.972449</td>\n",
       "      <td>1.815612</td>\n",
       "      <td>-0.805537</td>\n",
       "      <td>1.022320</td>\n",
       "      <td>-1.829569</td>\n",
       "      <td>0.756406</td>\n",
       "      <td>-0.444980</td>\n",
       "      <td>-2.338430</td>\n",
       "      <td>0.632880</td>\n",
       "      <td>-0.912492</td>\n",
       "      <td>1.094919</td>\n",
       "      <td>0.033690</td>\n",
       "      <td>0.674762</td>\n",
       "      <td>0.643271</td>\n",
       "      <td>-0.362390</td>\n",
       "      <td>0.543195</td>\n",
       "      <td>-1.660143</td>\n",
       "      <td>1.020524</td>\n",
       "      <td>1.949152</td>\n",
       "      <td>-0.949106</td>\n",
       "      <td>-0.750425</td>\n",
       "      <td>0.614402</td>\n",
       "      <td>1.703636</td>\n",
       "      <td>-0.747710</td>\n",
       "      <td>-0.966335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-04-07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.819792</td>\n",
       "      <td>-0.982411</td>\n",
       "      <td>-1.010481</td>\n",
       "      <td>1.050002</td>\n",
       "      <td>0.690168</td>\n",
       "      <td>0.794730</td>\n",
       "      <td>0.281609</td>\n",
       "      <td>-0.584068</td>\n",
       "      <td>0.522935</td>\n",
       "      <td>1.314278</td>\n",
       "      <td>-0.468990</td>\n",
       "      <td>1.015674</td>\n",
       "      <td>0.095780</td>\n",
       "      <td>-0.090131</td>\n",
       "      <td>0.121122</td>\n",
       "      <td>0.641263</td>\n",
       "      <td>1.195369</td>\n",
       "      <td>-0.043699</td>\n",
       "      <td>0.300918</td>\n",
       "      <td>-0.510973</td>\n",
       "      <td>0.033367</td>\n",
       "      <td>0.266118</td>\n",
       "      <td>0.286762</td>\n",
       "      <td>-0.413685</td>\n",
       "      <td>-1.853315</td>\n",
       "      <td>0.160174</td>\n",
       "      <td>-1.505841</td>\n",
       "      <td>-0.255259</td>\n",
       "      <td>1.073686</td>\n",
       "      <td>-0.130857</td>\n",
       "      <td>0.113276</td>\n",
       "      <td>-0.490394</td>\n",
       "      <td>0.355425</td>\n",
       "      <td>-1.346322</td>\n",
       "      <td>0.936334</td>\n",
       "      <td>-1.610474</td>\n",
       "      <td>-0.638695</td>\n",
       "      <td>-0.203596</td>\n",
       "      <td>1.830329</td>\n",
       "      <td>-0.354029</td>\n",
       "      <td>0.131519</td>\n",
       "      <td>-1.169916</td>\n",
       "      <td>0.146008</td>\n",
       "      <td>1.354220</td>\n",
       "      <td>0.128852</td>\n",
       "      <td>-0.306639</td>\n",
       "      <td>-0.527814</td>\n",
       "      <td>0.150390</td>\n",
       "      <td>0.098864</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.125737</td>\n",
       "      <td>-0.956597</td>\n",
       "      <td>0.800094</td>\n",
       "      <td>-0.947476</td>\n",
       "      <td>-1.217942</td>\n",
       "      <td>-0.685731</td>\n",
       "      <td>0.343844</td>\n",
       "      <td>0.290111</td>\n",
       "      <td>-0.847178</td>\n",
       "      <td>-0.035952</td>\n",
       "      <td>0.708540</td>\n",
       "      <td>0.377110</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>1.057412</td>\n",
       "      <td>0.114082</td>\n",
       "      <td>-0.368786</td>\n",
       "      <td>-0.383575</td>\n",
       "      <td>0.110381</td>\n",
       "      <td>-1.250354</td>\n",
       "      <td>-0.132147</td>\n",
       "      <td>0.602319</td>\n",
       "      <td>-0.741663</td>\n",
       "      <td>1.161139</td>\n",
       "      <td>0.772682</td>\n",
       "      <td>0.659802</td>\n",
       "      <td>-0.370310</td>\n",
       "      <td>0.504005</td>\n",
       "      <td>-1.390165</td>\n",
       "      <td>-1.299680</td>\n",
       "      <td>-0.612009</td>\n",
       "      <td>-1.319248</td>\n",
       "      <td>-0.379710</td>\n",
       "      <td>0.861513</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>1.025970</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.394066</td>\n",
       "      <td>-0.682618</td>\n",
       "      <td>1.512006</td>\n",
       "      <td>0.429191</td>\n",
       "      <td>-0.094298</td>\n",
       "      <td>0.603369</td>\n",
       "      <td>0.183042</td>\n",
       "      <td>0.871928</td>\n",
       "      <td>0.794578</td>\n",
       "      <td>0.589846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-09-08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.336588</td>\n",
       "      <td>0.124982</td>\n",
       "      <td>0.265170</td>\n",
       "      <td>0.702093</td>\n",
       "      <td>-0.806993</td>\n",
       "      <td>0.915643</td>\n",
       "      <td>-0.873193</td>\n",
       "      <td>0.532319</td>\n",
       "      <td>-1.443681</td>\n",
       "      <td>0.450639</td>\n",
       "      <td>-0.561673</td>\n",
       "      <td>-0.045912</td>\n",
       "      <td>0.461204</td>\n",
       "      <td>-0.131562</td>\n",
       "      <td>0.077892</td>\n",
       "      <td>-0.862214</td>\n",
       "      <td>0.354069</td>\n",
       "      <td>-0.058075</td>\n",
       "      <td>-0.284119</td>\n",
       "      <td>-2.005307</td>\n",
       "      <td>-0.728140</td>\n",
       "      <td>-0.122163</td>\n",
       "      <td>0.388809</td>\n",
       "      <td>-0.725412</td>\n",
       "      <td>-0.164666</td>\n",
       "      <td>-0.481027</td>\n",
       "      <td>-0.053525</td>\n",
       "      <td>1.137394</td>\n",
       "      <td>-0.665674</td>\n",
       "      <td>-0.085696</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>-0.143032</td>\n",
       "      <td>0.536505</td>\n",
       "      <td>-0.038190</td>\n",
       "      <td>0.683802</td>\n",
       "      <td>0.057937</td>\n",
       "      <td>-0.216116</td>\n",
       "      <td>-0.079253</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>-0.709941</td>\n",
       "      <td>0.157998</td>\n",
       "      <td>0.185480</td>\n",
       "      <td>0.370402</td>\n",
       "      <td>-0.753654</td>\n",
       "      <td>0.781240</td>\n",
       "      <td>0.886344</td>\n",
       "      <td>-0.357192</td>\n",
       "      <td>-1.061215</td>\n",
       "      <td>0.452014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714254</td>\n",
       "      <td>-0.367555</td>\n",
       "      <td>-1.874418</td>\n",
       "      <td>-0.115493</td>\n",
       "      <td>-0.505355</td>\n",
       "      <td>1.326566</td>\n",
       "      <td>0.931132</td>\n",
       "      <td>-1.040033</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>-0.922823</td>\n",
       "      <td>0.459259</td>\n",
       "      <td>0.136026</td>\n",
       "      <td>0.516101</td>\n",
       "      <td>-1.122543</td>\n",
       "      <td>-1.276170</td>\n",
       "      <td>0.537032</td>\n",
       "      <td>-0.185670</td>\n",
       "      <td>-0.037686</td>\n",
       "      <td>0.163978</td>\n",
       "      <td>1.703618</td>\n",
       "      <td>-0.464324</td>\n",
       "      <td>-0.760336</td>\n",
       "      <td>0.292739</td>\n",
       "      <td>-0.525850</td>\n",
       "      <td>1.064035</td>\n",
       "      <td>-1.654064</td>\n",
       "      <td>-0.097459</td>\n",
       "      <td>-0.879186</td>\n",
       "      <td>0.406081</td>\n",
       "      <td>-0.507800</td>\n",
       "      <td>-0.140618</td>\n",
       "      <td>0.504883</td>\n",
       "      <td>1.182153</td>\n",
       "      <td>1.805809</td>\n",
       "      <td>-0.803499</td>\n",
       "      <td>0.384380</td>\n",
       "      <td>-0.600851</td>\n",
       "      <td>0.237532</td>\n",
       "      <td>2.262692</td>\n",
       "      <td>1.360018</td>\n",
       "      <td>0.069672</td>\n",
       "      <td>0.544021</td>\n",
       "      <td>-1.105401</td>\n",
       "      <td>0.759985</td>\n",
       "      <td>1.659128</td>\n",
       "      <td>-0.180586</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2006-07-21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.562420</td>\n",
       "      <td>0.529299</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.916200</td>\n",
       "      <td>-1.044956</td>\n",
       "      <td>1.685090</td>\n",
       "      <td>-0.881126</td>\n",
       "      <td>0.370308</td>\n",
       "      <td>-0.712720</td>\n",
       "      <td>-0.208872</td>\n",
       "      <td>-0.542123</td>\n",
       "      <td>0.410522</td>\n",
       "      <td>0.639658</td>\n",
       "      <td>0.323089</td>\n",
       "      <td>0.323216</td>\n",
       "      <td>-0.525278</td>\n",
       "      <td>0.635510</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>-0.366100</td>\n",
       "      <td>-2.154320</td>\n",
       "      <td>-0.987386</td>\n",
       "      <td>-0.190409</td>\n",
       "      <td>-0.108063</td>\n",
       "      <td>-0.892028</td>\n",
       "      <td>0.631797</td>\n",
       "      <td>-0.194658</td>\n",
       "      <td>-0.279678</td>\n",
       "      <td>1.133721</td>\n",
       "      <td>-1.076537</td>\n",
       "      <td>-0.565393</td>\n",
       "      <td>1.026210</td>\n",
       "      <td>-0.527438</td>\n",
       "      <td>0.445320</td>\n",
       "      <td>-0.061565</td>\n",
       "      <td>0.775616</td>\n",
       "      <td>-0.266960</td>\n",
       "      <td>-0.385827</td>\n",
       "      <td>-0.361899</td>\n",
       "      <td>-0.194316</td>\n",
       "      <td>0.149040</td>\n",
       "      <td>-0.662051</td>\n",
       "      <td>0.961501</td>\n",
       "      <td>0.486169</td>\n",
       "      <td>0.150510</td>\n",
       "      <td>0.524191</td>\n",
       "      <td>0.604291</td>\n",
       "      <td>-0.843304</td>\n",
       "      <td>-0.798732</td>\n",
       "      <td>0.027233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.848535</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>-3.297358</td>\n",
       "      <td>0.044732</td>\n",
       "      <td>-0.230049</td>\n",
       "      <td>0.959104</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>-2.249763</td>\n",
       "      <td>1.543976</td>\n",
       "      <td>-0.602299</td>\n",
       "      <td>0.342353</td>\n",
       "      <td>-0.407926</td>\n",
       "      <td>2.106130</td>\n",
       "      <td>-0.573788</td>\n",
       "      <td>-0.499155</td>\n",
       "      <td>0.522528</td>\n",
       "      <td>-0.143081</td>\n",
       "      <td>0.093791</td>\n",
       "      <td>-1.170585</td>\n",
       "      <td>1.653108</td>\n",
       "      <td>-0.311750</td>\n",
       "      <td>-0.543931</td>\n",
       "      <td>-1.058488</td>\n",
       "      <td>-1.638842</td>\n",
       "      <td>-0.074263</td>\n",
       "      <td>-1.314339</td>\n",
       "      <td>-0.251519</td>\n",
       "      <td>-0.417725</td>\n",
       "      <td>1.145915</td>\n",
       "      <td>0.412533</td>\n",
       "      <td>-0.835132</td>\n",
       "      <td>1.147302</td>\n",
       "      <td>0.691795</td>\n",
       "      <td>0.688599</td>\n",
       "      <td>-0.304910</td>\n",
       "      <td>0.089377</td>\n",
       "      <td>-0.876775</td>\n",
       "      <td>-0.559781</td>\n",
       "      <td>2.978864</td>\n",
       "      <td>0.207917</td>\n",
       "      <td>-0.230468</td>\n",
       "      <td>1.791909</td>\n",
       "      <td>-0.451569</td>\n",
       "      <td>1.189610</td>\n",
       "      <td>0.329250</td>\n",
       "      <td>-1.143252</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2007-01-16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.914682 -0.601231 -1.025777  0.646176  1.011424  0.545620  0.413133   \n",
       "1 -0.551814 -0.979159 -1.117663  1.076771  0.625453  0.594147  0.200665   \n",
       "2 -0.819792 -0.982411 -1.010481  1.050002  0.690168  0.794730  0.281609   \n",
       "3 -1.336588  0.124982  0.265170  0.702093 -0.806993  0.915643 -0.873193   \n",
       "4 -1.562420  0.529299  0.027853  0.916200 -1.044956  1.685090 -0.881126   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  0.157729  0.240342  1.240933 -0.614915  1.991610  0.506536  0.278690   \n",
       "1 -0.109799  0.167253  1.085187 -0.908410  0.744116  0.728125 -0.159216   \n",
       "2 -0.584068  0.522935  1.314278 -0.468990  1.015674  0.095780 -0.090131   \n",
       "3  0.532319 -1.443681  0.450639 -0.561673 -0.045912  0.461204 -0.131562   \n",
       "4  0.370308 -0.712720 -0.208872 -0.542123  0.410522  0.639658  0.323089   \n",
       "\n",
       "         14        15        16        17        18        19        20  \\\n",
       "0  0.153647 -0.377177  1.514802 -0.019620  0.283497 -0.656795  0.221551   \n",
       "1  0.144665  0.513853  1.799283 -0.349975  0.227961 -0.650050  0.099265   \n",
       "2  0.121122  0.641263  1.195369 -0.043699  0.300918 -0.510973  0.033367   \n",
       "3  0.077892 -0.862214  0.354069 -0.058075 -0.284119 -2.005307 -0.728140   \n",
       "4  0.323216 -0.525278  0.635510  0.016097 -0.366100 -2.154320 -0.987386   \n",
       "\n",
       "         21        22        23        24        25        26        27  \\\n",
       "0  0.350008  0.196467 -0.604921 -1.521657 -0.185724 -1.876779 -0.208486   \n",
       "1  0.732234  0.033733 -0.569960 -1.853381  0.309617 -1.390592 -0.524115   \n",
       "2  0.266118  0.286762 -0.413685 -1.853315  0.160174 -1.505841 -0.255259   \n",
       "3 -0.122163  0.388809 -0.725412 -0.164666 -0.481027 -0.053525  1.137394   \n",
       "4 -0.190409 -0.108063 -0.892028  0.631797 -0.194658 -0.279678  1.133721   \n",
       "\n",
       "         28        29        30        31        32        33        34  \\\n",
       "0  0.673291 -0.596495  0.631191  0.029072  0.877182 -0.778522  0.761286   \n",
       "1  0.653087  0.003768  1.020155 -0.361017  1.143764 -0.811648  0.879657   \n",
       "2  1.073686 -0.130857  0.113276 -0.490394  0.355425 -1.346322  0.936334   \n",
       "3 -0.665674 -0.085696  0.019830 -0.143032  0.536505 -0.038190  0.683802   \n",
       "4 -1.076537 -0.565393  1.026210 -0.527438  0.445320 -0.061565  0.775616   \n",
       "\n",
       "         35        36        37        38        39        40        41  \\\n",
       "0 -1.076368 -0.941719  0.219719  2.491332 -1.257230 -0.303478 -1.353673   \n",
       "1 -0.934078 -0.719017 -0.174933  1.807723 -0.246274  0.197087 -0.588264   \n",
       "2 -1.610474 -0.638695 -0.203596  1.830329 -0.354029  0.131519 -1.169916   \n",
       "3  0.057937 -0.216116 -0.079253  0.046781 -0.709941  0.157998  0.185480   \n",
       "4 -0.266960 -0.385827 -0.361899 -0.194316  0.149040 -0.662051  0.961501   \n",
       "\n",
       "         42        43        44        45        46        47        48 ...  \\\n",
       "0 -0.387802  0.508279 -0.721791 -0.805647 -0.692598  0.739660  0.774142 ...   \n",
       "1 -0.028376  1.524079 -0.491840 -0.773806 -0.905032  0.587289  0.482128 ...   \n",
       "2  0.146008  1.354220  0.128852 -0.306639 -0.527814  0.150390  0.098864 ...   \n",
       "3  0.370402 -0.753654  0.781240  0.886344 -0.357192 -1.061215  0.452014 ...   \n",
       "4  0.486169  0.150510  0.524191  0.604291 -0.843304 -0.798732  0.027233 ...   \n",
       "\n",
       "        232       233       234       235       236       237       238  \\\n",
       "0 -1.111548 -1.504085  0.280236 -1.026748 -0.935085 -0.300665 -1.065586   \n",
       "1 -1.362359 -0.384883 -0.396091 -1.114439 -1.497867  0.073814 -1.047930   \n",
       "2 -2.125737 -0.956597  0.800094 -0.947476 -1.217942 -0.685731  0.343844   \n",
       "3  0.714254 -0.367555 -1.874418 -0.115493 -0.505355  1.326566  0.931132   \n",
       "4 -0.848535  1.310900 -3.297358  0.044732 -0.230049  0.959104  0.463500   \n",
       "\n",
       "        239       240       241       242       243       244       245  \\\n",
       "0 -0.794485  0.810877  0.240799 -0.403455 -0.313444 -0.471123  1.063699   \n",
       "1 -0.472296  0.541844 -0.597084 -0.788613 -0.759914 -0.559378  0.918252   \n",
       "2  0.290111 -0.847178 -0.035952  0.708540  0.377110  0.002219  1.057412   \n",
       "3 -1.040033  0.000094 -0.922823  0.459259  0.136026  0.516101 -1.122543   \n",
       "4 -2.249763  1.543976 -0.602299  0.342353 -0.407926  2.106130 -0.573788   \n",
       "\n",
       "        246       247       248       249       250       251       252  \\\n",
       "0 -0.801149 -0.386264  0.054835  0.332211 -2.399556  1.094873  2.336983   \n",
       "1 -0.153937 -0.072157  0.764843  0.574582 -1.299033  1.859001  0.993939   \n",
       "2  0.114082 -0.368786 -0.383575  0.110381 -1.250354 -0.132147  0.602319   \n",
       "3 -1.276170  0.537032 -0.185670 -0.037686  0.163978  1.703618 -0.464324   \n",
       "4 -0.499155  0.522528 -0.143081  0.093791 -1.170585  1.653108 -0.311750   \n",
       "\n",
       "        253       254       255       256       257       258       259  \\\n",
       "0 -2.151991  0.262187  0.434412  1.312646 -1.291214  0.415570 -1.990970   \n",
       "1 -0.972449  1.815612 -0.805537  1.022320 -1.829569  0.756406 -0.444980   \n",
       "2 -0.741663  1.161139  0.772682  0.659802 -0.370310  0.504005 -1.390165   \n",
       "3 -0.760336  0.292739 -0.525850  1.064035 -1.654064 -0.097459 -0.879186   \n",
       "4 -0.543931 -1.058488 -1.638842 -0.074263 -1.314339 -0.251519 -0.417725   \n",
       "\n",
       "        260       261       262       263       264       265       266  \\\n",
       "0 -1.362115 -0.359844 -0.691137  0.531001  0.796599  0.538899  0.668736   \n",
       "1 -2.338430  0.632880 -0.912492  1.094919  0.033690  0.674762  0.643271   \n",
       "2 -1.299680 -0.612009 -1.319248 -0.379710  0.861513  0.097416  1.025970   \n",
       "3  0.406081 -0.507800 -0.140618  0.504883  1.182153  1.805809 -0.803499   \n",
       "4  1.145915  0.412533 -0.835132  1.147302  0.691795  0.688599 -0.304910   \n",
       "\n",
       "        267       268       269       270       271       272       273  \\\n",
       "0 -0.362717  0.526381 -1.574725  1.199216  0.881033 -0.162544  1.078744   \n",
       "1 -0.362390  0.543195 -1.660143  1.020524  1.949152 -0.949106 -0.750425   \n",
       "2  0.005399  0.394066 -0.682618  1.512006  0.429191 -0.094298  0.603369   \n",
       "3  0.384380 -0.600851  0.237532  2.262692  1.360018  0.069672  0.544021   \n",
       "4  0.089377 -0.876775 -0.559781  2.978864  0.207917 -0.230468  1.791909   \n",
       "\n",
       "        274       275       276       277  PTID_Key   EXAMDATE  DX  \n",
       "0  1.362077  1.059333  0.267904 -0.928759       1.0 2010-12-10   2  \n",
       "1  0.614402  1.703636 -0.747710 -0.966335       1.0 2011-04-07   2  \n",
       "2  0.183042  0.871928  0.794578  0.589846       1.0 2011-09-08   2  \n",
       "3 -1.105401  0.759985  1.659128 -0.180586       2.0 2006-07-21   0  \n",
       "4 -0.451569  1.189610  0.329250 -1.143252       2.0 2007-01-16   0  \n",
       "\n",
       "[5 rows x 281 columns]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input_Data = pd.read_csv('data/Input_interp_filledCat_codedCat.csv')\n",
    "Input_Data = pd.read_csv('data/Input_pca_0.95thres.csv')\n",
    "# Input_Data = pd.read_csv('data/Input_remove_corr.csv')\n",
    "# normalize the format of EXAMDATE\n",
    "Input_Data['EXAMDATE'] = pd.to_datetime(Input_Data['EXAMDATE'], errors='coerce')\n",
    "Input_Data.head()\n",
    "# Input_Data['EXAMDATE'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Input_Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do linear interpolation if there is data before and after for the same object, then do ffil and bfil\n",
    "def linear_interp(data):\n",
    "    ID_list = np.unique(data.PTID_Key.values)\n",
    "    # Create an empty dataframe with all columns from data\n",
    "    Input_new=pd.DataFrame(columns=data.columns)\n",
    "#     print(ID_list)\n",
    "    for ID in ID_list:\n",
    "#         print(ID)\n",
    "        df=data[data['PTID_Key']==ID]\n",
    "        # interpolate only for numeric data\n",
    "        df=df.interpolate()\n",
    "        # ffill, bfill numeric data that can't be interpolate as well as categorical data\n",
    "        df=df.fillna(method='ffill')\n",
    "        df=df.fillna(method='bfill')\n",
    "        Input_new=pd.concat([Input_new, df], ignore_index=True)\n",
    "\n",
    "    print(\"Remaining missing values: \", Input_new.isnull().sum().sum() )\n",
    "    print(\"Filled percentage: \", (1- Input_new.isnull().sum().sum()/data.isnull().sum().sum())*100,\"%\")\n",
    "    return Input_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def scale(data):\n",
    "#     df=pd.DataFrame(columns=data.columns)\n",
    "#     print(df.shape)\n",
    "#     df=data.drop(['PTID_Key','EXAMDATE', 'EXAMDATE_bl'], axis=1)\n",
    "    features=data.select_dtypes(exclude=[\"datetime\"]).columns.tolist()\n",
    "    # Separating out the features\n",
    "    for feature in features:\n",
    "    # Standardizing the features\n",
    "#         print(feature)\n",
    "        data[feature] = StandardScaler().fit_transform(data[feature].values.reshape(-1,1))\n",
    "    # df = StandardScaler().fit_transform(df)\n",
    "    return data\n",
    "    # df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the date formate, check if there are objects in output but not in input, sort by PTID and date, then linear interpolate\n",
    "def output_prep(data):    \n",
    "    \n",
    "#     data.rename(index=str,columns={'Date':'EXAMDATE'},inplace=True)\n",
    "    # normalize the format of EXAMDATE\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.head()\n",
    "    \n",
    "    # We can see if there is objects in data but not in df_data\n",
    "    data_id = list(set(data['PTID_Key'].values))\n",
    "    IDlist=[]\n",
    "    for ID in data_id:\n",
    "        if ID not in Input_Data['PTID_Key'].values:\n",
    "            IDlist.append(ID)\n",
    "    if len(IDlist)==0:\n",
    "        print(\"All objects in the train target file are in input data file\")\n",
    "    else:\n",
    "        print(\"% objects in the train target file are not in input data file:\" % (len(IDlist)),IDlist)\n",
    "\n",
    "    # sort the data by DX_bl and Month, then fill the missing data\n",
    "    data = data.sort_values(by=['PTID_Key','Date'])\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    data_new=linear_interp(data)\n",
    "    \n",
    "    # Drop the rows where at least one element is missing.\n",
    "    data_new=data_new.dropna()\n",
    "    data_new = data_new.reset_index(drop=True)\n",
    "    print(\"After dropping missing values, the shape of the dataset is: \", data_new.shape)\n",
    "    print(\"After dropping missing values, the total number of missing values is: \", data_new.isnull().sum().sum())\n",
    "#     print(data_new.head())\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input and output data for feeding models: cross-product, remove PTID & date, then scale data\n",
    "def prep_for_models(Input,Output):\n",
    "#     Input=scale(Input)\n",
    "    # Cross-product (based on same PTID) the input (M rows) and output files (N rows) to get the data (MxN) for models\n",
    "    data=Input.merge(Output, left_on='PTID_Key', right_on='PTID_Key')\n",
    "    print(\"After merging input and output, the shape of the data is: \",data.shape)\n",
    "    \n",
    "    # Get the month between each input and each output\n",
    "    data['Month_inter']=np.ceil((data['Date']-data['EXAMDATE'])/np.timedelta64(1, 'M'))\n",
    "    print(\"Adding the month interval, the shape of the data is: \",data.shape)\n",
    "    # Scale numeric data\n",
    "    data = data.drop(\"PTID_Key\", axis=1) \n",
    "    data=data.select_dtypes(exclude=['object','datetime'])\n",
    "    print(\"Removing PTID_Key and Date columns, the shape of the data is: \",data.shape) \n",
    "    \n",
    "#     print(data_input.head())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>PTID_Key</th>\n",
       "      <th>CN_Diag</th>\n",
       "      <th>MCI_Diag</th>\n",
       "      <th>AD_Diag</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>Ventricles_Norm</th>\n",
       "      <th>MMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/10/13</td>\n",
       "      <td>785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/15/13</td>\n",
       "      <td>785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/17/14</td>\n",
       "      <td>785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/29/14</td>\n",
       "      <td>785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/24/15</td>\n",
       "      <td>785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.013934</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date  PTID_Key  CN_Diag  MCI_Diag  AD_Diag  ADAS13  Ventricles_Norm  \\\n",
       "0  7/10/13       785      0.0       1.0      0.0     5.0         0.012737   \n",
       "1  1/15/13       785      NaN       NaN      NaN     NaN              NaN   \n",
       "2  1/17/14       785      NaN       NaN      NaN     NaN              NaN   \n",
       "3  7/29/14       785      1.0       0.0      0.0     7.0              NaN   \n",
       "4  7/24/15       785      1.0       0.0      0.0    11.0         0.013934   \n",
       "\n",
       "   MMSE  \n",
       "0  28.0  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3  30.0  \n",
       "4  25.0  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/TADPOLE_TargetData_train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>PTID_Key</th>\n",
       "      <th>CN_Diag</th>\n",
       "      <th>MCI_Diag</th>\n",
       "      <th>AD_Diag</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>Ventricles_Norm</th>\n",
       "      <th>MMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>1603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>1603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-10-29</td>\n",
       "      <td>1603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>1603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-18</td>\n",
       "      <td>1603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  PTID_Key  CN_Diag  MCI_Diag  AD_Diag  ADAS13  Ventricles_Norm  \\\n",
       "0  2014-01-02      1603      NaN       NaN      NaN     NaN              NaN   \n",
       "1  2015-01-15      1603      NaN       NaN      NaN     NaN              NaN   \n",
       "2  2014-10-29      1603      NaN       NaN      NaN     NaN              NaN   \n",
       "3  2015-08-11      1603      NaN       NaN      NaN     NaN              NaN   \n",
       "4  2016-07-18      1603      NaN       NaN      NaN     NaN              NaN   \n",
       "\n",
       "   MMSE  \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val=pd.read_csv('data/TADPOLE_TargetData_validation.csv')\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>PTID_Key</th>\n",
       "      <th>CN_Diag</th>\n",
       "      <th>MCI_Diag</th>\n",
       "      <th>AD_Diag</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>Ventricles_Norm</th>\n",
       "      <th>MMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/25/13</td>\n",
       "      <td>583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/25/13</td>\n",
       "      <td>583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/10/14</td>\n",
       "      <td>583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/4/13</td>\n",
       "      <td>809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4/25/13</td>\n",
       "      <td>809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date  PTID_Key  CN_Diag  MCI_Diag  AD_Diag  ADAS13  Ventricles_Norm  \\\n",
       "0  1/25/13       583      NaN       NaN      NaN     NaN              NaN   \n",
       "1  6/25/13       583      NaN       NaN      NaN     NaN              NaN   \n",
       "2  3/10/14       583      NaN       NaN      NaN     NaN              NaN   \n",
       "3  12/4/13       809      NaN       NaN      NaN     NaN              NaN   \n",
       "4  4/25/13       809      NaN       NaN      NaN     NaN              NaN   \n",
       "\n",
       "   MMSE  \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv('data/TADPOLE_PredictTargetData_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>PTID_Key</th>\n",
       "      <th>CN_Diag</th>\n",
       "      <th>MCI_Diag</th>\n",
       "      <th>AD_Diag</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>Ventricles_Norm</th>\n",
       "      <th>MMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-10-31</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-04-28</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-04</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.020526</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.020526</td>\n",
       "      <td>29.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date PTID_Key  CN_Diag  MCI_Diag  AD_Diag  ADAS13  Ventricles_Norm  \\\n",
       "0 2013-03-28        8      0.0       1.0      0.0     5.0         0.012128   \n",
       "1 2013-10-31        8      0.0       1.0      0.0     5.0         0.012128   \n",
       "2 2014-04-28        8      0.0       1.0      0.0     5.0         0.012128   \n",
       "3 2013-02-04       18      0.0       1.0      0.0     9.0         0.020526   \n",
       "4 2013-09-03       18      0.0       1.0      0.0    10.5         0.020526   \n",
       "\n",
       "   MMSE  \n",
       "0  30.0  \n",
       "1  30.0  \n",
       "2  30.0  \n",
       "3  30.0  \n",
       "4  29.5  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_Data['EXAMDATE'].dtype\n",
    "train_proc['Date'].dtype\n",
    "train_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All objects in the train target file are in input data file\n",
      "Remaining missing values:  1037\n",
      "Filled percentage:  82.67624457066488 %\n",
      "After dropping missing values, the shape of the dataset is:  (1901, 8)\n",
      "After dropping missing values, the total number of missing values is:  0\n",
      "All objects in the train target file are in input data file\n",
      "Remaining missing values:  408\n",
      "Filled percentage:  80.17492711370262 %\n",
      "After dropping missing values, the shape of the dataset is:  (639, 8)\n",
      "After dropping missing values, the total number of missing values is:  0\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "train_proc=output_prep(train)\n",
    "train_proc.to_csv('data/train_preprocessed.csv',index=False)\n",
    "\n",
    "# Prepare validation data\n",
    "val_proc=output_prep(val)\n",
    "val_proc.to_csv('data/val_preprocessed.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging input and output, the shape of the data is:  (6716, 288)\n",
      "Adding the month interval, the shape of the data is:  (6716, 289)\n",
      "Removing PTID_Key and Date columns, the shape of the data is:  (6716, 286)\n"
     ]
    }
   ],
   "source": [
    "# Get x,y for training data\n",
    "data_con = ['ADAS13','Ventricles_Norm','MMSE']\n",
    "data_cat=['CN_Diag','MCI_Diag','AD_Diag']\n",
    "\n",
    "train_final=prep_for_models(Input_Data,train_proc)\n",
    "\n",
    "x_train=train_final.drop(data_con,axis=1).drop(data_cat,axis=1)\n",
    "\n",
    "y_train_adas13=train_final['ADAS13']\n",
    "y_train_ventricles=train_final['Ventricles_Norm']\n",
    "y_train_mmse=train_final['MMSE']\n",
    "\n",
    "# Diagnosis\n",
    "y_train_diag = train_final[['CN_Diag','MCI_Diag','AD_Diag']]\n",
    "\n",
    "y_train_diag['CN_Diag'] = y_train_diag['CN_Diag'].astype('int')\n",
    "y_train_diag['MCI_Diag'] = y_train_diag['MCI_Diag'].astype('int')\n",
    "y_train_diag['AD_Diag'] = y_train_diag['AD_Diag'].astype('int')\n",
    "\n",
    "# Encode one-hot encoding back to label encoding (0: CN_Diag, 1: MCI_Diag, 2: AD_Diag)\n",
    "y_train_diag['Diag'] = np.argmax(y_train_diag[['CN_Diag','MCI_Diag','AD_Diag']].values,axis=1)\n",
    "\n",
    "y_train_diag1 = y_train_diag['Diag']\n",
    "y_train_diag2 = y_train_diag[['CN_Diag','MCI_Diag','AD_Diag']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging input and output, the shape of the data is:  (2238, 288)\n",
      "Adding the month interval, the shape of the data is:  (2238, 289)\n",
      "Removing PTID_Key and Date columns, the shape of the data is:  (2238, 286)\n"
     ]
    }
   ],
   "source": [
    "# Get x,y for validation data\n",
    "data_con = ['ADAS13','Ventricles_Norm','MMSE']\n",
    "data_cat=['CN_Diag','MCI_Diag','AD_Diag']\n",
    "\n",
    "val_final=prep_for_models(Input_Data,val_proc)\n",
    "\n",
    "x_val=val_final.drop(data_con,axis=1).drop(data_cat,axis=1)\n",
    "\n",
    "y_val_adas13=val_final['ADAS13']\n",
    "y_val_ventricles=val_final['Ventricles_Norm']\n",
    "y_val_mmse=val_final['MMSE']\n",
    "\n",
    "# Diagnosis\n",
    "y_val_diag = val_final[['CN_Diag','MCI_Diag','AD_Diag']]\n",
    "\n",
    "y_val_diag['CN_Diag'] = y_val_diag['CN_Diag'].astype('int')\n",
    "y_val_diag['MCI_Diag'] = y_val_diag['MCI_Diag'].astype('int')\n",
    "y_val_diag['AD_Diag'] = y_val_diag['AD_Diag'].astype('int')\n",
    "\n",
    "# Encode one-hot encoding back to label encoding (0: CN_Diag, 1: MCI_Diag, 2: AD_Diag)\n",
    "y_val_diag['Diag'] = np.argmax(y_val_diag[['CN_Diag','MCI_Diag','AD_Diag']].values,axis=1)\n",
    "\n",
    "y_val_diag1 = y_val_diag['Diag']\n",
    "y_val_diag2 = y_val_diag[['CN_Diag','MCI_Diag','AD_Diag']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Models\n",
    "\n",
    "from sklearn import ensemble,tree,linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Print R2 and RMSE scores\n",
    "def get_score(prediction, labels):\n",
    "    print('R2: {}'.format(r2_score(prediction, labels)))\n",
    "    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction,labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show scores for train and validation sets\n",
    "def train_test(estimator, x_train, x_test, y_train, y_test):\n",
    "    prediction_train = estimator.predict(x_train)\n",
    "    print(estimator)\n",
    "    get_score(prediction_train,y_train)\n",
    "    \n",
    "    prediction_test = estimator.predict(x_test)\n",
    "    print('Test')\n",
    "    get_score(prediction_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Elastic Net\n",
    "def eNet(x_train,y_train,x_val,y_val):\n",
    "    ENSTest = linear_model.ElasticNetCV(alphas=[0.0001,0.0005,0.001,0.01,0.1,1,10],l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(x_train,y_train)\n",
    "    train_test(ENSTest, x_train,x_val,y_train,y_val)\n",
    "\n",
    "    # Average R2 score and standard deviation of 5-fold cross-validation\n",
    "    scores = cross_val_score(ENSTest, x_train, y_train, cv=5)\n",
    "    print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAS13\n",
      "ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True,\n",
      "       cv='warn', eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100,\n",
      "       n_jobs=None, normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0)\n",
      "R2: 0.7641671579373136\n",
      "RMSE: 4.781466562752626\n",
      "Test\n",
      "R2: 0.1476963594257218\n",
      "RMSE: 7.7586431625021275\n",
      "Accuracy: 0.58 (+/- 0.21)\n",
      "Ventricles_Norm\n",
      "ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True,\n",
      "       cv='warn', eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100,\n",
      "       n_jobs=None, normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0)\n",
      "R2: 0.9451548567935586\n",
      "RMSE: 0.0026289336879930557\n",
      "Test\n",
      "R2: 0.8358048867864725\n",
      "RMSE: 0.004725053013946156\n",
      "Accuracy: 0.80 (+/- 0.07)\n",
      "MMSE\n",
      "ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True,\n",
      "       cv='warn', eps=0.001, fit_intercept=True,\n",
      "       l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100,\n",
      "       n_jobs=None, normalize=False, positive=False, precompute='auto',\n",
      "       random_state=None, selection='cyclic', tol=0.0001, verbose=0)\n",
      "R2: 0.2904362254328061\n",
      "RMSE: 2.219104236393011\n",
      "Test\n",
      "R2: -0.8285642224632712\n",
      "RMSE: 2.8111858980631617\n",
      "Accuracy: 0.44 (+/- 0.17)\n"
     ]
    }
   ],
   "source": [
    "print(\"ADAS13\")\n",
    "eNet(x_train,y_train_adas13,x_val,y_val_adas13)\n",
    "print(\"Ventricles_Norm\")\n",
    "eNet(x_train,y_train_ventricles,x_val,y_val_ventricles)\n",
    "print(\"MMSE\")\n",
    "eNet(x_train,y_train_mmse,x_val,y_val_mmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr(x_train,y_train,x_val,y_val):    \n",
    "    svr = SVR(gamma='scale',C=1.0,epsilon=0.2).fit(x_train,y_train)\n",
    "    train_test(svr, x_train,x_val,y_train,y_val)\n",
    "\n",
    "    # Average R2 score and standard deviation of 5-fold cross-validation\n",
    "    scores = cross_val_score(svr, x_train, y_train, cv=5)\n",
    "    print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAS13\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='scale',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "R2: -3.2310939975634065\n",
      "RMSE: 8.860844062664619\n",
      "Test\n",
      "R2: -13.455743380289295\n",
      "RMSE: 10.634515434395105\n",
      "Accuracy: 0.14 (+/- 0.06)\n",
      "Ventricles_Norm\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='scale',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "R2: -7.747377135104545e+30\n",
      "RMSE: 0.019313794043997957\n",
      "Test\n",
      "R2: 0.0\n",
      "RMSE: 0.019544580340634176\n",
      "Accuracy: -1.82 (+/- 2.10)\n",
      "MMSE\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='scale',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "R2: -0.33893063899058706\n",
      "RMSE: 2.440142318678786\n",
      "Test\n",
      "R2: -3.686888628493006\n",
      "RMSE: 3.1361286337808343\n",
      "Accuracy: 0.20 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "print(\"ADAS13\")\n",
    "svr(x_train,y_train_adas13,x_val,y_val_adas13)\n",
    "print(\"Ventricles_Norm\")\n",
    "svr(x_train,y_train_ventricles,x_val,y_val_ventricles)\n",
    "print(\"MMSE\")\n",
    "svr(x_train,y_train_mmse,x_val,y_val_mmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "def gBoost(x_train,y_train,x_val,y_val):\n",
    "    GBest = ensemble.GradientBoostingRegressor(n_estimators=3000,learning_rate=0.05,max_depth=3,max_features='sqrt',\n",
    "                                              min_samples_leaf=15,min_samples_split=10,loss='huber').fit(x_train,y_train)\n",
    "    train_test(GBest,x_train,x_train,y_train,y_train)\n",
    "    # Average R2 score and standard deviation of 5-fold cross-validation\n",
    "    scores = cross_val_score(GBest, x_train, y_train, cv=5)\n",
    "    print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))\n",
    "\n",
    "    x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAS13\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.05, loss='huber', max_depth=3,\n",
      "             max_features='sqrt', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=15, min_samples_split=10,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
      "             n_iter_no_change=None, presort='auto', random_state=None,\n",
      "             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,\n",
      "             warm_start=False)\n",
      "R2: 0.9181307144646785\n",
      "RMSE: 3.0936169633756205\n",
      "Test\n",
      "R2: 0.9181307144646785\n",
      "RMSE: 3.0936169633756205\n",
      "Accuracy: 0.48 (+/- 0.12)\n",
      "Ventricles_Norm\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.05, loss='huber', max_depth=3,\n",
      "             max_features='sqrt', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=15, min_samples_split=10,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
      "             n_iter_no_change=None, presort='auto', random_state=None,\n",
      "             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,\n",
      "             warm_start=False)\n",
      "R2: 0.994676424288722\n",
      "RMSE: 0.0008654150608285345\n",
      "Test\n",
      "R2: 0.994676424288722\n",
      "RMSE: 0.0008654150608285345\n",
      "Accuracy: 0.49 (+/- 0.06)\n",
      "MMSE\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.05, loss='huber', max_depth=3,\n",
      "             max_features='sqrt', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=15, min_samples_split=10,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
      "             n_iter_no_change=None, presort='auto', random_state=None,\n",
      "             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,\n",
      "             warm_start=False)\n",
      "R2: 0.8556718247573666\n",
      "RMSE: 1.3235863059975193\n",
      "Test\n",
      "R2: 0.8556718247573666\n",
      "RMSE: 1.3235863059975193\n",
      "Accuracy: 0.34 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "print(\"ADAS13\")\n",
    "gBoost(x_train,y_train_adas13,x_val,y_val_adas13)\n",
    "print(\"Ventricles_Norm\")\n",
    "gBoost(x_train,y_train_ventricles,x_val,y_val_ventricles)\n",
    "print(\"MMSE\")\n",
    "gBoost(x_train,y_train_mmse,x_val,y_val_mmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Neural networks\n",
    "\n",
    "# # X_train = X_train_adas13[features].values\n",
    "# y_train = y_train.values.reshape(-1,1)\n",
    "# y_train.shape\n",
    "\n",
    "def random_batch(x_train, y_train, batch_size):\n",
    "    \n",
    "    num = x_train.shape[0]\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num,\n",
    "                           size=batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = x_train[idx]\n",
    "    y_batch = y_train[idx]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN(x_train,y_train,x_val,y_val):\n",
    "    # Convert y_train shape to ?x1\n",
    "    y_train = y_train.values.reshape(-1,1)\n",
    "    y_val = y_val.values.reshape(-1,1)\n",
    "\n",
    "    n_input = x_train.shape[1]\n",
    "    n_hidden1 = 128\n",
    "    n_hidden2 = 512\n",
    "    n_hidden3 = 1024\n",
    "    n_output = 1\n",
    "    learning_rate = 0.001\n",
    "    epochs = 40000\n",
    "    batch_size = 100\n",
    "    REGULARIZATION_RATE = 0.0001\n",
    "\n",
    "    X = tf.placeholder(tf.float32,[None,n_input])\n",
    "    y_gt = tf.placeholder(tf.float32,[None,n_output])\n",
    "    # regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False)\n",
    "    W1 = tf.Variable(initializer([n_input,n_hidden1]))\n",
    "    # tf.add_to_collection('losses',regularizer(W1))\n",
    "    b1 = tf.Variable(tf.constant(0.1,shape=[n_hidden1]))\n",
    "    H1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "    W2 = tf.Variable(initializer([n_hidden1,n_hidden2]))\n",
    "    # tf.add_to_collection('losses',regularizer(W2))\n",
    "    b2 = tf.Variable(tf.constant(0.1,shape=[n_hidden2]))\n",
    "    H2 = tf.nn.relu(tf.matmul(H1,W2)+b2)\n",
    "\n",
    "    W3 = tf.Variable(initializer([n_hidden2,n_hidden3]))\n",
    "    # tf.add_to_collection('losses',regularizer(W3))\n",
    "    b3 = tf.Variable(tf.constant(0.1,shape=[n_hidden3]))\n",
    "    H3 = tf.nn.relu(tf.matmul(H2,W3)+b3)\n",
    "\n",
    "    W_out = tf.Variable(initializer([n_hidden3,n_output]))\n",
    "    # tf.add_to_collection('losses',regularizer(W_out))\n",
    "    b_out = tf.Variable(tf.constant(0.1,shape=[n_output]))\n",
    "    y_pred = tf.matmul(H3,W_out)+b_out\n",
    "\n",
    "    tr_losses=[]\n",
    "    te_losses=[]\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_gt,predictions=y_pred)) \n",
    "    # + tf.add_n(tf.get_collection('losses'))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for iter in range(epochs):\n",
    "\n",
    "        sess.run(train_step, feed_dict={X:x_train, y_gt:y_train})\n",
    "        if iter%1 == 0:\n",
    "            train_loss = sess.run(loss, feed_dict={X:x_train, y_gt:y_train})\n",
    "            validation_loss = sess.run(loss, feed_dict={X:x_val, y_gt:y_val})\n",
    "            print(\"Iter %d, training loss %f, validation loss %f\" % (iter, train_loss, validation_loss))\n",
    "            tr_losses.append(train_loss)\n",
    "            te_losses.append(validation_loss)\n",
    "    train_pred = sess.run(y_pred,feed_dict={X:x_train})\n",
    "    val_pred =sess.run(y_pred, feed_dict={X:x_val})\n",
    "    print(\"Size of the model predictions for training is: \", train_pred.shape)\n",
    "    print(\"Size of the model predictions for validation is: \", val_pred.shape)\n",
    "    return train_pred, val_pred, tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_NN(epochs, tr_losses,te_losses):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import math\n",
    "    epoch=list(range(1,epochs+1))\n",
    "    plt.figure()\n",
    "    tr_loss=[math.sqrt(x) for x in tr_losses]\n",
    "    te_loss=[math.sqrt(x) for x in te_losses]\n",
    "    \n",
    "    plt.plot(epoch,tr_losses, label='training loss')\n",
    "    plt.plot(epoch,te_losses,label='validation loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return tr_loss, te_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAS13\n",
      "Iter 0, training loss 204.762421, validation loss 189.844833\n",
      "Iter 1, training loss 236.169571, validation loss 218.210022\n",
      "Iter 2, training loss 158.577820, validation loss 154.430954\n",
      "Iter 3, training loss 125.009277, validation loss 137.426758\n",
      "Iter 4, training loss 135.183350, validation loss 160.839218\n",
      "Iter 5, training loss 136.996155, validation loss 169.676239\n",
      "Iter 6, training loss 116.923965, validation loss 150.210312\n",
      "Iter 7, training loss 102.736534, validation loss 131.177704\n",
      "Iter 8, training loss 113.118828, validation loss 137.083710\n",
      "Iter 9, training loss 106.993179, validation loss 135.130585\n",
      "Iter 10, training loss 88.555771, validation loss 125.849014\n",
      "Iter 11, training loss 80.839355, validation loss 128.002594\n",
      "Iter 12, training loss 81.427505, validation loss 135.846985\n",
      "Iter 13, training loss 77.573479, validation loss 135.444778\n",
      "Iter 14, training loss 67.621544, validation loss 125.497513\n",
      "Iter 15, training loss 59.716953, validation loss 115.410149\n",
      "Iter 16, training loss 59.024906, validation loss 112.493889\n",
      "Iter 17, training loss 58.915440, validation loss 112.560509\n",
      "Iter 18, training loss 52.671978, validation loss 109.873482\n",
      "Iter 19, training loss 45.684128, validation loss 108.663277\n",
      "Iter 20, training loss 43.771282, validation loss 112.635941\n",
      "Iter 21, training loss 43.532516, validation loss 116.371468\n",
      "Iter 22, training loss 40.033833, validation loss 113.804985\n",
      "Iter 23, training loss 35.200283, validation loss 107.280159\n",
      "Iter 24, training loss 33.712036, validation loss 103.107834\n",
      "Iter 25, training loss 34.186333, validation loss 102.248688\n",
      "Iter 26, training loss 31.662413, validation loss 101.092201\n",
      "Iter 27, training loss 28.277121, validation loss 100.972786\n",
      "Iter 28, training loss 27.613323, validation loss 103.483070\n",
      "Iter 29, training loss 27.428175, validation loss 104.571854\n",
      "Iter 30, training loss 25.166931, validation loss 101.167725\n",
      "Iter 31, training loss 22.884308, validation loss 96.230057\n",
      "Iter 32, training loss 22.655655, validation loss 93.568230\n",
      "Iter 33, training loss 22.216335, validation loss 92.401505\n",
      "Iter 34, training loss 20.318140, validation loss 91.632416\n",
      "Iter 35, training loss 19.291182, validation loss 92.522278\n",
      "Iter 36, training loss 19.342264, validation loss 93.924370\n",
      "Iter 37, training loss 18.546059, validation loss 93.042480\n",
      "Iter 38, training loss 17.197775, validation loss 90.380836\n",
      "Iter 39, training loss 16.793352, validation loss 88.470909\n",
      "Iter 40, training loss 16.663382, validation loss 87.804237\n",
      "Iter 41, training loss 15.729388, validation loss 87.736839\n",
      "Iter 42, training loss 14.971402, validation loss 88.691238\n",
      "Iter 43, training loss 14.896338, validation loss 90.032654\n",
      "Iter 44, training loss 14.507139, validation loss 89.866997\n",
      "Iter 45, training loss 13.739215, validation loss 88.151604\n",
      "Iter 46, training loss 13.430519, validation loss 86.589897\n",
      "Iter 47, training loss 13.268508, validation loss 85.889603\n",
      "Iter 48, training loss 12.681479, validation loss 85.831757\n",
      "Iter 49, training loss 12.251092, validation loss 86.477501\n",
      "Iter 50, training loss 12.139063, validation loss 87.068565\n",
      "Iter 51, training loss 11.748043, validation loss 86.420670\n",
      "Iter 52, training loss 11.313065, validation loss 85.077042\n",
      "Iter 53, training loss 11.177318, validation loss 84.210335\n",
      "Iter 54, training loss 10.913523, validation loss 84.027756\n",
      "Iter 55, training loss 10.523151, validation loss 84.461571\n",
      "Iter 56, training loss 10.366387, validation loss 85.197395\n",
      "Iter 57, training loss 10.181608, validation loss 85.289093\n",
      "Iter 58, training loss 9.856587, validation loss 84.546349\n",
      "Iter 59, training loss 9.684813, validation loss 83.826263\n",
      "Iter 60, training loss 9.523781, validation loss 83.632378\n",
      "Iter 61, training loss 9.249964, validation loss 83.955795\n",
      "Iter 62, training loss 9.096698, validation loss 84.552307\n",
      "Iter 63, training loss 8.948687, validation loss 84.731842\n",
      "Iter 64, training loss 8.716379, validation loss 84.313187\n",
      "Iter 65, training loss 8.573988, validation loss 83.897316\n",
      "Iter 66, training loss 8.428521, validation loss 83.884468\n",
      "Iter 67, training loss 8.230936, validation loss 84.264923\n",
      "Iter 68, training loss 8.108003, validation loss 84.744751\n",
      "Iter 69, training loss 7.969545, validation loss 84.809692\n",
      "Iter 70, training loss 7.798584, validation loss 84.455528\n",
      "Iter 71, training loss 7.687019, validation loss 84.181633\n",
      "Iter 72, training loss 7.548442, validation loss 84.258125\n",
      "Iter 73, training loss 7.402068, validation loss 84.629646\n",
      "Iter 74, training loss 7.298221, validation loss 84.952095\n",
      "Iter 75, training loss 7.162168, validation loss 84.905685\n",
      "Iter 76, training loss 7.042879, validation loss 84.696899\n",
      "Iter 77, training loss 6.943079, validation loss 84.672577\n",
      "Iter 78, training loss 6.820485, validation loss 84.897926\n",
      "Iter 79, training loss 6.725086, validation loss 85.179337\n",
      "Iter 80, training loss 6.624002, validation loss 85.203476\n",
      "Iter 81, training loss 6.517734, validation loss 85.005249\n",
      "Iter 82, training loss 6.432516, validation loss 84.893105\n",
      "Iter 83, training loss 6.331582, validation loss 85.001839\n",
      "Iter 84, training loss 6.244324, validation loss 85.213852\n",
      "Iter 85, training loss 6.158875, validation loss 85.261108\n",
      "Iter 86, training loss 6.068206, validation loss 85.100784\n",
      "Iter 87, training loss 5.991097, validation loss 84.989120\n",
      "Iter 88, training loss 5.904803, validation loss 85.058334\n",
      "Iter 89, training loss 5.828692, validation loss 85.191940\n",
      "Iter 90, training loss 5.751342, validation loss 85.176361\n",
      "Iter 91, training loss 5.674314, validation loss 85.046791\n",
      "Iter 92, training loss 5.603982, validation loss 85.016167\n",
      "Iter 93, training loss 5.528970, validation loss 85.146530\n",
      "Iter 94, training loss 5.462494, validation loss 85.280701\n",
      "Iter 95, training loss 5.392116, validation loss 85.259628\n",
      "Iter 96, training loss 5.326711, validation loss 85.180397\n",
      "Iter 97, training loss 5.260362, validation loss 85.212166\n",
      "Iter 98, training loss 5.196086, validation loss 85.338799\n",
      "Iter 99, training loss 5.133717, validation loss 85.397728\n",
      "Iter 100, training loss 5.071091, validation loss 85.367149\n",
      "Iter 101, training loss 5.011653, validation loss 85.381805\n",
      "Iter 102, training loss 4.951003, validation loss 85.487221\n",
      "Iter 103, training loss 4.893844, validation loss 85.572205\n",
      "Iter 104, training loss 4.835739, validation loss 85.559418\n",
      "Iter 105, training loss 4.781240, validation loss 85.563438\n",
      "Iter 106, training loss 4.726108, validation loss 85.679939\n",
      "Iter 107, training loss 4.674003, validation loss 85.811264\n",
      "Iter 108, training loss 4.621100, validation loss 85.833435\n",
      "Iter 109, training loss 4.570601, validation loss 85.827377\n",
      "Iter 110, training loss 4.519235, validation loss 85.897774\n",
      "Iter 111, training loss 4.470224, validation loss 85.977959\n",
      "Iter 112, training loss 4.420937, validation loss 85.985748\n",
      "Iter 113, training loss 4.373627, validation loss 86.001930\n",
      "Iter 114, training loss 4.325992, validation loss 86.093025\n",
      "Iter 115, training loss 4.279746, validation loss 86.176765\n",
      "Iter 116, training loss 4.233934, validation loss 86.192322\n",
      "Iter 117, training loss 4.189811, validation loss 86.220139\n",
      "Iter 118, training loss 4.145742, validation loss 86.295563\n",
      "Iter 119, training loss 4.102833, validation loss 86.338364\n",
      "Iter 120, training loss 4.060989, validation loss 86.340584\n",
      "Iter 121, training loss 4.019703, validation loss 86.382355\n",
      "Iter 122, training loss 3.979065, validation loss 86.447723\n",
      "Iter 123, training loss 3.939214, validation loss 86.458908\n",
      "Iter 124, training loss 3.900056, validation loss 86.447845\n",
      "Iter 125, training loss 3.861472, validation loss 86.492226\n",
      "Iter 126, training loss 3.824149, validation loss 86.553604\n",
      "Iter 127, training loss 3.787189, validation loss 86.594551\n",
      "Iter 128, training loss 3.750769, validation loss 86.663956\n",
      "Iter 129, training loss 3.714992, validation loss 86.722694\n",
      "Iter 130, training loss 3.680060, validation loss 86.745384\n",
      "Iter 131, training loss 3.645550, validation loss 86.788467\n",
      "Iter 132, training loss 3.611403, validation loss 86.851273\n",
      "Iter 133, training loss 3.578102, validation loss 86.896172\n",
      "Iter 134, training loss 3.544965, validation loss 86.948479\n",
      "Iter 135, training loss 3.512043, validation loss 86.997070\n",
      "Iter 136, training loss 3.479746, validation loss 87.016052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 137, training loss 3.447742, validation loss 87.050072\n",
      "Iter 138, training loss 3.416695, validation loss 87.126930\n",
      "Iter 139, training loss 3.385295, validation loss 87.175507\n",
      "Iter 140, training loss 3.355039, validation loss 87.231216\n",
      "Iter 141, training loss 3.325225, validation loss 87.293304\n",
      "Iter 142, training loss 3.295336, validation loss 87.348022\n",
      "Iter 143, training loss 3.266437, validation loss 87.422012\n",
      "Iter 144, training loss 3.238490, validation loss 87.488708\n",
      "Iter 145, training loss 3.209986, validation loss 87.516777\n",
      "Iter 146, training loss 3.182663, validation loss 87.608505\n",
      "Iter 147, training loss 3.154908, validation loss 87.654594\n",
      "Iter 148, training loss 3.128135, validation loss 87.718460\n",
      "Iter 149, training loss 3.102059, validation loss 87.808479\n",
      "Iter 150, training loss 3.075979, validation loss 87.844391\n",
      "Iter 151, training loss 3.049562, validation loss 87.936363\n",
      "Iter 152, training loss 3.023714, validation loss 87.989845\n",
      "Iter 153, training loss 2.999698, validation loss 88.027237\n",
      "Iter 154, training loss 2.974189, validation loss 88.075829\n",
      "Iter 155, training loss 2.949459, validation loss 88.141937\n",
      "Iter 156, training loss 2.926122, validation loss 88.227386\n",
      "Iter 157, training loss 2.902575, validation loss 88.253616\n",
      "Iter 158, training loss 2.878625, validation loss 88.369896\n",
      "Iter 159, training loss 2.855716, validation loss 88.406204\n",
      "Iter 160, training loss 2.832996, validation loss 88.504433\n",
      "Iter 161, training loss 2.811740, validation loss 88.472443\n",
      "Iter 162, training loss 2.790193, validation loss 88.647057\n",
      "Iter 163, training loss 2.769678, validation loss 88.596664\n",
      "Iter 164, training loss 2.748898, validation loss 88.795753\n",
      "Iter 165, training loss 2.728445, validation loss 88.669609\n",
      "Iter 166, training loss 2.707336, validation loss 88.890167\n",
      "Iter 167, training loss 2.686049, validation loss 88.771629\n",
      "Iter 168, training loss 2.664411, validation loss 89.002869\n",
      "Iter 169, training loss 2.642734, validation loss 88.933876\n",
      "Iter 170, training loss 2.621524, validation loss 89.053680\n",
      "Iter 171, training loss 2.601212, validation loss 89.052979\n",
      "Iter 172, training loss 2.581593, validation loss 89.120552\n",
      "Iter 173, training loss 2.562435, validation loss 89.230896\n",
      "Iter 174, training loss 2.543605, validation loss 89.225372\n",
      "Iter 175, training loss 2.525515, validation loss 89.332573\n",
      "Iter 176, training loss 2.509646, validation loss 89.270767\n",
      "Iter 177, training loss 2.495629, validation loss 89.532066\n",
      "Iter 178, training loss 2.485648, validation loss 89.303368\n",
      "Iter 179, training loss 2.480016, validation loss 89.734055\n",
      "Iter 180, training loss 2.475496, validation loss 89.327095\n",
      "Iter 181, training loss 2.463434, validation loss 89.946083\n",
      "Iter 182, training loss 2.433700, validation loss 89.458855\n",
      "Iter 183, training loss 2.394585, validation loss 89.842041\n",
      "Iter 184, training loss 2.368189, validation loss 89.747375\n",
      "Iter 185, training loss 2.361320, validation loss 89.702095\n",
      "Iter 186, training loss 2.364641, validation loss 90.154968\n",
      "Iter 187, training loss 2.357413, validation loss 89.762802\n",
      "Iter 188, training loss 2.332389, validation loss 90.265869\n",
      "Iter 189, training loss 2.300908, validation loss 89.953758\n",
      "Iter 190, training loss 2.276014, validation loss 90.137169\n",
      "Iter 191, training loss 2.267088, validation loss 90.294365\n",
      "Iter 192, training loss 2.264735, validation loss 90.092270\n",
      "Iter 193, training loss 2.258118, validation loss 90.540253\n",
      "Iter 194, training loss 2.242940, validation loss 90.213280\n",
      "Iter 195, training loss 2.219522, validation loss 90.638931\n",
      "Iter 196, training loss 2.195164, validation loss 90.454880\n",
      "Iter 197, training loss 2.179174, validation loss 90.554070\n",
      "Iter 198, training loss 2.169016, validation loss 90.685143\n",
      "Iter 199, training loss 2.162808, validation loss 90.522125\n",
      "Iter 200, training loss 2.157325, validation loss 90.874039\n",
      "Iter 201, training loss 2.147707, validation loss 90.567009\n",
      "Iter 202, training loss 2.133426, validation loss 91.016541\n",
      "Iter 203, training loss 2.117345, validation loss 90.761963\n",
      "Iter 204, training loss 2.099422, validation loss 91.102547\n",
      "Iter 205, training loss 2.082245, validation loss 90.898445\n",
      "Iter 206, training loss 2.066583, validation loss 91.068710\n",
      "Iter 207, training loss 2.053730, validation loss 91.067993\n",
      "Iter 208, training loss 2.042059, validation loss 91.097580\n",
      "Iter 209, training loss 2.031814, validation loss 91.221481\n",
      "Iter 210, training loss 2.023877, validation loss 91.134544\n",
      "Iter 211, training loss 2.019972, validation loss 91.456116\n",
      "Iter 212, training loss 2.021346, validation loss 91.168190\n",
      "Iter 213, training loss 2.032472, validation loss 91.736076\n",
      "Iter 214, training loss 2.052202, validation loss 91.139412\n",
      "Iter 215, training loss 2.072370, validation loss 92.039398\n",
      "Iter 216, training loss 2.070611, validation loss 91.166885\n",
      "Iter 217, training loss 2.035671, validation loss 92.084282\n",
      "Iter 218, training loss 1.976473, validation loss 91.370598\n",
      "Iter 219, training loss 1.927917, validation loss 91.768044\n",
      "Iter 220, training loss 1.913624, validation loss 91.762131\n",
      "Iter 221, training loss 1.928501, validation loss 91.536636\n",
      "Iter 222, training loss 1.952081, validation loss 92.180702\n",
      "Iter 223, training loss 1.957106, validation loss 91.528191\n",
      "Iter 224, training loss 1.937118, validation loss 92.291801\n",
      "Iter 225, training loss 1.897294, validation loss 91.685738\n",
      "Iter 226, training loss 1.859548, validation loss 92.071259\n",
      "Iter 227, training loss 1.841441, validation loss 91.979782\n",
      "Iter 228, training loss 1.844215, validation loss 91.851852\n",
      "Iter 229, training loss 1.853149, validation loss 92.314560\n",
      "Iter 230, training loss 1.858897, validation loss 91.862953\n",
      "Iter 231, training loss 1.852415, validation loss 92.524414\n",
      "Iter 232, training loss 1.831443, validation loss 91.976929\n",
      "Iter 233, training loss 1.808327, validation loss 92.455811\n",
      "Iter 234, training loss 1.784075, validation loss 92.172928\n",
      "Iter 235, training loss 1.769226, validation loss 92.350700\n",
      "Iter 236, training loss 1.760534, validation loss 92.390457\n",
      "Iter 237, training loss 1.758078, validation loss 92.279030\n",
      "Iter 238, training loss 1.758615, validation loss 92.663139\n",
      "Iter 239, training loss 1.761432, validation loss 92.317497\n",
      "Iter 240, training loss 1.763103, validation loss 92.857895\n",
      "Iter 241, training loss 1.762366, validation loss 92.316330\n",
      "Iter 242, training loss 1.762090, validation loss 92.974358\n",
      "Iter 243, training loss 1.758177, validation loss 92.373848\n",
      "Iter 244, training loss 1.748447, validation loss 93.084152\n",
      "Iter 245, training loss 1.731707, validation loss 92.475868\n",
      "Iter 246, training loss 1.713127, validation loss 93.097992\n",
      "Iter 247, training loss 1.692725, validation loss 92.658020\n",
      "Iter 248, training loss 1.674853, validation loss 93.081573\n",
      "Iter 249, training loss 1.658626, validation loss 92.783516\n",
      "Iter 250, training loss 1.646463, validation loss 92.983009\n",
      "Iter 251, training loss 1.636129, validation loss 92.921066\n",
      "Iter 252, training loss 1.627186, validation loss 93.021095\n",
      "Iter 253, training loss 1.619890, validation loss 93.103973\n",
      "Iter 254, training loss 1.613830, validation loss 93.049927\n",
      "Iter 255, training loss 1.608399, validation loss 93.251068\n",
      "Iter 256, training loss 1.607347, validation loss 93.093208\n",
      "Iter 257, training loss 1.612625, validation loss 93.506134\n",
      "Iter 258, training loss 1.630773, validation loss 93.052086\n",
      "Iter 259, training loss 1.676237, validation loss 93.898949\n",
      "Iter 260, training loss 1.759311, validation loss 93.000038\n",
      "Iter 261, training loss 1.887205, validation loss 94.610382\n",
      "Iter 262, training loss 1.990880, validation loss 92.973236\n",
      "Iter 263, training loss 2.006013, validation loss 94.926285\n",
      "Iter 264, training loss 1.818688, validation loss 93.037872\n",
      "Iter 265, training loss 1.601618, validation loss 93.965950\n",
      "Iter 266, training loss 1.542047, validation loss 93.680458\n",
      "Iter 267, training loss 1.658439, validation loss 93.139824\n",
      "Iter 268, training loss 1.768408, validation loss 94.534065\n",
      "Iter 269, training loss 1.714137, validation loss 93.174026\n",
      "Iter 270, training loss 1.574688, validation loss 94.129654\n",
      "Iter 271, training loss 1.498807, validation loss 93.710594\n",
      "Iter 272, training loss 1.549985, validation loss 93.460732\n",
      "Iter 273, training loss 1.631738, validation loss 94.531952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 274, training loss 1.628008, validation loss 93.494453\n",
      "Iter 275, training loss 1.547215, validation loss 94.431046\n",
      "Iter 276, training loss 1.472618, validation loss 93.853905\n",
      "Iter 277, training loss 1.473813, validation loss 93.834496\n",
      "Iter 278, training loss 1.519933, validation loss 94.493645\n",
      "Iter 279, training loss 1.546035, validation loss 93.737320\n",
      "Iter 280, training loss 1.520754, validation loss 94.618050\n",
      "Iter 281, training loss 1.465034, validation loss 93.909409\n",
      "Iter 282, training loss 1.429361, validation loss 94.234169\n",
      "Iter 283, training loss 1.432262, validation loss 94.411041\n",
      "Iter 284, training loss 1.456160, validation loss 94.058258\n",
      "Iter 285, training loss 1.470268, validation loss 94.780373\n",
      "Iter 286, training loss 1.458133, validation loss 94.106369\n",
      "Iter 287, training loss 1.428704, validation loss 94.727898\n",
      "Iter 288, training loss 1.398429, validation loss 94.385071\n",
      "Iter 289, training loss 1.386188, validation loss 94.527557\n",
      "Iter 290, training loss 1.390611, validation loss 94.769386\n",
      "Iter 291, training loss 1.402102, validation loss 94.409912\n",
      "Iter 292, training loss 1.408336, validation loss 94.991798\n",
      "Iter 293, training loss 1.403506, validation loss 94.451027\n",
      "Iter 294, training loss 1.390625, validation loss 95.053818\n",
      "Iter 295, training loss 1.371577, validation loss 94.585770\n",
      "Iter 296, training loss 1.352901, validation loss 94.969658\n",
      "Iter 297, training loss 1.339585, validation loss 94.844185\n",
      "Iter 298, training loss 1.332564, validation loss 94.935158\n",
      "Iter 299, training loss 1.330144, validation loss 95.090294\n",
      "Iter 300, training loss 1.332058, validation loss 94.894592\n",
      "Iter 301, training loss 1.334863, validation loss 95.320953\n",
      "Iter 302, training loss 1.340364, validation loss 94.919647\n",
      "Iter 303, training loss 1.346031, validation loss 95.521881\n",
      "Iter 304, training loss 1.350668, validation loss 94.909798\n",
      "Iter 305, training loss 1.354961, validation loss 95.669533\n",
      "Iter 306, training loss 1.359792, validation loss 94.974792\n",
      "Iter 307, training loss 1.364275, validation loss 95.842628\n",
      "Iter 308, training loss 1.364677, validation loss 95.001854\n",
      "Iter 309, training loss 1.367522, validation loss 95.953400\n",
      "Iter 310, training loss 1.362723, validation loss 95.106491\n",
      "Iter 311, training loss 1.359145, validation loss 96.090668\n",
      "Iter 312, training loss 1.344488, validation loss 95.178246\n",
      "Iter 313, training loss 1.330700, validation loss 96.075455\n",
      "Iter 314, training loss 1.311731, validation loss 95.322502\n",
      "Iter 315, training loss 1.294634, validation loss 96.111855\n",
      "Iter 316, training loss 1.275302, validation loss 95.453568\n",
      "Iter 317, training loss 1.259998, validation loss 96.020996\n",
      "Iter 318, training loss 1.245374, validation loss 95.579254\n",
      "Iter 319, training loss 1.234350, validation loss 96.038612\n",
      "Iter 320, training loss 1.225274, validation loss 95.716850\n",
      "Iter 321, training loss 1.218554, validation loss 96.056938\n",
      "Iter 322, training loss 1.213674, validation loss 95.800690\n",
      "Iter 323, training loss 1.210984, validation loss 96.192436\n",
      "Iter 324, training loss 1.210754, validation loss 95.882439\n",
      "Iter 325, training loss 1.216821, validation loss 96.392479\n",
      "Iter 326, training loss 1.233567, validation loss 95.858307\n",
      "Iter 327, training loss 1.272036, validation loss 96.741119\n",
      "Iter 328, training loss 1.343946, validation loss 95.781479\n",
      "Iter 329, training loss 1.481609, validation loss 97.439240\n",
      "Iter 330, training loss 1.670960, validation loss 95.710075\n",
      "Iter 331, training loss 1.930640, validation loss 98.439430\n",
      "Iter 332, training loss 2.021949, validation loss 95.735649\n",
      "Iter 333, training loss 1.910578, validation loss 98.478302\n",
      "Iter 334, training loss 1.469201, validation loss 95.756721\n",
      "Iter 335, training loss 1.172461, validation loss 96.522804\n",
      "Iter 336, training loss 1.245465, validation loss 96.861404\n",
      "Iter 337, training loss 1.488197, validation loss 95.685989\n",
      "Iter 338, training loss 1.570353, validation loss 97.761887\n",
      "Iter 339, training loss 1.353763, validation loss 95.811600\n",
      "Iter 340, training loss 1.151148, validation loss 96.622482\n",
      "Iter 341, training loss 1.171895, validation loss 96.822876\n",
      "Iter 342, training loss 1.323753, validation loss 95.985245\n",
      "Iter 343, training loss 1.389480, validation loss 97.670601\n",
      "Iter 344, training loss 1.269080, validation loss 96.133430\n",
      "Iter 345, training loss 1.131334, validation loss 96.912621\n",
      "Iter 346, training loss 1.115080, validation loss 96.858139\n",
      "Iter 347, training loss 1.200882, validation loss 96.306343\n",
      "Iter 348, training loss 1.267247, validation loss 97.621017\n",
      "Iter 349, training loss 1.225317, validation loss 96.345345\n",
      "Iter 350, training loss 1.135943, validation loss 97.244781\n",
      "Iter 351, training loss 1.080799, validation loss 96.808960\n",
      "Iter 352, training loss 1.099200, validation loss 96.666695\n",
      "Iter 353, training loss 1.150106, validation loss 97.454468\n",
      "Iter 354, training loss 1.170710, validation loss 96.557976\n",
      "Iter 355, training loss 1.146091, validation loss 97.586945\n",
      "Iter 356, training loss 1.093058, validation loss 96.828377\n",
      "Iter 357, training loss 1.056684, validation loss 97.230827\n",
      "Iter 358, training loss 1.056276, validation loss 97.321617\n",
      "Iter 359, training loss 1.078748, validation loss 96.973885\n",
      "Iter 360, training loss 1.100250, validation loss 97.747612\n",
      "Iter 361, training loss 1.100180, validation loss 96.998550\n",
      "Iter 362, training loss 1.079483, validation loss 97.762764\n",
      "Iter 363, training loss 1.049705, validation loss 97.185509\n",
      "Iter 364, training loss 1.027679, validation loss 97.561432\n",
      "Iter 365, training loss 1.020310, validation loss 97.558022\n",
      "Iter 366, training loss 1.025148, validation loss 97.395248\n",
      "Iter 367, training loss 1.036080, validation loss 97.843193\n",
      "Iter 368, training loss 1.044005, validation loss 97.353477\n",
      "Iter 369, training loss 1.047007, validation loss 98.070625\n",
      "Iter 370, training loss 1.041476, validation loss 97.460594\n",
      "Iter 371, training loss 1.030724, validation loss 98.131744\n",
      "Iter 372, training loss 1.016576, validation loss 97.612579\n",
      "Iter 373, training loss 1.003638, validation loss 98.112747\n",
      "Iter 374, training loss 0.991222, validation loss 97.806175\n",
      "Iter 375, training loss 0.982099, validation loss 98.063042\n",
      "Iter 376, training loss 0.976095, validation loss 97.990173\n",
      "Iter 377, training loss 0.971920, validation loss 98.047661\n",
      "Iter 378, training loss 0.970056, validation loss 98.196579\n",
      "Iter 379, training loss 0.969333, validation loss 98.050713\n",
      "Iter 380, training loss 0.971775, validation loss 98.382904\n",
      "Iter 381, training loss 0.975879, validation loss 98.047897\n",
      "Iter 382, training loss 0.985439, validation loss 98.625626\n",
      "Iter 383, training loss 1.001213, validation loss 98.025581\n",
      "Iter 384, training loss 1.028336, validation loss 98.930763\n",
      "Iter 385, training loss 1.067309, validation loss 97.971176\n",
      "Iter 386, training loss 1.130742, validation loss 99.371292\n",
      "Iter 387, training loss 1.212094, validation loss 97.925339\n",
      "Iter 388, training loss 1.331415, validation loss 99.988174\n",
      "Iter 389, training loss 1.433827, validation loss 97.908722\n",
      "Iter 390, training loss 1.532599, validation loss 100.506828\n",
      "Iter 391, training loss 1.479083, validation loss 97.899796\n",
      "Iter 392, training loss 1.345239, validation loss 100.104073\n",
      "Iter 393, training loss 1.111652, validation loss 98.033432\n",
      "Iter 394, training loss 0.948935, validation loss 98.874870\n",
      "Iter 395, training loss 0.928768, validation loss 98.744728\n",
      "Iter 396, training loss 1.025666, validation loss 98.113892\n",
      "Iter 397, training loss 1.146239, validation loss 99.657997\n",
      "Iter 398, training loss 1.185474, validation loss 98.049438\n",
      "Iter 399, training loss 1.139895, validation loss 99.753075\n",
      "Iter 400, training loss 1.020954, validation loss 98.242004\n",
      "Iter 401, training loss 0.923409, validation loss 99.051514\n",
      "Iter 402, training loss 0.892629, validation loss 98.834579\n",
      "Iter 403, training loss 0.926691, validation loss 98.524483\n",
      "Iter 404, training loss 0.989597, validation loss 99.484138\n",
      "Iter 405, training loss 1.034660, validation loss 98.371658\n",
      "Iter 406, training loss 1.046620, validation loss 99.801186\n",
      "Iter 407, training loss 1.011983, validation loss 98.494034\n",
      "Iter 408, training loss 0.957699, validation loss 99.592690\n",
      "Iter 409, training loss 0.901908, validation loss 98.749634\n",
      "Iter 410, training loss 0.869600, validation loss 99.176239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 411, training loss 0.864298, validation loss 99.207016\n",
      "Iter 412, training loss 0.879498, validation loss 98.939011\n",
      "Iter 413, training loss 0.904584, validation loss 99.609154\n",
      "Iter 414, training loss 0.927159, validation loss 98.843468\n",
      "Iter 415, training loss 0.946099, validation loss 99.898689\n",
      "Iter 416, training loss 0.950091, validation loss 98.877464\n",
      "Iter 417, training loss 0.947017, validation loss 99.983894\n",
      "Iter 418, training loss 0.930898, validation loss 98.937187\n",
      "Iter 419, training loss 0.911771, validation loss 99.929642\n",
      "Iter 420, training loss 0.888916, validation loss 99.107536\n",
      "Iter 421, training loss 0.866824, validation loss 99.832420\n",
      "Iter 422, training loss 0.848223, validation loss 99.280022\n",
      "Iter 423, training loss 0.833046, validation loss 99.690231\n",
      "Iter 424, training loss 0.824760, validation loss 99.500946\n",
      "Iter 425, training loss 0.819406, validation loss 99.617165\n",
      "Iter 426, training loss 0.817228, validation loss 99.692108\n",
      "Iter 427, training loss 0.817431, validation loss 99.600128\n",
      "Iter 428, training loss 0.820455, validation loss 99.934700\n",
      "Iter 429, training loss 0.827499, validation loss 99.567291\n",
      "Iter 430, training loss 0.840026, validation loss 100.160660\n",
      "Iter 431, training loss 0.862894, validation loss 99.491638\n",
      "Iter 432, training loss 0.906348, validation loss 100.551064\n",
      "Iter 433, training loss 0.974328, validation loss 99.395538\n",
      "Iter 434, training loss 1.096398, validation loss 101.203362\n",
      "Iter 435, training loss 1.262384, validation loss 99.298592\n",
      "Iter 436, training loss 1.525282, validation loss 102.244530\n",
      "Iter 437, training loss 1.735786, validation loss 99.301979\n",
      "Iter 438, training loss 1.934233, validation loss 103.044563\n",
      "Iter 439, training loss 1.726044, validation loss 99.228088\n",
      "Iter 440, training loss 1.343650, validation loss 101.795761\n",
      "Iter 441, training loss 0.902548, validation loss 99.392395\n",
      "Iter 442, training loss 0.812809, validation loss 99.604042\n",
      "Iter 443, training loss 1.043617, validation loss 100.959908\n",
      "Iter 444, training loss 1.247591, validation loss 99.067993\n",
      "Iter 445, training loss 1.218388, validation loss 101.392967\n",
      "Iter 446, training loss 0.956047, validation loss 99.232834\n",
      "Iter 447, training loss 0.785364, validation loss 99.971344\n",
      "Iter 448, training loss 0.837448, validation loss 100.392220\n",
      "Iter 449, training loss 0.992022, validation loss 99.320374\n",
      "Iter 450, training loss 1.062588, validation loss 101.188271\n",
      "Iter 451, training loss 0.959548, validation loss 99.382698\n",
      "Iter 452, training loss 0.818635, validation loss 100.484642\n",
      "Iter 453, training loss 0.760088, validation loss 100.106552\n",
      "Iter 454, training loss 0.814241, validation loss 99.752052\n",
      "Iter 455, training loss 0.901526, validation loss 100.961349\n",
      "Iter 456, training loss 0.922499, validation loss 99.590622\n",
      "Iter 457, training loss 0.872714, validation loss 100.946136\n",
      "Iter 458, training loss 0.789444, validation loss 99.924088\n",
      "Iter 459, training loss 0.743969, validation loss 100.361366\n",
      "Iter 460, training loss 0.756547, validation loss 100.575684\n",
      "Iter 461, training loss 0.800709, validation loss 99.997314\n",
      "Iter 462, training loss 0.836572, validation loss 101.097626\n",
      "Iter 463, training loss 0.835513, validation loss 100.037048\n",
      "Iter 464, training loss 0.805529, validation loss 101.097778\n",
      "Iter 465, training loss 0.762152, validation loss 100.270218\n",
      "Iter 466, training loss 0.731480, validation loss 100.743134\n",
      "Iter 467, training loss 0.723105, validation loss 100.691650\n",
      "Iter 468, training loss 0.734020, validation loss 100.498360\n",
      "Iter 469, training loss 0.753359, validation loss 101.091469\n",
      "Iter 470, training loss 0.769786, validation loss 100.405098\n",
      "Iter 471, training loss 0.777023, validation loss 101.307030\n",
      "Iter 472, training loss 0.771349, validation loss 100.479156\n",
      "Iter 473, training loss 0.759046, validation loss 101.332825\n",
      "Iter 474, training loss 0.739967, validation loss 100.641960\n",
      "Iter 475, training loss 0.723102, validation loss 101.221443\n",
      "Iter 476, training loss 0.709662, validation loss 100.846443\n",
      "Iter 477, training loss 0.700891, validation loss 101.099838\n",
      "Iter 478, training loss 0.697078, validation loss 101.077774\n",
      "Iter 479, training loss 0.697236, validation loss 101.018471\n",
      "Iter 480, training loss 0.699456, validation loss 101.278343\n",
      "Iter 481, training loss 0.704405, validation loss 100.970879\n",
      "Iter 482, training loss 0.711146, validation loss 101.473343\n",
      "Iter 483, training loss 0.721261, validation loss 100.944168\n",
      "Iter 484, training loss 0.734947, validation loss 101.718910\n",
      "Iter 485, training loss 0.754534, validation loss 100.917511\n",
      "Iter 486, training loss 0.783778, validation loss 101.998825\n",
      "Iter 487, training loss 0.820337, validation loss 100.856895\n",
      "Iter 488, training loss 0.878437, validation loss 102.397987\n",
      "Iter 489, training loss 0.945035, validation loss 100.812851\n",
      "Iter 490, training loss 1.041687, validation loss 102.898460\n",
      "Iter 491, training loss 1.118730, validation loss 100.730339\n",
      "Iter 492, training loss 1.210923, validation loss 103.308945\n",
      "Iter 493, training loss 1.211632, validation loss 100.720192\n",
      "Iter 494, training loss 1.176236, validation loss 103.249260\n",
      "Iter 495, training loss 1.015798, validation loss 100.727119\n",
      "Iter 496, training loss 0.846571, validation loss 102.339394\n",
      "Iter 497, training loss 0.708570, validation loss 101.081436\n",
      "Iter 498, training loss 0.665515, validation loss 101.377480\n",
      "Iter 499, training loss 0.712218, validation loss 101.872162\n",
      "Iter 500, training loss 0.799218, validation loss 100.924561\n",
      "Iter 501, training loss 0.882250, validation loss 102.538895\n",
      "Iter 502, training loss 0.909189, validation loss 100.886932\n",
      "Iter 503, training loss 0.893278, validation loss 102.683907\n",
      "Iter 504, training loss 0.821610, validation loss 101.037682\n",
      "Iter 505, training loss 0.743638, validation loss 102.249115\n",
      "Iter 506, training loss 0.677125, validation loss 101.355103\n",
      "Iter 507, training loss 0.647215, validation loss 101.720856\n",
      "Iter 508, training loss 0.653532, validation loss 101.878082\n",
      "Iter 509, training loss 0.683174, validation loss 101.404976\n",
      "Iter 510, training loss 0.723343, validation loss 102.349846\n",
      "Iter 511, training loss 0.758155, validation loss 101.306854\n",
      "Iter 512, training loss 0.785779, validation loss 102.694046\n",
      "Iter 513, training loss 0.792838, validation loss 101.358902\n",
      "Iter 514, training loss 0.791075, validation loss 102.805946\n",
      "Iter 515, training loss 0.769365, validation loss 101.450424\n",
      "Iter 516, training loss 0.743616, validation loss 102.725021\n",
      "Iter 517, training loss 0.709048, validation loss 101.619896\n",
      "Iter 518, training loss 0.677831, validation loss 102.515182\n",
      "Iter 519, training loss 0.651075, validation loss 101.810890\n",
      "Iter 520, training loss 0.633242, validation loss 102.286842\n",
      "Iter 521, training loss 0.623191, validation loss 102.066536\n",
      "Iter 522, training loss 0.619552, validation loss 102.156693\n",
      "Iter 523, training loss 0.621342, validation loss 102.308098\n",
      "Iter 524, training loss 0.627051, validation loss 102.071106\n",
      "Iter 525, training loss 0.637076, validation loss 102.580238\n",
      "Iter 526, training loss 0.652670, validation loss 102.023079\n",
      "Iter 527, training loss 0.677153, validation loss 102.886612\n",
      "Iter 528, training loss 0.711806, validation loss 101.942909\n",
      "Iter 529, training loss 0.769327, validation loss 103.310661\n",
      "Iter 530, training loss 0.848982, validation loss 101.840973\n",
      "Iter 531, training loss 0.977901, validation loss 103.946648\n",
      "Iter 532, training loss 1.123979, validation loss 101.727837\n",
      "Iter 533, training loss 1.341655, validation loss 104.767273\n",
      "Iter 534, training loss 1.476051, validation loss 101.670525\n",
      "Iter 535, training loss 1.597641, validation loss 105.259071\n",
      "Iter 536, training loss 1.414342, validation loss 101.570374\n",
      "Iter 537, training loss 1.130683, validation loss 104.210388\n",
      "Iter 538, training loss 0.773270, validation loss 101.741356\n",
      "Iter 539, training loss 0.611399, validation loss 102.357811\n",
      "Iter 540, training loss 0.695481, validation loss 102.882782\n",
      "Iter 541, training loss 0.886901, validation loss 101.564293\n",
      "Iter 542, training loss 1.015524, validation loss 103.840721\n",
      "Iter 543, training loss 0.941702, validation loss 101.572609\n",
      "Iter 544, training loss 0.774795, validation loss 103.252953\n",
      "Iter 545, training loss 0.626471, validation loss 102.063530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 546, training loss 0.606251, validation loss 102.192474\n",
      "Iter 547, training loss 0.692734, validation loss 103.054558\n",
      "Iter 548, training loss 0.786657, validation loss 101.819817\n",
      "Iter 549, training loss 0.819831, validation loss 103.553902\n",
      "Iter 550, training loss 0.757630, validation loss 101.909180\n",
      "Iter 551, training loss 0.666810, validation loss 103.119621\n",
      "Iter 552, training loss 0.597935, validation loss 102.380836\n",
      "Iter 553, training loss 0.587819, validation loss 102.492325\n",
      "Iter 554, training loss 0.625648, validation loss 103.050148\n",
      "Iter 555, training loss 0.675609, validation loss 102.207008\n",
      "Iter 556, training loss 0.709227, validation loss 103.506020\n",
      "Iter 557, training loss 0.703402, validation loss 102.262459\n",
      "Iter 558, training loss 0.673123, validation loss 103.480751\n",
      "Iter 559, training loss 0.628468, validation loss 102.493431\n",
      "Iter 560, training loss 0.591609, validation loss 103.167313\n",
      "Iter 561, training loss 0.572390, validation loss 102.878220\n",
      "Iter 562, training loss 0.572954, validation loss 102.872612\n",
      "Iter 563, training loss 0.587376, validation loss 103.267090\n",
      "Iter 564, training loss 0.607476, validation loss 102.715286\n",
      "Iter 565, training loss 0.627616, validation loss 103.588524\n",
      "Iter 566, training loss 0.640552, validation loss 102.705421\n",
      "Iter 567, training loss 0.648002, validation loss 103.767662\n",
      "Iter 568, training loss 0.645136, validation loss 102.749329\n",
      "Iter 569, training loss 0.638262, validation loss 103.779320\n",
      "Iter 570, training loss 0.624855, validation loss 102.839386\n",
      "Iter 571, training loss 0.611235, validation loss 103.733284\n",
      "Iter 572, training loss 0.596151, validation loss 102.976707\n",
      "Iter 573, training loss 0.583529, validation loss 103.641266\n",
      "Iter 574, training loss 0.572545, validation loss 103.107376\n",
      "Iter 575, training loss 0.564076, validation loss 103.556419\n",
      "Iter 576, training loss 0.557912, validation loss 103.250450\n",
      "Iter 577, training loss 0.553804, validation loss 103.524132\n",
      "Iter 578, training loss 0.550698, validation loss 103.378494\n",
      "Iter 579, training loss 0.548466, validation loss 103.536041\n",
      "Iter 580, training loss 0.546805, validation loss 103.496300\n",
      "Iter 581, training loss 0.545431, validation loss 103.585648\n",
      "Iter 582, training loss 0.544096, validation loss 103.597115\n",
      "Iter 583, training loss 0.542901, validation loss 103.628716\n",
      "Iter 584, training loss 0.541776, validation loss 103.680069\n",
      "Iter 585, training loss 0.540724, validation loss 103.675957\n",
      "Iter 586, training loss 0.539816, validation loss 103.778862\n",
      "Iter 587, training loss 0.539245, validation loss 103.725174\n",
      "Iter 588, training loss 0.539385, validation loss 103.899033\n",
      "Iter 589, training loss 0.541216, validation loss 103.716362\n",
      "Iter 590, training loss 0.546565, validation loss 104.067680\n",
      "Iter 591, training loss 0.559171, validation loss 103.641487\n",
      "Iter 592, training loss 0.587272, validation loss 104.385414\n",
      "Iter 593, training loss 0.644811, validation loss 103.468773\n",
      "Iter 594, training loss 0.766147, validation loss 105.068390\n",
      "Iter 595, training loss 0.989351, validation loss 103.254959\n",
      "Iter 596, training loss 1.441149, validation loss 106.614525\n",
      "Iter 597, training loss 2.080504, validation loss 103.287048\n",
      "Iter 598, training loss 3.174914, validation loss 109.507622\n",
      "Iter 599, training loss 3.601867, validation loss 103.655754\n",
      "Iter 600, training loss 3.539704, validation loss 109.910980\n",
      "Iter 601, training loss 1.693859, validation loss 102.579346\n",
      "Iter 602, training loss 0.598061, validation loss 103.462204\n",
      "Iter 603, training loss 1.196875, validation loss 105.134903\n",
      "Iter 604, training loss 1.930662, validation loss 101.955093\n",
      "Iter 605, training loss 1.484881, validation loss 105.441963\n",
      "Iter 606, training loss 0.623960, validation loss 102.301353\n",
      "Iter 607, training loss 0.990064, validation loss 101.691078\n",
      "Iter 608, training loss 1.541713, validation loss 105.305092\n",
      "Iter 609, training loss 0.973036, validation loss 101.677567\n",
      "Iter 610, training loss 0.592470, validation loss 102.359451\n",
      "Iter 611, training loss 1.017276, validation loss 104.208221\n",
      "Iter 612, training loss 1.101134, validation loss 101.624649\n",
      "Iter 613, training loss 0.681315, validation loss 103.218201\n",
      "Iter 614, training loss 0.634875, validation loss 103.043983\n",
      "Iter 615, training loss 0.933718, validation loss 101.715111\n",
      "Iter 616, training loss 0.860748, validation loss 103.879845\n",
      "Iter 617, training loss 0.577095, validation loss 102.347656\n",
      "Iter 618, training loss 0.676042, validation loss 102.060081\n",
      "Iter 619, training loss 0.852233, validation loss 104.000740\n",
      "Iter 620, training loss 0.689302, validation loss 102.145638\n",
      "Iter 621, training loss 0.551858, validation loss 102.748825\n",
      "Iter 622, training loss 0.672470, validation loss 103.623230\n",
      "Iter 623, training loss 0.733910, validation loss 102.224464\n",
      "Iter 624, training loss 0.608472, validation loss 103.469353\n",
      "Iter 625, training loss 0.544979, validation loss 103.081329\n",
      "Iter 626, training loss 0.635319, validation loss 102.490067\n",
      "Iter 627, training loss 0.667511, validation loss 103.819572\n",
      "Iter 628, training loss 0.573299, validation loss 102.702095\n",
      "Iter 629, training loss 0.535681, validation loss 102.978882\n",
      "Iter 630, training loss 0.597017, validation loss 103.701332\n",
      "Iter 631, training loss 0.616577, validation loss 102.727280\n",
      "Iter 632, training loss 0.558881, validation loss 103.617889\n",
      "Iter 633, training loss 0.524466, validation loss 103.334579\n",
      "Iter 634, training loss 0.558053, validation loss 103.046608\n",
      "Iter 635, training loss 0.584776, validation loss 103.958168\n",
      "Iter 636, training loss 0.553939, validation loss 103.164825\n",
      "Iter 637, training loss 0.518828, validation loss 103.609802\n",
      "Iter 638, training loss 0.526769, validation loss 103.794838\n",
      "Iter 639, training loss 0.551718, validation loss 103.327858\n",
      "Iter 640, training loss 0.549272, validation loss 104.094498\n",
      "Iter 641, training loss 0.522395, validation loss 103.554176\n",
      "Iter 642, training loss 0.509035, validation loss 103.757584\n",
      "Iter 643, training loss 0.520210, validation loss 104.031509\n",
      "Iter 644, training loss 0.532571, validation loss 103.577858\n",
      "Iter 645, training loss 0.526908, validation loss 104.180077\n",
      "Iter 646, training loss 0.509985, validation loss 103.771011\n",
      "Iter 647, training loss 0.501931, validation loss 103.933922\n",
      "Iter 648, training loss 0.507746, validation loss 104.148682\n",
      "Iter 649, training loss 0.515693, validation loss 103.814194\n",
      "Iter 650, training loss 0.514460, validation loss 104.308708\n",
      "Iter 651, training loss 0.504850, validation loss 103.936020\n",
      "Iter 652, training loss 0.496663, validation loss 104.164917\n",
      "Iter 653, training loss 0.495952, validation loss 104.210365\n",
      "Iter 654, training loss 0.500327, validation loss 104.042015\n",
      "Iter 655, training loss 0.503251, validation loss 104.415199\n",
      "Iter 656, training loss 0.500774, validation loss 104.087379\n",
      "Iter 657, training loss 0.494981, validation loss 104.413574\n",
      "Iter 658, training loss 0.489972, validation loss 104.279686\n",
      "Iter 659, training loss 0.488513, validation loss 104.331673\n",
      "Iter 660, training loss 0.489949, validation loss 104.492119\n",
      "Iter 661, training loss 0.491739, validation loss 104.298859\n",
      "Iter 662, training loss 0.491681, validation loss 104.612061\n",
      "Iter 663, training loss 0.489297, validation loss 104.374313\n",
      "Iter 664, training loss 0.485810, validation loss 104.632454\n",
      "Iter 665, training loss 0.482756, validation loss 104.534111\n",
      "Iter 666, training loss 0.481220, validation loss 104.604897\n",
      "Iter 667, training loss 0.481029, validation loss 104.689140\n",
      "Iter 668, training loss 0.481478, validation loss 104.586487\n",
      "Iter 669, training loss 0.481795, validation loss 104.803261\n",
      "Iter 670, training loss 0.481472, validation loss 104.616425\n",
      "Iter 671, training loss 0.480338, validation loss 104.865303\n",
      "Iter 672, training loss 0.478509, validation loss 104.688683\n",
      "Iter 673, training loss 0.476590, validation loss 104.889954\n",
      "Iter 674, training loss 0.474738, validation loss 104.794800\n",
      "Iter 675, training loss 0.473259, validation loss 104.900742\n",
      "Iter 676, training loss 0.472141, validation loss 104.891701\n",
      "Iter 677, training loss 0.471365, validation loss 104.900475\n",
      "Iter 678, training loss 0.470849, validation loss 104.980911\n",
      "Iter 679, training loss 0.470526, validation loss 104.915581\n",
      "Iter 680, training loss 0.470404, validation loss 105.067139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 681, training loss 0.470459, validation loss 104.932060\n",
      "Iter 682, training loss 0.470609, validation loss 105.155380\n",
      "Iter 683, training loss 0.470975, validation loss 104.960106\n",
      "Iter 684, training loss 0.471508, validation loss 105.246262\n",
      "Iter 685, training loss 0.472243, validation loss 104.981583\n",
      "Iter 686, training loss 0.473392, validation loss 105.338409\n",
      "Iter 687, training loss 0.474980, validation loss 105.002899\n",
      "Iter 688, training loss 0.477241, validation loss 105.440079\n",
      "Iter 689, training loss 0.479896, validation loss 105.008362\n",
      "Iter 690, training loss 0.483783, validation loss 105.549240\n",
      "Iter 691, training loss 0.488508, validation loss 105.013657\n",
      "Iter 692, training loss 0.495308, validation loss 105.683464\n",
      "Iter 693, training loss 0.503106, validation loss 105.001099\n",
      "Iter 694, training loss 0.514358, validation loss 105.833397\n",
      "Iter 695, training loss 0.526805, validation loss 104.974182\n",
      "Iter 696, training loss 0.545156, validation loss 106.013184\n",
      "Iter 697, training loss 0.565273, validation loss 104.922455\n",
      "Iter 698, training loss 0.594615, validation loss 106.224159\n",
      "Iter 699, training loss 0.623369, validation loss 104.865471\n",
      "Iter 700, training loss 0.665056, validation loss 106.480637\n",
      "Iter 701, training loss 0.697705, validation loss 104.805443\n",
      "Iter 702, training loss 0.743295, validation loss 106.708115\n",
      "Iter 703, training loss 0.764582, validation loss 104.754333\n",
      "Iter 704, training loss 0.790638, validation loss 106.831390\n",
      "Iter 705, training loss 0.771747, validation loss 104.729095\n",
      "Iter 706, training loss 0.745687, validation loss 106.698936\n",
      "Iter 707, training loss 0.678348, validation loss 104.784142\n",
      "Iter 708, training loss 0.608709, validation loss 106.291855\n",
      "Iter 709, training loss 0.533922, validation loss 104.966087\n",
      "Iter 710, training loss 0.480229, validation loss 105.749771\n",
      "Iter 711, training loss 0.452595, validation loss 105.326752\n",
      "Iter 712, training loss 0.451559, validation loss 105.348389\n",
      "Iter 713, training loss 0.470137, validation loss 105.750778\n",
      "Iter 714, training loss 0.500172, validation loss 105.120590\n",
      "Iter 715, training loss 0.537038, validation loss 106.141098\n",
      "Iter 716, training loss 0.572011, validation loss 105.024055\n",
      "Iter 717, training loss 0.609447, validation loss 106.457993\n",
      "Iter 718, training loss 0.635546, validation loss 104.982376\n",
      "Iter 719, training loss 0.663941, validation loss 106.664001\n",
      "Iter 720, training loss 0.671428, validation loss 104.973358\n",
      "Iter 721, training loss 0.679241, validation loss 106.739754\n",
      "Iter 722, training loss 0.660513, validation loss 104.999725\n",
      "Iter 723, training loss 0.639904, validation loss 106.640907\n",
      "Iter 724, training loss 0.598964, validation loss 105.082237\n",
      "Iter 725, training loss 0.559530, validation loss 106.404297\n",
      "Iter 726, training loss 0.516143, validation loss 105.253487\n",
      "Iter 727, training loss 0.481358, validation loss 106.102753\n",
      "Iter 728, training loss 0.455590, validation loss 105.489532\n",
      "Iter 729, training loss 0.441432, validation loss 105.842781\n",
      "Iter 730, training loss 0.437058, validation loss 105.774643\n",
      "Iter 731, training loss 0.440239, validation loss 105.691422\n",
      "Iter 732, training loss 0.449067, validation loss 106.051498\n",
      "Iter 733, training loss 0.462612, validation loss 105.590096\n",
      "Iter 734, training loss 0.481469, validation loss 106.329994\n",
      "Iter 735, training loss 0.505549, validation loss 105.516182\n",
      "Iter 736, training loss 0.540107, validation loss 106.643097\n",
      "Iter 737, training loss 0.583546, validation loss 105.422791\n",
      "Iter 738, training loss 0.649447, validation loss 107.041191\n",
      "Iter 739, training loss 0.725990, validation loss 105.323730\n",
      "Iter 740, training loss 0.844622, validation loss 107.589348\n",
      "Iter 741, training loss 0.956326, validation loss 105.236526\n",
      "Iter 742, training loss 1.117738, validation loss 108.197678\n",
      "Iter 743, training loss 1.193383, validation loss 105.171104\n",
      "Iter 744, training loss 1.266091, validation loss 108.464104\n",
      "Iter 745, training loss 1.130155, validation loss 105.084961\n",
      "Iter 746, training loss 0.940252, validation loss 107.678131\n",
      "Iter 747, training loss 0.663463, validation loss 105.173088\n",
      "Iter 748, training loss 0.477695, validation loss 106.195374\n",
      "Iter 749, training loss 0.442880, validation loss 105.912308\n",
      "Iter 750, training loss 0.538733, validation loss 105.265610\n",
      "Iter 751, training loss 0.676338, validation loss 106.907967\n",
      "Iter 752, training loss 0.743824, validation loss 105.092476\n",
      "Iter 753, training loss 0.731184, validation loss 107.116570\n",
      "Iter 754, training loss 0.622978, validation loss 105.229645\n",
      "Iter 755, training loss 0.509001, validation loss 106.442429\n",
      "Iter 756, training loss 0.437857, validation loss 105.718979\n",
      "Iter 757, training loss 0.437912, validation loss 105.724998\n",
      "Iter 758, training loss 0.489394, validation loss 106.417580\n",
      "Iter 759, training loss 0.548590, validation loss 105.407036\n",
      "Iter 760, training loss 0.586596, validation loss 106.839394\n",
      "Iter 761, training loss 0.578123, validation loss 105.419052\n",
      "Iter 762, training loss 0.542690, validation loss 106.757240\n",
      "Iter 763, training loss 0.489319, validation loss 105.633423\n",
      "Iter 764, training loss 0.445790, validation loss 106.366898\n",
      "Iter 765, training loss 0.422981, validation loss 106.021111\n",
      "Iter 766, training loss 0.423904, validation loss 106.024956\n",
      "Iter 767, training loss 0.441733, validation loss 106.449768\n",
      "Iter 768, training loss 0.466531, validation loss 105.838493\n",
      "Iter 769, training loss 0.491835, validation loss 106.775925\n",
      "Iter 770, training loss 0.509322, validation loss 105.795120\n",
      "Iter 771, training loss 0.521080, validation loss 106.975784\n",
      "Iter 772, training loss 0.520482, validation loss 105.852654\n",
      "Iter 773, training loss 0.515625, validation loss 107.039047\n",
      "Iter 774, training loss 0.501779, validation loss 105.960693\n",
      "Iter 775, training loss 0.487254, validation loss 106.995163\n",
      "Iter 776, training loss 0.469490, validation loss 106.085854\n",
      "Iter 777, training loss 0.453822, validation loss 106.883842\n",
      "Iter 778, training loss 0.439409, validation loss 106.228767\n",
      "Iter 779, training loss 0.428347, validation loss 106.776192\n",
      "Iter 780, training loss 0.420039, validation loss 106.379669\n",
      "Iter 781, training loss 0.414484, validation loss 106.698463\n",
      "Iter 782, training loss 0.410977, validation loss 106.514320\n",
      "Iter 783, training loss 0.408876, validation loss 106.659973\n",
      "Iter 784, training loss 0.407650, validation loss 106.630386\n",
      "Iter 785, training loss 0.407056, validation loss 106.647728\n",
      "Iter 786, training loss 0.406892, validation loss 106.734528\n",
      "Iter 787, training loss 0.407217, validation loss 106.653397\n",
      "Iter 788, training loss 0.408398, validation loss 106.862038\n",
      "Iter 789, training loss 0.410924, validation loss 106.653610\n",
      "Iter 790, training loss 0.416029, validation loss 107.026001\n",
      "Iter 791, training loss 0.425645, validation loss 106.603462\n",
      "Iter 792, training loss 0.443881, validation loss 107.272240\n",
      "Iter 793, training loss 0.475661, validation loss 106.490692\n",
      "Iter 794, training loss 0.534950, validation loss 107.697655\n",
      "Iter 795, training loss 0.630652, validation loss 106.315002\n",
      "Iter 796, training loss 0.806992, validation loss 108.484711\n",
      "Iter 797, training loss 1.058217, validation loss 106.159592\n",
      "Iter 798, training loss 1.501905, validation loss 109.906616\n",
      "Iter 799, training loss 1.947757, validation loss 106.189445\n",
      "Iter 800, training loss 2.582442, validation loss 111.656227\n",
      "Iter 801, training loss 2.538788, validation loss 106.132965\n",
      "Iter 802, training loss 2.180328, validation loss 110.741631\n",
      "Iter 803, training loss 1.098240, validation loss 105.481224\n",
      "Iter 804, training loss 0.460919, validation loss 106.466339\n",
      "Iter 805, training loss 0.674842, validation loss 107.154526\n",
      "Iter 806, training loss 1.207541, validation loss 105.011520\n",
      "Iter 807, training loss 1.314816, validation loss 108.438683\n",
      "Iter 808, training loss 0.772119, validation loss 105.001602\n",
      "Iter 809, training loss 0.437168, validation loss 105.773468\n",
      "Iter 810, training loss 0.655932, validation loss 106.800621\n",
      "Iter 811, training loss 0.938989, validation loss 104.907043\n",
      "Iter 812, training loss 0.846960, validation loss 107.369324\n",
      "Iter 813, training loss 0.510944, validation loss 105.324249\n",
      "Iter 814, training loss 0.453522, validation loss 105.495003\n",
      "Iter 815, training loss 0.668881, validation loss 106.923264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 816, training loss 0.748645, validation loss 105.038414\n",
      "Iter 817, training loss 0.591776, validation loss 106.739159\n",
      "Iter 818, training loss 0.428201, validation loss 105.661636\n",
      "Iter 819, training loss 0.478512, validation loss 105.438232\n",
      "Iter 820, training loss 0.613304, validation loss 106.899506\n",
      "Iter 821, training loss 0.602841, validation loss 105.325882\n",
      "Iter 822, training loss 0.480967, validation loss 106.564659\n",
      "Iter 823, training loss 0.412104, validation loss 106.059334\n",
      "Iter 824, training loss 0.467733, validation loss 105.693268\n",
      "Iter 825, training loss 0.541368, validation loss 106.917061\n",
      "Iter 826, training loss 0.520640, validation loss 105.649620\n",
      "Iter 827, training loss 0.444516, validation loss 106.593300\n",
      "Iter 828, training loss 0.404773, validation loss 106.227829\n",
      "Iter 829, training loss 0.436149, validation loss 105.967148\n",
      "Iter 830, training loss 0.482095, validation loss 106.920692\n",
      "Iter 831, training loss 0.479764, validation loss 105.961342\n",
      "Iter 832, training loss 0.437969, validation loss 106.840584\n",
      "Iter 833, training loss 0.401876, validation loss 106.388184\n",
      "Iter 834, training loss 0.404484, validation loss 106.393944\n",
      "Iter 835, training loss 0.431650, validation loss 106.974869\n",
      "Iter 836, training loss 0.448022, validation loss 106.297760\n",
      "Iter 837, training loss 0.438491, validation loss 107.121941\n",
      "Iter 838, training loss 0.412452, validation loss 106.493835\n",
      "Iter 839, training loss 0.394010, validation loss 106.831589\n",
      "Iter 840, training loss 0.394537, validation loss 106.901375\n",
      "Iter 841, training loss 0.407852, validation loss 106.613899\n",
      "Iter 842, training loss 0.419694, validation loss 107.198349\n",
      "Iter 843, training loss 0.419736, validation loss 106.601852\n",
      "Iter 844, training loss 0.409532, validation loss 107.210892\n",
      "Iter 845, training loss 0.395863, validation loss 106.812386\n",
      "Iter 846, training loss 0.387159, validation loss 107.061241\n",
      "Iter 847, training loss 0.386412, validation loss 107.086609\n",
      "Iter 848, training loss 0.391616, validation loss 106.913116\n",
      "Iter 849, training loss 0.398099, validation loss 107.310318\n",
      "Iter 850, training loss 0.401507, validation loss 106.896614\n",
      "Iter 851, training loss 0.400614, validation loss 107.399620\n",
      "Iter 852, training loss 0.395723, validation loss 106.972450\n",
      "Iter 853, training loss 0.389573, validation loss 107.365181\n",
      "Iter 854, training loss 0.383974, validation loss 107.129173\n",
      "Iter 855, training loss 0.380461, validation loss 107.303642\n",
      "Iter 856, training loss 0.379277, validation loss 107.300385\n",
      "Iter 857, training loss 0.379992, validation loss 107.243347\n",
      "Iter 858, training loss 0.381823, validation loss 107.447090\n",
      "Iter 859, training loss 0.384104, validation loss 107.227882\n",
      "Iter 860, training loss 0.386242, validation loss 107.576767\n",
      "Iter 861, training loss 0.387747, validation loss 107.250832\n",
      "Iter 862, training loss 0.388761, validation loss 107.673149\n",
      "Iter 863, training loss 0.388948, validation loss 107.302696\n",
      "Iter 864, training loss 0.388834, validation loss 107.750893\n",
      "Iter 865, training loss 0.388211, validation loss 107.361488\n",
      "Iter 866, training loss 0.387572, validation loss 107.805458\n",
      "Iter 867, training loss 0.386829, validation loss 107.420921\n",
      "Iter 868, training loss 0.386443, validation loss 107.861458\n",
      "Iter 869, training loss 0.386192, validation loss 107.467789\n",
      "Iter 870, training loss 0.386567, validation loss 107.912231\n",
      "Iter 871, training loss 0.387464, validation loss 107.495071\n",
      "Iter 872, training loss 0.389600, validation loss 107.987999\n",
      "Iter 873, training loss 0.392872, validation loss 107.507545\n",
      "Iter 874, training loss 0.398409, validation loss 108.095306\n",
      "Iter 875, training loss 0.406083, validation loss 107.491501\n",
      "Iter 876, training loss 0.418319, validation loss 108.249901\n",
      "Iter 877, training loss 0.434759, validation loss 107.449463\n",
      "Iter 878, training loss 0.460537, validation loss 108.474823\n",
      "Iter 879, training loss 0.492669, validation loss 107.372047\n",
      "Iter 880, training loss 0.542540, validation loss 108.781761\n",
      "Iter 881, training loss 0.600075, validation loss 107.274918\n",
      "Iter 882, training loss 0.687350, validation loss 109.199692\n",
      "Iter 883, training loss 0.768323, validation loss 107.176422\n",
      "Iter 884, training loss 0.883586, validation loss 109.655182\n",
      "Iter 885, training loss 0.946440, validation loss 107.092468\n",
      "Iter 886, training loss 1.019247, validation loss 109.908142\n",
      "Iter 887, training loss 0.965805, validation loss 107.019043\n",
      "Iter 888, training loss 0.880160, validation loss 109.536949\n",
      "Iter 889, training loss 0.690841, validation loss 107.025238\n",
      "Iter 890, training loss 0.516959, validation loss 108.516159\n",
      "Iter 891, training loss 0.396122, validation loss 107.402809\n",
      "Iter 892, training loss 0.372714, validation loss 107.549324\n",
      "Iter 893, training loss 0.430686, validation loss 108.149284\n",
      "Iter 894, training loss 0.519382, validation loss 107.111946\n",
      "Iter 895, training loss 0.596703, validation loss 108.732605\n",
      "Iter 896, training loss 0.613762, validation loss 107.034447\n",
      "Iter 897, training loss 0.588782, validation loss 108.732857\n",
      "Iter 898, training loss 0.514804, validation loss 107.147339\n",
      "Iter 899, training loss 0.439543, validation loss 108.250977\n",
      "Iter 900, training loss 0.383022, validation loss 107.474457\n",
      "Iter 901, training loss 0.363609, validation loss 107.705925\n",
      "Iter 902, training loss 0.378337, validation loss 107.957817\n",
      "Iter 903, training loss 0.412163, validation loss 107.399498\n",
      "Iter 904, training loss 0.449537, validation loss 108.387154\n",
      "Iter 905, training loss 0.473870, validation loss 107.328987\n",
      "Iter 906, training loss 0.485868, validation loss 108.591232\n",
      "Iter 907, training loss 0.476757, validation loss 107.391182\n",
      "Iter 908, training loss 0.459903, validation loss 108.565765\n",
      "Iter 909, training loss 0.432610, validation loss 107.530914\n",
      "Iter 910, training loss 0.406599, validation loss 108.393135\n",
      "Iter 911, training loss 0.383011, validation loss 107.724380\n",
      "Iter 912, training loss 0.366652, validation loss 108.187576\n",
      "Iter 913, training loss 0.357753, validation loss 107.962219\n",
      "Iter 914, training loss 0.355635, validation loss 108.042259\n",
      "Iter 915, training loss 0.358577, validation loss 108.205818\n",
      "Iter 916, training loss 0.365221, validation loss 107.962868\n",
      "Iter 917, training loss 0.374890, validation loss 108.444939\n",
      "Iter 918, training loss 0.387264, validation loss 107.924812\n",
      "Iter 919, training loss 0.403724, validation loss 108.684479\n",
      "Iter 920, training loss 0.423805, validation loss 107.879646\n",
      "Iter 921, training loss 0.452431, validation loss 108.946571\n",
      "Iter 922, training loss 0.486387, validation loss 107.813194\n",
      "Iter 923, training loss 0.536401, validation loss 109.263405\n",
      "Iter 924, training loss 0.591059, validation loss 107.726051\n",
      "Iter 925, training loss 0.672373, validation loss 109.650879\n",
      "Iter 926, training loss 0.745733, validation loss 107.636078\n",
      "Iter 927, training loss 0.850475, validation loss 110.064011\n",
      "Iter 928, training loss 0.906934, validation loss 107.561729\n",
      "Iter 929, training loss 0.973382, validation loss 110.298843\n",
      "Iter 930, training loss 0.926302, validation loss 107.501030\n",
      "Iter 931, training loss 0.852326, validation loss 109.975342\n",
      "Iter 932, training loss 0.680907, validation loss 107.508141\n",
      "Iter 933, training loss 0.519218, validation loss 109.046417\n",
      "Iter 934, training loss 0.394565, validation loss 107.822266\n",
      "Iter 935, training loss 0.353652, validation loss 108.114304\n",
      "Iter 936, training loss 0.390110, validation loss 108.509430\n",
      "Iter 937, training loss 0.466878, validation loss 107.665604\n",
      "Iter 938, training loss 0.546735, validation loss 109.133972\n",
      "Iter 939, training loss 0.585669, validation loss 107.563301\n",
      "Iter 940, training loss 0.590409, validation loss 109.296463\n",
      "Iter 941, training loss 0.541841, validation loss 107.629860\n",
      "Iter 942, training loss 0.477991, validation loss 108.975243\n",
      "Iter 943, training loss 0.409251, validation loss 107.841080\n",
      "Iter 944, training loss 0.363295, validation loss 108.445763\n",
      "Iter 945, training loss 0.347846, validation loss 108.232788\n",
      "Iter 946, training loss 0.360137, validation loss 108.062057\n",
      "Iter 947, training loss 0.389151, validation loss 108.682228\n",
      "Iter 948, training loss 0.421398, validation loss 107.903992\n",
      "Iter 949, training loss 0.450895, validation loss 109.021393\n",
      "Iter 950, training loss 0.466989, validation loss 107.900055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 951, training loss 0.476489, validation loss 109.189636\n",
      "Iter 952, training loss 0.470244, validation loss 107.956139\n",
      "Iter 953, training loss 0.460361, validation loss 109.184837\n",
      "Iter 954, training loss 0.439797, validation loss 108.045731\n",
      "Iter 955, training loss 0.419864, validation loss 109.077744\n",
      "Iter 956, training loss 0.397257, validation loss 108.181007\n",
      "Iter 957, training loss 0.378242, validation loss 108.934303\n",
      "Iter 958, training loss 0.362275, validation loss 108.360947\n",
      "Iter 959, training loss 0.351101, validation loss 108.819565\n",
      "Iter 960, training loss 0.344017, validation loss 108.554901\n",
      "Iter 961, training loss 0.340277, validation loss 108.745850\n",
      "Iter 962, training loss 0.338916, validation loss 108.726082\n",
      "Iter 963, training loss 0.339250, validation loss 108.701698\n",
      "Iter 964, training loss 0.340955, validation loss 108.873253\n",
      "Iter 965, training loss 0.344022, validation loss 108.667786\n",
      "Iter 966, training loss 0.348954, validation loss 109.023613\n",
      "Iter 967, training loss 0.356708, validation loss 108.627350\n",
      "Iter 968, training loss 0.369216, validation loss 109.226601\n",
      "Iter 969, training loss 0.388588, validation loss 108.568764\n",
      "Iter 970, training loss 0.421052, validation loss 109.529266\n",
      "Iter 971, training loss 0.469997, validation loss 108.468277\n",
      "Iter 972, training loss 0.553394, validation loss 110.009193\n",
      "Iter 973, training loss 0.669515, validation loss 108.331512\n",
      "Iter 974, training loss 0.866980, validation loss 110.790749\n",
      "Iter 975, training loss 1.094149, validation loss 108.229378\n",
      "Iter 976, training loss 1.456756, validation loss 111.919304\n",
      "Iter 977, training loss 1.703495, validation loss 108.210457\n",
      "Iter 978, training loss 1.990757, validation loss 112.727036\n",
      "Iter 979, training loss 1.753793, validation loss 107.984909\n",
      "Iter 980, training loss 1.340941, validation loss 111.334213\n",
      "Iter 981, training loss 0.693969, validation loss 107.729118\n",
      "Iter 982, training loss 0.365591, validation loss 108.498413\n",
      "Iter 983, training loss 0.495878, validation loss 109.059425\n",
      "Iter 984, training loss 0.821935, validation loss 107.442017\n",
      "Iter 985, training loss 0.989540, validation loss 110.222488\n",
      "Iter 986, training loss 0.771772, validation loss 107.411430\n",
      "Iter 987, training loss 0.470475, validation loss 108.852142\n",
      "Iter 988, training loss 0.354966, validation loss 108.183426\n",
      "Iter 989, training loss 0.489598, validation loss 107.588982\n",
      "Iter 990, training loss 0.669547, validation loss 109.446098\n",
      "Iter 991, training loss 0.660527, validation loss 107.443214\n",
      "Iter 992, training loss 0.509210, validation loss 109.000755\n",
      "Iter 993, training loss 0.365684, validation loss 107.903809\n",
      "Iter 994, training loss 0.367799, validation loss 107.906143\n",
      "Iter 995, training loss 0.471953, validation loss 108.944168\n",
      "Iter 996, training loss 0.537208, validation loss 107.628418\n",
      "Iter 997, training loss 0.505091, validation loss 109.129623\n",
      "Iter 998, training loss 0.406172, validation loss 107.892220\n",
      "Iter 999, training loss 0.343763, validation loss 108.407211\n",
      "Size of the model predictions for training is:  (6716, 1)\n",
      "Size of the model predictions for validation is:  (2238, 1)\n"
     ]
    }
   ],
   "source": [
    "epochs=40000\n",
    "print(\"ADAS13\")\n",
    "adas_train_pred, adas_val_pred, tr_losses, te_losses=NN(x_train,y_train_adas13,x_val,y_val_adas13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEFCAYAAAABjYvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22309d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch number is:  60\n",
      "The lowest validation loss is:  9.145073954020914\n",
      "At the same epoch, the training loss is:  3.086062349135854\n"
     ]
    }
   ],
   "source": [
    "tr_loss, te_loss=plot_NN(epochs, tr_losses, te_losses)\n",
    "print(\"Best epoch number is: \", te_loss.index(min(te_loss)))\n",
    "print(\"The lowest validation loss is: \", min(te_loss))\n",
    "print(\"At the same epoch, the training loss is: \", tr_loss[te_loss.index(min(te_loss))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventricles_Norm\n",
      "Iter 0, training loss 295.494537, validation loss 286.139008\n",
      "Iter 1, training loss 38.278706, validation loss 36.705318\n",
      "Iter 2, training loss 30.270189, validation loss 30.478430\n",
      "Iter 3, training loss 114.650520, validation loss 114.120354\n",
      "Iter 4, training loss 93.249313, validation loss 93.251190\n",
      "Iter 5, training loss 32.032940, validation loss 32.467907\n",
      "Iter 6, training loss 1.475252, validation loss 1.646666\n",
      "Iter 7, training loss 10.065543, validation loss 9.759432\n",
      "Iter 8, training loss 31.831715, validation loss 31.168251\n",
      "Iter 9, training loss 42.233803, validation loss 41.493191\n",
      "Iter 10, training loss 35.407803, validation loss 34.796410\n",
      "Iter 11, training loss 19.641846, validation loss 19.235363\n",
      "Iter 12, training loss 5.910471, validation loss 5.713643\n",
      "Iter 13, training loss 0.470615, validation loss 0.487421\n",
      "Iter 14, training loss 3.193468, validation loss 3.433158\n",
      "Iter 15, training loss 9.685003, validation loss 10.130063\n",
      "Iter 16, training loss 14.856505, validation loss 15.440338\n",
      "Iter 17, training loss 15.780027, validation loss 16.398001\n",
      "Iter 18, training loss 12.553052, validation loss 13.100057\n",
      "Iter 19, training loss 7.343874, validation loss 7.745433\n",
      "Iter 20, training loss 2.733781, validation loss 2.963893\n",
      "Iter 21, training loss 0.409120, validation loss 0.493066\n",
      "Iter 22, training loss 0.653065, validation loss 0.641650\n",
      "Iter 23, training loss 2.550449, validation loss 2.495928\n",
      "Iter 24, training loss 4.669309, validation loss 4.606705\n",
      "Iter 25, training loss 5.856109, validation loss 5.798919\n",
      "Iter 26, training loss 5.651233, validation loss 5.599957\n",
      "Iter 27, training loss 4.305117, validation loss 4.257759\n",
      "Iter 28, training loss 2.505217, validation loss 2.466566\n",
      "Iter 29, training loss 0.988084, validation loss 0.972374\n",
      "Iter 30, training loss 0.229037, validation loss 0.254555\n",
      "Iter 31, training loss 0.310728, validation loss 0.389903\n",
      "Iter 32, training loss 0.955657, validation loss 1.090938\n",
      "Iter 33, training loss 1.704306, validation loss 1.887821\n",
      "Iter 34, training loss 2.167800, validation loss 2.381570\n",
      "Iter 35, training loss 2.160723, validation loss 2.381654\n",
      "Iter 36, training loss 1.730724, validation loss 1.935556\n",
      "Iter 37, training loss 1.091269, validation loss 1.261222\n",
      "Iter 38, training loss 0.506695, validation loss 0.632595\n",
      "Iter 39, training loss 0.174020, validation loss 0.256010\n",
      "Iter 40, training loss 0.153286, validation loss 0.198317\n",
      "Iter 41, training loss 0.367624, validation loss 0.385876\n",
      "Iter 42, training loss 0.659419, validation loss 0.661355\n",
      "Iter 43, training loss 0.869645, validation loss 0.864426\n",
      "Iter 44, training loss 0.904101, validation loss 0.899215\n",
      "Iter 45, training loss 0.761212, validation loss 0.762868\n",
      "Iter 46, training loss 0.516343, validation loss 0.529935\n",
      "Iter 47, training loss 0.276437, validation loss 0.306612\n",
      "Iter 48, training loss 0.129106, validation loss 0.179201\n",
      "Iter 49, training loss 0.108375, validation loss 0.179550\n",
      "Iter 50, training loss 0.188999, validation loss 0.279486\n",
      "Iter 51, training loss 0.306900, validation loss 0.412116\n",
      "Iter 52, training loss 0.393512, validation loss 0.506730\n",
      "Iter 53, training loss 0.407098, validation loss 0.520689\n",
      "Iter 54, training loss 0.346584, validation loss 0.453619\n",
      "Iter 55, training loss 0.244856, validation loss 0.340479\n",
      "Iter 56, training loss 0.148305, validation loss 0.230276\n",
      "Iter 57, training loss 0.093586, validation loss 0.162114\n",
      "Iter 58, training loss 0.092550, validation loss 0.149721\n",
      "Iter 59, training loss 0.130973, validation loss 0.179909\n",
      "Iter 60, training loss 0.179382, validation loss 0.223420\n",
      "Iter 61, training loss 0.209168, validation loss 0.251385\n",
      "Iter 62, training loss 0.205697, validation loss 0.248861\n",
      "Iter 63, training loss 0.172777, validation loss 0.219278\n",
      "Iter 64, training loss 0.127767, validation loss 0.179518\n",
      "Iter 65, training loss 0.091093, validation loss 0.149440\n",
      "Iter 66, training loss 0.075975, validation loss 0.141495\n",
      "Iter 67, training loss 0.083244, validation loss 0.155587\n",
      "Iter 68, training loss 0.102955, validation loss 0.180835\n",
      "Iter 69, training loss 0.120945, validation loss 0.202399\n",
      "Iter 70, training loss 0.126489, validation loss 0.209259\n",
      "Iter 71, training loss 0.117073, validation loss 0.198979\n",
      "Iter 72, training loss 0.098271, validation loss 0.177549\n",
      "Iter 73, training loss 0.079542, validation loss 0.155099\n",
      "Iter 74, training loss 0.068793, validation loss 0.140302\n",
      "Iter 75, training loss 0.068616, validation loss 0.136437\n",
      "Iter 76, training loss 0.075852, validation loss 0.140850\n",
      "Iter 77, training loss 0.084140, validation loss 0.147465\n",
      "Iter 78, training loss 0.087708, validation loss 0.150570\n",
      "Iter 79, training loss 0.084208, validation loss 0.147664\n",
      "Iter 80, training loss 0.075529, validation loss 0.140132\n",
      "Iter 81, training loss 0.066354, validation loss 0.132178\n",
      "Iter 82, training loss 0.061510, validation loss 0.128640\n",
      "Iter 83, training loss 0.063199, validation loss 0.131850\n",
      "Iter 84, training loss 0.066408, validation loss 0.136908\n",
      "Iter 85, training loss 0.067241, validation loss 0.139446\n",
      "Iter 86, training loss 0.065343, validation loss 0.138991\n",
      "Iter 87, training loss 0.061947, validation loss 0.136453\n",
      "Iter 88, training loss 0.058565, validation loss 0.133225\n",
      "Iter 89, training loss 0.056292, validation loss 0.130509\n",
      "Iter 90, training loss 0.055684, validation loss 0.128893\n",
      "Iter 91, training loss 0.056250, validation loss 0.128684\n",
      "Iter 92, training loss 0.056320, validation loss 0.128550\n",
      "Iter 93, training loss 0.055848, validation loss 0.128017\n",
      "Iter 94, training loss 0.054848, validation loss 0.127080\n",
      "Iter 95, training loss 0.053541, validation loss 0.126227\n",
      "Iter 96, training loss 0.052323, validation loss 0.125629\n",
      "Iter 97, training loss 0.051494, validation loss 0.125414\n",
      "Iter 98, training loss 0.051054, validation loss 0.125488\n",
      "Iter 99, training loss 0.050778, validation loss 0.125559\n",
      "Iter 100, training loss 0.050424, validation loss 0.125351\n",
      "Iter 101, training loss 0.049861, validation loss 0.124725\n",
      "Iter 102, training loss 0.049107, validation loss 0.123739\n",
      "Iter 103, training loss 0.048314, validation loss 0.122578\n",
      "Iter 104, training loss 0.047646, validation loss 0.121487\n",
      "Iter 105, training loss 0.047161, validation loss 0.120647\n",
      "Iter 106, training loss 0.046787, validation loss 0.120039\n",
      "Iter 107, training loss 0.046397, validation loss 0.119555\n",
      "Iter 108, training loss 0.045909, validation loss 0.119106\n",
      "Iter 109, training loss 0.045321, validation loss 0.118685\n",
      "Iter 110, training loss 0.044696, validation loss 0.118313\n",
      "Iter 111, training loss 0.044107, validation loss 0.118061\n",
      "Iter 112, training loss 0.043600, validation loss 0.117920\n",
      "Iter 113, training loss 0.043147, validation loss 0.117809\n",
      "Iter 114, training loss 0.042706, validation loss 0.117605\n",
      "Iter 115, training loss 0.042230, validation loss 0.117259\n",
      "Iter 116, training loss 0.041696, validation loss 0.116740\n",
      "Iter 117, training loss 0.041136, validation loss 0.116191\n",
      "Iter 118, training loss 0.040717, validation loss 0.115722\n",
      "Iter 119, training loss 0.040320, validation loss 0.115317\n",
      "Iter 120, training loss 0.039811, validation loss 0.114827\n",
      "Iter 121, training loss 0.039337, validation loss 0.114290\n",
      "Iter 122, training loss 0.038915, validation loss 0.113801\n",
      "Iter 123, training loss 0.038512, validation loss 0.113339\n",
      "Iter 124, training loss 0.038104, validation loss 0.112920\n",
      "Iter 125, training loss 0.037660, validation loss 0.112526\n",
      "Iter 126, training loss 0.037180, validation loss 0.112118\n",
      "Iter 127, training loss 0.036718, validation loss 0.111689\n",
      "Iter 128, training loss 0.036326, validation loss 0.111272\n",
      "Iter 129, training loss 0.035962, validation loss 0.110773\n",
      "Iter 130, training loss 0.035525, validation loss 0.110268\n",
      "Iter 131, training loss 0.035087, validation loss 0.109780\n",
      "Iter 132, training loss 0.034707, validation loss 0.109335\n",
      "Iter 133, training loss 0.034323, validation loss 0.108899\n",
      "Iter 134, training loss 0.033922, validation loss 0.108460\n",
      "Iter 135, training loss 0.033536, validation loss 0.107991\n",
      "Iter 136, training loss 0.033154, validation loss 0.107471\n",
      "Iter 137, training loss 0.032777, validation loss 0.106886\n",
      "Iter 138, training loss 0.032415, validation loss 0.106325\n",
      "Iter 139, training loss 0.032055, validation loss 0.105837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 140, training loss 0.031678, validation loss 0.105417\n",
      "Iter 141, training loss 0.031321, validation loss 0.105057\n",
      "Iter 142, training loss 0.030983, validation loss 0.104681\n",
      "Iter 143, training loss 0.030619, validation loss 0.104151\n",
      "Iter 144, training loss 0.030266, validation loss 0.103550\n",
      "Iter 145, training loss 0.029936, validation loss 0.103017\n",
      "Iter 146, training loss 0.029587, validation loss 0.102605\n",
      "Iter 147, training loss 0.029241, validation loss 0.102310\n",
      "Iter 148, training loss 0.028918, validation loss 0.101996\n",
      "Iter 149, training loss 0.028585, validation loss 0.101578\n",
      "Iter 150, training loss 0.028263, validation loss 0.101100\n",
      "Iter 151, training loss 0.027956, validation loss 0.100708\n",
      "Iter 152, training loss 0.027649, validation loss 0.100381\n",
      "Iter 153, training loss 0.027343, validation loss 0.100042\n",
      "Iter 154, training loss 0.027037, validation loss 0.099595\n",
      "Iter 155, training loss 0.026746, validation loss 0.099078\n",
      "Iter 156, training loss 0.026456, validation loss 0.098574\n",
      "Iter 157, training loss 0.026168, validation loss 0.098126\n",
      "Iter 158, training loss 0.025889, validation loss 0.097722\n",
      "Iter 159, training loss 0.025606, validation loss 0.097297\n",
      "Iter 160, training loss 0.025333, validation loss 0.096889\n",
      "Iter 161, training loss 0.025069, validation loss 0.096511\n",
      "Iter 162, training loss 0.024803, validation loss 0.096143\n",
      "Iter 163, training loss 0.024545, validation loss 0.095726\n",
      "Iter 164, training loss 0.024292, validation loss 0.095283\n",
      "Iter 165, training loss 0.024044, validation loss 0.094950\n",
      "Iter 166, training loss 0.023799, validation loss 0.094646\n",
      "Iter 167, training loss 0.023553, validation loss 0.094267\n",
      "Iter 168, training loss 0.023316, validation loss 0.093862\n",
      "Iter 169, training loss 0.023088, validation loss 0.093523\n",
      "Iter 170, training loss 0.022855, validation loss 0.093225\n",
      "Iter 171, training loss 0.022629, validation loss 0.092941\n",
      "Iter 172, training loss 0.022409, validation loss 0.092610\n",
      "Iter 173, training loss 0.022189, validation loss 0.092306\n",
      "Iter 174, training loss 0.021975, validation loss 0.092010\n",
      "Iter 175, training loss 0.021759, validation loss 0.091690\n",
      "Iter 176, training loss 0.021548, validation loss 0.091362\n",
      "Iter 177, training loss 0.021342, validation loss 0.091064\n",
      "Iter 178, training loss 0.021136, validation loss 0.090731\n",
      "Iter 179, training loss 0.020933, validation loss 0.090405\n",
      "Iter 180, training loss 0.020734, validation loss 0.090131\n",
      "Iter 181, training loss 0.020538, validation loss 0.089804\n",
      "Iter 182, training loss 0.020343, validation loss 0.089518\n",
      "Iter 183, training loss 0.020153, validation loss 0.089268\n",
      "Iter 184, training loss 0.019967, validation loss 0.088969\n",
      "Iter 185, training loss 0.019783, validation loss 0.088704\n",
      "Iter 186, training loss 0.019601, validation loss 0.088507\n",
      "Iter 187, training loss 0.019421, validation loss 0.088238\n",
      "Iter 188, training loss 0.019242, validation loss 0.087976\n",
      "Iter 189, training loss 0.019062, validation loss 0.087739\n",
      "Iter 190, training loss 0.018884, validation loss 0.087531\n",
      "Iter 191, training loss 0.018712, validation loss 0.087282\n",
      "Iter 192, training loss 0.018538, validation loss 0.087062\n",
      "Iter 193, training loss 0.018368, validation loss 0.086880\n",
      "Iter 194, training loss 0.018200, validation loss 0.086627\n",
      "Iter 195, training loss 0.018033, validation loss 0.086379\n",
      "Iter 196, training loss 0.017869, validation loss 0.086172\n",
      "Iter 197, training loss 0.017709, validation loss 0.085880\n",
      "Iter 198, training loss 0.017548, validation loss 0.085721\n",
      "Iter 199, training loss 0.017387, validation loss 0.085492\n",
      "Iter 200, training loss 0.017229, validation loss 0.085395\n",
      "Iter 201, training loss 0.017076, validation loss 0.085194\n",
      "Iter 202, training loss 0.016928, validation loss 0.085106\n",
      "Iter 203, training loss 0.016794, validation loss 0.084794\n",
      "Iter 204, training loss 0.016660, validation loss 0.084902\n",
      "Iter 205, training loss 0.016511, validation loss 0.084432\n",
      "Iter 206, training loss 0.016341, validation loss 0.084478\n",
      "Iter 207, training loss 0.016200, validation loss 0.084314\n",
      "Iter 208, training loss 0.016081, validation loss 0.083993\n",
      "Iter 209, training loss 0.015950, validation loss 0.084160\n",
      "Iter 210, training loss 0.015789, validation loss 0.083802\n",
      "Iter 211, training loss 0.015666, validation loss 0.083646\n",
      "Iter 212, training loss 0.015556, validation loss 0.083805\n",
      "Iter 213, training loss 0.015403, validation loss 0.083395\n",
      "Iter 214, training loss 0.015265, validation loss 0.083312\n",
      "Iter 215, training loss 0.015154, validation loss 0.083339\n",
      "Iter 216, training loss 0.015045, validation loss 0.083028\n",
      "Iter 217, training loss 0.014903, validation loss 0.083106\n",
      "Iter 218, training loss 0.014770, validation loss 0.082878\n",
      "Iter 219, training loss 0.014653, validation loss 0.082783\n",
      "Iter 220, training loss 0.014542, validation loss 0.082813\n",
      "Iter 221, training loss 0.014424, validation loss 0.082517\n",
      "Iter 222, training loss 0.014298, validation loss 0.082553\n",
      "Iter 223, training loss 0.014183, validation loss 0.082461\n",
      "Iter 224, training loss 0.014073, validation loss 0.082279\n",
      "Iter 225, training loss 0.013965, validation loss 0.082279\n",
      "Iter 226, training loss 0.013852, validation loss 0.082110\n",
      "Iter 227, training loss 0.013738, validation loss 0.082037\n",
      "Iter 228, training loss 0.013631, validation loss 0.081856\n",
      "Iter 229, training loss 0.013523, validation loss 0.081751\n",
      "Iter 230, training loss 0.013422, validation loss 0.081705\n",
      "Iter 231, training loss 0.013322, validation loss 0.081449\n",
      "Iter 232, training loss 0.013215, validation loss 0.081465\n",
      "Iter 233, training loss 0.013112, validation loss 0.081346\n",
      "Iter 234, training loss 0.013006, validation loss 0.081213\n",
      "Iter 235, training loss 0.012912, validation loss 0.081135\n",
      "Iter 236, training loss 0.012823, validation loss 0.080960\n",
      "Iter 237, training loss 0.012731, validation loss 0.081006\n",
      "Iter 238, training loss 0.012640, validation loss 0.080699\n",
      "Iter 239, training loss 0.012531, validation loss 0.080801\n",
      "Iter 240, training loss 0.012431, validation loss 0.080685\n",
      "Iter 241, training loss 0.012335, validation loss 0.080574\n",
      "Iter 242, training loss 0.012252, validation loss 0.080572\n",
      "Iter 243, training loss 0.012170, validation loss 0.080398\n",
      "Iter 244, training loss 0.012082, validation loss 0.080497\n",
      "Iter 245, training loss 0.011993, validation loss 0.080210\n",
      "Iter 246, training loss 0.011891, validation loss 0.080318\n",
      "Iter 247, training loss 0.011802, validation loss 0.080237\n",
      "Iter 248, training loss 0.011713, validation loss 0.080030\n",
      "Iter 249, training loss 0.011638, validation loss 0.080038\n",
      "Iter 250, training loss 0.011565, validation loss 0.079877\n",
      "Iter 251, training loss 0.011475, validation loss 0.079968\n",
      "Iter 252, training loss 0.011390, validation loss 0.079665\n",
      "Iter 253, training loss 0.011298, validation loss 0.079777\n",
      "Iter 254, training loss 0.011215, validation loss 0.079644\n",
      "Iter 255, training loss 0.011130, validation loss 0.079548\n",
      "Iter 256, training loss 0.011049, validation loss 0.079430\n",
      "Iter 257, training loss 0.010971, validation loss 0.079420\n",
      "Iter 258, training loss 0.010894, validation loss 0.079342\n",
      "Iter 259, training loss 0.010815, validation loss 0.079287\n",
      "Iter 260, training loss 0.010741, validation loss 0.079251\n",
      "Iter 261, training loss 0.010665, validation loss 0.079149\n",
      "Iter 262, training loss 0.010593, validation loss 0.079150\n",
      "Iter 263, training loss 0.010528, validation loss 0.079032\n",
      "Iter 264, training loss 0.010482, validation loss 0.079152\n",
      "Iter 265, training loss 0.010483, validation loss 0.078934\n",
      "Iter 266, training loss 0.010507, validation loss 0.079383\n",
      "Iter 267, training loss 0.010399, validation loss 0.078841\n",
      "Iter 268, training loss 0.010199, validation loss 0.078906\n",
      "Iter 269, training loss 0.010121, validation loss 0.078789\n",
      "Iter 270, training loss 0.010137, validation loss 0.078600\n",
      "Iter 271, training loss 0.010055, validation loss 0.078802\n",
      "Iter 272, training loss 0.009923, validation loss 0.078467\n",
      "Iter 273, training loss 0.009873, validation loss 0.078414\n",
      "Iter 274, training loss 0.009847, validation loss 0.078586\n",
      "Iter 275, training loss 0.009765, validation loss 0.078317\n",
      "Iter 276, training loss 0.009675, validation loss 0.078319\n",
      "Iter 277, training loss 0.009643, validation loss 0.078376\n",
      "Iter 278, training loss 0.009600, validation loss 0.078167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 279, training loss 0.009501, validation loss 0.078207\n",
      "Iter 280, training loss 0.009440, validation loss 0.078174\n",
      "Iter 281, training loss 0.009405, validation loss 0.078059\n",
      "Iter 282, training loss 0.009343, validation loss 0.078138\n",
      "Iter 283, training loss 0.009263, validation loss 0.077993\n",
      "Iter 284, training loss 0.009213, validation loss 0.077911\n",
      "Iter 285, training loss 0.009171, validation loss 0.077936\n",
      "Iter 286, training loss 0.009111, validation loss 0.077728\n",
      "Iter 287, training loss 0.009040, validation loss 0.077748\n",
      "Iter 288, training loss 0.008987, validation loss 0.077703\n",
      "Iter 289, training loss 0.008945, validation loss 0.077586\n",
      "Iter 290, training loss 0.008891, validation loss 0.077645\n",
      "Iter 291, training loss 0.008828, validation loss 0.077520\n",
      "Iter 292, training loss 0.008772, validation loss 0.077478\n",
      "Iter 293, training loss 0.008726, validation loss 0.077467\n",
      "Iter 294, training loss 0.008680, validation loss 0.077363\n",
      "Iter 295, training loss 0.008628, validation loss 0.077369\n",
      "Iter 296, training loss 0.008572, validation loss 0.077251\n",
      "Iter 297, training loss 0.008519, validation loss 0.077209\n",
      "Iter 298, training loss 0.008471, validation loss 0.077179\n",
      "Iter 299, training loss 0.008424, validation loss 0.077083\n",
      "Iter 300, training loss 0.008381, validation loss 0.077064\n",
      "Iter 301, training loss 0.008331, validation loss 0.077014\n",
      "Iter 302, training loss 0.008280, validation loss 0.076947\n",
      "Iter 303, training loss 0.008231, validation loss 0.076838\n",
      "Iter 304, training loss 0.008183, validation loss 0.076828\n",
      "Iter 305, training loss 0.008139, validation loss 0.076779\n",
      "Iter 306, training loss 0.008100, validation loss 0.076682\n",
      "Iter 307, training loss 0.008063, validation loss 0.076668\n",
      "Iter 308, training loss 0.008028, validation loss 0.076571\n",
      "Iter 309, training loss 0.007982, validation loss 0.076593\n",
      "Iter 310, training loss 0.007934, validation loss 0.076464\n",
      "Iter 311, training loss 0.007878, validation loss 0.076486\n",
      "Iter 312, training loss 0.007827, validation loss 0.076419\n",
      "Iter 313, training loss 0.007778, validation loss 0.076344\n",
      "Iter 314, training loss 0.007738, validation loss 0.076276\n",
      "Iter 315, training loss 0.007704, validation loss 0.076212\n",
      "Iter 316, training loss 0.007665, validation loss 0.076214\n",
      "Iter 317, training loss 0.007631, validation loss 0.076069\n",
      "Iter 318, training loss 0.007589, validation loss 0.076074\n",
      "Iter 319, training loss 0.007547, validation loss 0.075988\n",
      "Iter 320, training loss 0.007495, validation loss 0.075953\n",
      "Iter 321, training loss 0.007453, validation loss 0.075839\n",
      "Iter 322, training loss 0.007406, validation loss 0.075893\n",
      "Iter 323, training loss 0.007373, validation loss 0.075864\n",
      "Iter 324, training loss 0.007355, validation loss 0.075700\n",
      "Iter 325, training loss 0.007327, validation loss 0.075754\n",
      "Iter 326, training loss 0.007298, validation loss 0.075718\n",
      "Iter 327, training loss 0.007246, validation loss 0.075663\n",
      "Iter 328, training loss 0.007195, validation loss 0.075447\n",
      "Iter 329, training loss 0.007134, validation loss 0.075504\n",
      "Iter 330, training loss 0.007110, validation loss 0.075521\n",
      "Iter 331, training loss 0.007083, validation loss 0.075327\n",
      "Iter 332, training loss 0.007052, validation loss 0.075359\n",
      "Iter 333, training loss 0.007023, validation loss 0.075403\n",
      "Iter 334, training loss 0.006968, validation loss 0.075287\n",
      "Iter 335, training loss 0.006937, validation loss 0.075080\n",
      "Iter 336, training loss 0.006880, validation loss 0.075186\n",
      "Iter 337, training loss 0.006877, validation loss 0.075247\n",
      "Iter 338, training loss 0.006849, validation loss 0.075033\n",
      "Iter 339, training loss 0.006825, validation loss 0.075046\n",
      "Iter 340, training loss 0.006775, validation loss 0.075159\n",
      "Iter 341, training loss 0.006716, validation loss 0.074983\n",
      "Iter 342, training loss 0.006690, validation loss 0.074769\n",
      "Iter 343, training loss 0.006647, validation loss 0.074888\n",
      "Iter 344, training loss 0.006654, validation loss 0.074968\n",
      "Iter 345, training loss 0.006634, validation loss 0.074758\n",
      "Iter 346, training loss 0.006608, validation loss 0.074706\n",
      "Iter 347, training loss 0.006527, validation loss 0.074785\n",
      "Iter 348, training loss 0.006479, validation loss 0.074656\n",
      "Iter 349, training loss 0.006458, validation loss 0.074461\n",
      "Iter 350, training loss 0.006427, validation loss 0.074511\n",
      "Iter 351, training loss 0.006411, validation loss 0.074624\n",
      "Iter 352, training loss 0.006371, validation loss 0.074499\n",
      "Iter 353, training loss 0.006340, validation loss 0.074313\n",
      "Iter 354, training loss 0.006291, validation loss 0.074307\n",
      "Iter 355, training loss 0.006259, validation loss 0.074325\n",
      "Iter 356, training loss 0.006226, validation loss 0.074235\n",
      "Iter 357, training loss 0.006198, validation loss 0.074219\n",
      "Iter 358, training loss 0.006168, validation loss 0.074240\n",
      "Iter 359, training loss 0.006138, validation loss 0.074160\n",
      "Iter 360, training loss 0.006108, validation loss 0.074035\n",
      "Iter 361, training loss 0.006076, validation loss 0.074042\n",
      "Iter 362, training loss 0.006045, validation loss 0.074035\n",
      "Iter 363, training loss 0.006016, validation loss 0.073965\n",
      "Iter 364, training loss 0.005986, validation loss 0.073881\n",
      "Iter 365, training loss 0.005958, validation loss 0.073854\n",
      "Iter 366, training loss 0.005929, validation loss 0.073809\n",
      "Iter 367, training loss 0.005901, validation loss 0.073791\n",
      "Iter 368, training loss 0.005874, validation loss 0.073781\n",
      "Iter 369, training loss 0.005848, validation loss 0.073741\n",
      "Iter 370, training loss 0.005826, validation loss 0.073644\n",
      "Iter 371, training loss 0.005813, validation loss 0.073656\n",
      "Iter 372, training loss 0.005809, validation loss 0.073628\n",
      "Iter 373, training loss 0.005813, validation loss 0.073654\n",
      "Iter 374, training loss 0.005801, validation loss 0.073593\n",
      "Iter 375, training loss 0.005761, validation loss 0.073596\n",
      "Iter 376, training loss 0.005697, validation loss 0.073483\n",
      "Iter 377, training loss 0.005644, validation loss 0.073424\n",
      "Iter 378, training loss 0.005608, validation loss 0.073414\n",
      "Iter 379, training loss 0.005590, validation loss 0.073407\n",
      "Iter 380, training loss 0.005584, validation loss 0.073370\n",
      "Iter 381, training loss 0.005581, validation loss 0.073289\n",
      "Iter 382, training loss 0.005560, validation loss 0.073356\n",
      "Iter 383, training loss 0.005519, validation loss 0.073288\n",
      "Iter 384, training loss 0.005480, validation loss 0.073252\n",
      "Iter 385, training loss 0.005438, validation loss 0.073219\n",
      "Iter 386, training loss 0.005413, validation loss 0.073230\n",
      "Iter 387, training loss 0.005386, validation loss 0.073119\n",
      "Iter 388, training loss 0.005371, validation loss 0.073054\n",
      "Iter 389, training loss 0.005351, validation loss 0.073141\n",
      "Iter 390, training loss 0.005332, validation loss 0.073085\n",
      "Iter 391, training loss 0.005313, validation loss 0.073026\n",
      "Iter 392, training loss 0.005285, validation loss 0.072966\n",
      "Iter 393, training loss 0.005258, validation loss 0.073014\n",
      "Iter 394, training loss 0.005225, validation loss 0.072937\n",
      "Iter 395, training loss 0.005202, validation loss 0.072909\n",
      "Iter 396, training loss 0.005173, validation loss 0.072923\n",
      "Iter 397, training loss 0.005151, validation loss 0.072896\n",
      "Iter 398, training loss 0.005129, validation loss 0.072796\n",
      "Iter 399, training loss 0.005104, validation loss 0.072795\n",
      "Iter 400, training loss 0.005086, validation loss 0.072881\n",
      "Iter 401, training loss 0.005064, validation loss 0.072838\n",
      "Iter 402, training loss 0.005046, validation loss 0.072818\n",
      "Iter 403, training loss 0.005030, validation loss 0.072741\n",
      "Iter 404, training loss 0.005022, validation loss 0.072771\n",
      "Iter 405, training loss 0.005020, validation loss 0.072689\n",
      "Iter 406, training loss 0.005022, validation loss 0.072827\n",
      "Iter 407, training loss 0.005007, validation loss 0.072690\n",
      "Iter 408, training loss 0.004975, validation loss 0.072699\n",
      "Iter 409, training loss 0.004927, validation loss 0.072574\n",
      "Iter 410, training loss 0.004884, validation loss 0.072652\n",
      "Iter 411, training loss 0.004848, validation loss 0.072535\n",
      "Iter 412, training loss 0.004827, validation loss 0.072453\n",
      "Iter 413, training loss 0.004815, validation loss 0.072495\n",
      "Iter 414, training loss 0.004808, validation loss 0.072442\n",
      "Iter 415, training loss 0.004802, validation loss 0.072487\n",
      "Iter 416, training loss 0.004780, validation loss 0.072415\n",
      "Iter 417, training loss 0.004758, validation loss 0.072498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 418, training loss 0.004718, validation loss 0.072320\n",
      "Iter 419, training loss 0.004694, validation loss 0.072316\n",
      "Iter 420, training loss 0.004666, validation loss 0.072379\n",
      "Iter 421, training loss 0.004648, validation loss 0.072380\n",
      "Iter 422, training loss 0.004631, validation loss 0.072280\n",
      "Iter 423, training loss 0.004616, validation loss 0.072180\n",
      "Iter 424, training loss 0.004605, validation loss 0.072272\n",
      "Iter 425, training loss 0.004587, validation loss 0.072203\n",
      "Iter 426, training loss 0.004576, validation loss 0.072255\n",
      "Iter 427, training loss 0.004552, validation loss 0.072167\n",
      "Iter 428, training loss 0.004537, validation loss 0.072219\n",
      "Iter 429, training loss 0.004511, validation loss 0.072052\n",
      "Iter 430, training loss 0.004494, validation loss 0.072110\n",
      "Iter 431, training loss 0.004468, validation loss 0.072108\n",
      "Iter 432, training loss 0.004451, validation loss 0.072118\n",
      "Iter 433, training loss 0.004425, validation loss 0.071950\n",
      "Iter 434, training loss 0.004409, validation loss 0.071916\n",
      "Iter 435, training loss 0.004394, validation loss 0.072040\n",
      "Iter 436, training loss 0.004381, validation loss 0.071978\n",
      "Iter 437, training loss 0.004372, validation loss 0.071977\n",
      "Iter 438, training loss 0.004355, validation loss 0.071871\n",
      "Iter 439, training loss 0.004347, validation loss 0.071994\n",
      "Iter 440, training loss 0.004335, validation loss 0.071830\n",
      "Iter 441, training loss 0.004323, validation loss 0.071941\n",
      "Iter 442, training loss 0.004299, validation loss 0.071837\n",
      "Iter 443, training loss 0.004276, validation loss 0.071951\n",
      "Iter 444, training loss 0.004249, validation loss 0.071733\n",
      "Iter 445, training loss 0.004223, validation loss 0.071749\n",
      "Iter 446, training loss 0.004203, validation loss 0.071736\n",
      "Iter 447, training loss 0.004184, validation loss 0.071752\n",
      "Iter 448, training loss 0.004167, validation loss 0.071652\n",
      "Iter 449, training loss 0.004150, validation loss 0.071594\n",
      "Iter 450, training loss 0.004133, validation loss 0.071629\n",
      "Iter 451, training loss 0.004117, validation loss 0.071635\n",
      "Iter 452, training loss 0.004101, validation loss 0.071627\n",
      "Iter 453, training loss 0.004084, validation loss 0.071560\n",
      "Iter 454, training loss 0.004069, validation loss 0.071553\n",
      "Iter 455, training loss 0.004055, validation loss 0.071489\n",
      "Iter 456, training loss 0.004042, validation loss 0.071521\n",
      "Iter 457, training loss 0.004030, validation loss 0.071421\n",
      "Iter 458, training loss 0.004025, validation loss 0.071511\n",
      "Iter 459, training loss 0.004030, validation loss 0.071401\n",
      "Iter 460, training loss 0.004046, validation loss 0.071608\n",
      "Iter 461, training loss 0.004041, validation loss 0.071404\n",
      "Iter 462, training loss 0.004033, validation loss 0.071581\n",
      "Iter 463, training loss 0.003997, validation loss 0.071331\n",
      "Iter 464, training loss 0.003967, validation loss 0.071469\n",
      "Iter 465, training loss 0.003919, validation loss 0.071263\n",
      "Iter 466, training loss 0.003889, validation loss 0.071287\n",
      "Iter 467, training loss 0.003879, validation loss 0.071361\n",
      "Iter 468, training loss 0.003881, validation loss 0.071284\n",
      "Iter 469, training loss 0.003895, validation loss 0.071411\n",
      "Iter 470, training loss 0.003875, validation loss 0.071235\n",
      "Iter 471, training loss 0.003848, validation loss 0.071353\n",
      "Iter 472, training loss 0.003816, validation loss 0.071121\n",
      "Iter 473, training loss 0.003797, validation loss 0.071185\n",
      "Iter 474, training loss 0.003774, validation loss 0.071165\n",
      "Iter 475, training loss 0.003759, validation loss 0.071152\n",
      "Iter 476, training loss 0.003751, validation loss 0.071137\n",
      "Iter 477, training loss 0.003748, validation loss 0.071040\n",
      "Iter 478, training loss 0.003746, validation loss 0.071240\n",
      "Iter 479, training loss 0.003731, validation loss 0.071114\n",
      "Iter 480, training loss 0.003710, validation loss 0.071187\n",
      "Iter 481, training loss 0.003683, validation loss 0.070999\n",
      "Iter 482, training loss 0.003667, validation loss 0.071032\n",
      "Iter 483, training loss 0.003646, validation loss 0.070986\n",
      "Iter 484, training loss 0.003639, validation loss 0.070970\n",
      "Iter 485, training loss 0.003628, validation loss 0.071026\n",
      "Iter 486, training loss 0.003630, validation loss 0.070874\n",
      "Iter 487, training loss 0.003630, validation loss 0.070999\n",
      "Iter 488, training loss 0.003625, validation loss 0.070890\n",
      "Iter 489, training loss 0.003615, validation loss 0.071113\n",
      "Iter 490, training loss 0.003589, validation loss 0.070828\n",
      "Iter 491, training loss 0.003575, validation loss 0.070906\n",
      "Iter 492, training loss 0.003541, validation loss 0.070865\n",
      "Iter 493, training loss 0.003532, validation loss 0.070923\n",
      "Iter 494, training loss 0.003509, validation loss 0.070756\n",
      "Iter 495, training loss 0.003504, validation loss 0.070672\n",
      "Iter 496, training loss 0.003500, validation loss 0.070843\n",
      "Iter 497, training loss 0.003487, validation loss 0.070746\n",
      "Iter 498, training loss 0.003490, validation loss 0.070817\n",
      "Iter 499, training loss 0.003458, validation loss 0.070696\n",
      "Iter 500, training loss 0.003452, validation loss 0.070776\n",
      "Iter 501, training loss 0.003418, validation loss 0.070673\n",
      "Iter 502, training loss 0.003416, validation loss 0.070671\n",
      "Iter 503, training loss 0.003403, validation loss 0.070766\n",
      "Iter 504, training loss 0.003404, validation loss 0.070601\n",
      "Iter 505, training loss 0.003412, validation loss 0.070713\n",
      "Iter 506, training loss 0.003416, validation loss 0.070568\n",
      "Iter 507, training loss 0.003423, validation loss 0.070881\n",
      "Iter 508, training loss 0.003393, validation loss 0.070522\n",
      "Iter 509, training loss 0.003375, validation loss 0.070632\n",
      "Iter 510, training loss 0.003326, validation loss 0.070550\n",
      "Iter 511, training loss 0.003321, validation loss 0.070650\n",
      "Iter 512, training loss 0.003300, validation loss 0.070572\n",
      "Iter 513, training loss 0.003317, validation loss 0.070387\n",
      "Iter 514, training loss 0.003319, validation loss 0.070647\n",
      "Iter 515, training loss 0.003325, validation loss 0.070421\n",
      "Iter 516, training loss 0.003314, validation loss 0.070641\n",
      "Iter 517, training loss 0.003273, validation loss 0.070452\n",
      "Iter 518, training loss 0.003242, validation loss 0.070537\n",
      "Iter 519, training loss 0.003221, validation loss 0.070441\n",
      "Iter 520, training loss 0.003245, validation loss 0.070318\n",
      "Iter 521, training loss 0.003244, validation loss 0.070644\n",
      "Iter 522, training loss 0.003241, validation loss 0.070455\n",
      "Iter 523, training loss 0.003200, validation loss 0.070530\n",
      "Iter 524, training loss 0.003173, validation loss 0.070282\n",
      "Iter 525, training loss 0.003155, validation loss 0.070317\n",
      "Iter 526, training loss 0.003149, validation loss 0.070413\n",
      "Iter 527, training loss 0.003152, validation loss 0.070305\n",
      "Iter 528, training loss 0.003140, validation loss 0.070470\n",
      "Iter 529, training loss 0.003130, validation loss 0.070269\n",
      "Iter 530, training loss 0.003107, validation loss 0.070316\n",
      "Iter 531, training loss 0.003091, validation loss 0.070200\n",
      "Iter 532, training loss 0.003074, validation loss 0.070293\n",
      "Iter 533, training loss 0.003069, validation loss 0.070342\n",
      "Iter 534, training loss 0.003061, validation loss 0.070201\n",
      "Iter 535, training loss 0.003057, validation loss 0.070276\n",
      "Iter 536, training loss 0.003049, validation loss 0.070170\n",
      "Iter 537, training loss 0.003039, validation loss 0.070313\n",
      "Iter 538, training loss 0.003029, validation loss 0.070125\n",
      "Iter 539, training loss 0.003014, validation loss 0.070220\n",
      "Iter 540, training loss 0.003001, validation loss 0.070098\n",
      "Iter 541, training loss 0.002986, validation loss 0.070172\n",
      "Iter 542, training loss 0.002974, validation loss 0.070142\n",
      "Iter 543, training loss 0.002964, validation loss 0.070175\n",
      "Iter 544, training loss 0.002954, validation loss 0.070144\n",
      "Iter 545, training loss 0.002946, validation loss 0.070078\n",
      "Iter 546, training loss 0.002937, validation loss 0.070158\n",
      "Iter 547, training loss 0.002931, validation loss 0.070081\n",
      "Iter 548, training loss 0.002923, validation loss 0.070140\n",
      "Iter 549, training loss 0.002917, validation loss 0.069986\n",
      "Iter 550, training loss 0.002912, validation loss 0.070119\n",
      "Iter 551, training loss 0.002907, validation loss 0.069963\n",
      "Iter 552, training loss 0.002903, validation loss 0.070148\n",
      "Iter 553, training loss 0.002896, validation loss 0.069959\n",
      "Iter 554, training loss 0.002890, validation loss 0.070163\n",
      "Iter 555, training loss 0.002881, validation loss 0.069931\n",
      "Iter 556, training loss 0.002873, validation loss 0.070156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 557, training loss 0.002861, validation loss 0.069969\n",
      "Iter 558, training loss 0.002850, validation loss 0.070148\n",
      "Iter 559, training loss 0.002832, validation loss 0.069906\n",
      "Iter 560, training loss 0.002818, validation loss 0.070027\n",
      "Iter 561, training loss 0.002805, validation loss 0.069923\n",
      "Iter 562, training loss 0.002794, validation loss 0.070025\n",
      "Iter 563, training loss 0.002783, validation loss 0.069891\n",
      "Iter 564, training loss 0.002772, validation loss 0.069928\n",
      "Iter 565, training loss 0.002763, validation loss 0.069871\n",
      "Iter 566, training loss 0.002753, validation loss 0.069941\n",
      "Iter 567, training loss 0.002745, validation loss 0.069898\n",
      "Iter 568, training loss 0.002735, validation loss 0.069901\n",
      "Iter 569, training loss 0.002727, validation loss 0.069814\n",
      "Iter 570, training loss 0.002718, validation loss 0.069850\n",
      "Iter 571, training loss 0.002710, validation loss 0.069849\n",
      "Iter 572, training loss 0.002701, validation loss 0.069862\n",
      "Iter 573, training loss 0.002694, validation loss 0.069752\n",
      "Iter 574, training loss 0.002685, validation loss 0.069793\n",
      "Iter 575, training loss 0.002679, validation loss 0.069762\n",
      "Iter 576, training loss 0.002674, validation loss 0.069886\n",
      "Iter 577, training loss 0.002674, validation loss 0.069733\n",
      "Iter 578, training loss 0.002684, validation loss 0.069948\n",
      "Iter 579, training loss 0.002706, validation loss 0.069668\n",
      "Iter 580, training loss 0.002732, validation loss 0.070057\n",
      "Iter 581, training loss 0.002744, validation loss 0.069704\n",
      "Iter 582, training loss 0.002742, validation loss 0.070215\n",
      "Iter 583, training loss 0.002710, validation loss 0.069705\n",
      "Iter 584, training loss 0.002660, validation loss 0.069927\n",
      "Iter 585, training loss 0.002616, validation loss 0.069697\n",
      "Iter 586, training loss 0.002596, validation loss 0.069826\n",
      "Iter 587, training loss 0.002594, validation loss 0.069827\n",
      "Iter 588, training loss 0.002608, validation loss 0.069600\n",
      "Iter 589, training loss 0.002617, validation loss 0.069899\n",
      "Iter 590, training loss 0.002613, validation loss 0.069594\n",
      "Iter 591, training loss 0.002600, validation loss 0.069911\n",
      "Iter 592, training loss 0.002571, validation loss 0.069649\n",
      "Iter 593, training loss 0.002538, validation loss 0.069745\n",
      "Iter 594, training loss 0.002530, validation loss 0.069658\n",
      "Iter 595, training loss 0.002530, validation loss 0.069566\n",
      "Iter 596, training loss 0.002543, validation loss 0.069834\n",
      "Iter 597, training loss 0.002540, validation loss 0.069624\n",
      "Iter 598, training loss 0.002526, validation loss 0.069781\n",
      "Iter 599, training loss 0.002506, validation loss 0.069462\n",
      "Iter 600, training loss 0.002488, validation loss 0.069602\n",
      "Iter 601, training loss 0.002477, validation loss 0.069591\n",
      "Iter 602, training loss 0.002468, validation loss 0.069580\n",
      "Iter 603, training loss 0.002465, validation loss 0.069609\n",
      "Iter 604, training loss 0.002460, validation loss 0.069481\n",
      "Iter 605, training loss 0.002459, validation loss 0.069619\n",
      "Iter 606, training loss 0.002453, validation loss 0.069465\n",
      "Iter 607, training loss 0.002443, validation loss 0.069653\n",
      "Iter 608, training loss 0.002430, validation loss 0.069481\n",
      "Iter 609, training loss 0.002422, validation loss 0.069598\n",
      "Iter 610, training loss 0.002412, validation loss 0.069487\n",
      "Iter 611, training loss 0.002402, validation loss 0.069535\n",
      "Iter 612, training loss 0.002395, validation loss 0.069444\n",
      "Iter 613, training loss 0.002386, validation loss 0.069482\n",
      "Iter 614, training loss 0.002380, validation loss 0.069462\n",
      "Iter 615, training loss 0.002371, validation loss 0.069488\n",
      "Iter 616, training loss 0.002367, validation loss 0.069382\n",
      "Iter 617, training loss 0.002360, validation loss 0.069450\n",
      "Iter 618, training loss 0.002358, validation loss 0.069383\n",
      "Iter 619, training loss 0.002356, validation loss 0.069557\n",
      "Iter 620, training loss 0.002357, validation loss 0.069369\n",
      "Iter 621, training loss 0.002358, validation loss 0.069568\n",
      "Iter 622, training loss 0.002364, validation loss 0.069267\n",
      "Iter 623, training loss 0.002367, validation loss 0.069599\n",
      "Iter 624, training loss 0.002364, validation loss 0.069356\n",
      "Iter 625, training loss 0.002352, validation loss 0.069690\n",
      "Iter 626, training loss 0.002331, validation loss 0.069257\n",
      "Iter 627, training loss 0.002316, validation loss 0.069421\n",
      "Iter 628, training loss 0.002297, validation loss 0.069318\n",
      "Iter 629, training loss 0.002288, validation loss 0.069508\n",
      "Iter 630, training loss 0.002274, validation loss 0.069311\n",
      "Iter 631, training loss 0.002268, validation loss 0.069285\n",
      "Iter 632, training loss 0.002259, validation loss 0.069289\n",
      "Iter 633, training loss 0.002253, validation loss 0.069354\n",
      "Iter 634, training loss 0.002246, validation loss 0.069330\n",
      "Iter 635, training loss 0.002238, validation loss 0.069274\n",
      "Iter 636, training loss 0.002233, validation loss 0.069269\n",
      "Iter 637, training loss 0.002225, validation loss 0.069287\n",
      "Iter 638, training loss 0.002220, validation loss 0.069317\n",
      "Iter 639, training loss 0.002212, validation loss 0.069275\n",
      "Iter 640, training loss 0.002206, validation loss 0.069216\n",
      "Iter 641, training loss 0.002199, validation loss 0.069252\n",
      "Iter 642, training loss 0.002194, validation loss 0.069282\n",
      "Iter 643, training loss 0.002187, validation loss 0.069271\n",
      "Iter 644, training loss 0.002181, validation loss 0.069170\n",
      "Iter 645, training loss 0.002177, validation loss 0.069238\n",
      "Iter 646, training loss 0.002176, validation loss 0.069180\n",
      "Iter 647, training loss 0.002182, validation loss 0.069362\n",
      "Iter 648, training loss 0.002195, validation loss 0.069106\n",
      "Iter 649, training loss 0.002222, validation loss 0.069491\n",
      "Iter 650, training loss 0.002261, validation loss 0.069075\n",
      "Iter 651, training loss 0.002292, validation loss 0.069684\n",
      "Iter 652, training loss 0.002277, validation loss 0.069151\n",
      "Iter 653, training loss 0.002212, validation loss 0.069594\n",
      "Iter 654, training loss 0.002152, validation loss 0.069116\n",
      "Iter 655, training loss 0.002121, validation loss 0.069204\n",
      "Iter 656, training loss 0.002112, validation loss 0.069234\n",
      "Iter 657, training loss 0.002129, validation loss 0.069138\n",
      "Iter 658, training loss 0.002155, validation loss 0.069463\n",
      "Iter 659, training loss 0.002177, validation loss 0.069051\n",
      "Iter 660, training loss 0.002169, validation loss 0.069471\n",
      "Iter 661, training loss 0.002136, validation loss 0.069043\n",
      "Iter 662, training loss 0.002099, validation loss 0.069318\n",
      "Iter 663, training loss 0.002073, validation loss 0.069174\n",
      "Iter 664, training loss 0.002066, validation loss 0.069185\n",
      "Iter 665, training loss 0.002068, validation loss 0.069199\n",
      "Iter 666, training loss 0.002079, validation loss 0.068985\n",
      "Iter 667, training loss 0.002089, validation loss 0.069338\n",
      "Iter 668, training loss 0.002092, validation loss 0.069045\n",
      "Iter 669, training loss 0.002077, validation loss 0.069328\n",
      "Iter 670, training loss 0.002050, validation loss 0.068992\n",
      "Iter 671, training loss 0.002029, validation loss 0.069128\n",
      "Iter 672, training loss 0.002019, validation loss 0.069089\n",
      "Iter 673, training loss 0.002019, validation loss 0.069044\n",
      "Iter 674, training loss 0.002024, validation loss 0.069220\n",
      "Iter 675, training loss 0.002028, validation loss 0.068985\n",
      "Iter 676, training loss 0.002027, validation loss 0.069226\n",
      "Iter 677, training loss 0.002020, validation loss 0.068944\n",
      "Iter 678, training loss 0.002008, validation loss 0.069202\n",
      "Iter 679, training loss 0.001992, validation loss 0.068981\n",
      "Iter 680, training loss 0.001980, validation loss 0.069106\n",
      "Iter 681, training loss 0.001969, validation loss 0.069028\n",
      "Iter 682, training loss 0.001964, validation loss 0.069036\n",
      "Iter 683, training loss 0.001960, validation loss 0.069053\n",
      "Iter 684, training loss 0.001958, validation loss 0.068959\n",
      "Iter 685, training loss 0.001955, validation loss 0.069103\n",
      "Iter 686, training loss 0.001952, validation loss 0.068940\n",
      "Iter 687, training loss 0.001949, validation loss 0.069091\n",
      "Iter 688, training loss 0.001944, validation loss 0.068936\n",
      "Iter 689, training loss 0.001937, validation loss 0.069110\n",
      "Iter 690, training loss 0.001930, validation loss 0.068915\n",
      "Iter 691, training loss 0.001925, validation loss 0.069051\n",
      "Iter 692, training loss 0.001917, validation loss 0.068954\n",
      "Iter 693, training loss 0.001910, validation loss 0.069065\n",
      "Iter 694, training loss 0.001902, validation loss 0.068908\n",
      "Iter 695, training loss 0.001898, validation loss 0.068969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 696, training loss 0.001892, validation loss 0.068954\n",
      "Iter 697, training loss 0.001887, validation loss 0.069024\n",
      "Iter 698, training loss 0.001882, validation loss 0.068886\n",
      "Iter 699, training loss 0.001877, validation loss 0.068956\n",
      "Iter 700, training loss 0.001875, validation loss 0.068888\n",
      "Iter 701, training loss 0.001873, validation loss 0.069035\n",
      "Iter 702, training loss 0.001873, validation loss 0.068850\n",
      "Iter 703, training loss 0.001874, validation loss 0.069046\n",
      "Iter 704, training loss 0.001880, validation loss 0.068795\n",
      "Iter 705, training loss 0.001887, validation loss 0.069126\n",
      "Iter 706, training loss 0.001889, validation loss 0.068841\n",
      "Iter 707, training loss 0.001887, validation loss 0.069223\n",
      "Iter 708, training loss 0.001880, validation loss 0.068776\n",
      "Iter 709, training loss 0.001868, validation loss 0.069050\n",
      "Iter 710, training loss 0.001847, validation loss 0.068823\n",
      "Iter 711, training loss 0.001832, validation loss 0.069101\n",
      "Iter 712, training loss 0.001817, validation loss 0.068853\n",
      "Iter 713, training loss 0.001812, validation loss 0.068857\n",
      "Iter 714, training loss 0.001803, validation loss 0.068885\n",
      "Iter 715, training loss 0.001802, validation loss 0.068880\n",
      "Iter 716, training loss 0.001799, validation loss 0.068926\n",
      "Iter 717, training loss 0.001799, validation loss 0.068772\n",
      "Iter 718, training loss 0.001802, validation loss 0.069018\n",
      "Iter 719, training loss 0.001805, validation loss 0.068747\n",
      "Iter 720, training loss 0.001815, validation loss 0.069063\n",
      "Iter 721, training loss 0.001815, validation loss 0.068788\n",
      "Iter 722, training loss 0.001810, validation loss 0.069150\n",
      "Iter 723, training loss 0.001796, validation loss 0.068700\n",
      "Iter 724, training loss 0.001789, validation loss 0.068991\n",
      "Iter 725, training loss 0.001772, validation loss 0.068802\n",
      "Iter 726, training loss 0.001759, validation loss 0.069031\n",
      "Iter 727, training loss 0.001744, validation loss 0.068770\n",
      "Iter 728, training loss 0.001737, validation loss 0.068768\n",
      "Iter 729, training loss 0.001732, validation loss 0.068829\n",
      "Iter 730, training loss 0.001730, validation loss 0.068806\n",
      "Iter 731, training loss 0.001727, validation loss 0.068862\n",
      "Iter 732, training loss 0.001724, validation loss 0.068687\n",
      "Iter 733, training loss 0.001723, validation loss 0.068856\n",
      "Iter 734, training loss 0.001724, validation loss 0.068685\n",
      "Iter 735, training loss 0.001729, validation loss 0.068952\n",
      "Iter 736, training loss 0.001727, validation loss 0.068705\n",
      "Iter 737, training loss 0.001726, validation loss 0.068981\n",
      "Iter 738, training loss 0.001725, validation loss 0.068606\n",
      "Iter 739, training loss 0.001726, validation loss 0.068942\n",
      "Iter 740, training loss 0.001717, validation loss 0.068704\n",
      "Iter 741, training loss 0.001709, validation loss 0.069028\n",
      "Iter 742, training loss 0.001696, validation loss 0.068638\n",
      "Iter 743, training loss 0.001688, validation loss 0.068819\n",
      "Iter 744, training loss 0.001676, validation loss 0.068689\n",
      "Iter 745, training loss 0.001669, validation loss 0.068888\n",
      "Iter 746, training loss 0.001662, validation loss 0.068686\n",
      "Iter 747, training loss 0.001654, validation loss 0.068711\n",
      "Iter 748, training loss 0.001651, validation loss 0.068657\n",
      "Iter 749, training loss 0.001645, validation loss 0.068768\n",
      "Iter 750, training loss 0.001642, validation loss 0.068723\n",
      "Iter 751, training loss 0.001637, validation loss 0.068767\n",
      "Iter 752, training loss 0.001635, validation loss 0.068645\n",
      "Iter 753, training loss 0.001634, validation loss 0.068758\n",
      "Iter 754, training loss 0.001632, validation loss 0.068670\n",
      "Iter 755, training loss 0.001631, validation loss 0.068857\n",
      "Iter 756, training loss 0.001631, validation loss 0.068590\n",
      "Iter 757, training loss 0.001638, validation loss 0.068834\n",
      "Iter 758, training loss 0.001643, validation loss 0.068602\n",
      "Iter 759, training loss 0.001652, validation loss 0.068987\n",
      "Iter 760, training loss 0.001653, validation loss 0.068523\n",
      "Iter 761, training loss 0.001656, validation loss 0.068913\n",
      "Iter 762, training loss 0.001653, validation loss 0.068570\n",
      "Iter 763, training loss 0.001653, validation loss 0.069030\n",
      "Iter 764, training loss 0.001637, validation loss 0.068526\n",
      "Iter 765, training loss 0.001615, validation loss 0.068815\n",
      "Iter 766, training loss 0.001599, validation loss 0.068580\n",
      "Iter 767, training loss 0.001584, validation loss 0.068824\n",
      "Iter 768, training loss 0.001580, validation loss 0.068596\n",
      "Iter 769, training loss 0.001564, validation loss 0.068669\n",
      "Iter 770, training loss 0.001566, validation loss 0.068631\n",
      "Iter 771, training loss 0.001554, validation loss 0.068632\n",
      "Iter 772, training loss 0.001555, validation loss 0.068663\n",
      "Iter 773, training loss 0.001549, validation loss 0.068621\n",
      "Iter 774, training loss 0.001546, validation loss 0.068662\n",
      "Iter 775, training loss 0.001545, validation loss 0.068504\n",
      "Iter 776, training loss 0.001542, validation loss 0.068674\n",
      "Iter 777, training loss 0.001546, validation loss 0.068533\n",
      "Iter 778, training loss 0.001546, validation loss 0.068738\n",
      "Iter 779, training loss 0.001553, validation loss 0.068416\n",
      "Iter 780, training loss 0.001559, validation loss 0.068771\n",
      "Iter 781, training loss 0.001575, validation loss 0.068435\n",
      "Iter 782, training loss 0.001580, validation loss 0.068890\n",
      "Iter 783, training loss 0.001582, validation loss 0.068449\n",
      "Iter 784, training loss 0.001568, validation loss 0.068900\n",
      "Iter 785, training loss 0.001555, validation loss 0.068402\n",
      "Iter 786, training loss 0.001535, validation loss 0.068710\n",
      "Iter 787, training loss 0.001514, validation loss 0.068517\n",
      "Iter 788, training loss 0.001502, validation loss 0.068742\n",
      "Iter 789, training loss 0.001489, validation loss 0.068514\n",
      "Iter 790, training loss 0.001489, validation loss 0.068441\n",
      "Iter 791, training loss 0.001482, validation loss 0.068563\n",
      "Iter 792, training loss 0.001484, validation loss 0.068521\n",
      "Iter 793, training loss 0.001480, validation loss 0.068606\n",
      "Iter 794, training loss 0.001479, validation loss 0.068409\n",
      "Iter 795, training loss 0.001475, validation loss 0.068595\n",
      "Iter 796, training loss 0.001471, validation loss 0.068418\n",
      "Iter 797, training loss 0.001472, validation loss 0.068628\n",
      "Iter 798, training loss 0.001468, validation loss 0.068447\n",
      "Iter 799, training loss 0.001464, validation loss 0.068635\n",
      "Iter 800, training loss 0.001460, validation loss 0.068327\n",
      "Iter 801, training loss 0.001462, validation loss 0.068564\n",
      "Iter 802, training loss 0.001461, validation loss 0.068417\n",
      "Iter 803, training loss 0.001460, validation loss 0.068712\n",
      "Iter 804, training loss 0.001456, validation loss 0.068316\n",
      "Iter 805, training loss 0.001458, validation loss 0.068584\n",
      "Iter 806, training loss 0.001461, validation loss 0.068349\n",
      "Iter 807, training loss 0.001462, validation loss 0.068760\n",
      "Iter 808, training loss 0.001460, validation loss 0.068350\n",
      "Iter 809, training loss 0.001450, validation loss 0.068644\n",
      "Iter 810, training loss 0.001449, validation loss 0.068307\n",
      "Iter 811, training loss 0.001440, validation loss 0.068643\n",
      "Iter 812, training loss 0.001434, validation loss 0.068366\n",
      "Iter 813, training loss 0.001421, validation loss 0.068645\n",
      "Iter 814, training loss 0.001414, validation loss 0.068320\n",
      "Iter 815, training loss 0.001406, validation loss 0.068460\n",
      "Iter 816, training loss 0.001399, validation loss 0.068381\n",
      "Iter 817, training loss 0.001396, validation loss 0.068595\n",
      "Iter 818, training loss 0.001387, validation loss 0.068386\n",
      "Iter 819, training loss 0.001387, validation loss 0.068409\n",
      "Iter 820, training loss 0.001379, validation loss 0.068365\n",
      "Iter 821, training loss 0.001378, validation loss 0.068483\n",
      "Iter 822, training loss 0.001373, validation loss 0.068336\n",
      "Iter 823, training loss 0.001370, validation loss 0.068394\n",
      "Iter 824, training loss 0.001369, validation loss 0.068328\n",
      "Iter 825, training loss 0.001365, validation loss 0.068480\n",
      "Iter 826, training loss 0.001369, validation loss 0.068311\n",
      "Iter 827, training loss 0.001367, validation loss 0.068498\n",
      "Iter 828, training loss 0.001373, validation loss 0.068255\n",
      "Iter 829, training loss 0.001379, validation loss 0.068559\n",
      "Iter 830, training loss 0.001395, validation loss 0.068250\n",
      "Iter 831, training loss 0.001411, validation loss 0.068714\n",
      "Iter 832, training loss 0.001416, validation loss 0.068191\n",
      "Iter 833, training loss 0.001416, validation loss 0.068663\n",
      "Iter 834, training loss 0.001411, validation loss 0.068204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 835, training loss 0.001404, validation loss 0.068746\n",
      "Iter 836, training loss 0.001380, validation loss 0.068220\n",
      "Iter 837, training loss 0.001350, validation loss 0.068474\n",
      "Iter 838, training loss 0.001333, validation loss 0.068276\n",
      "Iter 839, training loss 0.001320, validation loss 0.068424\n",
      "Iter 840, training loss 0.001317, validation loss 0.068295\n",
      "Iter 841, training loss 0.001311, validation loss 0.068231\n",
      "Iter 842, training loss 0.001314, validation loss 0.068369\n",
      "Iter 843, training loss 0.001313, validation loss 0.068236\n",
      "Iter 844, training loss 0.001318, validation loss 0.068432\n",
      "Iter 845, training loss 0.001317, validation loss 0.068223\n",
      "Iter 846, training loss 0.001313, validation loss 0.068464\n",
      "Iter 847, training loss 0.001313, validation loss 0.068156\n",
      "Iter 848, training loss 0.001313, validation loss 0.068447\n",
      "Iter 849, training loss 0.001317, validation loss 0.068230\n",
      "Iter 850, training loss 0.001310, validation loss 0.068529\n",
      "Iter 851, training loss 0.001306, validation loss 0.068133\n",
      "Iter 852, training loss 0.001302, validation loss 0.068405\n",
      "Iter 853, training loss 0.001304, validation loss 0.068167\n",
      "Iter 854, training loss 0.001297, validation loss 0.068501\n",
      "Iter 855, training loss 0.001294, validation loss 0.068174\n",
      "Iter 856, training loss 0.001282, validation loss 0.068403\n",
      "Iter 857, training loss 0.001281, validation loss 0.068134\n",
      "Iter 858, training loss 0.001271, validation loss 0.068356\n",
      "Iter 859, training loss 0.001268, validation loss 0.068229\n",
      "Iter 860, training loss 0.001260, validation loss 0.068410\n",
      "Iter 861, training loss 0.001254, validation loss 0.068162\n",
      "Iter 862, training loss 0.001252, validation loss 0.068238\n",
      "Iter 863, training loss 0.001245, validation loss 0.068221\n",
      "Iter 864, training loss 0.001246, validation loss 0.068387\n",
      "Iter 865, training loss 0.001239, validation loss 0.068198\n",
      "Iter 866, training loss 0.001239, validation loss 0.068237\n",
      "Iter 867, training loss 0.001235, validation loss 0.068173\n",
      "Iter 868, training loss 0.001234, validation loss 0.068340\n",
      "Iter 869, training loss 0.001233, validation loss 0.068171\n",
      "Iter 870, training loss 0.001230, validation loss 0.068297\n",
      "Iter 871, training loss 0.001231, validation loss 0.068114\n",
      "Iter 872, training loss 0.001229, validation loss 0.068316\n",
      "Iter 873, training loss 0.001233, validation loss 0.068129\n",
      "Iter 874, training loss 0.001236, validation loss 0.068425\n",
      "Iter 875, training loss 0.001237, validation loss 0.068092\n",
      "Iter 876, training loss 0.001239, validation loss 0.068381\n",
      "Iter 877, training loss 0.001243, validation loss 0.068076\n",
      "Iter 878, training loss 0.001252, validation loss 0.068516\n",
      "Iter 879, training loss 0.001255, validation loss 0.068072\n",
      "Iter 880, training loss 0.001246, validation loss 0.068442\n",
      "Iter 881, training loss 0.001242, validation loss 0.068058\n",
      "Iter 882, training loss 0.001236, validation loss 0.068487\n",
      "Iter 883, training loss 0.001230, validation loss 0.068066\n",
      "Iter 884, training loss 0.001210, validation loss 0.068361\n",
      "Iter 885, training loss 0.001201, validation loss 0.068127\n",
      "Iter 886, training loss 0.001187, validation loss 0.068332\n",
      "Iter 887, training loss 0.001181, validation loss 0.068118\n",
      "Iter 888, training loss 0.001173, validation loss 0.068197\n",
      "Iter 889, training loss 0.001171, validation loss 0.068179\n",
      "Iter 890, training loss 0.001166, validation loss 0.068171\n",
      "Iter 891, training loss 0.001166, validation loss 0.068200\n",
      "Iter 892, training loss 0.001165, validation loss 0.068119\n",
      "Iter 893, training loss 0.001164, validation loss 0.068249\n",
      "Iter 894, training loss 0.001166, validation loss 0.068064\n",
      "Iter 895, training loss 0.001169, validation loss 0.068286\n",
      "Iter 896, training loss 0.001176, validation loss 0.068072\n",
      "Iter 897, training loss 0.001181, validation loss 0.068387\n",
      "Iter 898, training loss 0.001191, validation loss 0.068003\n",
      "Iter 899, training loss 0.001202, validation loss 0.068451\n",
      "Iter 900, training loss 0.001217, validation loss 0.068027\n",
      "Iter 901, training loss 0.001215, validation loss 0.068535\n",
      "Iter 902, training loss 0.001202, validation loss 0.068028\n",
      "Iter 903, training loss 0.001178, validation loss 0.068440\n",
      "Iter 904, training loss 0.001159, validation loss 0.068048\n",
      "Iter 905, training loss 0.001139, validation loss 0.068274\n",
      "Iter 906, training loss 0.001128, validation loss 0.068147\n",
      "Iter 907, training loss 0.001122, validation loss 0.068208\n",
      "Iter 908, training loss 0.001120, validation loss 0.068183\n",
      "Iter 909, training loss 0.001124, validation loss 0.068042\n",
      "Iter 910, training loss 0.001128, validation loss 0.068286\n",
      "Iter 911, training loss 0.001135, validation loss 0.068090\n",
      "Iter 912, training loss 0.001138, validation loss 0.068372\n",
      "Iter 913, training loss 0.001140, validation loss 0.068020\n",
      "Iter 914, training loss 0.001135, validation loss 0.068351\n",
      "Iter 915, training loss 0.001132, validation loss 0.068043\n",
      "Iter 916, training loss 0.001127, validation loss 0.068373\n",
      "Iter 917, training loss 0.001120, validation loss 0.068072\n",
      "Iter 918, training loss 0.001108, validation loss 0.068300\n",
      "Iter 919, training loss 0.001100, validation loss 0.068050\n",
      "Iter 920, training loss 0.001093, validation loss 0.068191\n",
      "Iter 921, training loss 0.001088, validation loss 0.068127\n",
      "Iter 922, training loss 0.001084, validation loss 0.068203\n",
      "Iter 923, training loss 0.001080, validation loss 0.068119\n",
      "Iter 924, training loss 0.001079, validation loss 0.068077\n",
      "Iter 925, training loss 0.001076, validation loss 0.068151\n",
      "Iter 926, training loss 0.001075, validation loss 0.068121\n",
      "Iter 927, training loss 0.001074, validation loss 0.068205\n",
      "Iter 928, training loss 0.001073, validation loss 0.068061\n",
      "Iter 929, training loss 0.001074, validation loss 0.068205\n",
      "Iter 930, training loss 0.001077, validation loss 0.068030\n",
      "Iter 931, training loss 0.001083, validation loss 0.068288\n",
      "Iter 932, training loss 0.001090, validation loss 0.068022\n",
      "Iter 933, training loss 0.001100, validation loss 0.068382\n",
      "Iter 934, training loss 0.001113, validation loss 0.067954\n",
      "Iter 935, training loss 0.001122, validation loss 0.068427\n",
      "Iter 936, training loss 0.001127, validation loss 0.067984\n",
      "Iter 937, training loss 0.001125, validation loss 0.068527\n",
      "Iter 938, training loss 0.001111, validation loss 0.067992\n",
      "Iter 939, training loss 0.001088, validation loss 0.068367\n",
      "Iter 940, training loss 0.001067, validation loss 0.068029\n",
      "Iter 941, training loss 0.001051, validation loss 0.068275\n",
      "Iter 942, training loss 0.001040, validation loss 0.068072\n",
      "Iter 943, training loss 0.001033, validation loss 0.068128\n",
      "Iter 944, training loss 0.001033, validation loss 0.068156\n",
      "Iter 945, training loss 0.001033, validation loss 0.068064\n",
      "Iter 946, training loss 0.001037, validation loss 0.068230\n",
      "Iter 947, training loss 0.001040, validation loss 0.068055\n",
      "Iter 948, training loss 0.001042, validation loss 0.068299\n",
      "Iter 949, training loss 0.001046, validation loss 0.067978\n",
      "Iter 950, training loss 0.001049, validation loss 0.068296\n",
      "Iter 951, training loss 0.001052, validation loss 0.068003\n",
      "Iter 952, training loss 0.001047, validation loss 0.068357\n",
      "Iter 953, training loss 0.001041, validation loss 0.067990\n",
      "Iter 954, training loss 0.001033, validation loss 0.068284\n",
      "Iter 955, training loss 0.001027, validation loss 0.068006\n",
      "Iter 956, training loss 0.001019, validation loss 0.068257\n",
      "Iter 957, training loss 0.001013, validation loss 0.068034\n",
      "Iter 958, training loss 0.001006, validation loss 0.068204\n",
      "Iter 959, training loss 0.001001, validation loss 0.068033\n",
      "Iter 960, training loss 0.000997, validation loss 0.068130\n",
      "Iter 961, training loss 0.000993, validation loss 0.068086\n",
      "Iter 962, training loss 0.000990, validation loss 0.068148\n",
      "Iter 963, training loss 0.000987, validation loss 0.068063\n",
      "Iter 964, training loss 0.000985, validation loss 0.068057\n",
      "Iter 965, training loss 0.000983, validation loss 0.068087\n",
      "Iter 966, training loss 0.000981, validation loss 0.068108\n",
      "Iter 967, training loss 0.000978, validation loss 0.068115\n",
      "Iter 968, training loss 0.000976, validation loss 0.068061\n",
      "Iter 969, training loss 0.000975, validation loss 0.068112\n",
      "Iter 970, training loss 0.000974, validation loss 0.068037\n",
      "Iter 971, training loss 0.000974, validation loss 0.068147\n",
      "Iter 972, training loss 0.000976, validation loss 0.068025\n",
      "Iter 973, training loss 0.000981, validation loss 0.068229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 974, training loss 0.000993, validation loss 0.067947\n",
      "Iter 975, training loss 0.001012, validation loss 0.068326\n",
      "Iter 976, training loss 0.001037, validation loss 0.067906\n",
      "Iter 977, training loss 0.001062, validation loss 0.068503\n",
      "Iter 978, training loss 0.001081, validation loss 0.067898\n",
      "Iter 979, training loss 0.001085, validation loss 0.068571\n",
      "Iter 980, training loss 0.001064, validation loss 0.067919\n",
      "Iter 981, training loss 0.001026, validation loss 0.068472\n",
      "Iter 982, training loss 0.000986, validation loss 0.067991\n",
      "Iter 983, training loss 0.000956, validation loss 0.068225\n",
      "Iter 984, training loss 0.000943, validation loss 0.068071\n",
      "Iter 985, training loss 0.000944, validation loss 0.068044\n",
      "Iter 986, training loss 0.000955, validation loss 0.068240\n",
      "Iter 987, training loss 0.000973, validation loss 0.067965\n",
      "Iter 988, training loss 0.000991, validation loss 0.068373\n",
      "Iter 989, training loss 0.001001, validation loss 0.067911\n",
      "Iter 990, training loss 0.000996, validation loss 0.068384\n",
      "Iter 991, training loss 0.000981, validation loss 0.067931\n",
      "Iter 992, training loss 0.000960, validation loss 0.068318\n",
      "Iter 993, training loss 0.000940, validation loss 0.068010\n",
      "Iter 994, training loss 0.000926, validation loss 0.068151\n",
      "Iter 995, training loss 0.000920, validation loss 0.068077\n",
      "Iter 996, training loss 0.000922, validation loss 0.068016\n",
      "Iter 997, training loss 0.000929, validation loss 0.068202\n",
      "Iter 998, training loss 0.000938, validation loss 0.067968\n",
      "Iter 999, training loss 0.000946, validation loss 0.068292\n",
      "Iter 1000, training loss 0.000949, validation loss 0.067932\n",
      "Iter 1001, training loss 0.000947, validation loss 0.068294\n",
      "Iter 1002, training loss 0.000940, validation loss 0.067943\n",
      "Iter 1003, training loss 0.000928, validation loss 0.068256\n",
      "Iter 1004, training loss 0.000916, validation loss 0.067986\n",
      "Iter 1005, training loss 0.000906, validation loss 0.068153\n",
      "Iter 1006, training loss 0.000899, validation loss 0.068033\n",
      "Iter 1007, training loss 0.000896, validation loss 0.068051\n",
      "Iter 1008, training loss 0.000895, validation loss 0.068095\n",
      "Iter 1009, training loss 0.000896, validation loss 0.068008\n",
      "Iter 1010, training loss 0.000898, validation loss 0.068159\n",
      "Iter 1011, training loss 0.000902, validation loss 0.067966\n",
      "Iter 1012, training loss 0.000906, validation loss 0.068211\n",
      "Iter 1013, training loss 0.000909, validation loss 0.067950\n",
      "Iter 1014, training loss 0.000912, validation loss 0.068274\n",
      "Iter 1015, training loss 0.000913, validation loss 0.067945\n",
      "Iter 1016, training loss 0.000910, validation loss 0.068268\n",
      "Iter 1017, training loss 0.000906, validation loss 0.067938\n",
      "Iter 1018, training loss 0.000901, validation loss 0.068253\n",
      "Iter 1019, training loss 0.000894, validation loss 0.067951\n",
      "Iter 1020, training loss 0.000887, validation loss 0.068190\n",
      "Iter 1021, training loss 0.000880, validation loss 0.067982\n",
      "Iter 1022, training loss 0.000875, validation loss 0.068165\n",
      "Iter 1023, training loss 0.000869, validation loss 0.068014\n",
      "Iter 1024, training loss 0.000865, validation loss 0.068099\n",
      "Iter 1025, training loss 0.000862, validation loss 0.068065\n",
      "Iter 1026, training loss 0.000860, validation loss 0.068091\n",
      "Iter 1027, training loss 0.000858, validation loss 0.068058\n",
      "Iter 1028, training loss 0.000856, validation loss 0.068028\n",
      "Iter 1029, training loss 0.000855, validation loss 0.068082\n",
      "Iter 1030, training loss 0.000853, validation loss 0.068032\n",
      "Iter 1031, training loss 0.000853, validation loss 0.068096\n",
      "Iter 1032, training loss 0.000851, validation loss 0.068008\n",
      "Iter 1033, training loss 0.000852, validation loss 0.068144\n",
      "Iter 1034, training loss 0.000855, validation loss 0.067975\n",
      "Iter 1035, training loss 0.000861, validation loss 0.068196\n",
      "Iter 1036, training loss 0.000869, validation loss 0.067940\n",
      "Iter 1037, training loss 0.000880, validation loss 0.068297\n",
      "Iter 1038, training loss 0.000897, validation loss 0.067885\n",
      "Iter 1039, training loss 0.000918, validation loss 0.068419\n",
      "Iter 1040, training loss 0.000937, validation loss 0.067883\n",
      "Iter 1041, training loss 0.000939, validation loss 0.068495\n",
      "Iter 1042, training loss 0.000924, validation loss 0.067895\n",
      "Iter 1043, training loss 0.000900, validation loss 0.068426\n",
      "Iter 1044, training loss 0.000875, validation loss 0.067937\n",
      "Iter 1045, training loss 0.000848, validation loss 0.068231\n",
      "Iter 1046, training loss 0.000832, validation loss 0.068010\n",
      "Iter 1047, training loss 0.000823, validation loss 0.068111\n",
      "Iter 1048, training loss 0.000822, validation loss 0.068089\n",
      "Iter 1049, training loss 0.000825, validation loss 0.067959\n",
      "Iter 1050, training loss 0.000831, validation loss 0.068168\n",
      "Iter 1051, training loss 0.000839, validation loss 0.067952\n",
      "Iter 1052, training loss 0.000848, validation loss 0.068272\n",
      "Iter 1053, training loss 0.000854, validation loss 0.067898\n",
      "Iter 1054, training loss 0.000851, validation loss 0.068260\n",
      "Iter 1055, training loss 0.000846, validation loss 0.067891\n",
      "Iter 1056, training loss 0.000839, validation loss 0.068259\n",
      "Iter 1057, training loss 0.000832, validation loss 0.067947\n",
      "Iter 1058, training loss 0.000819, validation loss 0.068197\n",
      "Iter 1059, training loss 0.000812, validation loss 0.067964\n",
      "Iter 1060, training loss 0.000804, validation loss 0.068114\n",
      "Iter 1061, training loss 0.000801, validation loss 0.067995\n",
      "Iter 1062, training loss 0.000797, validation loss 0.068056\n",
      "Iter 1063, training loss 0.000795, validation loss 0.068033\n",
      "Iter 1064, training loss 0.000793, validation loss 0.068016\n",
      "Iter 1065, training loss 0.000793, validation loss 0.068049\n",
      "Iter 1066, training loss 0.000792, validation loss 0.067974\n",
      "Iter 1067, training loss 0.000792, validation loss 0.068089\n",
      "Iter 1068, training loss 0.000793, validation loss 0.067964\n",
      "Iter 1069, training loss 0.000794, validation loss 0.068132\n",
      "Iter 1070, training loss 0.000798, validation loss 0.067937\n",
      "Iter 1071, training loss 0.000801, validation loss 0.068194\n",
      "Iter 1072, training loss 0.000809, validation loss 0.067915\n",
      "Iter 1073, training loss 0.000818, validation loss 0.068277\n",
      "Iter 1074, training loss 0.000829, validation loss 0.067887\n",
      "Iter 1075, training loss 0.000836, validation loss 0.068344\n",
      "Iter 1076, training loss 0.000841, validation loss 0.067873\n",
      "Iter 1077, training loss 0.000840, validation loss 0.068380\n",
      "Iter 1078, training loss 0.000833, validation loss 0.067885\n",
      "Iter 1079, training loss 0.000818, validation loss 0.068318\n",
      "Iter 1080, training loss 0.000803, validation loss 0.067921\n",
      "Iter 1081, training loss 0.000788, validation loss 0.068218\n",
      "Iter 1082, training loss 0.000777, validation loss 0.067931\n",
      "Iter 1083, training loss 0.000768, validation loss 0.068094\n",
      "Iter 1084, training loss 0.000763, validation loss 0.068008\n",
      "Iter 1085, training loss 0.000759, validation loss 0.068030\n",
      "Iter 1086, training loss 0.000758, validation loss 0.068015\n",
      "Iter 1087, training loss 0.000758, validation loss 0.067969\n",
      "Iter 1088, training loss 0.000760, validation loss 0.068115\n",
      "Iter 1089, training loss 0.000762, validation loss 0.067948\n",
      "Iter 1090, training loss 0.000766, validation loss 0.068140\n",
      "Iter 1091, training loss 0.000768, validation loss 0.067922\n",
      "Iter 1092, training loss 0.000772, validation loss 0.068220\n",
      "Iter 1093, training loss 0.000777, validation loss 0.067900\n",
      "Iter 1094, training loss 0.000784, validation loss 0.068262\n",
      "Iter 1095, training loss 0.000788, validation loss 0.067891\n",
      "Iter 1096, training loss 0.000789, validation loss 0.068302\n",
      "Iter 1097, training loss 0.000787, validation loss 0.067875\n",
      "Iter 1098, training loss 0.000784, validation loss 0.068289\n",
      "Iter 1099, training loss 0.000779, validation loss 0.067895\n",
      "Iter 1100, training loss 0.000771, validation loss 0.068247\n",
      "Iter 1101, training loss 0.000761, validation loss 0.067895\n",
      "Iter 1102, training loss 0.000752, validation loss 0.068187\n",
      "Iter 1103, training loss 0.000745, validation loss 0.067940\n",
      "Iter 1104, training loss 0.000739, validation loss 0.068118\n",
      "Iter 1105, training loss 0.000734, validation loss 0.067947\n",
      "Iter 1106, training loss 0.000729, validation loss 0.068066\n",
      "Iter 1107, training loss 0.000726, validation loss 0.067985\n",
      "Iter 1108, training loss 0.000724, validation loss 0.068016\n",
      "Iter 1109, training loss 0.000722, validation loss 0.068022\n",
      "Iter 1110, training loss 0.000721, validation loss 0.067994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1111, training loss 0.000720, validation loss 0.068042\n",
      "Iter 1112, training loss 0.000720, validation loss 0.067958\n",
      "Iter 1113, training loss 0.000720, validation loss 0.068088\n",
      "Iter 1114, training loss 0.000721, validation loss 0.067952\n",
      "Iter 1115, training loss 0.000723, validation loss 0.068113\n",
      "Iter 1116, training loss 0.000726, validation loss 0.067909\n",
      "Iter 1117, training loss 0.000733, validation loss 0.068191\n",
      "Iter 1118, training loss 0.000742, validation loss 0.067883\n",
      "Iter 1119, training loss 0.000756, validation loss 0.068271\n",
      "Iter 1120, training loss 0.000769, validation loss 0.067834\n",
      "Iter 1121, training loss 0.000784, validation loss 0.068377\n",
      "Iter 1122, training loss 0.000793, validation loss 0.067842\n",
      "Iter 1123, training loss 0.000797, validation loss 0.068436\n",
      "Iter 1124, training loss 0.000786, validation loss 0.067861\n",
      "Iter 1125, training loss 0.000767, validation loss 0.068376\n",
      "Iter 1126, training loss 0.000742, validation loss 0.067894\n",
      "Iter 1127, training loss 0.000722, validation loss 0.068224\n",
      "Iter 1128, training loss 0.000705, validation loss 0.067969\n",
      "Iter 1129, training loss 0.000696, validation loss 0.068102\n",
      "Iter 1130, training loss 0.000692, validation loss 0.068038\n",
      "Iter 1131, training loss 0.000695, validation loss 0.067946\n",
      "Iter 1132, training loss 0.000701, validation loss 0.068136\n",
      "Iter 1133, training loss 0.000709, validation loss 0.067924\n",
      "Iter 1134, training loss 0.000718, validation loss 0.068237\n",
      "Iter 1135, training loss 0.000725, validation loss 0.067872\n",
      "Iter 1136, training loss 0.000729, validation loss 0.068278\n",
      "Iter 1137, training loss 0.000727, validation loss 0.067872\n",
      "Iter 1138, training loss 0.000724, validation loss 0.068290\n",
      "Iter 1139, training loss 0.000715, validation loss 0.067915\n",
      "Iter 1140, training loss 0.000704, validation loss 0.068240\n",
      "Iter 1141, training loss 0.000693, validation loss 0.067925\n",
      "Iter 1142, training loss 0.000685, validation loss 0.068129\n",
      "Iter 1143, training loss 0.000678, validation loss 0.067977\n",
      "Iter 1144, training loss 0.000674, validation loss 0.068067\n",
      "Iter 1145, training loss 0.000671, validation loss 0.067985\n",
      "Iter 1146, training loss 0.000670, validation loss 0.067970\n",
      "Iter 1147, training loss 0.000670, validation loss 0.068047\n",
      "Iter 1148, training loss 0.000671, validation loss 0.067974\n",
      "Iter 1149, training loss 0.000672, validation loss 0.068111\n",
      "Iter 1150, training loss 0.000674, validation loss 0.067936\n",
      "Iter 1151, training loss 0.000677, validation loss 0.068150\n",
      "Iter 1152, training loss 0.000681, validation loss 0.067902\n",
      "Iter 1153, training loss 0.000686, validation loss 0.068203\n",
      "Iter 1154, training loss 0.000689, validation loss 0.067905\n",
      "Iter 1155, training loss 0.000693, validation loss 0.068267\n",
      "Iter 1156, training loss 0.000695, validation loss 0.067880\n",
      "Iter 1157, training loss 0.000699, validation loss 0.068269\n",
      "Iter 1158, training loss 0.000699, validation loss 0.067874\n",
      "Iter 1159, training loss 0.000699, validation loss 0.068302\n",
      "Iter 1160, training loss 0.000695, validation loss 0.067879\n",
      "Iter 1161, training loss 0.000690, validation loss 0.068258\n",
      "Iter 1162, training loss 0.000682, validation loss 0.067888\n",
      "Iter 1163, training loss 0.000674, validation loss 0.068224\n",
      "Iter 1164, training loss 0.000666, validation loss 0.067917\n",
      "Iter 1165, training loss 0.000658, validation loss 0.068158\n",
      "Iter 1166, training loss 0.000652, validation loss 0.067958\n",
      "Iter 1167, training loss 0.000647, validation loss 0.068095\n",
      "Iter 1168, training loss 0.000643, validation loss 0.067966\n",
      "Iter 1169, training loss 0.000641, validation loss 0.068044\n",
      "Iter 1170, training loss 0.000639, validation loss 0.068025\n",
      "Iter 1171, training loss 0.000637, validation loss 0.068017\n",
      "Iter 1172, training loss 0.000637, validation loss 0.068047\n",
      "Iter 1173, training loss 0.000636, validation loss 0.067984\n",
      "Iter 1174, training loss 0.000637, validation loss 0.068094\n",
      "Iter 1175, training loss 0.000637, validation loss 0.067963\n",
      "Iter 1176, training loss 0.000640, validation loss 0.068134\n",
      "Iter 1177, training loss 0.000643, validation loss 0.067938\n",
      "Iter 1178, training loss 0.000647, validation loss 0.068190\n",
      "Iter 1179, training loss 0.000654, validation loss 0.067902\n",
      "Iter 1180, training loss 0.000664, validation loss 0.068271\n",
      "Iter 1181, training loss 0.000675, validation loss 0.067884\n",
      "Iter 1182, training loss 0.000688, validation loss 0.068359\n",
      "Iter 1183, training loss 0.000697, validation loss 0.067866\n",
      "Iter 1184, training loss 0.000705, validation loss 0.068430\n",
      "Iter 1185, training loss 0.000705, validation loss 0.067875\n",
      "Iter 1186, training loss 0.000698, validation loss 0.068419\n",
      "Iter 1187, training loss 0.000682, validation loss 0.067901\n",
      "Iter 1188, training loss 0.000664, validation loss 0.068348\n",
      "Iter 1189, training loss 0.000645, validation loss 0.067937\n",
      "Iter 1190, training loss 0.000629, validation loss 0.068189\n",
      "Iter 1191, training loss 0.000618, validation loss 0.067997\n",
      "Iter 1192, training loss 0.000613, validation loss 0.068090\n",
      "Iter 1193, training loss 0.000611, validation loss 0.068059\n",
      "Iter 1194, training loss 0.000612, validation loss 0.067984\n",
      "Iter 1195, training loss 0.000616, validation loss 0.068152\n",
      "Iter 1196, training loss 0.000621, validation loss 0.067963\n",
      "Iter 1197, training loss 0.000627, validation loss 0.068219\n",
      "Iter 1198, training loss 0.000632, validation loss 0.067918\n",
      "Iter 1199, training loss 0.000635, validation loss 0.068269\n",
      "Iter 1200, training loss 0.000637, validation loss 0.067915\n",
      "Iter 1201, training loss 0.000639, validation loss 0.068286\n",
      "Iter 1202, training loss 0.000636, validation loss 0.067921\n",
      "Iter 1203, training loss 0.000630, validation loss 0.068280\n",
      "Iter 1204, training loss 0.000624, validation loss 0.067925\n",
      "Iter 1205, training loss 0.000617, validation loss 0.068214\n",
      "Iter 1206, training loss 0.000611, validation loss 0.067955\n",
      "Iter 1207, training loss 0.000604, validation loss 0.068182\n",
      "Iter 1208, training loss 0.000599, validation loss 0.067980\n",
      "Iter 1209, training loss 0.000596, validation loss 0.068106\n",
      "Iter 1210, training loss 0.000593, validation loss 0.068005\n",
      "Iter 1211, training loss 0.000590, validation loss 0.068091\n",
      "Iter 1212, training loss 0.000588, validation loss 0.068031\n",
      "Iter 1213, training loss 0.000587, validation loss 0.068055\n",
      "Iter 1214, training loss 0.000586, validation loss 0.068064\n",
      "Iter 1215, training loss 0.000585, validation loss 0.068062\n",
      "Iter 1216, training loss 0.000583, validation loss 0.068075\n",
      "Iter 1217, training loss 0.000583, validation loss 0.068048\n",
      "Iter 1218, training loss 0.000582, validation loss 0.068117\n",
      "Iter 1219, training loss 0.000582, validation loss 0.068027\n",
      "Iter 1220, training loss 0.000584, validation loss 0.068136\n",
      "Iter 1221, training loss 0.000586, validation loss 0.067982\n",
      "Iter 1222, training loss 0.000592, validation loss 0.068214\n",
      "Iter 1223, training loss 0.000602, validation loss 0.067940\n",
      "Iter 1224, training loss 0.000617, validation loss 0.068307\n",
      "Iter 1225, training loss 0.000635, validation loss 0.067890\n",
      "Iter 1226, training loss 0.000659, validation loss 0.068457\n",
      "Iter 1227, training loss 0.000680, validation loss 0.067884\n",
      "Iter 1228, training loss 0.000696, validation loss 0.068573\n",
      "Iter 1229, training loss 0.000695, validation loss 0.067901\n",
      "Iter 1230, training loss 0.000680, validation loss 0.068554\n",
      "Iter 1231, training loss 0.000647, validation loss 0.067927\n",
      "Iter 1232, training loss 0.000612, validation loss 0.068374\n",
      "Iter 1233, training loss 0.000584, validation loss 0.068019\n",
      "Iter 1234, training loss 0.000568, validation loss 0.068194\n",
      "Iter 1235, training loss 0.000563, validation loss 0.068110\n",
      "Iter 1236, training loss 0.000566, validation loss 0.068038\n",
      "Iter 1237, training loss 0.000575, validation loss 0.068239\n",
      "Iter 1238, training loss 0.000587, validation loss 0.067968\n",
      "Iter 1239, training loss 0.000600, validation loss 0.068321\n",
      "Iter 1240, training loss 0.000607, validation loss 0.067915\n",
      "Iter 1241, training loss 0.000609, validation loss 0.068373\n",
      "Iter 1242, training loss 0.000605, validation loss 0.067939\n",
      "Iter 1243, training loss 0.000596, validation loss 0.068352\n",
      "Iter 1244, training loss 0.000582, validation loss 0.067982\n",
      "Iter 1245, training loss 0.000568, validation loss 0.068262\n",
      "Iter 1246, training loss 0.000557, validation loss 0.068050\n",
      "Iter 1247, training loss 0.000551, validation loss 0.068160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1248, training loss 0.000549, validation loss 0.068111\n",
      "Iter 1249, training loss 0.000549, validation loss 0.068079\n",
      "Iter 1250, training loss 0.000550, validation loss 0.068156\n",
      "Iter 1251, training loss 0.000553, validation loss 0.068026\n",
      "Iter 1252, training loss 0.000558, validation loss 0.068244\n",
      "Iter 1253, training loss 0.000562, validation loss 0.068009\n",
      "Iter 1254, training loss 0.000566, validation loss 0.068285\n",
      "Iter 1255, training loss 0.000569, validation loss 0.067969\n",
      "Iter 1256, training loss 0.000573, validation loss 0.068327\n",
      "Iter 1257, training loss 0.000574, validation loss 0.067986\n",
      "Iter 1258, training loss 0.000572, validation loss 0.068340\n",
      "Iter 1259, training loss 0.000567, validation loss 0.067991\n",
      "Iter 1260, training loss 0.000562, validation loss 0.068316\n",
      "Iter 1261, training loss 0.000556, validation loss 0.068009\n",
      "Iter 1262, training loss 0.000552, validation loss 0.068271\n",
      "Iter 1263, training loss 0.000547, validation loss 0.068034\n",
      "Iter 1264, training loss 0.000543, validation loss 0.068244\n",
      "Iter 1265, training loss 0.000540, validation loss 0.068034\n",
      "Iter 1266, training loss 0.000537, validation loss 0.068188\n",
      "Iter 1267, training loss 0.000534, validation loss 0.068062\n",
      "Iter 1268, training loss 0.000533, validation loss 0.068203\n",
      "Iter 1269, training loss 0.000531, validation loss 0.068056\n",
      "Iter 1270, training loss 0.000530, validation loss 0.068170\n",
      "Iter 1271, training loss 0.000529, validation loss 0.068077\n",
      "Iter 1272, training loss 0.000529, validation loss 0.068218\n",
      "Iter 1273, training loss 0.000529, validation loss 0.068068\n",
      "Iter 1274, training loss 0.000530, validation loss 0.068226\n",
      "Iter 1275, training loss 0.000532, validation loss 0.068052\n",
      "Iter 1276, training loss 0.000535, validation loss 0.068278\n",
      "Iter 1277, training loss 0.000540, validation loss 0.068034\n",
      "Iter 1278, training loss 0.000547, validation loss 0.068350\n",
      "Iter 1279, training loss 0.000556, validation loss 0.067994\n",
      "Iter 1280, training loss 0.000567, validation loss 0.068401\n",
      "Iter 1281, training loss 0.000578, validation loss 0.067972\n",
      "Iter 1282, training loss 0.000589, validation loss 0.068518\n",
      "Iter 1283, training loss 0.000591, validation loss 0.067990\n",
      "Iter 1284, training loss 0.000589, validation loss 0.068503\n",
      "Iter 1285, training loss 0.000580, validation loss 0.067987\n",
      "Iter 1286, training loss 0.000571, validation loss 0.068487\n",
      "Iter 1287, training loss 0.000555, validation loss 0.068028\n",
      "Iter 1288, training loss 0.000538, validation loss 0.068372\n",
      "Iter 1289, training loss 0.000525, validation loss 0.068087\n",
      "Iter 1290, training loss 0.000516, validation loss 0.068287\n",
      "Iter 1291, training loss 0.000510, validation loss 0.068111\n",
      "Iter 1292, training loss 0.000506, validation loss 0.068172\n",
      "Iter 1293, training loss 0.000505, validation loss 0.068193\n",
      "Iter 1294, training loss 0.000505, validation loss 0.068155\n",
      "Iter 1295, training loss 0.000507, validation loss 0.068235\n",
      "Iter 1296, training loss 0.000509, validation loss 0.068090\n",
      "Iter 1297, training loss 0.000512, validation loss 0.068286\n",
      "Iter 1298, training loss 0.000516, validation loss 0.068066\n",
      "Iter 1299, training loss 0.000522, validation loss 0.068350\n",
      "Iter 1300, training loss 0.000527, validation loss 0.068059\n",
      "Iter 1301, training loss 0.000531, validation loss 0.068405\n",
      "Iter 1302, training loss 0.000533, validation loss 0.068024\n",
      "Iter 1303, training loss 0.000537, validation loss 0.068426\n",
      "Iter 1304, training loss 0.000539, validation loss 0.068048\n",
      "Iter 1305, training loss 0.000539, validation loss 0.068458\n",
      "Iter 1306, training loss 0.000534, validation loss 0.068033\n",
      "Iter 1307, training loss 0.000528, validation loss 0.068407\n",
      "Iter 1308, training loss 0.000522, validation loss 0.068056\n",
      "Iter 1309, training loss 0.000515, validation loss 0.068385\n",
      "Iter 1310, training loss 0.000509, validation loss 0.068094\n",
      "Iter 1311, training loss 0.000502, validation loss 0.068338\n",
      "Iter 1312, training loss 0.000497, validation loss 0.068112\n",
      "Iter 1313, training loss 0.000493, validation loss 0.068273\n",
      "Iter 1314, training loss 0.000490, validation loss 0.068151\n",
      "Iter 1315, training loss 0.000487, validation loss 0.068272\n",
      "Iter 1316, training loss 0.000485, validation loss 0.068156\n",
      "Iter 1317, training loss 0.000484, validation loss 0.068214\n",
      "Iter 1318, training loss 0.000482, validation loss 0.068179\n",
      "Iter 1319, training loss 0.000482, validation loss 0.068253\n",
      "Iter 1320, training loss 0.000481, validation loss 0.068191\n",
      "Iter 1321, training loss 0.000480, validation loss 0.068227\n",
      "Iter 1322, training loss 0.000479, validation loss 0.068186\n",
      "Iter 1323, training loss 0.000478, validation loss 0.068245\n",
      "Iter 1324, training loss 0.000478, validation loss 0.068184\n",
      "Iter 1325, training loss 0.000477, validation loss 0.068262\n",
      "Iter 1326, training loss 0.000478, validation loss 0.068173\n",
      "Iter 1327, training loss 0.000479, validation loss 0.068289\n",
      "Iter 1328, training loss 0.000482, validation loss 0.068124\n",
      "Iter 1329, training loss 0.000489, validation loss 0.068352\n",
      "Iter 1330, training loss 0.000498, validation loss 0.068085\n",
      "Iter 1331, training loss 0.000512, validation loss 0.068455\n",
      "Iter 1332, training loss 0.000531, validation loss 0.068041\n",
      "Iter 1333, training loss 0.000557, validation loss 0.068609\n",
      "Iter 1334, training loss 0.000581, validation loss 0.068026\n",
      "Iter 1335, training loss 0.000600, validation loss 0.068723\n",
      "Iter 1336, training loss 0.000608, validation loss 0.068028\n",
      "Iter 1337, training loss 0.000601, validation loss 0.068757\n",
      "Iter 1338, training loss 0.000575, validation loss 0.068063\n",
      "Iter 1339, training loss 0.000535, validation loss 0.068597\n",
      "Iter 1340, training loss 0.000498, validation loss 0.068125\n",
      "Iter 1341, training loss 0.000472, validation loss 0.068371\n",
      "Iter 1342, training loss 0.000462, validation loss 0.068233\n",
      "Iter 1343, training loss 0.000464, validation loss 0.068193\n",
      "Iter 1344, training loss 0.000475, validation loss 0.068375\n",
      "Iter 1345, training loss 0.000491, validation loss 0.068092\n",
      "Iter 1346, training loss 0.000508, validation loss 0.068482\n",
      "Iter 1347, training loss 0.000520, validation loss 0.068038\n",
      "Iter 1348, training loss 0.000523, validation loss 0.068542\n",
      "Iter 1349, training loss 0.000516, validation loss 0.068064\n",
      "Iter 1350, training loss 0.000501, validation loss 0.068503\n",
      "Iter 1351, training loss 0.000483, validation loss 0.068116\n",
      "Iter 1352, training loss 0.000467, validation loss 0.068380\n",
      "Iter 1353, training loss 0.000456, validation loss 0.068199\n",
      "Iter 1354, training loss 0.000452, validation loss 0.068265\n",
      "Iter 1355, training loss 0.000452, validation loss 0.068287\n",
      "Iter 1356, training loss 0.000456, validation loss 0.068173\n",
      "Iter 1357, training loss 0.000462, validation loss 0.068355\n",
      "Iter 1358, training loss 0.000468, validation loss 0.068114\n",
      "Iter 1359, training loss 0.000474, validation loss 0.068417\n",
      "Iter 1360, training loss 0.000477, validation loss 0.068096\n",
      "Iter 1361, training loss 0.000478, validation loss 0.068438\n",
      "Iter 1362, training loss 0.000475, validation loss 0.068096\n",
      "Iter 1363, training loss 0.000470, validation loss 0.068425\n",
      "Iter 1364, training loss 0.000464, validation loss 0.068127\n",
      "Iter 1365, training loss 0.000457, validation loss 0.068374\n",
      "Iter 1366, training loss 0.000451, validation loss 0.068160\n",
      "Iter 1367, training loss 0.000447, validation loss 0.068328\n",
      "Iter 1368, training loss 0.000443, validation loss 0.068202\n",
      "Iter 1369, training loss 0.000441, validation loss 0.068292\n",
      "Iter 1370, training loss 0.000439, validation loss 0.068234\n",
      "Iter 1371, training loss 0.000438, validation loss 0.068249\n",
      "Iter 1372, training loss 0.000437, validation loss 0.068252\n",
      "Iter 1373, training loss 0.000437, validation loss 0.068217\n",
      "Iter 1374, training loss 0.000437, validation loss 0.068292\n",
      "Iter 1375, training loss 0.000438, validation loss 0.068197\n",
      "Iter 1376, training loss 0.000439, validation loss 0.068328\n",
      "Iter 1377, training loss 0.000442, validation loss 0.068169\n",
      "Iter 1378, training loss 0.000445, validation loss 0.068379\n",
      "Iter 1379, training loss 0.000450, validation loss 0.068144\n",
      "Iter 1380, training loss 0.000457, validation loss 0.068451\n",
      "Iter 1381, training loss 0.000466, validation loss 0.068122\n",
      "Iter 1382, training loss 0.000476, validation loss 0.068523\n",
      "Iter 1383, training loss 0.000485, validation loss 0.068088\n",
      "Iter 1384, training loss 0.000495, validation loss 0.068588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1385, training loss 0.000501, validation loss 0.068093\n",
      "Iter 1386, training loss 0.000504, validation loss 0.068641\n",
      "Iter 1387, training loss 0.000499, validation loss 0.068100\n",
      "Iter 1388, training loss 0.000490, validation loss 0.068607\n",
      "Iter 1389, training loss 0.000475, validation loss 0.068132\n",
      "Iter 1390, training loss 0.000458, validation loss 0.068517\n",
      "Iter 1391, training loss 0.000443, validation loss 0.068174\n",
      "Iter 1392, training loss 0.000431, validation loss 0.068397\n",
      "Iter 1393, training loss 0.000424, validation loss 0.068232\n",
      "Iter 1394, training loss 0.000420, validation loss 0.068281\n",
      "Iter 1395, training loss 0.000420, validation loss 0.068282\n",
      "Iter 1396, training loss 0.000422, validation loss 0.068212\n",
      "Iter 1397, training loss 0.000425, validation loss 0.068356\n",
      "Iter 1398, training loss 0.000431, validation loss 0.068162\n",
      "Iter 1399, training loss 0.000437, validation loss 0.068423\n",
      "Iter 1400, training loss 0.000443, validation loss 0.068134\n",
      "Iter 1401, training loss 0.000449, validation loss 0.068486\n",
      "Iter 1402, training loss 0.000454, validation loss 0.068119\n",
      "Iter 1403, training loss 0.000458, validation loss 0.068533\n",
      "Iter 1404, training loss 0.000458, validation loss 0.068127\n",
      "Iter 1405, training loss 0.000455, validation loss 0.068530\n",
      "Iter 1406, training loss 0.000449, validation loss 0.068120\n",
      "Iter 1407, training loss 0.000442, validation loss 0.068481\n",
      "Iter 1408, training loss 0.000434, validation loss 0.068153\n",
      "Iter 1409, training loss 0.000426, validation loss 0.068424\n",
      "Iter 1410, training loss 0.000419, validation loss 0.068180\n",
      "Iter 1411, training loss 0.000413, validation loss 0.068346\n",
      "Iter 1412, training loss 0.000410, validation loss 0.068213\n",
      "Iter 1413, training loss 0.000407, validation loss 0.068285\n",
      "Iter 1414, training loss 0.000405, validation loss 0.068245\n",
      "Iter 1415, training loss 0.000405, validation loss 0.068248\n",
      "Iter 1416, training loss 0.000405, validation loss 0.068288\n",
      "Iter 1417, training loss 0.000405, validation loss 0.068220\n",
      "Iter 1418, training loss 0.000407, validation loss 0.068335\n",
      "Iter 1419, training loss 0.000409, validation loss 0.068186\n",
      "Iter 1420, training loss 0.000414, validation loss 0.068385\n",
      "Iter 1421, training loss 0.000419, validation loss 0.068140\n",
      "Iter 1422, training loss 0.000427, validation loss 0.068448\n",
      "Iter 1423, training loss 0.000437, validation loss 0.068112\n",
      "Iter 1424, training loss 0.000449, validation loss 0.068541\n",
      "Iter 1425, training loss 0.000461, validation loss 0.068087\n",
      "Iter 1426, training loss 0.000473, validation loss 0.068626\n",
      "Iter 1427, training loss 0.000482, validation loss 0.068090\n",
      "Iter 1428, training loss 0.000487, validation loss 0.068696\n",
      "Iter 1429, training loss 0.000484, validation loss 0.068110\n",
      "Iter 1430, training loss 0.000473, validation loss 0.068675\n",
      "Iter 1431, training loss 0.000456, validation loss 0.068146\n",
      "Iter 1432, training loss 0.000436, validation loss 0.068578\n",
      "Iter 1433, training loss 0.000417, validation loss 0.068193\n",
      "Iter 1434, training loss 0.000402, validation loss 0.068429\n",
      "Iter 1435, training loss 0.000393, validation loss 0.068265\n",
      "Iter 1436, training loss 0.000390, validation loss 0.068306\n",
      "Iter 1437, training loss 0.000391, validation loss 0.068332\n",
      "Iter 1438, training loss 0.000395, validation loss 0.068220\n",
      "Iter 1439, training loss 0.000401, validation loss 0.068414\n",
      "Iter 1440, training loss 0.000408, validation loss 0.068166\n",
      "Iter 1441, training loss 0.000416, validation loss 0.068488\n",
      "Iter 1442, training loss 0.000422, validation loss 0.068153\n",
      "Iter 1443, training loss 0.000426, validation loss 0.068552\n",
      "Iter 1444, training loss 0.000427, validation loss 0.068154\n",
      "Iter 1445, training loss 0.000426, validation loss 0.068559\n",
      "Iter 1446, training loss 0.000422, validation loss 0.068164\n",
      "Iter 1447, training loss 0.000416, validation loss 0.068532\n",
      "Iter 1448, training loss 0.000407, validation loss 0.068191\n",
      "Iter 1449, training loss 0.000398, validation loss 0.068472\n",
      "Iter 1450, training loss 0.000391, validation loss 0.068228\n",
      "Iter 1451, training loss 0.000386, validation loss 0.068392\n",
      "Iter 1452, training loss 0.000382, validation loss 0.068255\n",
      "Iter 1453, training loss 0.000380, validation loss 0.068342\n",
      "Iter 1454, training loss 0.000378, validation loss 0.068288\n",
      "Iter 1455, training loss 0.000377, validation loss 0.068296\n",
      "Iter 1456, training loss 0.000377, validation loss 0.068318\n",
      "Iter 1457, training loss 0.000377, validation loss 0.068277\n",
      "Iter 1458, training loss 0.000377, validation loss 0.068355\n",
      "Iter 1459, training loss 0.000379, validation loss 0.068242\n",
      "Iter 1460, training loss 0.000381, validation loss 0.068397\n",
      "Iter 1461, training loss 0.000384, validation loss 0.068216\n",
      "Iter 1462, training loss 0.000390, validation loss 0.068451\n",
      "Iter 1463, training loss 0.000396, validation loss 0.068166\n",
      "Iter 1464, training loss 0.000405, validation loss 0.068517\n",
      "Iter 1465, training loss 0.000416, validation loss 0.068142\n",
      "Iter 1466, training loss 0.000429, validation loss 0.068611\n",
      "Iter 1467, training loss 0.000441, validation loss 0.068126\n",
      "Iter 1468, training loss 0.000453, validation loss 0.068707\n",
      "Iter 1469, training loss 0.000460, validation loss 0.068138\n",
      "Iter 1470, training loss 0.000460, validation loss 0.068747\n",
      "Iter 1471, training loss 0.000452, validation loss 0.068162\n",
      "Iter 1472, training loss 0.000439, validation loss 0.068711\n",
      "Iter 1473, training loss 0.000419, validation loss 0.068200\n",
      "Iter 1474, training loss 0.000399, validation loss 0.068586\n",
      "Iter 1475, training loss 0.000382, validation loss 0.068259\n",
      "Iter 1476, training loss 0.000370, validation loss 0.068455\n",
      "Iter 1477, training loss 0.000364, validation loss 0.068319\n",
      "Iter 1478, training loss 0.000363, validation loss 0.068327\n",
      "Iter 1479, training loss 0.000364, validation loss 0.068400\n",
      "Iter 1480, training loss 0.000369, validation loss 0.068276\n",
      "Iter 1481, training loss 0.000374, validation loss 0.068464\n",
      "Iter 1482, training loss 0.000381, validation loss 0.068204\n",
      "Iter 1483, training loss 0.000388, validation loss 0.068531\n",
      "Iter 1484, training loss 0.000395, validation loss 0.068194\n",
      "Iter 1485, training loss 0.000400, validation loss 0.068593\n",
      "Iter 1486, training loss 0.000402, validation loss 0.068192\n",
      "Iter 1487, training loss 0.000400, validation loss 0.068612\n",
      "Iter 1488, training loss 0.000395, validation loss 0.068203\n",
      "Iter 1489, training loss 0.000389, validation loss 0.068575\n",
      "Iter 1490, training loss 0.000381, validation loss 0.068236\n",
      "Iter 1491, training loss 0.000373, validation loss 0.068530\n",
      "Iter 1492, training loss 0.000366, validation loss 0.068268\n",
      "Iter 1493, training loss 0.000360, validation loss 0.068441\n",
      "Iter 1494, training loss 0.000356, validation loss 0.068294\n",
      "Iter 1495, training loss 0.000354, validation loss 0.068396\n",
      "Iter 1496, training loss 0.000352, validation loss 0.068319\n",
      "Iter 1497, training loss 0.000351, validation loss 0.068344\n",
      "Iter 1498, training loss 0.000350, validation loss 0.068349\n",
      "Iter 1499, training loss 0.000350, validation loss 0.068335\n",
      "Iter 1500, training loss 0.000350, validation loss 0.068384\n",
      "Iter 1501, training loss 0.000350, validation loss 0.068313\n",
      "Iter 1502, training loss 0.000351, validation loss 0.068415\n",
      "Iter 1503, training loss 0.000352, validation loss 0.068284\n",
      "Iter 1504, training loss 0.000355, validation loss 0.068451\n",
      "Iter 1505, training loss 0.000359, validation loss 0.068255\n",
      "Iter 1506, training loss 0.000364, validation loss 0.068509\n",
      "Iter 1507, training loss 0.000371, validation loss 0.068206\n",
      "Iter 1508, training loss 0.000382, validation loss 0.068578\n",
      "Iter 1509, training loss 0.000397, validation loss 0.068183\n",
      "Iter 1510, training loss 0.000414, validation loss 0.068705\n",
      "Iter 1511, training loss 0.000431, validation loss 0.068168\n",
      "Iter 1512, training loss 0.000447, validation loss 0.068816\n",
      "Iter 1513, training loss 0.000457, validation loss 0.068181\n",
      "Iter 1514, training loss 0.000459, validation loss 0.068879\n",
      "Iter 1515, training loss 0.000447, validation loss 0.068208\n",
      "Iter 1516, training loss 0.000424, validation loss 0.068808\n",
      "Iter 1517, training loss 0.000396, validation loss 0.068262\n",
      "Iter 1518, training loss 0.000370, validation loss 0.068643\n",
      "Iter 1519, training loss 0.000350, validation loss 0.068326\n",
      "Iter 1520, training loss 0.000340, validation loss 0.068463\n",
      "Iter 1521, training loss 0.000338, validation loss 0.068425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1522, training loss 0.000342, validation loss 0.068323\n",
      "Iter 1523, training loss 0.000350, validation loss 0.068517\n",
      "Iter 1524, training loss 0.000361, validation loss 0.068265\n",
      "Iter 1525, training loss 0.000371, validation loss 0.068630\n",
      "Iter 1526, training loss 0.000380, validation loss 0.068230\n",
      "Iter 1527, training loss 0.000385, validation loss 0.068682\n",
      "Iter 1528, training loss 0.000385, validation loss 0.068246\n",
      "Iter 1529, training loss 0.000379, validation loss 0.068692\n",
      "Iter 1530, training loss 0.000368, validation loss 0.068280\n",
      "Iter 1531, training loss 0.000357, validation loss 0.068628\n",
      "Iter 1532, training loss 0.000346, validation loss 0.068336\n",
      "Iter 1533, training loss 0.000338, validation loss 0.068526\n",
      "Iter 1534, training loss 0.000333, validation loss 0.068367\n",
      "Iter 1535, training loss 0.000330, validation loss 0.068432\n",
      "Iter 1536, training loss 0.000329, validation loss 0.068432\n",
      "Iter 1537, training loss 0.000330, validation loss 0.068369\n",
      "Iter 1538, training loss 0.000332, validation loss 0.068478\n",
      "Iter 1539, training loss 0.000336, validation loss 0.068324\n",
      "Iter 1540, training loss 0.000340, validation loss 0.068537\n",
      "Iter 1541, training loss 0.000344, validation loss 0.068283\n",
      "Iter 1542, training loss 0.000349, validation loss 0.068583\n",
      "Iter 1543, training loss 0.000354, validation loss 0.068274\n",
      "Iter 1544, training loss 0.000359, validation loss 0.068641\n",
      "Iter 1545, training loss 0.000361, validation loss 0.068252\n",
      "Iter 1546, training loss 0.000362, validation loss 0.068649\n",
      "Iter 1547, training loss 0.000362, validation loss 0.068262\n",
      "Iter 1548, training loss 0.000362, validation loss 0.068661\n",
      "Iter 1549, training loss 0.000358, validation loss 0.068261\n",
      "Iter 1550, training loss 0.000352, validation loss 0.068628\n",
      "Iter 1551, training loss 0.000347, validation loss 0.068288\n",
      "Iter 1552, training loss 0.000342, validation loss 0.068589\n",
      "Iter 1553, training loss 0.000337, validation loss 0.068299\n",
      "Iter 1554, training loss 0.000332, validation loss 0.068549\n",
      "Iter 1555, training loss 0.000329, validation loss 0.068328\n",
      "Iter 1556, training loss 0.000325, validation loss 0.068503\n",
      "Iter 1557, training loss 0.000323, validation loss 0.068343\n",
      "Iter 1558, training loss 0.000321, validation loss 0.068486\n",
      "Iter 1559, training loss 0.000319, validation loss 0.068373\n",
      "Iter 1560, training loss 0.000318, validation loss 0.068454\n",
      "Iter 1561, training loss 0.000316, validation loss 0.068377\n",
      "Iter 1562, training loss 0.000316, validation loss 0.068449\n",
      "Iter 1563, training loss 0.000315, validation loss 0.068383\n",
      "Iter 1564, training loss 0.000315, validation loss 0.068433\n",
      "Iter 1565, training loss 0.000314, validation loss 0.068381\n",
      "Iter 1566, training loss 0.000314, validation loss 0.068454\n",
      "Iter 1567, training loss 0.000314, validation loss 0.068375\n",
      "Iter 1568, training loss 0.000314, validation loss 0.068462\n",
      "Iter 1569, training loss 0.000315, validation loss 0.068358\n",
      "Iter 1570, training loss 0.000316, validation loss 0.068506\n",
      "Iter 1571, training loss 0.000319, validation loss 0.068328\n",
      "Iter 1572, training loss 0.000325, validation loss 0.068557\n",
      "Iter 1573, training loss 0.000333, validation loss 0.068292\n",
      "Iter 1574, training loss 0.000346, validation loss 0.068659\n",
      "Iter 1575, training loss 0.000364, validation loss 0.068241\n",
      "Iter 1576, training loss 0.000389, validation loss 0.068802\n",
      "Iter 1577, training loss 0.000419, validation loss 0.068225\n",
      "Iter 1578, training loss 0.000452, validation loss 0.068993\n",
      "Iter 1579, training loss 0.000478, validation loss 0.068228\n",
      "Iter 1580, training loss 0.000488, validation loss 0.069102\n",
      "Iter 1581, training loss 0.000476, validation loss 0.068267\n",
      "Iter 1582, training loss 0.000436, validation loss 0.069011\n",
      "Iter 1583, training loss 0.000386, validation loss 0.068316\n",
      "Iter 1584, training loss 0.000341, validation loss 0.068752\n",
      "Iter 1585, training loss 0.000312, validation loss 0.068429\n",
      "Iter 1586, training loss 0.000303, validation loss 0.068488\n",
      "Iter 1587, training loss 0.000311, validation loss 0.068577\n",
      "Iter 1588, training loss 0.000329, validation loss 0.068329\n",
      "Iter 1589, training loss 0.000353, validation loss 0.068735\n",
      "Iter 1590, training loss 0.000374, validation loss 0.068255\n",
      "Iter 1591, training loss 0.000385, validation loss 0.068835\n",
      "Iter 1592, training loss 0.000384, validation loss 0.068269\n",
      "Iter 1593, training loss 0.000371, validation loss 0.068839\n",
      "Iter 1594, training loss 0.000350, validation loss 0.068336\n",
      "Iter 1595, training loss 0.000326, validation loss 0.068711\n",
      "Iter 1596, training loss 0.000307, validation loss 0.068438\n",
      "Iter 1597, training loss 0.000298, validation loss 0.068547\n",
      "Iter 1598, training loss 0.000299, validation loss 0.068552\n",
      "Iter 1599, training loss 0.000306, validation loss 0.068408\n",
      "Iter 1600, training loss 0.000316, validation loss 0.068654\n",
      "Iter 1601, training loss 0.000327, validation loss 0.068330\n",
      "Iter 1602, training loss 0.000337, validation loss 0.068717\n",
      "Iter 1603, training loss 0.000341, validation loss 0.068301\n",
      "Iter 1604, training loss 0.000336, validation loss 0.068725\n",
      "Iter 1605, training loss 0.000328, validation loss 0.068339\n",
      "Iter 1606, training loss 0.000317, validation loss 0.068680\n",
      "Iter 1607, training loss 0.000307, validation loss 0.068395\n",
      "Iter 1608, training loss 0.000298, validation loss 0.068584\n",
      "Iter 1609, training loss 0.000293, validation loss 0.068472\n",
      "Iter 1610, training loss 0.000291, validation loss 0.068493\n",
      "Iter 1611, training loss 0.000292, validation loss 0.068534\n",
      "Iter 1612, training loss 0.000295, validation loss 0.068421\n",
      "Iter 1613, training loss 0.000299, validation loss 0.068596\n",
      "Iter 1614, training loss 0.000303, validation loss 0.068371\n",
      "Iter 1615, training loss 0.000308, validation loss 0.068629\n",
      "Iter 1616, training loss 0.000311, validation loss 0.068348\n",
      "Iter 1617, training loss 0.000311, validation loss 0.068659\n",
      "Iter 1618, training loss 0.000310, validation loss 0.068349\n",
      "Iter 1619, training loss 0.000309, validation loss 0.068648\n",
      "Iter 1620, training loss 0.000306, validation loss 0.068359\n",
      "Iter 1621, training loss 0.000303, validation loss 0.068637\n",
      "Iter 1622, training loss 0.000299, validation loss 0.068384\n",
      "Iter 1623, training loss 0.000296, validation loss 0.068604\n",
      "Iter 1624, training loss 0.000292, validation loss 0.068405\n",
      "Iter 1625, training loss 0.000290, validation loss 0.068570\n",
      "Iter 1626, training loss 0.000287, validation loss 0.068426\n",
      "Iter 1627, training loss 0.000285, validation loss 0.068535\n",
      "Iter 1628, training loss 0.000284, validation loss 0.068446\n",
      "Iter 1629, training loss 0.000283, validation loss 0.068512\n",
      "Iter 1630, training loss 0.000282, validation loss 0.068451\n",
      "Iter 1631, training loss 0.000281, validation loss 0.068489\n",
      "Iter 1632, training loss 0.000281, validation loss 0.068469\n",
      "Iter 1633, training loss 0.000280, validation loss 0.068490\n",
      "Iter 1634, training loss 0.000280, validation loss 0.068490\n",
      "Iter 1635, training loss 0.000279, validation loss 0.068487\n",
      "Iter 1636, training loss 0.000279, validation loss 0.068507\n",
      "Iter 1637, training loss 0.000279, validation loss 0.068484\n",
      "Iter 1638, training loss 0.000278, validation loss 0.068525\n",
      "Iter 1639, training loss 0.000278, validation loss 0.068472\n",
      "Iter 1640, training loss 0.000279, validation loss 0.068544\n",
      "Iter 1641, training loss 0.000280, validation loss 0.068440\n",
      "Iter 1642, training loss 0.000282, validation loss 0.068583\n",
      "Iter 1643, training loss 0.000286, validation loss 0.068406\n",
      "Iter 1644, training loss 0.000293, validation loss 0.068652\n",
      "Iter 1645, training loss 0.000304, validation loss 0.068355\n",
      "Iter 1646, training loss 0.000320, validation loss 0.068773\n",
      "Iter 1647, training loss 0.000343, validation loss 0.068326\n",
      "Iter 1648, training loss 0.000371, validation loss 0.068937\n",
      "Iter 1649, training loss 0.000404, validation loss 0.068297\n",
      "Iter 1650, training loss 0.000434, validation loss 0.069108\n",
      "Iter 1651, training loss 0.000457, validation loss 0.068308\n",
      "Iter 1652, training loss 0.000459, validation loss 0.069183\n",
      "Iter 1653, training loss 0.000438, validation loss 0.068337\n",
      "Iter 1654, training loss 0.000393, validation loss 0.069060\n",
      "Iter 1655, training loss 0.000341, validation loss 0.068390\n",
      "Iter 1656, training loss 0.000298, validation loss 0.068773\n",
      "Iter 1657, training loss 0.000275, validation loss 0.068513\n",
      "Iter 1658, training loss 0.000271, validation loss 0.068534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1659, training loss 0.000282, validation loss 0.068658\n",
      "Iter 1660, training loss 0.000302, validation loss 0.068368\n",
      "Iter 1661, training loss 0.000325, validation loss 0.068816\n",
      "Iter 1662, training loss 0.000345, validation loss 0.068325\n",
      "Iter 1663, training loss 0.000355, validation loss 0.068908\n",
      "Iter 1664, training loss 0.000351, validation loss 0.068326\n",
      "Iter 1665, training loss 0.000335, validation loss 0.068880\n",
      "Iter 1666, training loss 0.000313, validation loss 0.068396\n",
      "Iter 1667, training loss 0.000290, validation loss 0.068747\n",
      "Iter 1668, training loss 0.000273, validation loss 0.068490\n",
      "Iter 1669, training loss 0.000265, validation loss 0.068588\n",
      "Iter 1670, training loss 0.000267, validation loss 0.068615\n",
      "Iter 1671, training loss 0.000275, validation loss 0.068447\n",
      "Iter 1672, training loss 0.000286, validation loss 0.068704\n",
      "Iter 1673, training loss 0.000297, validation loss 0.068377\n",
      "Iter 1674, training loss 0.000305, validation loss 0.068772\n",
      "Iter 1675, training loss 0.000307, validation loss 0.068349\n",
      "Iter 1676, training loss 0.000303, validation loss 0.068767\n",
      "Iter 1677, training loss 0.000295, validation loss 0.068391\n",
      "Iter 1678, training loss 0.000285, validation loss 0.068725\n",
      "Iter 1679, training loss 0.000274, validation loss 0.068435\n",
      "Iter 1680, training loss 0.000266, validation loss 0.068630\n",
      "Iter 1681, training loss 0.000261, validation loss 0.068519\n",
      "Iter 1682, training loss 0.000259, validation loss 0.068558\n",
      "Iter 1683, training loss 0.000260, validation loss 0.068569\n",
      "Iter 1684, training loss 0.000261, validation loss 0.068484\n",
      "Iter 1685, training loss 0.000264, validation loss 0.068622\n",
      "Iter 1686, training loss 0.000268, validation loss 0.068433\n",
      "Iter 1687, training loss 0.000271, validation loss 0.068653\n",
      "Iter 1688, training loss 0.000274, validation loss 0.068406\n",
      "Iter 1689, training loss 0.000275, validation loss 0.068681\n",
      "Iter 1690, training loss 0.000276, validation loss 0.068395\n",
      "Iter 1691, training loss 0.000277, validation loss 0.068693\n",
      "Iter 1692, training loss 0.000277, validation loss 0.068407\n",
      "Iter 1693, training loss 0.000276, validation loss 0.068709\n",
      "Iter 1694, training loss 0.000274, validation loss 0.068409\n",
      "Iter 1695, training loss 0.000273, validation loss 0.068696\n",
      "Iter 1696, training loss 0.000271, validation loss 0.068430\n",
      "Iter 1697, training loss 0.000268, validation loss 0.068688\n",
      "Iter 1698, training loss 0.000265, validation loss 0.068440\n",
      "Iter 1699, training loss 0.000263, validation loss 0.068652\n",
      "Iter 1700, training loss 0.000260, validation loss 0.068449\n",
      "Iter 1701, training loss 0.000258, validation loss 0.068616\n",
      "Iter 1702, training loss 0.000256, validation loss 0.068457\n",
      "Iter 1703, training loss 0.000254, validation loss 0.068595\n",
      "Iter 1704, training loss 0.000253, validation loss 0.068476\n",
      "Iter 1705, training loss 0.000252, validation loss 0.068584\n",
      "Iter 1706, training loss 0.000251, validation loss 0.068489\n",
      "Iter 1707, training loss 0.000251, validation loss 0.068590\n",
      "Iter 1708, training loss 0.000250, validation loss 0.068501\n",
      "Iter 1709, training loss 0.000250, validation loss 0.068587\n",
      "Iter 1710, training loss 0.000250, validation loss 0.068500\n",
      "Iter 1711, training loss 0.000249, validation loss 0.068596\n",
      "Iter 1712, training loss 0.000250, validation loss 0.068491\n",
      "Iter 1713, training loss 0.000250, validation loss 0.068602\n",
      "Iter 1714, training loss 0.000251, validation loss 0.068469\n",
      "Iter 1715, training loss 0.000253, validation loss 0.068636\n",
      "Iter 1716, training loss 0.000256, validation loss 0.068453\n",
      "Iter 1717, training loss 0.000261, validation loss 0.068693\n",
      "Iter 1718, training loss 0.000269, validation loss 0.068410\n",
      "Iter 1719, training loss 0.000281, validation loss 0.068780\n",
      "Iter 1720, training loss 0.000298, validation loss 0.068374\n",
      "Iter 1721, training loss 0.000321, validation loss 0.068916\n",
      "Iter 1722, training loss 0.000347, validation loss 0.068354\n",
      "Iter 1723, training loss 0.000376, validation loss 0.069083\n",
      "Iter 1724, training loss 0.000401, validation loss 0.068349\n",
      "Iter 1725, training loss 0.000415, validation loss 0.069182\n",
      "Iter 1726, training loss 0.000414, validation loss 0.068371\n",
      "Iter 1727, training loss 0.000390, validation loss 0.069165\n",
      "Iter 1728, training loss 0.000351, validation loss 0.068420\n",
      "Iter 1729, training loss 0.000306, validation loss 0.068961\n",
      "Iter 1730, training loss 0.000268, validation loss 0.068496\n",
      "Iter 1731, training loss 0.000246, validation loss 0.068707\n",
      "Iter 1732, training loss 0.000240, validation loss 0.068617\n",
      "Iter 1733, training loss 0.000246, validation loss 0.068524\n",
      "Iter 1734, training loss 0.000261, validation loss 0.068761\n",
      "Iter 1735, training loss 0.000280, validation loss 0.068415\n",
      "Iter 1736, training loss 0.000299, validation loss 0.068879\n",
      "Iter 1737, training loss 0.000312, validation loss 0.068381\n",
      "Iter 1738, training loss 0.000317, validation loss 0.068954\n",
      "Iter 1739, training loss 0.000312, validation loss 0.068415\n",
      "Iter 1740, training loss 0.000297, validation loss 0.068934\n",
      "Iter 1741, training loss 0.000277, validation loss 0.068474\n",
      "Iter 1742, training loss 0.000257, validation loss 0.068799\n",
      "Iter 1743, training loss 0.000242, validation loss 0.068556\n",
      "Iter 1744, training loss 0.000236, validation loss 0.068646\n",
      "Iter 1745, training loss 0.000236, validation loss 0.068655\n",
      "Iter 1746, training loss 0.000242, validation loss 0.068518\n",
      "Iter 1747, training loss 0.000250, validation loss 0.068729\n",
      "Iter 1748, training loss 0.000259, validation loss 0.068444\n",
      "Iter 1749, training loss 0.000266, validation loss 0.068796\n",
      "Iter 1750, training loss 0.000270, validation loss 0.068430\n",
      "Iter 1751, training loss 0.000271, validation loss 0.068822\n",
      "Iter 1752, training loss 0.000268, validation loss 0.068441\n",
      "Iter 1753, training loss 0.000262, validation loss 0.068807\n",
      "Iter 1754, training loss 0.000254, validation loss 0.068472\n",
      "Iter 1755, training loss 0.000246, validation loss 0.068743\n",
      "Iter 1756, training loss 0.000239, validation loss 0.068525\n",
      "Iter 1757, training loss 0.000234, validation loss 0.068674\n",
      "Iter 1758, training loss 0.000231, validation loss 0.068570\n",
      "Iter 1759, training loss 0.000229, validation loss 0.068600\n",
      "Iter 1760, training loss 0.000229, validation loss 0.068614\n",
      "Iter 1761, training loss 0.000230, validation loss 0.068548\n",
      "Iter 1762, training loss 0.000232, validation loss 0.068638\n",
      "Iter 1763, training loss 0.000234, validation loss 0.068496\n",
      "Iter 1764, training loss 0.000235, validation loss 0.068666\n",
      "Iter 1765, training loss 0.000237, validation loss 0.068475\n",
      "Iter 1766, training loss 0.000239, validation loss 0.068698\n",
      "Iter 1767, training loss 0.000241, validation loss 0.068472\n",
      "Iter 1768, training loss 0.000243, validation loss 0.068733\n",
      "Iter 1769, training loss 0.000245, validation loss 0.068461\n",
      "Iter 1770, training loss 0.000247, validation loss 0.068751\n",
      "Iter 1771, training loss 0.000249, validation loss 0.068447\n",
      "Iter 1772, training loss 0.000253, validation loss 0.068778\n",
      "Iter 1773, training loss 0.000255, validation loss 0.068432\n",
      "Iter 1774, training loss 0.000258, validation loss 0.068798\n",
      "Iter 1775, training loss 0.000260, validation loss 0.068427\n",
      "Iter 1776, training loss 0.000264, validation loss 0.068828\n",
      "Iter 1777, training loss 0.000268, validation loss 0.068414\n",
      "Iter 1778, training loss 0.000270, validation loss 0.068846\n",
      "Iter 1779, training loss 0.000271, validation loss 0.068412\n",
      "Iter 1780, training loss 0.000274, validation loss 0.068876\n",
      "Iter 1781, training loss 0.000274, validation loss 0.068422\n",
      "Iter 1782, training loss 0.000272, validation loss 0.068878\n",
      "Iter 1783, training loss 0.000267, validation loss 0.068436\n",
      "Iter 1784, training loss 0.000262, validation loss 0.068855\n",
      "Iter 1785, training loss 0.000255, validation loss 0.068446\n",
      "Iter 1786, training loss 0.000248, validation loss 0.068788\n",
      "Iter 1787, training loss 0.000240, validation loss 0.068464\n",
      "Iter 1788, training loss 0.000234, validation loss 0.068731\n",
      "Iter 1789, training loss 0.000229, validation loss 0.068496\n",
      "Iter 1790, training loss 0.000225, validation loss 0.068674\n",
      "Iter 1791, training loss 0.000221, validation loss 0.068535\n",
      "Iter 1792, training loss 0.000219, validation loss 0.068629\n",
      "Iter 1793, training loss 0.000218, validation loss 0.068567\n",
      "Iter 1794, training loss 0.000217, validation loss 0.068586\n",
      "Iter 1795, training loss 0.000217, validation loss 0.068605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1796, training loss 0.000217, validation loss 0.068563\n",
      "Iter 1797, training loss 0.000218, validation loss 0.068635\n",
      "Iter 1798, training loss 0.000218, validation loss 0.068541\n",
      "Iter 1799, training loss 0.000219, validation loss 0.068663\n",
      "Iter 1800, training loss 0.000221, validation loss 0.068517\n",
      "Iter 1801, training loss 0.000224, validation loss 0.068698\n",
      "Iter 1802, training loss 0.000227, validation loss 0.068488\n",
      "Iter 1803, training loss 0.000233, validation loss 0.068751\n",
      "Iter 1804, training loss 0.000240, validation loss 0.068454\n",
      "Iter 1805, training loss 0.000250, validation loss 0.068824\n",
      "Iter 1806, training loss 0.000264, validation loss 0.068422\n",
      "Iter 1807, training loss 0.000283, validation loss 0.068946\n",
      "Iter 1808, training loss 0.000307, validation loss 0.068401\n",
      "Iter 1809, training loss 0.000332, validation loss 0.069089\n",
      "Iter 1810, training loss 0.000357, validation loss 0.068397\n",
      "Iter 1811, training loss 0.000376, validation loss 0.069216\n",
      "Iter 1812, training loss 0.000384, validation loss 0.068416\n",
      "Iter 1813, training loss 0.000371, validation loss 0.069227\n",
      "Iter 1814, training loss 0.000341, validation loss 0.068445\n",
      "Iter 1815, training loss 0.000299, validation loss 0.069068\n",
      "Iter 1816, training loss 0.000258, validation loss 0.068508\n",
      "Iter 1817, training loss 0.000227, validation loss 0.068826\n",
      "Iter 1818, training loss 0.000211, validation loss 0.068625\n",
      "Iter 1819, training loss 0.000211, validation loss 0.068607\n",
      "Iter 1820, training loss 0.000221, validation loss 0.068750\n",
      "Iter 1821, training loss 0.000237, validation loss 0.068473\n",
      "Iter 1822, training loss 0.000255, validation loss 0.068880\n",
      "Iter 1823, training loss 0.000271, validation loss 0.068426\n",
      "Iter 1824, training loss 0.000281, validation loss 0.068972\n",
      "Iter 1825, training loss 0.000283, validation loss 0.068444\n",
      "Iter 1826, training loss 0.000274, validation loss 0.068989\n",
      "Iter 1827, training loss 0.000259, validation loss 0.068483\n",
      "Iter 1828, training loss 0.000240, validation loss 0.068884\n",
      "Iter 1829, training loss 0.000222, validation loss 0.068556\n",
      "Iter 1830, training loss 0.000211, validation loss 0.068745\n",
      "Iter 1831, training loss 0.000206, validation loss 0.068636\n",
      "Iter 1832, training loss 0.000206, validation loss 0.068603\n",
      "Iter 1833, training loss 0.000211, validation loss 0.068725\n",
      "Iter 1834, training loss 0.000218, validation loss 0.068518\n",
      "Iter 1835, training loss 0.000226, validation loss 0.068788\n",
      "Iter 1836, training loss 0.000233, validation loss 0.068462\n",
      "Iter 1837, training loss 0.000238, validation loss 0.068839\n",
      "Iter 1838, training loss 0.000241, validation loss 0.068465\n",
      "Iter 1839, training loss 0.000240, validation loss 0.068868\n",
      "Iter 1840, training loss 0.000235, validation loss 0.068490\n",
      "Iter 1841, training loss 0.000230, validation loss 0.068843\n",
      "Iter 1842, training loss 0.000222, validation loss 0.068514\n",
      "Iter 1843, training loss 0.000215, validation loss 0.068770\n",
      "Iter 1844, training loss 0.000209, validation loss 0.068554\n",
      "Iter 1845, training loss 0.000205, validation loss 0.068710\n",
      "Iter 1846, training loss 0.000202, validation loss 0.068600\n",
      "Iter 1847, training loss 0.000200, validation loss 0.068638\n",
      "Iter 1848, training loss 0.000200, validation loss 0.068635\n",
      "Iter 1849, training loss 0.000200, validation loss 0.068582\n",
      "Iter 1850, training loss 0.000202, validation loss 0.068658\n",
      "Iter 1851, training loss 0.000203, validation loss 0.068539\n",
      "Iter 1852, training loss 0.000204, validation loss 0.068689\n",
      "Iter 1853, training loss 0.000206, validation loss 0.068531\n",
      "Iter 1854, training loss 0.000207, validation loss 0.068729\n",
      "Iter 1855, training loss 0.000209, validation loss 0.068521\n",
      "Iter 1856, training loss 0.000212, validation loss 0.068761\n",
      "Iter 1857, training loss 0.000214, validation loss 0.068506\n",
      "Iter 1858, training loss 0.000217, validation loss 0.068790\n",
      "Iter 1859, training loss 0.000220, validation loss 0.068493\n",
      "Iter 1860, training loss 0.000224, validation loss 0.068830\n",
      "Iter 1861, training loss 0.000229, validation loss 0.068467\n",
      "Iter 1862, training loss 0.000235, validation loss 0.068863\n",
      "Iter 1863, training loss 0.000242, validation loss 0.068449\n",
      "Iter 1864, training loss 0.000251, validation loss 0.068929\n",
      "Iter 1865, training loss 0.000261, validation loss 0.068439\n",
      "Iter 1866, training loss 0.000268, validation loss 0.068985\n",
      "Iter 1867, training loss 0.000276, validation loss 0.068438\n",
      "Iter 1868, training loss 0.000282, validation loss 0.069050\n",
      "Iter 1869, training loss 0.000285, validation loss 0.068459\n",
      "Iter 1870, training loss 0.000279, validation loss 0.069060\n",
      "Iter 1871, training loss 0.000270, validation loss 0.068482\n",
      "Iter 1872, training loss 0.000257, validation loss 0.069007\n",
      "Iter 1873, training loss 0.000241, validation loss 0.068501\n",
      "Iter 1874, training loss 0.000225, validation loss 0.068892\n",
      "Iter 1875, training loss 0.000210, validation loss 0.068560\n",
      "Iter 1876, training loss 0.000200, validation loss 0.068776\n",
      "Iter 1877, training loss 0.000193, validation loss 0.068622\n",
      "Iter 1878, training loss 0.000191, validation loss 0.068671\n",
      "Iter 1879, training loss 0.000192, validation loss 0.068703\n",
      "Iter 1880, training loss 0.000195, validation loss 0.068594\n",
      "Iter 1881, training loss 0.000199, validation loss 0.068756\n",
      "Iter 1882, training loss 0.000204, validation loss 0.068539\n",
      "Iter 1883, training loss 0.000210, validation loss 0.068819\n",
      "Iter 1884, training loss 0.000215, validation loss 0.068518\n",
      "Iter 1885, training loss 0.000219, validation loss 0.068871\n",
      "Iter 1886, training loss 0.000223, validation loss 0.068514\n",
      "Iter 1887, training loss 0.000227, validation loss 0.068911\n",
      "Iter 1888, training loss 0.000230, validation loss 0.068495\n",
      "Iter 1889, training loss 0.000231, validation loss 0.068914\n",
      "Iter 1890, training loss 0.000231, validation loss 0.068501\n",
      "Iter 1891, training loss 0.000231, validation loss 0.068935\n",
      "Iter 1892, training loss 0.000229, validation loss 0.068504\n",
      "Iter 1893, training loss 0.000226, validation loss 0.068908\n",
      "Iter 1894, training loss 0.000222, validation loss 0.068514\n",
      "Iter 1895, training loss 0.000218, validation loss 0.068889\n",
      "Iter 1896, training loss 0.000214, validation loss 0.068525\n",
      "Iter 1897, training loss 0.000209, validation loss 0.068852\n",
      "Iter 1898, training loss 0.000205, validation loss 0.068551\n",
      "Iter 1899, training loss 0.000200, validation loss 0.068816\n",
      "Iter 1900, training loss 0.000196, validation loss 0.068569\n",
      "Iter 1901, training loss 0.000193, validation loss 0.068776\n",
      "Iter 1902, training loss 0.000190, validation loss 0.068601\n",
      "Iter 1903, training loss 0.000188, validation loss 0.068741\n",
      "Iter 1904, training loss 0.000186, validation loss 0.068620\n",
      "Iter 1905, training loss 0.000184, validation loss 0.068709\n",
      "Iter 1906, training loss 0.000183, validation loss 0.068638\n",
      "Iter 1907, training loss 0.000183, validation loss 0.068671\n",
      "Iter 1908, training loss 0.000182, validation loss 0.068645\n",
      "Iter 1909, training loss 0.000182, validation loss 0.068655\n",
      "Iter 1910, training loss 0.000182, validation loss 0.068678\n",
      "Iter 1911, training loss 0.000182, validation loss 0.068644\n",
      "Iter 1912, training loss 0.000182, validation loss 0.068689\n",
      "Iter 1913, training loss 0.000182, validation loss 0.068628\n",
      "Iter 1914, training loss 0.000182, validation loss 0.068713\n",
      "Iter 1915, training loss 0.000183, validation loss 0.068612\n",
      "Iter 1916, training loss 0.000184, validation loss 0.068738\n",
      "Iter 1917, training loss 0.000186, validation loss 0.068594\n",
      "Iter 1918, training loss 0.000189, validation loss 0.068783\n",
      "Iter 1919, training loss 0.000194, validation loss 0.068554\n",
      "Iter 1920, training loss 0.000203, validation loss 0.068849\n",
      "Iter 1921, training loss 0.000215, validation loss 0.068510\n",
      "Iter 1922, training loss 0.000235, validation loss 0.068974\n",
      "Iter 1923, training loss 0.000264, validation loss 0.068467\n",
      "Iter 1924, training loss 0.000305, validation loss 0.069182\n",
      "Iter 1925, training loss 0.000360, validation loss 0.068456\n",
      "Iter 1926, training loss 0.000421, validation loss 0.069454\n",
      "Iter 1927, training loss 0.000482, validation loss 0.068482\n",
      "Iter 1928, training loss 0.000510, validation loss 0.069674\n",
      "Iter 1929, training loss 0.000496, validation loss 0.068542\n",
      "Iter 1930, training loss 0.000417, validation loss 0.069549\n",
      "Iter 1931, training loss 0.000316, validation loss 0.068593\n",
      "Iter 1932, training loss 0.000225, validation loss 0.069102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1933, training loss 0.000180, validation loss 0.068766\n",
      "Iter 1934, training loss 0.000187, validation loss 0.068703\n",
      "Iter 1935, training loss 0.000229, validation loss 0.069052\n",
      "Iter 1936, training loss 0.000286, validation loss 0.068509\n",
      "Iter 1937, training loss 0.000335, validation loss 0.069278\n",
      "Iter 1938, training loss 0.000356, validation loss 0.068463\n",
      "Iter 1939, training loss 0.000331, validation loss 0.069274\n",
      "Iter 1940, training loss 0.000281, validation loss 0.068521\n",
      "Iter 1941, training loss 0.000223, validation loss 0.069046\n",
      "Iter 1942, training loss 0.000184, validation loss 0.068699\n",
      "Iter 1943, training loss 0.000176, validation loss 0.068754\n",
      "Iter 1944, training loss 0.000193, validation loss 0.068945\n",
      "Iter 1945, training loss 0.000223, validation loss 0.068595\n",
      "Iter 1946, training loss 0.000248, validation loss 0.069120\n",
      "Iter 1947, training loss 0.000255, validation loss 0.068545\n",
      "Iter 1948, training loss 0.000243, validation loss 0.069086\n",
      "Iter 1949, training loss 0.000218, validation loss 0.068552\n",
      "Iter 1950, training loss 0.000192, validation loss 0.068890\n",
      "Iter 1951, training loss 0.000176, validation loss 0.068663\n",
      "Iter 1952, training loss 0.000172, validation loss 0.068714\n",
      "Iter 1953, training loss 0.000179, validation loss 0.068820\n",
      "Iter 1954, training loss 0.000190, validation loss 0.068593\n",
      "Iter 1955, training loss 0.000200, validation loss 0.068930\n",
      "Iter 1956, training loss 0.000205, validation loss 0.068570\n",
      "Iter 1957, training loss 0.000203, validation loss 0.068949\n",
      "Iter 1958, training loss 0.000194, validation loss 0.068587\n",
      "Iter 1959, training loss 0.000184, validation loss 0.068866\n",
      "Iter 1960, training loss 0.000175, validation loss 0.068656\n",
      "Iter 1961, training loss 0.000170, validation loss 0.068761\n",
      "Iter 1962, training loss 0.000169, validation loss 0.068743\n",
      "Iter 1963, training loss 0.000171, validation loss 0.068666\n",
      "Iter 1964, training loss 0.000176, validation loss 0.068817\n",
      "Iter 1965, training loss 0.000180, validation loss 0.068600\n",
      "Iter 1966, training loss 0.000183, validation loss 0.068836\n",
      "Iter 1967, training loss 0.000183, validation loss 0.068575\n",
      "Iter 1968, training loss 0.000182, validation loss 0.068838\n",
      "Iter 1969, training loss 0.000178, validation loss 0.068598\n",
      "Iter 1970, training loss 0.000174, validation loss 0.068803\n",
      "Iter 1971, training loss 0.000170, validation loss 0.068646\n",
      "Iter 1972, training loss 0.000168, validation loss 0.068763\n",
      "Iter 1973, training loss 0.000166, validation loss 0.068698\n",
      "Iter 1974, training loss 0.000165, validation loss 0.068700\n",
      "Iter 1975, training loss 0.000166, validation loss 0.068736\n",
      "Iter 1976, training loss 0.000167, validation loss 0.068654\n",
      "Iter 1977, training loss 0.000168, validation loss 0.068762\n",
      "Iter 1978, training loss 0.000169, validation loss 0.068618\n",
      "Iter 1979, training loss 0.000170, validation loss 0.068777\n",
      "Iter 1980, training loss 0.000171, validation loss 0.068608\n",
      "Iter 1981, training loss 0.000171, validation loss 0.068795\n",
      "Iter 1982, training loss 0.000171, validation loss 0.068615\n",
      "Iter 1983, training loss 0.000171, validation loss 0.068817\n",
      "Iter 1984, training loss 0.000171, validation loss 0.068629\n",
      "Iter 1985, training loss 0.000171, validation loss 0.068824\n",
      "Iter 1986, training loss 0.000170, validation loss 0.068634\n",
      "Iter 1987, training loss 0.000169, validation loss 0.068818\n",
      "Iter 1988, training loss 0.000169, validation loss 0.068635\n",
      "Iter 1989, training loss 0.000168, validation loss 0.068807\n",
      "Iter 1990, training loss 0.000168, validation loss 0.068637\n",
      "Iter 1991, training loss 0.000167, validation loss 0.068806\n",
      "Iter 1992, training loss 0.000167, validation loss 0.068635\n",
      "Iter 1993, training loss 0.000167, validation loss 0.068807\n",
      "Iter 1994, training loss 0.000167, validation loss 0.068644\n",
      "Iter 1995, training loss 0.000167, validation loss 0.068826\n",
      "Iter 1996, training loss 0.000168, validation loss 0.068640\n",
      "Iter 1997, training loss 0.000169, validation loss 0.068838\n",
      "Iter 1998, training loss 0.000170, validation loss 0.068631\n",
      "Iter 1999, training loss 0.000172, validation loss 0.068863\n",
      "Iter 2000, training loss 0.000174, validation loss 0.068616\n",
      "Iter 2001, training loss 0.000178, validation loss 0.068900\n",
      "Iter 2002, training loss 0.000182, validation loss 0.068597\n",
      "Iter 2003, training loss 0.000189, validation loss 0.068951\n",
      "Iter 2004, training loss 0.000197, validation loss 0.068570\n",
      "Iter 2005, training loss 0.000207, validation loss 0.069022\n",
      "Iter 2006, training loss 0.000218, validation loss 0.068552\n",
      "Iter 2007, training loss 0.000230, validation loss 0.069102\n",
      "Iter 2008, training loss 0.000242, validation loss 0.068540\n",
      "Iter 2009, training loss 0.000254, validation loss 0.069183\n",
      "Iter 2010, training loss 0.000263, validation loss 0.068547\n",
      "Iter 2011, training loss 0.000267, validation loss 0.069240\n",
      "Iter 2012, training loss 0.000265, validation loss 0.068567\n",
      "Iter 2013, training loss 0.000257, validation loss 0.069236\n",
      "Iter 2014, training loss 0.000242, validation loss 0.068594\n",
      "Iter 2015, training loss 0.000222, validation loss 0.069145\n",
      "Iter 2016, training loss 0.000201, validation loss 0.068632\n",
      "Iter 2017, training loss 0.000182, validation loss 0.068999\n",
      "Iter 2018, training loss 0.000167, validation loss 0.068685\n",
      "Iter 2019, training loss 0.000158, validation loss 0.068848\n",
      "Iter 2020, training loss 0.000155, validation loss 0.068765\n",
      "Iter 2021, training loss 0.000155, validation loss 0.068736\n",
      "Iter 2022, training loss 0.000159, validation loss 0.068846\n",
      "Iter 2023, training loss 0.000165, validation loss 0.068661\n",
      "Iter 2024, training loss 0.000172, validation loss 0.068921\n",
      "Iter 2025, training loss 0.000178, validation loss 0.068625\n",
      "Iter 2026, training loss 0.000184, validation loss 0.068986\n",
      "Iter 2027, training loss 0.000189, validation loss 0.068610\n",
      "Iter 2028, training loss 0.000192, validation loss 0.069024\n",
      "Iter 2029, training loss 0.000193, validation loss 0.068611\n",
      "Iter 2030, training loss 0.000192, validation loss 0.069032\n",
      "Iter 2031, training loss 0.000189, validation loss 0.068619\n",
      "Iter 2032, training loss 0.000185, validation loss 0.069008\n",
      "Iter 2033, training loss 0.000181, validation loss 0.068628\n",
      "Iter 2034, training loss 0.000176, validation loss 0.068966\n",
      "Iter 2035, training loss 0.000170, validation loss 0.068649\n",
      "Iter 2036, training loss 0.000166, validation loss 0.068918\n",
      "Iter 2037, training loss 0.000162, validation loss 0.068667\n",
      "Iter 2038, training loss 0.000158, validation loss 0.068860\n",
      "Iter 2039, training loss 0.000155, validation loss 0.068682\n",
      "Iter 2040, training loss 0.000153, validation loss 0.068817\n",
      "Iter 2041, training loss 0.000151, validation loss 0.068712\n",
      "Iter 2042, training loss 0.000150, validation loss 0.068788\n",
      "Iter 2043, training loss 0.000149, validation loss 0.068739\n",
      "Iter 2044, training loss 0.000149, validation loss 0.068757\n",
      "Iter 2045, training loss 0.000149, validation loss 0.068761\n",
      "Iter 2046, training loss 0.000149, validation loss 0.068737\n",
      "Iter 2047, training loss 0.000149, validation loss 0.068779\n",
      "Iter 2048, training loss 0.000149, validation loss 0.068715\n",
      "Iter 2049, training loss 0.000149, validation loss 0.068792\n",
      "Iter 2050, training loss 0.000150, validation loss 0.068697\n",
      "Iter 2051, training loss 0.000151, validation loss 0.068810\n",
      "Iter 2052, training loss 0.000152, validation loss 0.068677\n",
      "Iter 2053, training loss 0.000153, validation loss 0.068837\n",
      "Iter 2054, training loss 0.000155, validation loss 0.068657\n",
      "Iter 2055, training loss 0.000159, validation loss 0.068886\n",
      "Iter 2056, training loss 0.000165, validation loss 0.068632\n",
      "Iter 2057, training loss 0.000174, validation loss 0.068965\n",
      "Iter 2058, training loss 0.000187, validation loss 0.068589\n",
      "Iter 2059, training loss 0.000206, validation loss 0.069079\n",
      "Iter 2060, training loss 0.000233, validation loss 0.068552\n",
      "Iter 2061, training loss 0.000271, validation loss 0.069271\n",
      "Iter 2062, training loss 0.000321, validation loss 0.068534\n",
      "Iter 2063, training loss 0.000378, validation loss 0.069530\n",
      "Iter 2064, training loss 0.000438, validation loss 0.068559\n",
      "Iter 2065, training loss 0.000472, validation loss 0.069752\n",
      "Iter 2066, training loss 0.000473, validation loss 0.068617\n",
      "Iter 2067, training loss 0.000415, validation loss 0.069712\n",
      "Iter 2068, training loss 0.000328, validation loss 0.068665\n",
      "Iter 2069, training loss 0.000231, validation loss 0.069313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2070, training loss 0.000164, validation loss 0.068777\n",
      "Iter 2071, training loss 0.000145, validation loss 0.068859\n",
      "Iter 2072, training loss 0.000167, validation loss 0.069027\n",
      "Iter 2073, training loss 0.000213, validation loss 0.068618\n",
      "Iter 2074, training loss 0.000265, validation loss 0.069291\n",
      "Iter 2075, training loss 0.000307, validation loss 0.068552\n",
      "Iter 2076, training loss 0.000314, validation loss 0.069392\n",
      "Iter 2077, training loss 0.000290, validation loss 0.068563\n",
      "Iter 2078, training loss 0.000242, validation loss 0.069260\n",
      "Iter 2079, training loss 0.000190, validation loss 0.068684\n",
      "Iter 2080, training loss 0.000153, validation loss 0.068990\n",
      "Iter 2081, training loss 0.000143, validation loss 0.068896\n",
      "Iter 2082, training loss 0.000156, validation loss 0.068762\n",
      "Iter 2083, training loss 0.000181, validation loss 0.069111\n",
      "Iter 2084, training loss 0.000205, validation loss 0.068645\n",
      "Iter 2085, training loss 0.000216, validation loss 0.069186\n",
      "Iter 2086, training loss 0.000213, validation loss 0.068604\n",
      "Iter 2087, training loss 0.000196, validation loss 0.069110\n",
      "Iter 2088, training loss 0.000174, validation loss 0.068647\n",
      "Iter 2089, training loss 0.000154, validation loss 0.068939\n",
      "Iter 2090, training loss 0.000142, validation loss 0.068770\n",
      "Iter 2091, training loss 0.000141, validation loss 0.068791\n",
      "Iter 2092, training loss 0.000147, validation loss 0.068918\n",
      "Iter 2093, training loss 0.000157, validation loss 0.068689\n",
      "Iter 2094, training loss 0.000166, validation loss 0.069009\n",
      "Iter 2095, training loss 0.000170, validation loss 0.068655\n",
      "Iter 2096, training loss 0.000168, validation loss 0.069020\n",
      "Iter 2097, training loss 0.000162, validation loss 0.068665\n",
      "Iter 2098, training loss 0.000154, validation loss 0.068957\n",
      "Iter 2099, training loss 0.000146, validation loss 0.068729\n",
      "Iter 2100, training loss 0.000140, validation loss 0.068872\n",
      "Iter 2101, training loss 0.000138, validation loss 0.068812\n",
      "Iter 2102, training loss 0.000138, validation loss 0.068779\n",
      "Iter 2103, training loss 0.000141, validation loss 0.068876\n",
      "Iter 2104, training loss 0.000145, validation loss 0.068698\n",
      "Iter 2105, training loss 0.000148, validation loss 0.068905\n",
      "Iter 2106, training loss 0.000150, validation loss 0.068666\n",
      "Iter 2107, training loss 0.000151, validation loss 0.068927\n",
      "Iter 2108, training loss 0.000150, validation loss 0.068677\n",
      "Iter 2109, training loss 0.000147, validation loss 0.068917\n",
      "Iter 2110, training loss 0.000144, validation loss 0.068710\n",
      "Iter 2111, training loss 0.000141, validation loss 0.068889\n",
      "Iter 2112, training loss 0.000138, validation loss 0.068752\n",
      "Iter 2113, training loss 0.000136, validation loss 0.068831\n",
      "Iter 2114, training loss 0.000135, validation loss 0.068778\n",
      "Iter 2115, training loss 0.000135, validation loss 0.068768\n",
      "Iter 2116, training loss 0.000135, validation loss 0.068801\n",
      "Iter 2117, training loss 0.000136, validation loss 0.068725\n",
      "Iter 2118, training loss 0.000137, validation loss 0.068829\n",
      "Iter 2119, training loss 0.000138, validation loss 0.068707\n",
      "Iter 2120, training loss 0.000139, validation loss 0.068856\n",
      "Iter 2121, training loss 0.000139, validation loss 0.068701\n",
      "Iter 2122, training loss 0.000140, validation loss 0.068879\n",
      "Iter 2123, training loss 0.000140, validation loss 0.068703\n",
      "Iter 2124, training loss 0.000141, validation loss 0.068892\n",
      "Iter 2125, training loss 0.000141, validation loss 0.068694\n",
      "Iter 2126, training loss 0.000141, validation loss 0.068895\n",
      "Iter 2127, training loss 0.000142, validation loss 0.068686\n",
      "Iter 2128, training loss 0.000143, validation loss 0.068904\n",
      "Iter 2129, training loss 0.000144, validation loss 0.068675\n",
      "Iter 2130, training loss 0.000145, validation loss 0.068923\n",
      "Iter 2131, training loss 0.000147, validation loss 0.068667\n",
      "Iter 2132, training loss 0.000150, validation loss 0.068952\n",
      "Iter 2133, training loss 0.000153, validation loss 0.068653\n",
      "Iter 2134, training loss 0.000157, validation loss 0.068990\n",
      "Iter 2135, training loss 0.000161, validation loss 0.068641\n",
      "Iter 2136, training loss 0.000166, validation loss 0.069032\n",
      "Iter 2137, training loss 0.000172, validation loss 0.068623\n",
      "Iter 2138, training loss 0.000179, validation loss 0.069082\n",
      "Iter 2139, training loss 0.000186, validation loss 0.068610\n",
      "Iter 2140, training loss 0.000194, validation loss 0.069145\n",
      "Iter 2141, training loss 0.000203, validation loss 0.068606\n",
      "Iter 2142, training loss 0.000213, validation loss 0.069209\n",
      "Iter 2143, training loss 0.000221, validation loss 0.068602\n",
      "Iter 2144, training loss 0.000226, validation loss 0.069258\n",
      "Iter 2145, training loss 0.000229, validation loss 0.068610\n",
      "Iter 2146, training loss 0.000228, validation loss 0.069281\n",
      "Iter 2147, training loss 0.000224, validation loss 0.068627\n",
      "Iter 2148, training loss 0.000215, validation loss 0.069255\n",
      "Iter 2149, training loss 0.000202, validation loss 0.068653\n",
      "Iter 2150, training loss 0.000186, validation loss 0.069180\n",
      "Iter 2151, training loss 0.000170, validation loss 0.068685\n",
      "Iter 2152, training loss 0.000155, validation loss 0.069054\n",
      "Iter 2153, training loss 0.000142, validation loss 0.068735\n",
      "Iter 2154, training loss 0.000133, validation loss 0.068933\n",
      "Iter 2155, training loss 0.000129, validation loss 0.068802\n",
      "Iter 2156, training loss 0.000127, validation loss 0.068830\n",
      "Iter 2157, training loss 0.000128, validation loss 0.068877\n",
      "Iter 2158, training loss 0.000132, validation loss 0.068762\n",
      "Iter 2159, training loss 0.000136, validation loss 0.068942\n",
      "Iter 2160, training loss 0.000140, validation loss 0.068718\n",
      "Iter 2161, training loss 0.000144, validation loss 0.068996\n",
      "Iter 2162, training loss 0.000148, validation loss 0.068700\n",
      "Iter 2163, training loss 0.000152, validation loss 0.069035\n",
      "Iter 2164, training loss 0.000154, validation loss 0.068691\n",
      "Iter 2165, training loss 0.000156, validation loss 0.069064\n",
      "Iter 2166, training loss 0.000158, validation loss 0.068685\n",
      "Iter 2167, training loss 0.000159, validation loss 0.069071\n",
      "Iter 2168, training loss 0.000160, validation loss 0.068673\n",
      "Iter 2169, training loss 0.000161, validation loss 0.069078\n",
      "Iter 2170, training loss 0.000162, validation loss 0.068668\n",
      "Iter 2171, training loss 0.000162, validation loss 0.069079\n",
      "Iter 2172, training loss 0.000162, validation loss 0.068664\n",
      "Iter 2173, training loss 0.000163, validation loss 0.069082\n",
      "Iter 2174, training loss 0.000163, validation loss 0.068662\n",
      "Iter 2175, training loss 0.000163, validation loss 0.069085\n",
      "Iter 2176, training loss 0.000163, validation loss 0.068670\n",
      "Iter 2177, training loss 0.000163, validation loss 0.069094\n",
      "Iter 2178, training loss 0.000163, validation loss 0.068666\n",
      "Iter 2179, training loss 0.000163, validation loss 0.069088\n",
      "Iter 2180, training loss 0.000163, validation loss 0.068666\n",
      "Iter 2181, training loss 0.000162, validation loss 0.069086\n",
      "Iter 2182, training loss 0.000162, validation loss 0.068666\n",
      "Iter 2183, training loss 0.000162, validation loss 0.069091\n",
      "Iter 2184, training loss 0.000162, validation loss 0.068665\n",
      "Iter 2185, training loss 0.000162, validation loss 0.069083\n",
      "Iter 2186, training loss 0.000162, validation loss 0.068665\n",
      "Iter 2187, training loss 0.000161, validation loss 0.069097\n",
      "Iter 2188, training loss 0.000161, validation loss 0.068673\n",
      "Iter 2189, training loss 0.000161, validation loss 0.069096\n",
      "Iter 2190, training loss 0.000162, validation loss 0.068674\n",
      "Iter 2191, training loss 0.000162, validation loss 0.069100\n",
      "Iter 2192, training loss 0.000162, validation loss 0.068660\n",
      "Iter 2193, training loss 0.000162, validation loss 0.069093\n",
      "Iter 2194, training loss 0.000162, validation loss 0.068679\n",
      "Iter 2195, training loss 0.000163, validation loss 0.069120\n",
      "Iter 2196, training loss 0.000164, validation loss 0.068665\n",
      "Iter 2197, training loss 0.000164, validation loss 0.069101\n",
      "Iter 2198, training loss 0.000165, validation loss 0.068661\n",
      "Iter 2199, training loss 0.000166, validation loss 0.069122\n",
      "Iter 2200, training loss 0.000168, validation loss 0.068662\n",
      "Iter 2201, training loss 0.000169, validation loss 0.069143\n",
      "Iter 2202, training loss 0.000171, validation loss 0.068660\n",
      "Iter 2203, training loss 0.000173, validation loss 0.069139\n",
      "Iter 2204, training loss 0.000175, validation loss 0.068651\n",
      "Iter 2205, training loss 0.000177, validation loss 0.069173\n",
      "Iter 2206, training loss 0.000179, validation loss 0.068657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2207, training loss 0.000180, validation loss 0.069177\n",
      "Iter 2208, training loss 0.000181, validation loss 0.068661\n",
      "Iter 2209, training loss 0.000183, validation loss 0.069205\n",
      "Iter 2210, training loss 0.000183, validation loss 0.068651\n",
      "Iter 2211, training loss 0.000181, validation loss 0.069181\n",
      "Iter 2212, training loss 0.000180, validation loss 0.068662\n",
      "Iter 2213, training loss 0.000178, validation loss 0.069199\n",
      "Iter 2214, training loss 0.000175, validation loss 0.068668\n",
      "Iter 2215, training loss 0.000169, validation loss 0.069154\n",
      "Iter 2216, training loss 0.000162, validation loss 0.068683\n",
      "Iter 2217, training loss 0.000156, validation loss 0.069112\n",
      "Iter 2218, training loss 0.000148, validation loss 0.068696\n",
      "Iter 2219, training loss 0.000141, validation loss 0.069044\n",
      "Iter 2220, training loss 0.000133, validation loss 0.068732\n",
      "Iter 2221, training loss 0.000127, validation loss 0.068973\n",
      "Iter 2222, training loss 0.000122, validation loss 0.068769\n",
      "Iter 2223, training loss 0.000119, validation loss 0.068917\n",
      "Iter 2224, training loss 0.000116, validation loss 0.068826\n",
      "Iter 2225, training loss 0.000115, validation loss 0.068853\n",
      "Iter 2226, training loss 0.000115, validation loss 0.068851\n",
      "Iter 2227, training loss 0.000116, validation loss 0.068808\n",
      "Iter 2228, training loss 0.000117, validation loss 0.068903\n",
      "Iter 2229, training loss 0.000119, validation loss 0.068780\n",
      "Iter 2230, training loss 0.000120, validation loss 0.068924\n",
      "Iter 2231, training loss 0.000122, validation loss 0.068747\n",
      "Iter 2232, training loss 0.000124, validation loss 0.068953\n",
      "Iter 2233, training loss 0.000126, validation loss 0.068725\n",
      "Iter 2234, training loss 0.000129, validation loss 0.068983\n",
      "Iter 2235, training loss 0.000132, validation loss 0.068705\n",
      "Iter 2236, training loss 0.000137, validation loss 0.069025\n",
      "Iter 2237, training loss 0.000143, validation loss 0.068677\n",
      "Iter 2238, training loss 0.000151, validation loss 0.069078\n",
      "Iter 2239, training loss 0.000162, validation loss 0.068648\n",
      "Iter 2240, training loss 0.000178, validation loss 0.069175\n",
      "Iter 2241, training loss 0.000200, validation loss 0.068616\n",
      "Iter 2242, training loss 0.000231, validation loss 0.069334\n",
      "Iter 2243, training loss 0.000273, validation loss 0.068595\n",
      "Iter 2244, training loss 0.000321, validation loss 0.069559\n",
      "Iter 2245, training loss 0.000376, validation loss 0.068605\n",
      "Iter 2246, training loss 0.000419, validation loss 0.069793\n",
      "Iter 2247, training loss 0.000448, validation loss 0.068658\n",
      "Iter 2248, training loss 0.000429, validation loss 0.069873\n",
      "Iter 2249, training loss 0.000377, validation loss 0.068696\n",
      "Iter 2250, training loss 0.000286, validation loss 0.069603\n",
      "Iter 2251, training loss 0.000198, validation loss 0.068756\n",
      "Iter 2252, training loss 0.000135, validation loss 0.069141\n",
      "Iter 2253, training loss 0.000112, validation loss 0.068922\n",
      "Iter 2254, training loss 0.000126, validation loss 0.068788\n",
      "Iter 2255, training loss 0.000163, validation loss 0.069171\n",
      "Iter 2256, training loss 0.000210, validation loss 0.068618\n",
      "Iter 2257, training loss 0.000248, validation loss 0.069374\n",
      "Iter 2258, training loss 0.000271, validation loss 0.068595\n",
      "Iter 2259, training loss 0.000267, validation loss 0.069465\n",
      "Iter 2260, training loss 0.000241, validation loss 0.068667\n",
      "Iter 2261, training loss 0.000194, validation loss 0.069334\n",
      "Iter 2262, training loss 0.000148, validation loss 0.068789\n",
      "Iter 2263, training loss 0.000118, validation loss 0.069054\n",
      "Iter 2264, training loss 0.000111, validation loss 0.068966\n",
      "Iter 2265, training loss 0.000124, validation loss 0.068810\n",
      "Iter 2266, training loss 0.000146, validation loss 0.069153\n",
      "Iter 2267, training loss 0.000168, validation loss 0.068687\n",
      "Iter 2268, training loss 0.000182, validation loss 0.069228\n",
      "Iter 2269, training loss 0.000185, validation loss 0.068635\n",
      "Iter 2270, training loss 0.000175, validation loss 0.069206\n",
      "Iter 2271, training loss 0.000158, validation loss 0.068691\n",
      "Iter 2272, training loss 0.000136, validation loss 0.069090\n",
      "Iter 2273, training loss 0.000119, validation loss 0.068790\n",
      "Iter 2274, training loss 0.000109, validation loss 0.068938\n",
      "Iter 2275, training loss 0.000109, validation loss 0.068935\n",
      "Iter 2276, training loss 0.000115, validation loss 0.068806\n",
      "Iter 2277, training loss 0.000124, validation loss 0.069037\n",
      "Iter 2278, training loss 0.000132, validation loss 0.068733\n",
      "Iter 2279, training loss 0.000137, validation loss 0.069088\n",
      "Iter 2280, training loss 0.000136, validation loss 0.068711\n",
      "Iter 2281, training loss 0.000132, validation loss 0.069061\n",
      "Iter 2282, training loss 0.000125, validation loss 0.068750\n",
      "Iter 2283, training loss 0.000117, validation loss 0.069003\n",
      "Iter 2284, training loss 0.000111, validation loss 0.068815\n",
      "Iter 2285, training loss 0.000107, validation loss 0.068909\n",
      "Iter 2286, training loss 0.000106, validation loss 0.068884\n",
      "Iter 2287, training loss 0.000108, validation loss 0.068826\n",
      "Iter 2288, training loss 0.000110, validation loss 0.068933\n",
      "Iter 2289, training loss 0.000113, validation loss 0.068770\n",
      "Iter 2290, training loss 0.000115, validation loss 0.068980\n",
      "Iter 2291, training loss 0.000117, validation loss 0.068766\n",
      "Iter 2292, training loss 0.000117, validation loss 0.068997\n",
      "Iter 2293, training loss 0.000116, validation loss 0.068770\n",
      "Iter 2294, training loss 0.000114, validation loss 0.068988\n",
      "Iter 2295, training loss 0.000112, validation loss 0.068795\n",
      "Iter 2296, training loss 0.000109, validation loss 0.068952\n",
      "Iter 2297, training loss 0.000107, validation loss 0.068813\n",
      "Iter 2298, training loss 0.000106, validation loss 0.068908\n",
      "Iter 2299, training loss 0.000105, validation loss 0.068838\n",
      "Iter 2300, training loss 0.000104, validation loss 0.068868\n",
      "Iter 2301, training loss 0.000104, validation loss 0.068865\n",
      "Iter 2302, training loss 0.000104, validation loss 0.068847\n",
      "Iter 2303, training loss 0.000104, validation loss 0.068899\n",
      "Iter 2304, training loss 0.000105, validation loss 0.068835\n",
      "Iter 2305, training loss 0.000105, validation loss 0.068921\n",
      "Iter 2306, training loss 0.000105, validation loss 0.068824\n",
      "Iter 2307, training loss 0.000106, validation loss 0.068933\n",
      "Iter 2308, training loss 0.000106, validation loss 0.068807\n",
      "Iter 2309, training loss 0.000107, validation loss 0.068941\n",
      "Iter 2310, training loss 0.000107, validation loss 0.068794\n",
      "Iter 2311, training loss 0.000108, validation loss 0.068957\n",
      "Iter 2312, training loss 0.000109, validation loss 0.068785\n",
      "Iter 2313, training loss 0.000110, validation loss 0.068979\n",
      "Iter 2314, training loss 0.000112, validation loss 0.068776\n",
      "Iter 2315, training loss 0.000114, validation loss 0.069004\n",
      "Iter 2316, training loss 0.000116, validation loss 0.068753\n",
      "Iter 2317, training loss 0.000120, validation loss 0.069032\n",
      "Iter 2318, training loss 0.000124, validation loss 0.068729\n",
      "Iter 2319, training loss 0.000130, validation loss 0.069085\n",
      "Iter 2320, training loss 0.000139, validation loss 0.068700\n",
      "Iter 2321, training loss 0.000150, validation loss 0.069163\n",
      "Iter 2322, training loss 0.000166, validation loss 0.068669\n",
      "Iter 2323, training loss 0.000186, validation loss 0.069284\n",
      "Iter 2324, training loss 0.000213, validation loss 0.068643\n",
      "Iter 2325, training loss 0.000246, validation loss 0.069453\n",
      "Iter 2326, training loss 0.000286, validation loss 0.068630\n",
      "Iter 2327, training loss 0.000324, validation loss 0.069652\n",
      "Iter 2328, training loss 0.000361, validation loss 0.068650\n",
      "Iter 2329, training loss 0.000376, validation loss 0.069804\n",
      "Iter 2330, training loss 0.000374, validation loss 0.068687\n",
      "Iter 2331, training loss 0.000336, validation loss 0.069752\n",
      "Iter 2332, training loss 0.000281, validation loss 0.068721\n",
      "Iter 2333, training loss 0.000212, validation loss 0.069472\n",
      "Iter 2334, training loss 0.000152, validation loss 0.068790\n",
      "Iter 2335, training loss 0.000113, validation loss 0.069100\n",
      "Iter 2336, training loss 0.000100, validation loss 0.068931\n",
      "Iter 2337, training loss 0.000109, validation loss 0.068808\n",
      "Iter 2338, training loss 0.000132, validation loss 0.069109\n",
      "Iter 2339, training loss 0.000162, validation loss 0.068671\n",
      "Iter 2340, training loss 0.000189, validation loss 0.069297\n",
      "Iter 2341, training loss 0.000211, validation loss 0.068651\n",
      "Iter 2342, training loss 0.000220, validation loss 0.069419\n",
      "Iter 2343, training loss 0.000215, validation loss 0.068695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2344, training loss 0.000192, validation loss 0.069385\n",
      "Iter 2345, training loss 0.000162, validation loss 0.068761\n",
      "Iter 2346, training loss 0.000131, validation loss 0.069204\n",
      "Iter 2347, training loss 0.000109, validation loss 0.068871\n",
      "Iter 2348, training loss 0.000099, validation loss 0.068984\n",
      "Iter 2349, training loss 0.000101, validation loss 0.068995\n",
      "Iter 2350, training loss 0.000111, validation loss 0.068807\n",
      "Iter 2351, training loss 0.000124, validation loss 0.069105\n",
      "Iter 2352, training loss 0.000138, validation loss 0.068708\n",
      "Iter 2353, training loss 0.000147, validation loss 0.069174\n",
      "Iter 2354, training loss 0.000153, validation loss 0.068691\n",
      "Iter 2355, training loss 0.000151, validation loss 0.069208\n",
      "Iter 2356, training loss 0.000144, validation loss 0.068723\n",
      "Iter 2357, training loss 0.000134, validation loss 0.069170\n",
      "Iter 2358, training loss 0.000121, validation loss 0.068787\n",
      "Iter 2359, training loss 0.000110, validation loss 0.069074\n",
      "Iter 2360, training loss 0.000101, validation loss 0.068857\n",
      "Iter 2361, training loss 0.000097, validation loss 0.068954\n",
      "Iter 2362, training loss 0.000096, validation loss 0.068939\n",
      "Iter 2363, training loss 0.000098, validation loss 0.068864\n",
      "Iter 2364, training loss 0.000102, validation loss 0.069007\n",
      "Iter 2365, training loss 0.000107, validation loss 0.068800\n",
      "Iter 2366, training loss 0.000110, validation loss 0.069048\n",
      "Iter 2367, training loss 0.000113, validation loss 0.068769\n",
      "Iter 2368, training loss 0.000114, validation loss 0.069066\n",
      "Iter 2369, training loss 0.000114, validation loss 0.068767\n",
      "Iter 2370, training loss 0.000113, validation loss 0.069070\n",
      "Iter 2371, training loss 0.000111, validation loss 0.068783\n",
      "Iter 2372, training loss 0.000108, validation loss 0.069054\n",
      "Iter 2373, training loss 0.000106, validation loss 0.068812\n",
      "Iter 2374, training loss 0.000103, validation loss 0.069029\n",
      "Iter 2375, training loss 0.000100, validation loss 0.068846\n",
      "Iter 2376, training loss 0.000098, validation loss 0.068990\n",
      "Iter 2377, training loss 0.000096, validation loss 0.068872\n",
      "Iter 2378, training loss 0.000095, validation loss 0.068945\n",
      "Iter 2379, training loss 0.000094, validation loss 0.068889\n",
      "Iter 2380, training loss 0.000093, validation loss 0.068901\n",
      "Iter 2381, training loss 0.000093, validation loss 0.068906\n",
      "Iter 2382, training loss 0.000094, validation loss 0.068874\n",
      "Iter 2383, training loss 0.000094, validation loss 0.068930\n",
      "Iter 2384, training loss 0.000094, validation loss 0.068859\n",
      "Iter 2385, training loss 0.000095, validation loss 0.068957\n",
      "Iter 2386, training loss 0.000096, validation loss 0.068853\n",
      "Iter 2387, training loss 0.000096, validation loss 0.068988\n",
      "Iter 2388, training loss 0.000097, validation loss 0.068846\n",
      "Iter 2389, training loss 0.000099, validation loss 0.069016\n",
      "Iter 2390, training loss 0.000100, validation loss 0.068828\n",
      "Iter 2391, training loss 0.000103, validation loss 0.069039\n",
      "Iter 2392, training loss 0.000106, validation loss 0.068797\n",
      "Iter 2393, training loss 0.000110, validation loss 0.069075\n",
      "Iter 2394, training loss 0.000116, validation loss 0.068760\n",
      "Iter 2395, training loss 0.000124, validation loss 0.069137\n",
      "Iter 2396, training loss 0.000137, validation loss 0.068722\n",
      "Iter 2397, training loss 0.000154, validation loss 0.069250\n",
      "Iter 2398, training loss 0.000178, validation loss 0.068683\n",
      "Iter 2399, training loss 0.000210, validation loss 0.069422\n",
      "Iter 2400, training loss 0.000255, validation loss 0.068662\n",
      "Iter 2401, training loss 0.000308, validation loss 0.069680\n",
      "Iter 2402, training loss 0.000374, validation loss 0.068665\n",
      "Iter 2403, training loss 0.000434, validation loss 0.069963\n",
      "Iter 2404, training loss 0.000486, validation loss 0.068709\n",
      "Iter 2405, training loss 0.000488, validation loss 0.070114\n",
      "Iter 2406, training loss 0.000452, validation loss 0.068748\n",
      "Iter 2407, training loss 0.000357, validation loss 0.069897\n",
      "Iter 2408, training loss 0.000250, validation loss 0.068786\n",
      "Iter 2409, training loss 0.000153, validation loss 0.069384\n",
      "Iter 2410, training loss 0.000099, validation loss 0.068943\n",
      "Iter 2411, training loss 0.000096, validation loss 0.068934\n",
      "Iter 2412, training loss 0.000131, validation loss 0.069228\n",
      "Iter 2413, training loss 0.000187, validation loss 0.068687\n",
      "Iter 2414, training loss 0.000244, validation loss 0.069497\n",
      "Iter 2415, training loss 0.000289, validation loss 0.068636\n",
      "Iter 2416, training loss 0.000298, validation loss 0.069658\n",
      "Iter 2417, training loss 0.000275, validation loss 0.068697\n",
      "Iter 2418, training loss 0.000218, validation loss 0.069537\n",
      "Iter 2419, training loss 0.000156, validation loss 0.068826\n",
      "Iter 2420, training loss 0.000107, validation loss 0.069207\n",
      "Iter 2421, training loss 0.000090, validation loss 0.069030\n",
      "Iter 2422, training loss 0.000103, validation loss 0.068899\n",
      "Iter 2423, training loss 0.000132, validation loss 0.069265\n",
      "Iter 2424, training loss 0.000164, validation loss 0.068748\n",
      "Iter 2425, training loss 0.000185, validation loss 0.069394\n",
      "Iter 2426, training loss 0.000192, validation loss 0.068689\n",
      "Iter 2427, training loss 0.000179, validation loss 0.069363\n",
      "Iter 2428, training loss 0.000155, validation loss 0.068732\n",
      "Iter 2429, training loss 0.000125, validation loss 0.069202\n",
      "Iter 2430, training loss 0.000101, validation loss 0.068856\n",
      "Iter 2431, training loss 0.000089, validation loss 0.069006\n",
      "Iter 2432, training loss 0.000090, validation loss 0.069033\n",
      "Iter 2433, training loss 0.000101, validation loss 0.068854\n",
      "Iter 2434, training loss 0.000115, validation loss 0.069168\n",
      "Iter 2435, training loss 0.000126, validation loss 0.068775\n",
      "Iter 2436, training loss 0.000130, validation loss 0.069218\n",
      "Iter 2437, training loss 0.000128, validation loss 0.068761\n",
      "Iter 2438, training loss 0.000119, validation loss 0.069176\n",
      "Iter 2439, training loss 0.000108, validation loss 0.068813\n",
      "Iter 2440, training loss 0.000097, validation loss 0.069084\n",
      "Iter 2441, training loss 0.000090, validation loss 0.068902\n",
      "Iter 2442, training loss 0.000087, validation loss 0.068968\n",
      "Iter 2443, training loss 0.000088, validation loss 0.068995\n",
      "Iter 2444, training loss 0.000091, validation loss 0.068870\n",
      "Iter 2445, training loss 0.000096, validation loss 0.069055\n",
      "Iter 2446, training loss 0.000100, validation loss 0.068808\n",
      "Iter 2447, training loss 0.000103, validation loss 0.069092\n",
      "Iter 2448, training loss 0.000103, validation loss 0.068807\n",
      "Iter 2449, training loss 0.000101, validation loss 0.069094\n",
      "Iter 2450, training loss 0.000098, validation loss 0.068834\n",
      "Iter 2451, training loss 0.000094, validation loss 0.069062\n",
      "Iter 2452, training loss 0.000090, validation loss 0.068884\n",
      "Iter 2453, training loss 0.000087, validation loss 0.069008\n",
      "Iter 2454, training loss 0.000085, validation loss 0.068926\n",
      "Iter 2455, training loss 0.000085, validation loss 0.068940\n",
      "Iter 2456, training loss 0.000085, validation loss 0.068956\n",
      "Iter 2457, training loss 0.000086, validation loss 0.068886\n",
      "Iter 2458, training loss 0.000087, validation loss 0.068990\n",
      "Iter 2459, training loss 0.000088, validation loss 0.068864\n",
      "Iter 2460, training loss 0.000089, validation loss 0.069022\n",
      "Iter 2461, training loss 0.000090, validation loss 0.068857\n",
      "Iter 2462, training loss 0.000091, validation loss 0.069042\n",
      "Iter 2463, training loss 0.000091, validation loss 0.068862\n",
      "Iter 2464, training loss 0.000091, validation loss 0.069050\n",
      "Iter 2465, training loss 0.000090, validation loss 0.068864\n",
      "Iter 2466, training loss 0.000090, validation loss 0.069038\n",
      "Iter 2467, training loss 0.000089, validation loss 0.068861\n",
      "Iter 2468, training loss 0.000089, validation loss 0.069025\n",
      "Iter 2469, training loss 0.000088, validation loss 0.068865\n",
      "Iter 2470, training loss 0.000087, validation loss 0.069019\n",
      "Iter 2471, training loss 0.000087, validation loss 0.068874\n",
      "Iter 2472, training loss 0.000087, validation loss 0.069019\n",
      "Iter 2473, training loss 0.000086, validation loss 0.068881\n",
      "Iter 2474, training loss 0.000086, validation loss 0.069021\n",
      "Iter 2475, training loss 0.000086, validation loss 0.068882\n",
      "Iter 2476, training loss 0.000086, validation loss 0.069019\n",
      "Iter 2477, training loss 0.000086, validation loss 0.068872\n",
      "Iter 2478, training loss 0.000087, validation loss 0.069023\n",
      "Iter 2479, training loss 0.000088, validation loss 0.068866\n",
      "Iter 2480, training loss 0.000089, validation loss 0.069048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2481, training loss 0.000090, validation loss 0.068856\n",
      "Iter 2482, training loss 0.000092, validation loss 0.069076\n",
      "Iter 2483, training loss 0.000096, validation loss 0.068828\n",
      "Iter 2484, training loss 0.000100, validation loss 0.069118\n",
      "Iter 2485, training loss 0.000107, validation loss 0.068793\n",
      "Iter 2486, training loss 0.000116, validation loss 0.069193\n",
      "Iter 2487, training loss 0.000129, validation loss 0.068755\n",
      "Iter 2488, training loss 0.000147, validation loss 0.069308\n",
      "Iter 2489, training loss 0.000172, validation loss 0.068714\n",
      "Iter 2490, training loss 0.000204, validation loss 0.069479\n",
      "Iter 2491, training loss 0.000248, validation loss 0.068684\n",
      "Iter 2492, training loss 0.000300, validation loss 0.069734\n",
      "Iter 2493, training loss 0.000367, validation loss 0.068691\n",
      "Iter 2494, training loss 0.000428, validation loss 0.070029\n",
      "Iter 2495, training loss 0.000483, validation loss 0.068736\n",
      "Iter 2496, training loss 0.000492, validation loss 0.070188\n",
      "Iter 2497, training loss 0.000465, validation loss 0.068771\n",
      "Iter 2498, training loss 0.000380, validation loss 0.070010\n",
      "Iter 2499, training loss 0.000275, validation loss 0.068801\n",
      "Iter 2500, training loss 0.000169, validation loss 0.069508\n",
      "Iter 2501, training loss 0.000101, validation loss 0.068927\n",
      "Iter 2502, training loss 0.000081, validation loss 0.069013\n",
      "Iter 2503, training loss 0.000104, validation loss 0.069183\n",
      "Iter 2504, training loss 0.000152, validation loss 0.068735\n",
      "Iter 2505, training loss 0.000207, validation loss 0.069477\n",
      "Iter 2506, training loss 0.000258, validation loss 0.068667\n",
      "Iter 2507, training loss 0.000284, validation loss 0.069691\n",
      "Iter 2508, training loss 0.000282, validation loss 0.068703\n",
      "Iter 2509, training loss 0.000240, validation loss 0.069637\n",
      "Iter 2510, training loss 0.000182, validation loss 0.068799\n",
      "Iter 2511, training loss 0.000124, validation loss 0.069353\n",
      "Iter 2512, training loss 0.000088, validation loss 0.068989\n",
      "Iter 2513, training loss 0.000081, validation loss 0.069023\n",
      "Iter 2514, training loss 0.000099, validation loss 0.069214\n",
      "Iter 2515, training loss 0.000129, validation loss 0.068810\n",
      "Iter 2516, training loss 0.000157, validation loss 0.069382\n",
      "Iter 2517, training loss 0.000177, validation loss 0.068716\n",
      "Iter 2518, training loss 0.000180, validation loss 0.069428\n",
      "Iter 2519, training loss 0.000170, validation loss 0.068728\n",
      "Iter 2520, training loss 0.000146, validation loss 0.069344\n",
      "Iter 2521, training loss 0.000118, validation loss 0.068814\n",
      "Iter 2522, training loss 0.000093, validation loss 0.069160\n",
      "Iter 2523, training loss 0.000080, validation loss 0.068971\n",
      "Iter 2524, training loss 0.000079, validation loss 0.068985\n",
      "Iter 2525, training loss 0.000087, validation loss 0.069129\n",
      "Iter 2526, training loss 0.000100, validation loss 0.068847\n",
      "Iter 2527, training loss 0.000111, validation loss 0.069221\n",
      "Iter 2528, training loss 0.000118, validation loss 0.068786\n",
      "Iter 2529, training loss 0.000119, validation loss 0.069245\n",
      "Iter 2530, training loss 0.000115, validation loss 0.068793\n",
      "Iter 2531, training loss 0.000106, validation loss 0.069198\n",
      "Iter 2532, training loss 0.000095, validation loss 0.068849\n",
      "Iter 2533, training loss 0.000086, validation loss 0.069099\n",
      "Iter 2534, training loss 0.000079, validation loss 0.068935\n",
      "Iter 2535, training loss 0.000077, validation loss 0.068998\n",
      "Iter 2536, training loss 0.000078, validation loss 0.069026\n",
      "Iter 2537, training loss 0.000081, validation loss 0.068904\n",
      "Iter 2538, training loss 0.000085, validation loss 0.069083\n",
      "Iter 2539, training loss 0.000089, validation loss 0.068848\n",
      "Iter 2540, training loss 0.000092, validation loss 0.069125\n",
      "Iter 2541, training loss 0.000093, validation loss 0.068842\n",
      "Iter 2542, training loss 0.000092, validation loss 0.069138\n",
      "Iter 2543, training loss 0.000090, validation loss 0.068863\n",
      "Iter 2544, training loss 0.000087, validation loss 0.069119\n",
      "Iter 2545, training loss 0.000084, validation loss 0.068891\n",
      "Iter 2546, training loss 0.000081, validation loss 0.069071\n",
      "Iter 2547, training loss 0.000078, validation loss 0.068920\n",
      "Iter 2548, training loss 0.000076, validation loss 0.069016\n",
      "Iter 2549, training loss 0.000075, validation loss 0.068950\n",
      "Iter 2550, training loss 0.000075, validation loss 0.068968\n",
      "Iter 2551, training loss 0.000075, validation loss 0.068990\n",
      "Iter 2552, training loss 0.000075, validation loss 0.068946\n",
      "Iter 2553, training loss 0.000076, validation loss 0.069028\n",
      "Iter 2554, training loss 0.000077, validation loss 0.068927\n",
      "Iter 2555, training loss 0.000078, validation loss 0.069052\n",
      "Iter 2556, training loss 0.000079, validation loss 0.068911\n",
      "Iter 2557, training loss 0.000079, validation loss 0.069066\n",
      "Iter 2558, training loss 0.000080, validation loss 0.068893\n",
      "Iter 2559, training loss 0.000081, validation loss 0.069073\n",
      "Iter 2560, training loss 0.000081, validation loss 0.068880\n",
      "Iter 2561, training loss 0.000082, validation loss 0.069088\n",
      "Iter 2562, training loss 0.000083, validation loss 0.068874\n",
      "Iter 2563, training loss 0.000085, validation loss 0.069113\n",
      "Iter 2564, training loss 0.000086, validation loss 0.068868\n",
      "Iter 2565, training loss 0.000088, validation loss 0.069140\n",
      "Iter 2566, training loss 0.000091, validation loss 0.068848\n",
      "Iter 2567, training loss 0.000095, validation loss 0.069171\n",
      "Iter 2568, training loss 0.000100, validation loss 0.068822\n",
      "Iter 2569, training loss 0.000105, validation loss 0.069216\n",
      "Iter 2570, training loss 0.000113, validation loss 0.068793\n",
      "Iter 2571, training loss 0.000123, validation loss 0.069288\n",
      "Iter 2572, training loss 0.000135, validation loss 0.068763\n",
      "Iter 2573, training loss 0.000151, validation loss 0.069386\n",
      "Iter 2574, training loss 0.000172, validation loss 0.068738\n",
      "Iter 2575, training loss 0.000196, validation loss 0.069533\n",
      "Iter 2576, training loss 0.000227, validation loss 0.068723\n",
      "Iter 2577, training loss 0.000260, validation loss 0.069703\n",
      "Iter 2578, training loss 0.000298, validation loss 0.068714\n",
      "Iter 2579, training loss 0.000329, validation loss 0.069880\n",
      "Iter 2580, training loss 0.000357, validation loss 0.068732\n",
      "Iter 2581, training loss 0.000363, validation loss 0.069970\n",
      "Iter 2582, training loss 0.000353, validation loss 0.068753\n",
      "Iter 2583, training loss 0.000313, validation loss 0.069892\n",
      "Iter 2584, training loss 0.000259, validation loss 0.068785\n",
      "Iter 2585, training loss 0.000192, validation loss 0.069600\n",
      "Iter 2586, training loss 0.000133, validation loss 0.068853\n",
      "Iter 2587, training loss 0.000092, validation loss 0.069234\n",
      "Iter 2588, training loss 0.000073, validation loss 0.068987\n",
      "Iter 2589, training loss 0.000075, validation loss 0.068947\n",
      "Iter 2590, training loss 0.000091, validation loss 0.069179\n",
      "Iter 2591, training loss 0.000116, validation loss 0.068802\n",
      "Iter 2592, training loss 0.000142, validation loss 0.069370\n",
      "Iter 2593, training loss 0.000165, validation loss 0.068749\n",
      "Iter 2594, training loss 0.000180, validation loss 0.069505\n",
      "Iter 2595, training loss 0.000186, validation loss 0.068769\n",
      "Iter 2596, training loss 0.000178, validation loss 0.069540\n",
      "Iter 2597, training loss 0.000161, validation loss 0.068817\n",
      "Iter 2598, training loss 0.000136, validation loss 0.069433\n",
      "Iter 2599, training loss 0.000110, validation loss 0.068882\n",
      "Iter 2600, training loss 0.000089, validation loss 0.069231\n",
      "Iter 2601, training loss 0.000075, validation loss 0.068969\n",
      "Iter 2602, training loss 0.000070, validation loss 0.069036\n",
      "Iter 2603, training loss 0.000073, validation loss 0.069072\n",
      "Iter 2604, training loss 0.000080, validation loss 0.068896\n",
      "Iter 2605, training loss 0.000090, validation loss 0.069176\n",
      "Iter 2606, training loss 0.000100, validation loss 0.068829\n",
      "Iter 2607, training loss 0.000109, validation loss 0.069269\n",
      "Iter 2608, training loss 0.000116, validation loss 0.068811\n",
      "Iter 2609, training loss 0.000118, validation loss 0.069320\n",
      "Iter 2610, training loss 0.000118, validation loss 0.068824\n",
      "Iter 2611, training loss 0.000114, validation loss 0.069322\n",
      "Iter 2612, training loss 0.000107, validation loss 0.068853\n",
      "Iter 2613, training loss 0.000099, validation loss 0.069269\n",
      "Iter 2614, training loss 0.000090, validation loss 0.068890\n",
      "Iter 2615, training loss 0.000082, validation loss 0.069178\n",
      "Iter 2616, training loss 0.000076, validation loss 0.068929\n",
      "Iter 2617, training loss 0.000072, validation loss 0.069083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2618, training loss 0.000069, validation loss 0.068977\n",
      "Iter 2619, training loss 0.000069, validation loss 0.069011\n",
      "Iter 2620, training loss 0.000069, validation loss 0.069026\n",
      "Iter 2621, training loss 0.000070, validation loss 0.068959\n",
      "Iter 2622, training loss 0.000072, validation loss 0.069071\n",
      "Iter 2623, training loss 0.000073, validation loss 0.068924\n",
      "Iter 2624, training loss 0.000076, validation loss 0.069113\n",
      "Iter 2625, training loss 0.000078, validation loss 0.068905\n",
      "Iter 2626, training loss 0.000080, validation loss 0.069155\n",
      "Iter 2627, training loss 0.000083, validation loss 0.068894\n",
      "Iter 2628, training loss 0.000085, validation loss 0.069191\n",
      "Iter 2629, training loss 0.000088, validation loss 0.068878\n",
      "Iter 2630, training loss 0.000091, validation loss 0.069221\n",
      "Iter 2631, training loss 0.000095, validation loss 0.068858\n",
      "Iter 2632, training loss 0.000099, validation loss 0.069258\n",
      "Iter 2633, training loss 0.000106, validation loss 0.068827\n",
      "Iter 2634, training loss 0.000113, validation loss 0.069304\n",
      "Iter 2635, training loss 0.000122, validation loss 0.068796\n",
      "Iter 2636, training loss 0.000133, validation loss 0.069378\n",
      "Iter 2637, training loss 0.000148, validation loss 0.068774\n",
      "Iter 2638, training loss 0.000165, validation loss 0.069488\n",
      "Iter 2639, training loss 0.000187, validation loss 0.068755\n",
      "Iter 2640, training loss 0.000211, validation loss 0.069622\n",
      "Iter 2641, training loss 0.000241, validation loss 0.068737\n",
      "Iter 2642, training loss 0.000272, validation loss 0.069784\n",
      "Iter 2643, training loss 0.000307, validation loss 0.068742\n",
      "Iter 2644, training loss 0.000333, validation loss 0.069939\n",
      "Iter 2645, training loss 0.000353, validation loss 0.068755\n",
      "Iter 2646, training loss 0.000350, validation loss 0.069997\n",
      "Iter 2647, training loss 0.000334, validation loss 0.068772\n",
      "Iter 2648, training loss 0.000291, validation loss 0.069875\n",
      "Iter 2649, training loss 0.000237, validation loss 0.068792\n",
      "Iter 2650, training loss 0.000175, validation loss 0.069583\n",
      "Iter 2651, training loss 0.000122, validation loss 0.068868\n",
      "Iter 2652, training loss 0.000084, validation loss 0.069241\n",
      "Iter 2653, training loss 0.000067, validation loss 0.069005\n",
      "Iter 2654, training loss 0.000069, validation loss 0.068966\n",
      "Iter 2655, training loss 0.000084, validation loss 0.069184\n",
      "Iter 2656, training loss 0.000106, validation loss 0.068818\n",
      "Iter 2657, training loss 0.000129, validation loss 0.069367\n",
      "Iter 2658, training loss 0.000150, validation loss 0.068777\n",
      "Iter 2659, training loss 0.000165, validation loss 0.069512\n",
      "Iter 2660, training loss 0.000173, validation loss 0.068790\n",
      "Iter 2661, training loss 0.000169, validation loss 0.069547\n",
      "Iter 2662, training loss 0.000157, validation loss 0.068814\n",
      "Iter 2663, training loss 0.000137, validation loss 0.069460\n",
      "Iter 2664, training loss 0.000114, validation loss 0.068870\n",
      "Iter 2665, training loss 0.000093, validation loss 0.069296\n",
      "Iter 2666, training loss 0.000077, validation loss 0.068950\n",
      "Iter 2667, training loss 0.000067, validation loss 0.069110\n",
      "Iter 2668, training loss 0.000065, validation loss 0.069039\n",
      "Iter 2669, training loss 0.000067, validation loss 0.068968\n",
      "Iter 2670, training loss 0.000073, validation loss 0.069134\n",
      "Iter 2671, training loss 0.000081, validation loss 0.068882\n",
      "Iter 2672, training loss 0.000088, validation loss 0.069221\n",
      "Iter 2673, training loss 0.000096, validation loss 0.068849\n",
      "Iter 2674, training loss 0.000101, validation loss 0.069291\n",
      "Iter 2675, training loss 0.000104, validation loss 0.068845\n",
      "Iter 2676, training loss 0.000105, validation loss 0.069321\n",
      "Iter 2677, training loss 0.000104, validation loss 0.068855\n",
      "Iter 2678, training loss 0.000100, validation loss 0.069306\n",
      "Iter 2679, training loss 0.000095, validation loss 0.068875\n",
      "Iter 2680, training loss 0.000089, validation loss 0.069258\n",
      "Iter 2681, training loss 0.000083, validation loss 0.068902\n",
      "Iter 2682, training loss 0.000077, validation loss 0.069193\n",
      "Iter 2683, training loss 0.000073, validation loss 0.068928\n",
      "Iter 2684, training loss 0.000069, validation loss 0.069125\n",
      "Iter 2685, training loss 0.000067, validation loss 0.068958\n",
      "Iter 2686, training loss 0.000065, validation loss 0.069074\n",
      "Iter 2687, training loss 0.000064, validation loss 0.068991\n",
      "Iter 2688, training loss 0.000063, validation loss 0.069037\n",
      "Iter 2689, training loss 0.000063, validation loss 0.069026\n",
      "Iter 2690, training loss 0.000063, validation loss 0.069015\n",
      "Iter 2691, training loss 0.000063, validation loss 0.069059\n",
      "Iter 2692, training loss 0.000063, validation loss 0.068996\n",
      "Iter 2693, training loss 0.000064, validation loss 0.069084\n",
      "Iter 2694, training loss 0.000065, validation loss 0.068980\n",
      "Iter 2695, training loss 0.000065, validation loss 0.069106\n",
      "Iter 2696, training loss 0.000066, validation loss 0.068962\n",
      "Iter 2697, training loss 0.000068, validation loss 0.069128\n",
      "Iter 2698, training loss 0.000069, validation loss 0.068940\n",
      "Iter 2699, training loss 0.000072, validation loss 0.069157\n",
      "Iter 2700, training loss 0.000075, validation loss 0.068912\n",
      "Iter 2701, training loss 0.000079, validation loss 0.069201\n",
      "Iter 2702, training loss 0.000085, validation loss 0.068881\n",
      "Iter 2703, training loss 0.000093, validation loss 0.069274\n",
      "Iter 2704, training loss 0.000105, validation loss 0.068838\n",
      "Iter 2705, training loss 0.000121, validation loss 0.069385\n",
      "Iter 2706, training loss 0.000145, validation loss 0.068788\n",
      "Iter 2707, training loss 0.000178, validation loss 0.069561\n",
      "Iter 2708, training loss 0.000226, validation loss 0.068746\n",
      "Iter 2709, training loss 0.000292, validation loss 0.069861\n",
      "Iter 2710, training loss 0.000387, validation loss 0.068737\n",
      "Iter 2711, training loss 0.000503, validation loss 0.070307\n",
      "Iter 2712, training loss 0.000647, validation loss 0.068809\n",
      "Iter 2713, training loss 0.000762, validation loss 0.070814\n",
      "Iter 2714, training loss 0.000846, validation loss 0.068919\n",
      "Iter 2715, training loss 0.000789, validation loss 0.070896\n",
      "Iter 2716, training loss 0.000628, validation loss 0.068906\n",
      "Iter 2717, training loss 0.000369, validation loss 0.070173\n",
      "Iter 2718, training loss 0.000157, validation loss 0.068945\n",
      "Iter 2719, training loss 0.000067, validation loss 0.069232\n",
      "Iter 2720, training loss 0.000106, validation loss 0.069420\n",
      "Iter 2721, training loss 0.000228, validation loss 0.068760\n",
      "Iter 2722, training loss 0.000375, validation loss 0.070005\n",
      "Iter 2723, training loss 0.000504, validation loss 0.068699\n",
      "Iter 2724, training loss 0.000540, validation loss 0.070333\n",
      "Iter 2725, training loss 0.000485, validation loss 0.068747\n",
      "Iter 2726, training loss 0.000327, validation loss 0.069973\n",
      "Iter 2727, training loss 0.000166, validation loss 0.068864\n",
      "Iter 2728, training loss 0.000074, validation loss 0.069256\n",
      "Iter 2729, training loss 0.000081, validation loss 0.069296\n",
      "Iter 2730, training loss 0.000157, validation loss 0.068841\n",
      "Iter 2731, training loss 0.000241, validation loss 0.069798\n",
      "Iter 2732, training loss 0.000290, validation loss 0.068774\n",
      "Iter 2733, training loss 0.000271, validation loss 0.069845\n",
      "Iter 2734, training loss 0.000208, validation loss 0.068768\n",
      "Iter 2735, training loss 0.000128, validation loss 0.069441\n",
      "Iter 2736, training loss 0.000074, validation loss 0.068929\n",
      "Iter 2737, training loss 0.000063, validation loss 0.068996\n",
      "Iter 2738, training loss 0.000089, validation loss 0.069258\n",
      "Iter 2739, training loss 0.000127, validation loss 0.068812\n",
      "Iter 2740, training loss 0.000154, validation loss 0.069524\n",
      "Iter 2741, training loss 0.000156, validation loss 0.068803\n",
      "Iter 2742, training loss 0.000132, validation loss 0.069464\n",
      "Iter 2743, training loss 0.000099, validation loss 0.068878\n",
      "Iter 2744, training loss 0.000071, validation loss 0.069204\n",
      "Iter 2745, training loss 0.000060, validation loss 0.069040\n",
      "Iter 2746, training loss 0.000066, validation loss 0.068948\n",
      "Iter 2747, training loss 0.000081, validation loss 0.069225\n",
      "Iter 2748, training loss 0.000096, validation loss 0.068847\n",
      "Iter 2749, training loss 0.000101, validation loss 0.069332\n",
      "Iter 2750, training loss 0.000096, validation loss 0.068863\n",
      "Iter 2751, training loss 0.000084, validation loss 0.069267\n",
      "Iter 2752, training loss 0.000070, validation loss 0.068933\n",
      "Iter 2753, training loss 0.000061, validation loss 0.069099\n",
      "Iter 2754, training loss 0.000058, validation loss 0.069029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2755, training loss 0.000062, validation loss 0.068952\n",
      "Iter 2756, training loss 0.000067, validation loss 0.069144\n",
      "Iter 2757, training loss 0.000073, validation loss 0.068897\n",
      "Iter 2758, training loss 0.000076, validation loss 0.069206\n",
      "Iter 2759, training loss 0.000074, validation loss 0.068899\n",
      "Iter 2760, training loss 0.000070, validation loss 0.069184\n",
      "Iter 2761, training loss 0.000065, validation loss 0.068943\n",
      "Iter 2762, training loss 0.000061, validation loss 0.069099\n",
      "Iter 2763, training loss 0.000058, validation loss 0.068997\n",
      "Iter 2764, training loss 0.000057, validation loss 0.069004\n",
      "Iter 2765, training loss 0.000059, validation loss 0.069064\n",
      "Iter 2766, training loss 0.000061, validation loss 0.068947\n",
      "Iter 2767, training loss 0.000063, validation loss 0.069113\n",
      "Iter 2768, training loss 0.000064, validation loss 0.068924\n",
      "Iter 2769, training loss 0.000064, validation loss 0.069126\n",
      "Iter 2770, training loss 0.000063, validation loss 0.068929\n",
      "Iter 2771, training loss 0.000062, validation loss 0.069106\n",
      "Iter 2772, training loss 0.000060, validation loss 0.068957\n",
      "Iter 2773, training loss 0.000058, validation loss 0.069074\n",
      "Iter 2774, training loss 0.000057, validation loss 0.069002\n",
      "Iter 2775, training loss 0.000056, validation loss 0.069032\n",
      "Iter 2776, training loss 0.000056, validation loss 0.069034\n",
      "Iter 2777, training loss 0.000057, validation loss 0.068989\n",
      "Iter 2778, training loss 0.000057, validation loss 0.069055\n",
      "Iter 2779, training loss 0.000058, validation loss 0.068963\n",
      "Iter 2780, training loss 0.000058, validation loss 0.069072\n",
      "Iter 2781, training loss 0.000059, validation loss 0.068958\n",
      "Iter 2782, training loss 0.000059, validation loss 0.069089\n",
      "Iter 2783, training loss 0.000058, validation loss 0.068966\n",
      "Iter 2784, training loss 0.000058, validation loss 0.069089\n",
      "Iter 2785, training loss 0.000058, validation loss 0.068970\n",
      "Iter 2786, training loss 0.000057, validation loss 0.069076\n",
      "Iter 2787, training loss 0.000057, validation loss 0.068977\n",
      "Iter 2788, training loss 0.000056, validation loss 0.069062\n",
      "Iter 2789, training loss 0.000056, validation loss 0.068988\n",
      "Iter 2790, training loss 0.000056, validation loss 0.069050\n",
      "Iter 2791, training loss 0.000055, validation loss 0.069000\n",
      "Iter 2792, training loss 0.000055, validation loss 0.069037\n",
      "Iter 2793, training loss 0.000055, validation loss 0.069010\n",
      "Iter 2794, training loss 0.000055, validation loss 0.069031\n",
      "Iter 2795, training loss 0.000055, validation loss 0.069024\n",
      "Iter 2796, training loss 0.000055, validation loss 0.069026\n",
      "Iter 2797, training loss 0.000055, validation loss 0.069030\n",
      "Iter 2798, training loss 0.000055, validation loss 0.069019\n",
      "Iter 2799, training loss 0.000054, validation loss 0.069037\n",
      "Iter 2800, training loss 0.000054, validation loss 0.069014\n",
      "Iter 2801, training loss 0.000054, validation loss 0.069045\n",
      "Iter 2802, training loss 0.000054, validation loss 0.069009\n",
      "Iter 2803, training loss 0.000055, validation loss 0.069057\n",
      "Iter 2804, training loss 0.000055, validation loss 0.069002\n",
      "Iter 2805, training loss 0.000055, validation loss 0.069067\n",
      "Iter 2806, training loss 0.000055, validation loss 0.068988\n",
      "Iter 2807, training loss 0.000056, validation loss 0.069083\n",
      "Iter 2808, training loss 0.000056, validation loss 0.068973\n",
      "Iter 2809, training loss 0.000057, validation loss 0.069106\n",
      "Iter 2810, training loss 0.000059, validation loss 0.068950\n",
      "Iter 2811, training loss 0.000061, validation loss 0.069143\n",
      "Iter 2812, training loss 0.000064, validation loss 0.068922\n",
      "Iter 2813, training loss 0.000068, validation loss 0.069195\n",
      "Iter 2814, training loss 0.000075, validation loss 0.068884\n",
      "Iter 2815, training loss 0.000083, validation loss 0.069272\n",
      "Iter 2816, training loss 0.000096, validation loss 0.068835\n",
      "Iter 2817, training loss 0.000112, validation loss 0.069387\n",
      "Iter 2818, training loss 0.000136, validation loss 0.068781\n",
      "Iter 2819, training loss 0.000170, validation loss 0.069571\n",
      "Iter 2820, training loss 0.000218, validation loss 0.068733\n",
      "Iter 2821, training loss 0.000285, validation loss 0.069871\n",
      "Iter 2822, training loss 0.000377, validation loss 0.068722\n",
      "Iter 2823, training loss 0.000491, validation loss 0.070323\n",
      "Iter 2824, training loss 0.000633, validation loss 0.068789\n",
      "Iter 2825, training loss 0.000757, validation loss 0.070837\n",
      "Iter 2826, training loss 0.000859, validation loss 0.068905\n",
      "Iter 2827, training loss 0.000840, validation loss 0.071030\n",
      "Iter 2828, training loss 0.000723, validation loss 0.068919\n",
      "Iter 2829, training loss 0.000483, validation loss 0.070434\n",
      "Iter 2830, training loss 0.000245, validation loss 0.068884\n",
      "Iter 2831, training loss 0.000090, validation loss 0.069455\n",
      "Iter 2832, training loss 0.000061, validation loss 0.069246\n",
      "Iter 2833, training loss 0.000135, validation loss 0.068840\n",
      "Iter 2834, training loss 0.000265, validation loss 0.069819\n",
      "Iter 2835, training loss 0.000407, validation loss 0.068683\n",
      "Iter 2836, training loss 0.000503, validation loss 0.070305\n",
      "Iter 2837, training loss 0.000534, validation loss 0.068736\n",
      "Iter 2838, training loss 0.000449, validation loss 0.070248\n",
      "Iter 2839, training loss 0.000308, validation loss 0.068782\n",
      "Iter 2840, training loss 0.000156, validation loss 0.069624\n",
      "Iter 2841, training loss 0.000067, validation loss 0.069040\n",
      "Iter 2842, training loss 0.000068, validation loss 0.069022\n",
      "Iter 2843, training loss 0.000135, validation loss 0.069567\n",
      "Iter 2844, training loss 0.000220, validation loss 0.068797\n",
      "Iter 2845, training loss 0.000276, validation loss 0.069906\n",
      "Iter 2846, training loss 0.000286, validation loss 0.068738\n",
      "Iter 2847, training loss 0.000239, validation loss 0.069781\n",
      "Iter 2848, training loss 0.000166, validation loss 0.068765\n",
      "Iter 2849, training loss 0.000096, validation loss 0.069346\n",
      "Iter 2850, training loss 0.000058, validation loss 0.068985\n",
      "Iter 2851, training loss 0.000059, validation loss 0.068981\n",
      "Iter 2852, training loss 0.000088, validation loss 0.069334\n",
      "Iter 2853, training loss 0.000124, validation loss 0.068821\n",
      "Iter 2854, training loss 0.000147, validation loss 0.069545\n",
      "Iter 2855, training loss 0.000149, validation loss 0.068794\n",
      "Iter 2856, training loss 0.000129, validation loss 0.069484\n",
      "Iter 2857, training loss 0.000099, validation loss 0.068848\n",
      "Iter 2858, training loss 0.000071, validation loss 0.069243\n",
      "Iter 2859, training loss 0.000055, validation loss 0.068986\n",
      "Iter 2860, training loss 0.000053, validation loss 0.069008\n",
      "Iter 2861, training loss 0.000062, validation loss 0.069185\n",
      "Iter 2862, training loss 0.000076, validation loss 0.068891\n",
      "Iter 2863, training loss 0.000086, validation loss 0.069324\n",
      "Iter 2864, training loss 0.000090, validation loss 0.068861\n",
      "Iter 2865, training loss 0.000086, validation loss 0.069318\n",
      "Iter 2866, training loss 0.000077, validation loss 0.068876\n",
      "Iter 2867, training loss 0.000066, validation loss 0.069200\n",
      "Iter 2868, training loss 0.000057, validation loss 0.068943\n",
      "Iter 2869, training loss 0.000051, validation loss 0.069064\n",
      "Iter 2870, training loss 0.000051, validation loss 0.069048\n",
      "Iter 2871, training loss 0.000054, validation loss 0.068965\n",
      "Iter 2872, training loss 0.000058, validation loss 0.069150\n",
      "Iter 2873, training loss 0.000062, validation loss 0.068922\n",
      "Iter 2874, training loss 0.000064, validation loss 0.069200\n",
      "Iter 2875, training loss 0.000064, validation loss 0.068909\n",
      "Iter 2876, training loss 0.000062, validation loss 0.069177\n",
      "Iter 2877, training loss 0.000059, validation loss 0.068917\n",
      "Iter 2878, training loss 0.000055, validation loss 0.069117\n",
      "Iter 2879, training loss 0.000053, validation loss 0.068957\n",
      "Iter 2880, training loss 0.000050, validation loss 0.069060\n",
      "Iter 2881, training loss 0.000050, validation loss 0.069017\n",
      "Iter 2882, training loss 0.000050, validation loss 0.069012\n",
      "Iter 2883, training loss 0.000050, validation loss 0.069069\n",
      "Iter 2884, training loss 0.000051, validation loss 0.068974\n",
      "Iter 2885, training loss 0.000052, validation loss 0.069100\n",
      "Iter 2886, training loss 0.000053, validation loss 0.068951\n",
      "Iter 2887, training loss 0.000054, validation loss 0.069114\n",
      "Iter 2888, training loss 0.000054, validation loss 0.068943\n",
      "Iter 2889, training loss 0.000054, validation loss 0.069112\n",
      "Iter 2890, training loss 0.000053, validation loss 0.068947\n",
      "Iter 2891, training loss 0.000052, validation loss 0.069099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2892, training loss 0.000052, validation loss 0.068960\n",
      "Iter 2893, training loss 0.000051, validation loss 0.069083\n",
      "Iter 2894, training loss 0.000050, validation loss 0.068979\n",
      "Iter 2895, training loss 0.000050, validation loss 0.069068\n",
      "Iter 2896, training loss 0.000049, validation loss 0.068996\n",
      "Iter 2897, training loss 0.000049, validation loss 0.069050\n",
      "Iter 2898, training loss 0.000048, validation loss 0.069010\n",
      "Iter 2899, training loss 0.000048, validation loss 0.069032\n",
      "Iter 2900, training loss 0.000048, validation loss 0.069022\n",
      "Iter 2901, training loss 0.000048, validation loss 0.069020\n",
      "Iter 2902, training loss 0.000048, validation loss 0.069034\n",
      "Iter 2903, training loss 0.000048, validation loss 0.069010\n",
      "Iter 2904, training loss 0.000048, validation loss 0.069044\n",
      "Iter 2905, training loss 0.000048, validation loss 0.069003\n",
      "Iter 2906, training loss 0.000048, validation loss 0.069056\n",
      "Iter 2907, training loss 0.000048, validation loss 0.068996\n",
      "Iter 2908, training loss 0.000049, validation loss 0.069067\n",
      "Iter 2909, training loss 0.000049, validation loss 0.068987\n",
      "Iter 2910, training loss 0.000049, validation loss 0.069079\n",
      "Iter 2911, training loss 0.000050, validation loss 0.068973\n",
      "Iter 2912, training loss 0.000050, validation loss 0.069095\n",
      "Iter 2913, training loss 0.000051, validation loss 0.068956\n",
      "Iter 2914, training loss 0.000052, validation loss 0.069122\n",
      "Iter 2915, training loss 0.000054, validation loss 0.068935\n",
      "Iter 2916, training loss 0.000057, validation loss 0.069159\n",
      "Iter 2917, training loss 0.000060, validation loss 0.068906\n",
      "Iter 2918, training loss 0.000065, validation loss 0.069213\n",
      "Iter 2919, training loss 0.000071, validation loss 0.068871\n",
      "Iter 2920, training loss 0.000079, validation loss 0.069289\n",
      "Iter 2921, training loss 0.000091, validation loss 0.068825\n",
      "Iter 2922, training loss 0.000108, validation loss 0.069399\n",
      "Iter 2923, training loss 0.000131, validation loss 0.068772\n",
      "Iter 2924, training loss 0.000162, validation loss 0.069571\n",
      "Iter 2925, training loss 0.000206, validation loss 0.068723\n",
      "Iter 2926, training loss 0.000265, validation loss 0.069848\n",
      "Iter 2927, training loss 0.000347, validation loss 0.068710\n",
      "Iter 2928, training loss 0.000448, validation loss 0.070271\n",
      "Iter 2929, training loss 0.000576, validation loss 0.068755\n",
      "Iter 2930, training loss 0.000700, validation loss 0.070765\n",
      "Iter 2931, training loss 0.000824, validation loss 0.068868\n",
      "Iter 2932, training loss 0.000866, validation loss 0.071107\n",
      "Iter 2933, training loss 0.000834, validation loss 0.068923\n",
      "Iter 2934, training loss 0.000659, validation loss 0.070773\n",
      "Iter 2935, training loss 0.000431, validation loss 0.068849\n",
      "Iter 2936, training loss 0.000208, validation loss 0.069866\n",
      "Iter 2937, training loss 0.000075, validation loss 0.069022\n",
      "Iter 2938, training loss 0.000055, validation loss 0.069063\n",
      "Iter 2939, training loss 0.000127, validation loss 0.069525\n",
      "Iter 2940, training loss 0.000249, validation loss 0.068710\n",
      "Iter 2941, training loss 0.000379, validation loss 0.070083\n",
      "Iter 2942, training loss 0.000492, validation loss 0.068681\n",
      "Iter 2943, training loss 0.000529, validation loss 0.070396\n",
      "Iter 2944, training loss 0.000494, validation loss 0.068744\n",
      "Iter 2945, training loss 0.000363, validation loss 0.070138\n",
      "Iter 2946, training loss 0.000210, validation loss 0.068823\n",
      "Iter 2947, training loss 0.000091, validation loss 0.069447\n",
      "Iter 2948, training loss 0.000050, validation loss 0.069154\n",
      "Iter 2949, training loss 0.000084, validation loss 0.068932\n",
      "Iter 2950, training loss 0.000158, validation loss 0.069641\n",
      "Iter 2951, training loss 0.000234, validation loss 0.068754\n",
      "Iter 2952, training loss 0.000278, validation loss 0.069920\n",
      "Iter 2953, training loss 0.000282, validation loss 0.068711\n",
      "Iter 2954, training loss 0.000238, validation loss 0.069792\n",
      "Iter 2955, training loss 0.000171, validation loss 0.068743\n",
      "Iter 2956, training loss 0.000103, validation loss 0.069399\n",
      "Iter 2957, training loss 0.000059, validation loss 0.068944\n",
      "Iter 2958, training loss 0.000048, validation loss 0.069039\n",
      "Iter 2959, training loss 0.000067, validation loss 0.069277\n",
      "Iter 2960, training loss 0.000099, validation loss 0.068845\n",
      "Iter 2961, training loss 0.000128, validation loss 0.069506\n",
      "Iter 2962, training loss 0.000143, validation loss 0.068770\n",
      "Iter 2963, training loss 0.000137, validation loss 0.069514\n",
      "Iter 2964, training loss 0.000118, validation loss 0.068781\n",
      "Iter 2965, training loss 0.000091, validation loss 0.069352\n",
      "Iter 2966, training loss 0.000067, validation loss 0.068881\n",
      "Iter 2967, training loss 0.000051, validation loss 0.069138\n",
      "Iter 2968, training loss 0.000046, validation loss 0.069058\n",
      "Iter 2969, training loss 0.000050, validation loss 0.068972\n",
      "Iter 2970, training loss 0.000061, validation loss 0.069228\n",
      "Iter 2971, training loss 0.000072, validation loss 0.068876\n",
      "Iter 2972, training loss 0.000079, validation loss 0.069309\n",
      "Iter 2973, training loss 0.000082, validation loss 0.068833\n",
      "Iter 2974, training loss 0.000078, validation loss 0.069289\n",
      "Iter 2975, training loss 0.000071, validation loss 0.068847\n",
      "Iter 2976, training loss 0.000062, validation loss 0.069204\n",
      "Iter 2977, training loss 0.000054, validation loss 0.068910\n",
      "Iter 2978, training loss 0.000048, validation loss 0.069098\n",
      "Iter 2979, training loss 0.000045, validation loss 0.069009\n",
      "Iter 2980, training loss 0.000045, validation loss 0.069013\n",
      "Iter 2981, training loss 0.000047, validation loss 0.069108\n",
      "Iter 2982, training loss 0.000050, validation loss 0.068946\n",
      "Iter 2983, training loss 0.000053, validation loss 0.069160\n",
      "Iter 2984, training loss 0.000056, validation loss 0.068895\n",
      "Iter 2985, training loss 0.000057, validation loss 0.069173\n",
      "Iter 2986, training loss 0.000058, validation loss 0.068880\n",
      "Iter 2987, training loss 0.000056, validation loss 0.069169\n",
      "Iter 2988, training loss 0.000055, validation loss 0.068901\n",
      "Iter 2989, training loss 0.000052, validation loss 0.069150\n",
      "Iter 2990, training loss 0.000049, validation loss 0.068940\n",
      "Iter 2991, training loss 0.000047, validation loss 0.069114\n",
      "Iter 2992, training loss 0.000045, validation loss 0.068979\n",
      "Iter 2993, training loss 0.000044, validation loss 0.069064\n",
      "Iter 2994, training loss 0.000043, validation loss 0.069009\n",
      "Iter 2995, training loss 0.000043, validation loss 0.069014\n",
      "Iter 2996, training loss 0.000043, validation loss 0.069036\n",
      "Iter 2997, training loss 0.000044, validation loss 0.068979\n",
      "Iter 2998, training loss 0.000044, validation loss 0.069065\n",
      "Iter 2999, training loss 0.000045, validation loss 0.068960\n",
      "Iter 3000, training loss 0.000046, validation loss 0.069095\n",
      "Iter 3001, training loss 0.000047, validation loss 0.068950\n",
      "Iter 3002, training loss 0.000047, validation loss 0.069116\n",
      "Iter 3003, training loss 0.000048, validation loss 0.068939\n",
      "Iter 3004, training loss 0.000049, validation loss 0.069128\n",
      "Iter 3005, training loss 0.000049, validation loss 0.068928\n",
      "Iter 3006, training loss 0.000050, validation loss 0.069138\n",
      "Iter 3007, training loss 0.000051, validation loss 0.068918\n",
      "Iter 3008, training loss 0.000051, validation loss 0.069150\n",
      "Iter 3009, training loss 0.000052, validation loss 0.068906\n",
      "Iter 3010, training loss 0.000054, validation loss 0.069171\n",
      "Iter 3011, training loss 0.000056, validation loss 0.068894\n",
      "Iter 3012, training loss 0.000058, validation loss 0.069203\n",
      "Iter 3013, training loss 0.000061, validation loss 0.068877\n",
      "Iter 3014, training loss 0.000065, validation loss 0.069245\n",
      "Iter 3015, training loss 0.000070, validation loss 0.068853\n",
      "Iter 3016, training loss 0.000076, validation loss 0.069299\n",
      "Iter 3017, training loss 0.000085, validation loss 0.068819\n",
      "Iter 3018, training loss 0.000095, validation loss 0.069375\n",
      "Iter 3019, training loss 0.000109, validation loss 0.068780\n",
      "Iter 3020, training loss 0.000127, validation loss 0.069488\n",
      "Iter 3021, training loss 0.000152, validation loss 0.068742\n",
      "Iter 3022, training loss 0.000183, validation loss 0.069661\n",
      "Iter 3023, training loss 0.000226, validation loss 0.068711\n",
      "Iter 3024, training loss 0.000279, validation loss 0.069914\n",
      "Iter 3025, training loss 0.000348, validation loss 0.068696\n",
      "Iter 3026, training loss 0.000429, validation loss 0.070256\n",
      "Iter 3027, training loss 0.000526, validation loss 0.068729\n",
      "Iter 3028, training loss 0.000614, validation loss 0.070637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3029, training loss 0.000698, validation loss 0.068793\n",
      "Iter 3030, training loss 0.000724, validation loss 0.070856\n",
      "Iter 3031, training loss 0.000707, validation loss 0.068831\n",
      "Iter 3032, training loss 0.000603, validation loss 0.070680\n",
      "Iter 3033, training loss 0.000458, validation loss 0.068807\n",
      "Iter 3034, training loss 0.000286, validation loss 0.070044\n",
      "Iter 3035, training loss 0.000144, validation loss 0.068852\n",
      "Iter 3036, training loss 0.000061, validation loss 0.069295\n",
      "Iter 3037, training loss 0.000045, validation loss 0.069132\n",
      "Iter 3038, training loss 0.000082, validation loss 0.068847\n",
      "Iter 3039, training loss 0.000150, validation loss 0.069557\n",
      "Iter 3040, training loss 0.000232, validation loss 0.068673\n",
      "Iter 3041, training loss 0.000307, validation loss 0.069947\n",
      "Iter 3042, training loss 0.000365, validation loss 0.068676\n",
      "Iter 3043, training loss 0.000378, validation loss 0.070146\n",
      "Iter 3044, training loss 0.000353, validation loss 0.068715\n",
      "Iter 3045, training loss 0.000283, validation loss 0.069975\n",
      "Iter 3046, training loss 0.000195, validation loss 0.068788\n",
      "Iter 3047, training loss 0.000112, validation loss 0.069529\n",
      "Iter 3048, training loss 0.000058, validation loss 0.068978\n",
      "Iter 3049, training loss 0.000042, validation loss 0.069085\n",
      "Iter 3050, training loss 0.000060, validation loss 0.069267\n",
      "Iter 3051, training loss 0.000097, validation loss 0.068817\n",
      "Iter 3052, training loss 0.000139, validation loss 0.069532\n",
      "Iter 3053, training loss 0.000177, validation loss 0.068698\n",
      "Iter 3054, training loss 0.000201, validation loss 0.069688\n",
      "Iter 3055, training loss 0.000210, validation loss 0.068690\n",
      "Iter 3056, training loss 0.000196, validation loss 0.069700\n",
      "Iter 3057, training loss 0.000168, validation loss 0.068742\n",
      "Iter 3058, training loss 0.000128, validation loss 0.069531\n",
      "Iter 3059, training loss 0.000090, validation loss 0.068846\n",
      "Iter 3060, training loss 0.000060, validation loss 0.069261\n",
      "Iter 3061, training loss 0.000043, validation loss 0.068999\n",
      "Iter 3062, training loss 0.000042, validation loss 0.069008\n",
      "Iter 3063, training loss 0.000051, validation loss 0.069166\n",
      "Iter 3064, training loss 0.000066, validation loss 0.068840\n",
      "Iter 3065, training loss 0.000082, validation loss 0.069312\n",
      "Iter 3066, training loss 0.000098, validation loss 0.068767\n",
      "Iter 3067, training loss 0.000107, validation loss 0.069412\n",
      "Iter 3068, training loss 0.000112, validation loss 0.068762\n",
      "Iter 3069, training loss 0.000109, validation loss 0.069439\n",
      "Iter 3070, training loss 0.000102, validation loss 0.068796\n",
      "Iter 3071, training loss 0.000089, validation loss 0.069384\n",
      "Iter 3072, training loss 0.000076, validation loss 0.068848\n",
      "Iter 3073, training loss 0.000063, validation loss 0.069258\n",
      "Iter 3074, training loss 0.000052, validation loss 0.068902\n",
      "Iter 3075, training loss 0.000044, validation loss 0.069111\n",
      "Iter 3076, training loss 0.000040, validation loss 0.068974\n",
      "Iter 3077, training loss 0.000039, validation loss 0.068997\n",
      "Iter 3078, training loss 0.000041, validation loss 0.069059\n",
      "Iter 3079, training loss 0.000044, validation loss 0.068926\n",
      "Iter 3080, training loss 0.000048, validation loss 0.069140\n",
      "Iter 3081, training loss 0.000052, validation loss 0.068883\n",
      "Iter 3082, training loss 0.000056, validation loss 0.069203\n",
      "Iter 3083, training loss 0.000060, validation loss 0.068860\n",
      "Iter 3084, training loss 0.000063, validation loss 0.069246\n",
      "Iter 3085, training loss 0.000066, validation loss 0.068843\n",
      "Iter 3086, training loss 0.000068, validation loss 0.069270\n",
      "Iter 3087, training loss 0.000070, validation loss 0.068832\n",
      "Iter 3088, training loss 0.000071, validation loss 0.069284\n",
      "Iter 3089, training loss 0.000072, validation loss 0.068826\n",
      "Iter 3090, training loss 0.000072, validation loss 0.069290\n",
      "Iter 3091, training loss 0.000073, validation loss 0.068825\n",
      "Iter 3092, training loss 0.000074, validation loss 0.069302\n",
      "Iter 3093, training loss 0.000075, validation loss 0.068823\n",
      "Iter 3094, training loss 0.000077, validation loss 0.069315\n",
      "Iter 3095, training loss 0.000079, validation loss 0.068811\n",
      "Iter 3096, training loss 0.000082, validation loss 0.069331\n",
      "Iter 3097, training loss 0.000086, validation loss 0.068794\n",
      "Iter 3098, training loss 0.000091, validation loss 0.069369\n",
      "Iter 3099, training loss 0.000098, validation loss 0.068775\n",
      "Iter 3100, training loss 0.000107, validation loss 0.069428\n",
      "Iter 3101, training loss 0.000118, validation loss 0.068750\n",
      "Iter 3102, training loss 0.000132, validation loss 0.069512\n",
      "Iter 3103, training loss 0.000150, validation loss 0.068723\n",
      "Iter 3104, training loss 0.000172, validation loss 0.069634\n",
      "Iter 3105, training loss 0.000200, validation loss 0.068695\n",
      "Iter 3106, training loss 0.000234, validation loss 0.069807\n",
      "Iter 3107, training loss 0.000279, validation loss 0.068674\n",
      "Iter 3108, training loss 0.000331, validation loss 0.070044\n",
      "Iter 3109, training loss 0.000398, validation loss 0.068676\n",
      "Iter 3110, training loss 0.000469, validation loss 0.070352\n",
      "Iter 3111, training loss 0.000551, validation loss 0.068710\n",
      "Iter 3112, training loss 0.000614, validation loss 0.070644\n",
      "Iter 3113, training loss 0.000667, validation loss 0.068759\n",
      "Iter 3114, training loss 0.000664, validation loss 0.070763\n",
      "Iter 3115, training loss 0.000625, validation loss 0.068773\n",
      "Iter 3116, training loss 0.000525, validation loss 0.070523\n",
      "Iter 3117, training loss 0.000401, validation loss 0.068748\n",
      "Iter 3118, training loss 0.000259, validation loss 0.069943\n",
      "Iter 3119, training loss 0.000142, validation loss 0.068791\n",
      "Iter 3120, training loss 0.000066, validation loss 0.069312\n",
      "Iter 3121, training loss 0.000039, validation loss 0.069026\n",
      "Iter 3122, training loss 0.000053, validation loss 0.068884\n",
      "Iter 3123, training loss 0.000094, validation loss 0.069364\n",
      "Iter 3124, training loss 0.000150, validation loss 0.068689\n",
      "Iter 3125, training loss 0.000207, validation loss 0.069703\n",
      "Iter 3126, training loss 0.000260, validation loss 0.068646\n",
      "Iter 3127, training loss 0.000295, validation loss 0.069947\n",
      "Iter 3128, training loss 0.000310, validation loss 0.068668\n",
      "Iter 3129, training loss 0.000292, validation loss 0.069967\n",
      "Iter 3130, training loss 0.000252, validation loss 0.068696\n",
      "Iter 3131, training loss 0.000193, validation loss 0.069738\n",
      "Iter 3132, training loss 0.000133, validation loss 0.068778\n",
      "Iter 3133, training loss 0.000083, validation loss 0.069391\n",
      "Iter 3134, training loss 0.000051, validation loss 0.068929\n",
      "Iter 3135, training loss 0.000038, validation loss 0.069064\n",
      "Iter 3136, training loss 0.000042, validation loss 0.069112\n",
      "Iter 3137, training loss 0.000058, validation loss 0.068836\n",
      "Iter 3138, training loss 0.000080, validation loss 0.069295\n",
      "Iter 3139, training loss 0.000104, validation loss 0.068722\n",
      "Iter 3140, training loss 0.000124, validation loss 0.069452\n",
      "Iter 3141, training loss 0.000140, validation loss 0.068696\n",
      "Iter 3142, training loss 0.000149, validation loss 0.069552\n",
      "Iter 3143, training loss 0.000151, validation loss 0.068704\n",
      "Iter 3144, training loss 0.000144, validation loss 0.069549\n",
      "Iter 3145, training loss 0.000131, validation loss 0.068727\n",
      "Iter 3146, training loss 0.000112, validation loss 0.069449\n",
      "Iter 3147, training loss 0.000093, validation loss 0.068770\n",
      "Iter 3148, training loss 0.000075, validation loss 0.069300\n",
      "Iter 3149, training loss 0.000059, validation loss 0.068833\n",
      "Iter 3150, training loss 0.000048, validation loss 0.069146\n",
      "Iter 3151, training loss 0.000040, validation loss 0.068911\n",
      "Iter 3152, training loss 0.000037, validation loss 0.069018\n",
      "Iter 3153, training loss 0.000036, validation loss 0.068999\n",
      "Iter 3154, training loss 0.000038, validation loss 0.068931\n",
      "Iter 3155, training loss 0.000041, validation loss 0.069083\n",
      "Iter 3156, training loss 0.000045, validation loss 0.068870\n",
      "Iter 3157, training loss 0.000050, validation loss 0.069152\n",
      "Iter 3158, training loss 0.000055, validation loss 0.068823\n",
      "Iter 3159, training loss 0.000060, validation loss 0.069205\n",
      "Iter 3160, training loss 0.000065, validation loss 0.068789\n",
      "Iter 3161, training loss 0.000070, validation loss 0.069251\n",
      "Iter 3162, training loss 0.000075, validation loss 0.068770\n",
      "Iter 3163, training loss 0.000079, validation loss 0.069299\n",
      "Iter 3164, training loss 0.000084, validation loss 0.068757\n",
      "Iter 3165, training loss 0.000090, validation loss 0.069346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3166, training loss 0.000096, validation loss 0.068740\n",
      "Iter 3167, training loss 0.000104, validation loss 0.069402\n",
      "Iter 3168, training loss 0.000114, validation loss 0.068721\n",
      "Iter 3169, training loss 0.000125, validation loss 0.069477\n",
      "Iter 3170, training loss 0.000140, validation loss 0.068696\n",
      "Iter 3171, training loss 0.000158, validation loss 0.069578\n",
      "Iter 3172, training loss 0.000181, validation loss 0.068664\n",
      "Iter 3173, training loss 0.000209, validation loss 0.069718\n",
      "Iter 3174, training loss 0.000246, validation loss 0.068635\n",
      "Iter 3175, training loss 0.000290, validation loss 0.069922\n",
      "Iter 3176, training loss 0.000346, validation loss 0.068629\n",
      "Iter 3177, training loss 0.000409, validation loss 0.070208\n",
      "Iter 3178, training loss 0.000487, validation loss 0.068648\n",
      "Iter 3179, training loss 0.000563, validation loss 0.070521\n",
      "Iter 3180, training loss 0.000646, validation loss 0.068701\n",
      "Iter 3181, training loss 0.000700, validation loss 0.070809\n",
      "Iter 3182, training loss 0.000738, validation loss 0.068757\n",
      "Iter 3183, training loss 0.000717, validation loss 0.070864\n",
      "Iter 3184, training loss 0.000654, validation loss 0.068754\n",
      "Iter 3185, training loss 0.000526, validation loss 0.070513\n",
      "Iter 3186, training loss 0.000379, validation loss 0.068704\n",
      "Iter 3187, training loss 0.000231, validation loss 0.069867\n",
      "Iter 3188, training loss 0.000118, validation loss 0.068783\n",
      "Iter 3189, training loss 0.000052, validation loss 0.069212\n",
      "Iter 3190, training loss 0.000037, validation loss 0.069028\n",
      "Iter 3191, training loss 0.000061, validation loss 0.068799\n",
      "Iter 3192, training loss 0.000110, validation loss 0.069400\n",
      "Iter 3193, training loss 0.000171, validation loss 0.068636\n",
      "Iter 3194, training loss 0.000230, validation loss 0.069748\n",
      "Iter 3195, training loss 0.000283, validation loss 0.068606\n",
      "Iter 3196, training loss 0.000313, validation loss 0.069977\n",
      "Iter 3197, training loss 0.000323, validation loss 0.068627\n",
      "Iter 3198, training loss 0.000300, validation loss 0.069964\n",
      "Iter 3199, training loss 0.000255, validation loss 0.068651\n",
      "Iter 3200, training loss 0.000192, validation loss 0.069712\n",
      "Iter 3201, training loss 0.000130, validation loss 0.068739\n",
      "Iter 3202, training loss 0.000079, validation loss 0.069353\n",
      "Iter 3203, training loss 0.000047, validation loss 0.068887\n",
      "Iter 3204, training loss 0.000035, validation loss 0.069017\n",
      "Iter 3205, training loss 0.000040, validation loss 0.069074\n",
      "Iter 3206, training loss 0.000056, validation loss 0.068793\n",
      "Iter 3207, training loss 0.000078, validation loss 0.069260\n",
      "Iter 3208, training loss 0.000102, validation loss 0.068679\n",
      "Iter 3209, training loss 0.000123, validation loss 0.069428\n",
      "Iter 3210, training loss 0.000140, validation loss 0.068654\n",
      "Iter 3211, training loss 0.000151, validation loss 0.069535\n",
      "Iter 3212, training loss 0.000155, validation loss 0.068651\n",
      "Iter 3213, training loss 0.000151, validation loss 0.069540\n",
      "Iter 3214, training loss 0.000142, validation loss 0.068665\n",
      "Iter 3215, training loss 0.000126, validation loss 0.069465\n",
      "Iter 3216, training loss 0.000109, validation loss 0.068696\n",
      "Iter 3217, training loss 0.000090, validation loss 0.069333\n",
      "Iter 3218, training loss 0.000073, validation loss 0.068749\n",
      "Iter 3219, training loss 0.000059, validation loss 0.069194\n",
      "Iter 3220, training loss 0.000048, validation loss 0.068816\n",
      "Iter 3221, training loss 0.000040, validation loss 0.069065\n",
      "Iter 3222, training loss 0.000036, validation loss 0.068890\n",
      "Iter 3223, training loss 0.000033, validation loss 0.068966\n",
      "Iter 3224, training loss 0.000033, validation loss 0.068965\n",
      "Iter 3225, training loss 0.000035, validation loss 0.068888\n",
      "Iter 3226, training loss 0.000037, validation loss 0.069029\n",
      "Iter 3227, training loss 0.000041, validation loss 0.068833\n",
      "Iter 3228, training loss 0.000044, validation loss 0.069085\n",
      "Iter 3229, training loss 0.000048, validation loss 0.068791\n",
      "Iter 3230, training loss 0.000052, validation loss 0.069133\n",
      "Iter 3231, training loss 0.000057, validation loss 0.068760\n",
      "Iter 3232, training loss 0.000061, validation loss 0.069187\n",
      "Iter 3233, training loss 0.000067, validation loss 0.068738\n",
      "Iter 3234, training loss 0.000073, validation loss 0.069247\n",
      "Iter 3235, training loss 0.000081, validation loss 0.068717\n",
      "Iter 3236, training loss 0.000089, validation loss 0.069322\n",
      "Iter 3237, training loss 0.000101, validation loss 0.068689\n",
      "Iter 3238, training loss 0.000115, validation loss 0.069416\n",
      "Iter 3239, training loss 0.000135, validation loss 0.068649\n",
      "Iter 3240, training loss 0.000159, validation loss 0.069549\n",
      "Iter 3241, training loss 0.000192, validation loss 0.068609\n",
      "Iter 3242, training loss 0.000233, validation loss 0.069754\n",
      "Iter 3243, training loss 0.000291, validation loss 0.068575\n",
      "Iter 3244, training loss 0.000364, validation loss 0.070065\n",
      "Iter 3245, training loss 0.000465, validation loss 0.068580\n",
      "Iter 3246, training loss 0.000581, validation loss 0.070521\n",
      "Iter 3247, training loss 0.000732, validation loss 0.068662\n",
      "Iter 3248, training loss 0.000888, validation loss 0.071107\n",
      "Iter 3249, training loss 0.001064, validation loss 0.068824\n",
      "Iter 3250, training loss 0.001171, validation loss 0.071604\n",
      "Iter 3251, training loss 0.001224, validation loss 0.068915\n",
      "Iter 3252, training loss 0.001129, validation loss 0.071575\n",
      "Iter 3253, training loss 0.000933, validation loss 0.068831\n",
      "Iter 3254, training loss 0.000636, validation loss 0.070735\n",
      "Iter 3255, training loss 0.000348, validation loss 0.068706\n",
      "Iter 3256, training loss 0.000134, validation loss 0.069604\n",
      "Iter 3257, training loss 0.000040, validation loss 0.068956\n",
      "Iter 3258, training loss 0.000062, validation loss 0.068787\n",
      "Iter 3259, training loss 0.000163, validation loss 0.069538\n",
      "Iter 3260, training loss 0.000303, validation loss 0.068540\n",
      "Iter 3261, training loss 0.000438, validation loss 0.070185\n",
      "Iter 3262, training loss 0.000558, validation loss 0.068551\n",
      "Iter 3263, training loss 0.000609, validation loss 0.070535\n",
      "Iter 3264, training loss 0.000593, validation loss 0.068601\n",
      "Iter 3265, training loss 0.000483, validation loss 0.070330\n",
      "Iter 3266, training loss 0.000327, validation loss 0.068618\n",
      "Iter 3267, training loss 0.000171, validation loss 0.069643\n",
      "Iter 3268, training loss 0.000067, validation loss 0.068824\n",
      "Iter 3269, training loss 0.000035, validation loss 0.068999\n",
      "Iter 3270, training loss 0.000066, validation loss 0.069259\n",
      "Iter 3271, training loss 0.000135, validation loss 0.068641\n",
      "Iter 3272, training loss 0.000211, validation loss 0.069663\n",
      "Iter 3273, training loss 0.000276, validation loss 0.068525\n",
      "Iter 3274, training loss 0.000310, validation loss 0.069888\n",
      "Iter 3275, training loss 0.000315, validation loss 0.068517\n",
      "Iter 3276, training loss 0.000279, validation loss 0.069818\n",
      "Iter 3277, training loss 0.000219, validation loss 0.068562\n",
      "Iter 3278, training loss 0.000146, validation loss 0.069496\n",
      "Iter 3279, training loss 0.000083, validation loss 0.068714\n",
      "Iter 3280, training loss 0.000043, validation loss 0.069094\n",
      "Iter 3281, training loss 0.000033, validation loss 0.068961\n",
      "Iter 3282, training loss 0.000047, validation loss 0.068777\n",
      "Iter 3283, training loss 0.000075, validation loss 0.069214\n",
      "Iter 3284, training loss 0.000106, validation loss 0.068602\n",
      "Iter 3285, training loss 0.000132, validation loss 0.069386\n",
      "Iter 3286, training loss 0.000148, validation loss 0.068543\n",
      "Iter 3287, training loss 0.000151, validation loss 0.069449\n",
      "Iter 3288, training loss 0.000144, validation loss 0.068564\n",
      "Iter 3289, training loss 0.000125, validation loss 0.069396\n",
      "Iter 3290, training loss 0.000101, validation loss 0.068639\n",
      "Iter 3291, training loss 0.000076, validation loss 0.069245\n",
      "Iter 3292, training loss 0.000054, validation loss 0.068753\n",
      "Iter 3293, training loss 0.000039, validation loss 0.069047\n",
      "Iter 3294, training loss 0.000032, validation loss 0.068873\n",
      "Iter 3295, training loss 0.000032, validation loss 0.068856\n",
      "Iter 3296, training loss 0.000037, validation loss 0.068985\n",
      "Iter 3297, training loss 0.000045, validation loss 0.068724\n",
      "Iter 3298, training loss 0.000054, validation loss 0.069078\n",
      "Iter 3299, training loss 0.000062, validation loss 0.068652\n",
      "Iter 3300, training loss 0.000069, validation loss 0.069150\n",
      "Iter 3301, training loss 0.000074, validation loss 0.068639\n",
      "Iter 3302, training loss 0.000075, validation loss 0.069204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3303, training loss 0.000075, validation loss 0.068664\n",
      "Iter 3304, training loss 0.000073, validation loss 0.069217\n",
      "Iter 3305, training loss 0.000069, validation loss 0.068690\n",
      "Iter 3306, training loss 0.000064, validation loss 0.069179\n",
      "Iter 3307, training loss 0.000058, validation loss 0.068707\n",
      "Iter 3308, training loss 0.000053, validation loss 0.069111\n",
      "Iter 3309, training loss 0.000048, validation loss 0.068724\n",
      "Iter 3310, training loss 0.000044, validation loss 0.069042\n",
      "Iter 3311, training loss 0.000040, validation loss 0.068748\n",
      "Iter 3312, training loss 0.000037, validation loss 0.068984\n",
      "Iter 3313, training loss 0.000035, validation loss 0.068784\n",
      "Iter 3314, training loss 0.000033, validation loss 0.068947\n",
      "Iter 3315, training loss 0.000031, validation loss 0.068824\n",
      "Iter 3316, training loss 0.000031, validation loss 0.068919\n",
      "Iter 3317, training loss 0.000030, validation loss 0.068854\n",
      "Iter 3318, training loss 0.000030, validation loss 0.068893\n",
      "Iter 3319, training loss 0.000030, validation loss 0.068876\n",
      "Iter 3320, training loss 0.000030, validation loss 0.068869\n",
      "Iter 3321, training loss 0.000030, validation loss 0.068889\n",
      "Iter 3322, training loss 0.000030, validation loss 0.068844\n",
      "Iter 3323, training loss 0.000030, validation loss 0.068900\n",
      "Iter 3324, training loss 0.000030, validation loss 0.068827\n",
      "Iter 3325, training loss 0.000031, validation loss 0.068918\n",
      "Iter 3326, training loss 0.000031, validation loss 0.068815\n",
      "Iter 3327, training loss 0.000032, validation loss 0.068943\n",
      "Iter 3328, training loss 0.000033, validation loss 0.068803\n",
      "Iter 3329, training loss 0.000034, validation loss 0.068973\n",
      "Iter 3330, training loss 0.000035, validation loss 0.068784\n",
      "Iter 3331, training loss 0.000037, validation loss 0.069012\n",
      "Iter 3332, training loss 0.000041, validation loss 0.068750\n",
      "Iter 3333, training loss 0.000045, validation loss 0.069063\n",
      "Iter 3334, training loss 0.000052, validation loss 0.068700\n",
      "Iter 3335, training loss 0.000062, validation loss 0.069146\n",
      "Iter 3336, training loss 0.000076, validation loss 0.068638\n",
      "Iter 3337, training loss 0.000097, validation loss 0.069290\n",
      "Iter 3338, training loss 0.000129, validation loss 0.068567\n",
      "Iter 3339, training loss 0.000177, validation loss 0.069543\n",
      "Iter 3340, training loss 0.000251, validation loss 0.068502\n",
      "Iter 3341, training loss 0.000363, validation loss 0.070010\n",
      "Iter 3342, training loss 0.000542, validation loss 0.068506\n",
      "Iter 3343, training loss 0.000802, validation loss 0.070877\n",
      "Iter 3344, training loss 0.001203, validation loss 0.068757\n",
      "Iter 3345, training loss 0.001735, validation loss 0.072411\n",
      "Iter 3346, training loss 0.002451, validation loss 0.069481\n",
      "Iter 3347, training loss 0.003040, validation loss 0.074342\n",
      "Iter 3348, training loss 0.003395, validation loss 0.070211\n",
      "Iter 3349, training loss 0.002939, validation loss 0.074362\n",
      "Iter 3350, training loss 0.001942, validation loss 0.069437\n",
      "Iter 3351, training loss 0.000780, validation loss 0.071133\n",
      "Iter 3352, training loss 0.000121, validation loss 0.068996\n",
      "Iter 3353, training loss 0.000174, validation loss 0.068804\n",
      "Iter 3354, training loss 0.000733, validation loss 0.070888\n",
      "Iter 3355, training loss 0.001464, validation loss 0.068821\n",
      "Iter 3356, training loss 0.002084, validation loss 0.072921\n",
      "Iter 3357, training loss 0.002388, validation loss 0.069399\n",
      "Iter 3358, training loss 0.001954, validation loss 0.072726\n",
      "Iter 3359, training loss 0.001123, validation loss 0.068835\n",
      "Iter 3360, training loss 0.000328, validation loss 0.070029\n",
      "Iter 3361, training loss 0.000066, validation loss 0.069127\n",
      "Iter 3362, training loss 0.000360, validation loss 0.068590\n",
      "Iter 3363, training loss 0.000884, validation loss 0.071042\n",
      "Iter 3364, training loss 0.001247, validation loss 0.068669\n",
      "Iter 3365, training loss 0.001199, validation loss 0.071436\n",
      "Iter 3366, training loss 0.000814, validation loss 0.068501\n",
      "Iter 3367, training loss 0.000322, validation loss 0.069893\n",
      "Iter 3368, training loss 0.000060, validation loss 0.068854\n",
      "Iter 3369, training loss 0.000129, validation loss 0.068669\n",
      "Iter 3370, training loss 0.000394, validation loss 0.070067\n",
      "Iter 3371, training loss 0.000644, validation loss 0.068446\n",
      "Iter 3372, training loss 0.000726, validation loss 0.070646\n",
      "Iter 3373, training loss 0.000597, validation loss 0.068452\n",
      "Iter 3374, training loss 0.000320, validation loss 0.069773\n",
      "Iter 3375, training loss 0.000093, validation loss 0.068556\n",
      "Iter 3376, training loss 0.000046, validation loss 0.068722\n",
      "Iter 3377, training loss 0.000162, validation loss 0.069468\n",
      "Iter 3378, training loss 0.000318, validation loss 0.068464\n",
      "Iter 3379, training loss 0.000400, validation loss 0.070050\n",
      "Iter 3380, training loss 0.000363, validation loss 0.068431\n",
      "Iter 3381, training loss 0.000236, validation loss 0.069595\n",
      "Iter 3382, training loss 0.000104, validation loss 0.068523\n",
      "Iter 3383, training loss 0.000036, validation loss 0.068806\n",
      "Iter 3384, training loss 0.000058, validation loss 0.068948\n",
      "Iter 3385, training loss 0.000133, validation loss 0.068393\n",
      "Iter 3386, training loss 0.000197, validation loss 0.069447\n",
      "Iter 3387, training loss 0.000211, validation loss 0.068410\n",
      "Iter 3388, training loss 0.000170, validation loss 0.069442\n",
      "Iter 3389, training loss 0.000105, validation loss 0.068547\n",
      "Iter 3390, training loss 0.000050, validation loss 0.069000\n",
      "Iter 3391, training loss 0.000032, validation loss 0.068823\n",
      "Iter 3392, training loss 0.000052, validation loss 0.068591\n",
      "Iter 3393, training loss 0.000089, validation loss 0.069088\n",
      "Iter 3394, training loss 0.000116, validation loss 0.068380\n",
      "Iter 3395, training loss 0.000118, validation loss 0.069175\n",
      "Iter 3396, training loss 0.000097, validation loss 0.068451\n",
      "Iter 3397, training loss 0.000067, validation loss 0.069030\n",
      "Iter 3398, training loss 0.000041, validation loss 0.068620\n",
      "Iter 3399, training loss 0.000030, validation loss 0.068740\n",
      "Iter 3400, training loss 0.000036, validation loss 0.068836\n",
      "Iter 3401, training loss 0.000051, validation loss 0.068543\n",
      "Iter 3402, training loss 0.000066, validation loss 0.069001\n",
      "Iter 3403, training loss 0.000071, validation loss 0.068479\n",
      "Iter 3404, training loss 0.000066, validation loss 0.069009\n",
      "Iter 3405, training loss 0.000054, validation loss 0.068544\n",
      "Iter 3406, training loss 0.000041, validation loss 0.068886\n",
      "Iter 3407, training loss 0.000031, validation loss 0.068658\n",
      "Iter 3408, training loss 0.000028, validation loss 0.068695\n",
      "Iter 3409, training loss 0.000032, validation loss 0.068780\n",
      "Iter 3410, training loss 0.000039, validation loss 0.068570\n",
      "Iter 3411, training loss 0.000044, validation loss 0.068878\n",
      "Iter 3412, training loss 0.000046, validation loss 0.068531\n",
      "Iter 3413, training loss 0.000045, validation loss 0.068887\n",
      "Iter 3414, training loss 0.000041, validation loss 0.068571\n",
      "Iter 3415, training loss 0.000035, validation loss 0.068833\n",
      "Iter 3416, training loss 0.000030, validation loss 0.068649\n",
      "Iter 3417, training loss 0.000028, validation loss 0.068727\n",
      "Iter 3418, training loss 0.000028, validation loss 0.068725\n",
      "Iter 3419, training loss 0.000030, validation loss 0.068638\n",
      "Iter 3420, training loss 0.000032, validation loss 0.068790\n",
      "Iter 3421, training loss 0.000034, validation loss 0.068588\n",
      "Iter 3422, training loss 0.000035, validation loss 0.068808\n",
      "Iter 3423, training loss 0.000034, validation loss 0.068577\n",
      "Iter 3424, training loss 0.000033, validation loss 0.068794\n",
      "Iter 3425, training loss 0.000031, validation loss 0.068610\n",
      "Iter 3426, training loss 0.000029, validation loss 0.068758\n",
      "Iter 3427, training loss 0.000028, validation loss 0.068659\n",
      "Iter 3428, training loss 0.000027, validation loss 0.068714\n",
      "Iter 3429, training loss 0.000027, validation loss 0.068709\n",
      "Iter 3430, training loss 0.000027, validation loss 0.068671\n",
      "Iter 3431, training loss 0.000028, validation loss 0.068740\n",
      "Iter 3432, training loss 0.000028, validation loss 0.068637\n",
      "Iter 3433, training loss 0.000029, validation loss 0.068758\n",
      "Iter 3434, training loss 0.000029, validation loss 0.068626\n",
      "Iter 3435, training loss 0.000029, validation loss 0.068764\n",
      "Iter 3436, training loss 0.000029, validation loss 0.068629\n",
      "Iter 3437, training loss 0.000028, validation loss 0.068755\n",
      "Iter 3438, training loss 0.000028, validation loss 0.068642\n",
      "Iter 3439, training loss 0.000027, validation loss 0.068739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3440, training loss 0.000027, validation loss 0.068658\n",
      "Iter 3441, training loss 0.000026, validation loss 0.068716\n",
      "Iter 3442, training loss 0.000026, validation loss 0.068679\n",
      "Iter 3443, training loss 0.000026, validation loss 0.068701\n",
      "Iter 3444, training loss 0.000026, validation loss 0.068700\n",
      "Iter 3445, training loss 0.000026, validation loss 0.068686\n",
      "Iter 3446, training loss 0.000026, validation loss 0.068712\n",
      "Iter 3447, training loss 0.000026, validation loss 0.068671\n",
      "Iter 3448, training loss 0.000026, validation loss 0.068720\n",
      "Iter 3449, training loss 0.000026, validation loss 0.068661\n",
      "Iter 3450, training loss 0.000026, validation loss 0.068728\n",
      "Iter 3451, training loss 0.000026, validation loss 0.068660\n",
      "Iter 3452, training loss 0.000026, validation loss 0.068739\n",
      "Iter 3453, training loss 0.000027, validation loss 0.068659\n",
      "Iter 3454, training loss 0.000027, validation loss 0.068742\n",
      "Iter 3455, training loss 0.000027, validation loss 0.068653\n",
      "Iter 3456, training loss 0.000027, validation loss 0.068743\n",
      "Iter 3457, training loss 0.000027, validation loss 0.068649\n",
      "Iter 3458, training loss 0.000027, validation loss 0.068747\n",
      "Iter 3459, training loss 0.000027, validation loss 0.068644\n",
      "Iter 3460, training loss 0.000027, validation loss 0.068755\n",
      "Iter 3461, training loss 0.000027, validation loss 0.068640\n",
      "Iter 3462, training loss 0.000027, validation loss 0.068765\n",
      "Iter 3463, training loss 0.000028, validation loss 0.068632\n",
      "Iter 3464, training loss 0.000028, validation loss 0.068778\n",
      "Iter 3465, training loss 0.000029, validation loss 0.068620\n",
      "Iter 3466, training loss 0.000030, validation loss 0.068797\n",
      "Iter 3467, training loss 0.000031, validation loss 0.068603\n",
      "Iter 3468, training loss 0.000033, validation loss 0.068825\n",
      "Iter 3469, training loss 0.000035, validation loss 0.068578\n",
      "Iter 3470, training loss 0.000038, validation loss 0.068866\n",
      "Iter 3471, training loss 0.000042, validation loss 0.068545\n",
      "Iter 3472, training loss 0.000048, validation loss 0.068927\n",
      "Iter 3473, training loss 0.000056, validation loss 0.068500\n",
      "Iter 3474, training loss 0.000067, validation loss 0.069021\n",
      "Iter 3475, training loss 0.000082, validation loss 0.068446\n",
      "Iter 3476, training loss 0.000104, validation loss 0.069168\n",
      "Iter 3477, training loss 0.000136, validation loss 0.068381\n",
      "Iter 3478, training loss 0.000181, validation loss 0.069404\n",
      "Iter 3479, training loss 0.000247, validation loss 0.068320\n",
      "Iter 3480, training loss 0.000340, validation loss 0.069797\n",
      "Iter 3481, training loss 0.000477, validation loss 0.068305\n",
      "Iter 3482, training loss 0.000663, validation loss 0.070461\n",
      "Iter 3483, training loss 0.000934, validation loss 0.068435\n",
      "Iter 3484, training loss 0.001285, validation loss 0.071554\n",
      "Iter 3485, training loss 0.001760, validation loss 0.068855\n",
      "Iter 3486, training loss 0.002267, validation loss 0.073093\n",
      "Iter 3487, training loss 0.002804, validation loss 0.069545\n",
      "Iter 3488, training loss 0.003065, validation loss 0.074305\n",
      "Iter 3489, training loss 0.003049, validation loss 0.069782\n",
      "Iter 3490, training loss 0.002495, validation loss 0.073578\n",
      "Iter 3491, training loss 0.001648, validation loss 0.069011\n",
      "Iter 3492, training loss 0.000762, validation loss 0.070908\n",
      "Iter 3493, training loss 0.000186, validation loss 0.068652\n",
      "Iter 3494, training loss 0.000050, validation loss 0.068826\n",
      "Iter 3495, training loss 0.000287, validation loss 0.069801\n",
      "Iter 3496, training loss 0.000731, validation loss 0.068352\n",
      "Iter 3497, training loss 0.001225, validation loss 0.071434\n",
      "Iter 3498, training loss 0.001671, validation loss 0.068712\n",
      "Iter 3499, training loss 0.001854, validation loss 0.072403\n",
      "Iter 3500, training loss 0.001761, validation loss 0.068839\n",
      "Iter 3501, training loss 0.001319, validation loss 0.071626\n",
      "Iter 3502, training loss 0.000741, validation loss 0.068450\n",
      "Iter 3503, training loss 0.000256, validation loss 0.069653\n",
      "Iter 3504, training loss 0.000049, validation loss 0.068709\n",
      "Iter 3505, training loss 0.000136, validation loss 0.068410\n",
      "Iter 3506, training loss 0.000400, validation loss 0.069870\n",
      "Iter 3507, training loss 0.000681, validation loss 0.068233\n",
      "Iter 3508, training loss 0.000851, validation loss 0.070715\n",
      "Iter 3509, training loss 0.000870, validation loss 0.068313\n",
      "Iter 3510, training loss 0.000715, validation loss 0.070504\n",
      "Iter 3511, training loss 0.000478, validation loss 0.068282\n",
      "Iter 3512, training loss 0.000233, validation loss 0.069525\n",
      "Iter 3513, training loss 0.000072, validation loss 0.068463\n",
      "Iter 3514, training loss 0.000033, validation loss 0.068613\n",
      "Iter 3515, training loss 0.000097, validation loss 0.069045\n",
      "Iter 3516, training loss 0.000209, validation loss 0.068232\n",
      "Iter 3517, training loss 0.000309, validation loss 0.069583\n",
      "Iter 3518, training loss 0.000355, validation loss 0.068163\n",
      "Iter 3519, training loss 0.000331, validation loss 0.069650\n",
      "Iter 3520, training loss 0.000258, validation loss 0.068215\n",
      "Iter 3521, training loss 0.000163, validation loss 0.069264\n",
      "Iter 3522, training loss 0.000081, validation loss 0.068380\n",
      "Iter 3523, training loss 0.000034, validation loss 0.068733\n",
      "Iter 3524, training loss 0.000030, validation loss 0.068665\n",
      "Iter 3525, training loss 0.000058, validation loss 0.068357\n",
      "Iter 3526, training loss 0.000100, validation loss 0.068972\n",
      "Iter 3527, training loss 0.000138, validation loss 0.068204\n",
      "Iter 3528, training loss 0.000159, validation loss 0.069159\n",
      "Iter 3529, training loss 0.000158, validation loss 0.068211\n",
      "Iter 3530, training loss 0.000137, validation loss 0.069140\n",
      "Iter 3531, training loss 0.000104, validation loss 0.068309\n",
      "Iter 3532, training loss 0.000069, validation loss 0.068928\n",
      "Iter 3533, training loss 0.000042, validation loss 0.068445\n",
      "Iter 3534, training loss 0.000028, validation loss 0.068639\n",
      "Iter 3535, training loss 0.000027, validation loss 0.068592\n",
      "Iter 3536, training loss 0.000035, validation loss 0.068398\n",
      "Iter 3537, training loss 0.000049, validation loss 0.068731\n",
      "Iter 3538, training loss 0.000063, validation loss 0.068275\n",
      "Iter 3539, training loss 0.000073, validation loss 0.068848\n",
      "Iter 3540, training loss 0.000077, validation loss 0.068271\n",
      "Iter 3541, training loss 0.000076, validation loss 0.068897\n",
      "Iter 3542, training loss 0.000069, validation loss 0.068319\n",
      "Iter 3543, training loss 0.000059, validation loss 0.068845\n",
      "Iter 3544, training loss 0.000049, validation loss 0.068375\n",
      "Iter 3545, training loss 0.000039, validation loss 0.068725\n",
      "Iter 3546, training loss 0.000031, validation loss 0.068433\n",
      "Iter 3547, training loss 0.000026, validation loss 0.068590\n",
      "Iter 3548, training loss 0.000024, validation loss 0.068505\n",
      "Iter 3549, training loss 0.000025, validation loss 0.068490\n",
      "Iter 3550, training loss 0.000026, validation loss 0.068590\n",
      "Iter 3551, training loss 0.000029, validation loss 0.068432\n",
      "Iter 3552, training loss 0.000032, validation loss 0.068660\n",
      "Iter 3553, training loss 0.000035, validation loss 0.068399\n",
      "Iter 3554, training loss 0.000037, validation loss 0.068700\n",
      "Iter 3555, training loss 0.000039, validation loss 0.068380\n",
      "Iter 3556, training loss 0.000039, validation loss 0.068715\n",
      "Iter 3557, training loss 0.000040, validation loss 0.068375\n",
      "Iter 3558, training loss 0.000039, validation loss 0.068717\n",
      "Iter 3559, training loss 0.000039, validation loss 0.068382\n",
      "Iter 3560, training loss 0.000038, validation loss 0.068708\n",
      "Iter 3561, training loss 0.000037, validation loss 0.068389\n",
      "Iter 3562, training loss 0.000035, validation loss 0.068689\n",
      "Iter 3563, training loss 0.000034, validation loss 0.068393\n",
      "Iter 3564, training loss 0.000033, validation loss 0.068666\n",
      "Iter 3565, training loss 0.000032, validation loss 0.068397\n",
      "Iter 3566, training loss 0.000032, validation loss 0.068651\n",
      "Iter 3567, training loss 0.000031, validation loss 0.068406\n",
      "Iter 3568, training loss 0.000031, validation loss 0.068651\n",
      "Iter 3569, training loss 0.000031, validation loss 0.068417\n",
      "Iter 3570, training loss 0.000031, validation loss 0.068659\n",
      "Iter 3571, training loss 0.000031, validation loss 0.068422\n",
      "Iter 3572, training loss 0.000031, validation loss 0.068669\n",
      "Iter 3573, training loss 0.000032, validation loss 0.068411\n",
      "Iter 3574, training loss 0.000033, validation loss 0.068680\n",
      "Iter 3575, training loss 0.000035, validation loss 0.068388\n",
      "Iter 3576, training loss 0.000038, validation loss 0.068705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3577, training loss 0.000041, validation loss 0.068359\n",
      "Iter 3578, training loss 0.000046, validation loss 0.068753\n",
      "Iter 3579, training loss 0.000052, validation loss 0.068324\n",
      "Iter 3580, training loss 0.000061, validation loss 0.068834\n",
      "Iter 3581, training loss 0.000074, validation loss 0.068279\n",
      "Iter 3582, training loss 0.000092, validation loss 0.068960\n",
      "Iter 3583, training loss 0.000117, validation loss 0.068223\n",
      "Iter 3584, training loss 0.000152, validation loss 0.069157\n",
      "Iter 3585, training loss 0.000203, validation loss 0.068159\n",
      "Iter 3586, training loss 0.000274, validation loss 0.069477\n",
      "Iter 3587, training loss 0.000379, validation loss 0.068115\n",
      "Iter 3588, training loss 0.000527, validation loss 0.070023\n",
      "Iter 3589, training loss 0.000746, validation loss 0.068170\n",
      "Iter 3590, training loss 0.001050, validation loss 0.070985\n",
      "Iter 3591, training loss 0.001494, validation loss 0.068497\n",
      "Iter 3592, training loss 0.002065, validation loss 0.072606\n",
      "Iter 3593, training loss 0.002830, validation loss 0.069319\n",
      "Iter 3594, training loss 0.003610, validation loss 0.074852\n",
      "Iter 3595, training loss 0.004397, validation loss 0.070455\n",
      "Iter 3596, training loss 0.004706, validation loss 0.076413\n",
      "Iter 3597, training loss 0.004489, validation loss 0.070636\n",
      "Iter 3598, training loss 0.003448, validation loss 0.074803\n",
      "Iter 3599, training loss 0.002062, validation loss 0.069069\n",
      "Iter 3600, training loss 0.000791, validation loss 0.070754\n",
      "Iter 3601, training loss 0.000122, validation loss 0.068529\n",
      "Iter 3602, training loss 0.000149, validation loss 0.068400\n",
      "Iter 3603, training loss 0.000681, validation loss 0.070433\n",
      "Iter 3604, training loss 0.001426, validation loss 0.068400\n",
      "Iter 3605, training loss 0.002153, validation loss 0.072751\n",
      "Iter 3606, training loss 0.002706, validation loss 0.069172\n",
      "Iter 3607, training loss 0.002736, validation loss 0.073610\n",
      "Iter 3608, training loss 0.002309, validation loss 0.068995\n",
      "Iter 3609, training loss 0.001480, validation loss 0.071744\n",
      "Iter 3610, training loss 0.000649, validation loss 0.068242\n",
      "Iter 3611, training loss 0.000137, validation loss 0.069133\n",
      "Iter 3612, training loss 0.000084, validation loss 0.068903\n",
      "Iter 3613, training loss 0.000390, validation loss 0.068099\n",
      "Iter 3614, training loss 0.000830, validation loss 0.070477\n",
      "Iter 3615, training loss 0.001180, validation loss 0.068144\n",
      "Iter 3616, training loss 0.001284, validation loss 0.071179\n",
      "Iter 3617, training loss 0.001151, validation loss 0.068165\n",
      "Iter 3618, training loss 0.000817, validation loss 0.070464\n",
      "Iter 3619, training loss 0.000442, validation loss 0.068071\n",
      "Iter 3620, training loss 0.000153, validation loss 0.069115\n",
      "Iter 3621, training loss 0.000035, validation loss 0.068468\n",
      "Iter 3622, training loss 0.000086, validation loss 0.068245\n",
      "Iter 3623, training loss 0.000241, validation loss 0.069297\n",
      "Iter 3624, training loss 0.000410, validation loss 0.067993\n",
      "Iter 3625, training loss 0.000517, validation loss 0.069813\n",
      "Iter 3626, training loss 0.000524, validation loss 0.067931\n",
      "Iter 3627, training loss 0.000432, validation loss 0.069627\n",
      "Iter 3628, training loss 0.000290, validation loss 0.067960\n",
      "Iter 3629, training loss 0.000149, validation loss 0.068998\n",
      "Iter 3630, training loss 0.000056, validation loss 0.068233\n",
      "Iter 3631, training loss 0.000028, validation loss 0.068411\n",
      "Iter 3632, training loss 0.000058, validation loss 0.068687\n",
      "Iter 3633, training loss 0.000120, validation loss 0.068077\n",
      "Iter 3634, training loss 0.000185, validation loss 0.069048\n",
      "Iter 3635, training loss 0.000228, validation loss 0.067936\n",
      "Iter 3636, training loss 0.000237, validation loss 0.069141\n",
      "Iter 3637, training loss 0.000214, validation loss 0.067930\n",
      "Iter 3638, training loss 0.000167, validation loss 0.068987\n",
      "Iter 3639, training loss 0.000112, validation loss 0.068051\n",
      "Iter 3640, training loss 0.000065, validation loss 0.068683\n",
      "Iter 3641, training loss 0.000034, validation loss 0.068259\n",
      "Iter 3642, training loss 0.000024, validation loss 0.068371\n",
      "Iter 3643, training loss 0.000033, validation loss 0.068488\n",
      "Iter 3644, training loss 0.000051, validation loss 0.068139\n",
      "Iter 3645, training loss 0.000073, validation loss 0.068649\n",
      "Iter 3646, training loss 0.000091, validation loss 0.068001\n",
      "Iter 3647, training loss 0.000101, validation loss 0.068725\n",
      "Iter 3648, training loss 0.000102, validation loss 0.067980\n",
      "Iter 3649, training loss 0.000094, validation loss 0.068723\n",
      "Iter 3650, training loss 0.000081, validation loss 0.068042\n",
      "Iter 3651, training loss 0.000065, validation loss 0.068635\n",
      "Iter 3652, training loss 0.000049, validation loss 0.068133\n",
      "Iter 3653, training loss 0.000036, validation loss 0.068490\n",
      "Iter 3654, training loss 0.000027, validation loss 0.068226\n",
      "Iter 3655, training loss 0.000023, validation loss 0.068332\n",
      "Iter 3656, training loss 0.000023, validation loss 0.068316\n",
      "Iter 3657, training loss 0.000026, validation loss 0.068210\n",
      "Iter 3658, training loss 0.000030, validation loss 0.068405\n",
      "Iter 3659, training loss 0.000035, validation loss 0.068141\n",
      "Iter 3660, training loss 0.000040, validation loss 0.068481\n",
      "Iter 3661, training loss 0.000044, validation loss 0.068112\n",
      "Iter 3662, training loss 0.000046, validation loss 0.068526\n",
      "Iter 3663, training loss 0.000048, validation loss 0.068103\n",
      "Iter 3664, training loss 0.000048, validation loss 0.068533\n",
      "Iter 3665, training loss 0.000047, validation loss 0.068100\n",
      "Iter 3666, training loss 0.000045, validation loss 0.068515\n",
      "Iter 3667, training loss 0.000043, validation loss 0.068107\n",
      "Iter 3668, training loss 0.000041, validation loss 0.068492\n",
      "Iter 3669, training loss 0.000039, validation loss 0.068122\n",
      "Iter 3670, training loss 0.000037, validation loss 0.068464\n",
      "Iter 3671, training loss 0.000035, validation loss 0.068137\n",
      "Iter 3672, training loss 0.000033, validation loss 0.068434\n",
      "Iter 3673, training loss 0.000031, validation loss 0.068148\n",
      "Iter 3674, training loss 0.000030, validation loss 0.068407\n",
      "Iter 3675, training loss 0.000029, validation loss 0.068156\n",
      "Iter 3676, training loss 0.000028, validation loss 0.068390\n",
      "Iter 3677, training loss 0.000028, validation loss 0.068164\n",
      "Iter 3678, training loss 0.000028, validation loss 0.068385\n",
      "Iter 3679, training loss 0.000027, validation loss 0.068171\n",
      "Iter 3680, training loss 0.000027, validation loss 0.068391\n",
      "Iter 3681, training loss 0.000028, validation loss 0.068172\n",
      "Iter 3682, training loss 0.000028, validation loss 0.068402\n",
      "Iter 3683, training loss 0.000029, validation loss 0.068162\n",
      "Iter 3684, training loss 0.000030, validation loss 0.068418\n",
      "Iter 3685, training loss 0.000032, validation loss 0.068140\n",
      "Iter 3686, training loss 0.000035, validation loss 0.068446\n",
      "Iter 3687, training loss 0.000038, validation loss 0.068109\n",
      "Iter 3688, training loss 0.000043, validation loss 0.068495\n",
      "Iter 3689, training loss 0.000050, validation loss 0.068069\n",
      "Iter 3690, training loss 0.000059, validation loss 0.068577\n",
      "Iter 3691, training loss 0.000073, validation loss 0.068019\n",
      "Iter 3692, training loss 0.000092, validation loss 0.068710\n",
      "Iter 3693, training loss 0.000119, validation loss 0.067956\n",
      "Iter 3694, training loss 0.000159, validation loss 0.068923\n",
      "Iter 3695, training loss 0.000216, validation loss 0.067886\n",
      "Iter 3696, training loss 0.000299, validation loss 0.069280\n",
      "Iter 3697, training loss 0.000424, validation loss 0.067844\n",
      "Iter 3698, training loss 0.000605, validation loss 0.069917\n",
      "Iter 3699, training loss 0.000879, validation loss 0.067937\n",
      "Iter 3700, training loss 0.001272, validation loss 0.071093\n",
      "Iter 3701, training loss 0.001863, validation loss 0.068415\n",
      "Iter 3702, training loss 0.002669, validation loss 0.073239\n",
      "Iter 3703, training loss 0.003811, validation loss 0.069707\n",
      "Iter 3704, training loss 0.005142, validation loss 0.076686\n",
      "Iter 3705, training loss 0.006668, validation loss 0.071914\n",
      "Iter 3706, training loss 0.007668, validation loss 0.080036\n",
      "Iter 3707, training loss 0.007959, validation loss 0.073022\n",
      "Iter 3708, training loss 0.006819, validation loss 0.079021\n",
      "Iter 3709, training loss 0.004702, validation loss 0.070629\n",
      "Iter 3710, training loss 0.002261, validation loss 0.072953\n",
      "Iter 3711, training loss 0.000558, validation loss 0.068350\n",
      "Iter 3712, training loss 0.000097, validation loss 0.068579\n",
      "Iter 3713, training loss 0.000751, validation loss 0.070472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3714, training loss 0.001989, validation loss 0.068502\n",
      "Iter 3715, training loss 0.003284, validation loss 0.074265\n",
      "Iter 3716, training loss 0.004253, validation loss 0.069902\n",
      "Iter 3717, training loss 0.004398, validation loss 0.075752\n",
      "Iter 3718, training loss 0.003783, validation loss 0.069704\n",
      "Iter 3719, training loss 0.002470, validation loss 0.073045\n",
      "Iter 3720, training loss 0.001096, validation loss 0.068214\n",
      "Iter 3721, training loss 0.000221, validation loss 0.069210\n",
      "Iter 3722, training loss 0.000114, validation loss 0.068819\n",
      "Iter 3723, training loss 0.000615, validation loss 0.067890\n",
      "Iter 3724, training loss 0.001330, validation loss 0.071126\n",
      "Iter 3725, training loss 0.001876, validation loss 0.068176\n",
      "Iter 3726, training loss 0.002014, validation loss 0.072002\n",
      "Iter 3727, training loss 0.001729, validation loss 0.068092\n",
      "Iter 3728, training loss 0.001133, validation loss 0.070604\n",
      "Iter 3729, training loss 0.000524, validation loss 0.067720\n",
      "Iter 3730, training loss 0.000128, validation loss 0.068684\n",
      "Iter 3731, training loss 0.000050, validation loss 0.068357\n",
      "Iter 3732, training loss 0.000234, validation loss 0.067811\n",
      "Iter 3733, training loss 0.000533, validation loss 0.069687\n",
      "Iter 3734, training loss 0.000796, validation loss 0.067790\n",
      "Iter 3735, training loss 0.000917, validation loss 0.070334\n",
      "Iter 3736, training loss 0.000861, validation loss 0.067781\n",
      "Iter 3737, training loss 0.000646, validation loss 0.069741\n",
      "Iter 3738, training loss 0.000374, validation loss 0.067637\n",
      "Iter 3739, training loss 0.000147, validation loss 0.068629\n",
      "Iter 3740, training loss 0.000038, validation loss 0.067950\n",
      "Iter 3741, training loss 0.000055, validation loss 0.067880\n",
      "Iter 3742, training loss 0.000157, validation loss 0.068687\n",
      "Iter 3743, training loss 0.000281, validation loss 0.067651\n",
      "Iter 3744, training loss 0.000377, validation loss 0.069233\n",
      "Iter 3745, training loss 0.000413, validation loss 0.067649\n",
      "Iter 3746, training loss 0.000381, validation loss 0.069203\n",
      "Iter 3747, training loss 0.000298, validation loss 0.067622\n",
      "Iter 3748, training loss 0.000193, validation loss 0.068720\n",
      "Iter 3749, training loss 0.000101, validation loss 0.067704\n",
      "Iter 3750, training loss 0.000043, validation loss 0.068186\n",
      "Iter 3751, training loss 0.000027, validation loss 0.068010\n",
      "Iter 3752, training loss 0.000044, validation loss 0.067840\n",
      "Iter 3753, training loss 0.000081, validation loss 0.068388\n",
      "Iter 3754, training loss 0.000121, validation loss 0.067696\n",
      "Iter 3755, training loss 0.000152, validation loss 0.068617\n",
      "Iter 3756, training loss 0.000164, validation loss 0.067645\n",
      "Iter 3757, training loss 0.000155, validation loss 0.068596\n",
      "Iter 3758, training loss 0.000132, validation loss 0.067631\n",
      "Iter 3759, training loss 0.000102, validation loss 0.068407\n",
      "Iter 3760, training loss 0.000071, validation loss 0.067702\n",
      "Iter 3761, training loss 0.000046, validation loss 0.068183\n",
      "Iter 3762, training loss 0.000030, validation loss 0.067855\n",
      "Iter 3763, training loss 0.000023, validation loss 0.067995\n",
      "Iter 3764, training loss 0.000024, validation loss 0.068034\n",
      "Iter 3765, training loss 0.000031, validation loss 0.067862\n",
      "Iter 3766, training loss 0.000041, validation loss 0.068175\n",
      "Iter 3767, training loss 0.000050, validation loss 0.067769\n",
      "Iter 3768, training loss 0.000058, validation loss 0.068247\n",
      "Iter 3769, training loss 0.000063, validation loss 0.067717\n",
      "Iter 3770, training loss 0.000065, validation loss 0.068266\n",
      "Iter 3771, training loss 0.000063, validation loss 0.067711\n",
      "Iter 3772, training loss 0.000060, validation loss 0.068248\n",
      "Iter 3773, training loss 0.000054, validation loss 0.067738\n",
      "Iter 3774, training loss 0.000048, validation loss 0.068201\n",
      "Iter 3775, training loss 0.000042, validation loss 0.067780\n",
      "Iter 3776, training loss 0.000036, validation loss 0.068132\n",
      "Iter 3777, training loss 0.000031, validation loss 0.067821\n",
      "Iter 3778, training loss 0.000027, validation loss 0.068054\n",
      "Iter 3779, training loss 0.000024, validation loss 0.067862\n",
      "Iter 3780, training loss 0.000022, validation loss 0.067988\n",
      "Iter 3781, training loss 0.000021, validation loss 0.067907\n",
      "Iter 3782, training loss 0.000021, validation loss 0.067938\n",
      "Iter 3783, training loss 0.000021, validation loss 0.067948\n",
      "Iter 3784, training loss 0.000021, validation loss 0.067899\n",
      "Iter 3785, training loss 0.000022, validation loss 0.067982\n",
      "Iter 3786, training loss 0.000023, validation loss 0.067868\n",
      "Iter 3787, training loss 0.000023, validation loss 0.068009\n",
      "Iter 3788, training loss 0.000025, validation loss 0.067842\n",
      "Iter 3789, training loss 0.000026, validation loss 0.068037\n",
      "Iter 3790, training loss 0.000027, validation loss 0.067823\n",
      "Iter 3791, training loss 0.000029, validation loss 0.068067\n",
      "Iter 3792, training loss 0.000031, validation loss 0.067802\n",
      "Iter 3793, training loss 0.000033, validation loss 0.068099\n",
      "Iter 3794, training loss 0.000036, validation loss 0.067777\n",
      "Iter 3795, training loss 0.000039, validation loss 0.068138\n",
      "Iter 3796, training loss 0.000043, validation loss 0.067745\n",
      "Iter 3797, training loss 0.000049, validation loss 0.068190\n",
      "Iter 3798, training loss 0.000056, validation loss 0.067707\n",
      "Iter 3799, training loss 0.000067, validation loss 0.068271\n",
      "Iter 3800, training loss 0.000080, validation loss 0.067660\n",
      "Iter 3801, training loss 0.000099, validation loss 0.068394\n",
      "Iter 3802, training loss 0.000125, validation loss 0.067604\n",
      "Iter 3803, training loss 0.000160, validation loss 0.068587\n",
      "Iter 3804, training loss 0.000210, validation loss 0.067544\n",
      "Iter 3805, training loss 0.000280, validation loss 0.068898\n",
      "Iter 3806, training loss 0.000380, validation loss 0.067504\n",
      "Iter 3807, training loss 0.000522, validation loss 0.069420\n",
      "Iter 3808, training loss 0.000726, validation loss 0.067548\n",
      "Iter 3809, training loss 0.001017, validation loss 0.070330\n",
      "Iter 3810, training loss 0.001442, validation loss 0.067838\n",
      "Iter 3811, training loss 0.002030, validation loss 0.071954\n",
      "Iter 3812, training loss 0.002879, validation loss 0.068705\n",
      "Iter 3813, training loss 0.003987, validation loss 0.074771\n",
      "Iter 3814, training loss 0.005481, validation loss 0.070597\n",
      "Iter 3815, training loss 0.007164, validation loss 0.079002\n",
      "Iter 3816, training loss 0.009036, validation loss 0.073463\n",
      "Iter 3817, training loss 0.010363, validation loss 0.083066\n",
      "Iter 3818, training loss 0.010868, validation loss 0.075082\n",
      "Iter 3819, training loss 0.009807, validation loss 0.082460\n",
      "Iter 3820, training loss 0.007471, validation loss 0.072479\n",
      "Iter 3821, training loss 0.004369, validation loss 0.075590\n",
      "Iter 3822, training loss 0.001699, validation loss 0.068473\n",
      "Iter 3823, training loss 0.000256, validation loss 0.069148\n",
      "Iter 3824, training loss 0.000271, validation loss 0.069143\n",
      "Iter 3825, training loss 0.001357, validation loss 0.067958\n",
      "Iter 3826, training loss 0.002867, validation loss 0.073386\n",
      "Iter 3827, training loss 0.004248, validation loss 0.069498\n",
      "Iter 3828, training loss 0.005122, validation loss 0.076430\n",
      "Iter 3829, training loss 0.005290, validation loss 0.070388\n",
      "Iter 3830, training loss 0.004498, validation loss 0.075640\n",
      "Iter 3831, training loss 0.003101, validation loss 0.069039\n",
      "Iter 3832, training loss 0.001557, validation loss 0.071432\n",
      "Iter 3833, training loss 0.000445, validation loss 0.067754\n",
      "Iter 3834, training loss 0.000067, validation loss 0.068133\n",
      "Iter 3835, training loss 0.000375, validation loss 0.069149\n",
      "Iter 3836, training loss 0.001058, validation loss 0.067592\n",
      "Iter 3837, training loss 0.001750, validation loss 0.071417\n",
      "Iter 3838, training loss 0.002168, validation loss 0.067976\n",
      "Iter 3839, training loss 0.002173, validation loss 0.071896\n",
      "Iter 3840, training loss 0.001815, validation loss 0.067804\n",
      "Iter 3841, training loss 0.001218, validation loss 0.070442\n",
      "Iter 3842, training loss 0.000620, validation loss 0.067399\n",
      "Iter 3843, training loss 0.000197, validation loss 0.068545\n",
      "Iter 3844, training loss 0.000041, validation loss 0.067806\n",
      "Iter 3845, training loss 0.000129, validation loss 0.067538\n",
      "Iter 3846, training loss 0.000364, validation loss 0.068986\n",
      "Iter 3847, training loss 0.000630, validation loss 0.067416\n",
      "Iter 3848, training loss 0.000833, validation loss 0.069886\n",
      "Iter 3849, training loss 0.000915, validation loss 0.067475\n",
      "Iter 3850, training loss 0.000850, validation loss 0.069823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3851, training loss 0.000675, validation loss 0.067347\n",
      "Iter 3852, training loss 0.000444, validation loss 0.069003\n",
      "Iter 3853, training loss 0.000231, validation loss 0.067300\n",
      "Iter 3854, training loss 0.000087, validation loss 0.068072\n",
      "Iter 3855, training loss 0.000031, validation loss 0.067644\n",
      "Iter 3856, training loss 0.000055, validation loss 0.067503\n",
      "Iter 3857, training loss 0.000128, validation loss 0.068232\n",
      "Iter 3858, training loss 0.000219, validation loss 0.067314\n",
      "Iter 3859, training loss 0.000300, validation loss 0.068690\n",
      "Iter 3860, training loss 0.000352, validation loss 0.067279\n",
      "Iter 3861, training loss 0.000362, validation loss 0.068804\n",
      "Iter 3862, training loss 0.000335, validation loss 0.067251\n",
      "Iter 3863, training loss 0.000280, validation loss 0.068602\n",
      "Iter 3864, training loss 0.000213, validation loss 0.067254\n",
      "Iter 3865, training loss 0.000146, validation loss 0.068239\n",
      "Iter 3866, training loss 0.000090, validation loss 0.067357\n",
      "Iter 3867, training loss 0.000050, validation loss 0.067885\n",
      "Iter 3868, training loss 0.000028, validation loss 0.067561\n",
      "Iter 3869, training loss 0.000024, validation loss 0.067621\n",
      "Iter 3870, training loss 0.000033, validation loss 0.067788\n",
      "Iter 3871, training loss 0.000050, validation loss 0.067446\n",
      "Iter 3872, training loss 0.000070, validation loss 0.067968\n",
      "Iter 3873, training loss 0.000089, validation loss 0.067336\n",
      "Iter 3874, training loss 0.000105, validation loss 0.068080\n",
      "Iter 3875, training loss 0.000116, validation loss 0.067283\n",
      "Iter 3876, training loss 0.000123, validation loss 0.068139\n",
      "Iter 3877, training loss 0.000125, validation loss 0.067279\n",
      "Iter 3878, training loss 0.000123, validation loss 0.068153\n",
      "Iter 3879, training loss 0.000118, validation loss 0.067301\n",
      "Iter 3880, training loss 0.000110, validation loss 0.068121\n",
      "Iter 3881, training loss 0.000101, validation loss 0.067325\n",
      "Iter 3882, training loss 0.000092, validation loss 0.068051\n",
      "Iter 3883, training loss 0.000082, validation loss 0.067342\n",
      "Iter 3884, training loss 0.000073, validation loss 0.067965\n",
      "Iter 3885, training loss 0.000065, validation loss 0.067356\n",
      "Iter 3886, training loss 0.000058, validation loss 0.067888\n",
      "Iter 3887, training loss 0.000052, validation loss 0.067378\n",
      "Iter 3888, training loss 0.000047, validation loss 0.067836\n",
      "Iter 3889, training loss 0.000043, validation loss 0.067406\n",
      "Iter 3890, training loss 0.000041, validation loss 0.067804\n",
      "Iter 3891, training loss 0.000038, validation loss 0.067428\n",
      "Iter 3892, training loss 0.000037, validation loss 0.067784\n",
      "Iter 3893, training loss 0.000036, validation loss 0.067436\n",
      "Iter 3894, training loss 0.000035, validation loss 0.067772\n",
      "Iter 3895, training loss 0.000036, validation loss 0.067432\n",
      "Iter 3896, training loss 0.000036, validation loss 0.067775\n",
      "Iter 3897, training loss 0.000038, validation loss 0.067417\n",
      "Iter 3898, training loss 0.000040, validation loss 0.067795\n",
      "Iter 3899, training loss 0.000044, validation loss 0.067390\n",
      "Iter 3900, training loss 0.000048, validation loss 0.067836\n",
      "Iter 3901, training loss 0.000055, validation loss 0.067355\n",
      "Iter 3902, training loss 0.000064, validation loss 0.067908\n",
      "Iter 3903, training loss 0.000077, validation loss 0.067309\n",
      "Iter 3904, training loss 0.000094, validation loss 0.068024\n",
      "Iter 3905, training loss 0.000119, validation loss 0.067251\n",
      "Iter 3906, training loss 0.000154, validation loss 0.068213\n",
      "Iter 3907, training loss 0.000203, validation loss 0.067187\n",
      "Iter 3908, training loss 0.000274, validation loss 0.068527\n",
      "Iter 3909, training loss 0.000378, validation loss 0.067139\n",
      "Iter 3910, training loss 0.000527, validation loss 0.069070\n",
      "Iter 3911, training loss 0.000746, validation loss 0.067181\n",
      "Iter 3912, training loss 0.001064, validation loss 0.070048\n",
      "Iter 3913, training loss 0.001539, validation loss 0.067508\n",
      "Iter 3914, training loss 0.002225, validation loss 0.071891\n",
      "Iter 3915, training loss 0.003246, validation loss 0.068562\n",
      "Iter 3916, training loss 0.004678, validation loss 0.075349\n",
      "Iter 3917, training loss 0.006735, validation loss 0.071162\n",
      "Iter 3918, training loss 0.009382, validation loss 0.081419\n",
      "Iter 3919, training loss 0.012797, validation loss 0.076213\n",
      "Iter 3920, training loss 0.016287, validation loss 0.089833\n",
      "Iter 3921, training loss 0.019367, validation loss 0.082083\n",
      "Iter 3922, training loss 0.020353, validation loss 0.094632\n",
      "Iter 3923, training loss 0.018787, validation loss 0.081833\n",
      "Iter 3924, training loss 0.014023, validation loss 0.087274\n",
      "Iter 3925, training loss 0.007999, validation loss 0.072917\n",
      "Iter 3926, training loss 0.002743, validation loss 0.073200\n",
      "Iter 3927, training loss 0.000311, validation loss 0.068307\n",
      "Iter 3928, training loss 0.001027, validation loss 0.068094\n",
      "Iter 3929, training loss 0.003723, validation loss 0.074565\n",
      "Iter 3930, training loss 0.006752, validation loss 0.071280\n",
      "Iter 3931, training loss 0.008770, validation loss 0.080937\n",
      "Iter 3932, training loss 0.009166, validation loss 0.072980\n",
      "Iter 3933, training loss 0.007756, validation loss 0.079513\n",
      "Iter 3934, training loss 0.005244, validation loss 0.070031\n",
      "Iter 3935, training loss 0.002518, validation loss 0.072464\n",
      "Iter 3936, training loss 0.000633, validation loss 0.067472\n",
      "Iter 3937, training loss 0.000115, validation loss 0.067779\n",
      "Iter 3938, training loss 0.000816, validation loss 0.069798\n",
      "Iter 3939, training loss 0.002091, validation loss 0.067885\n",
      "Iter 3940, training loss 0.003177, validation loss 0.073316\n",
      "Iter 3941, training loss 0.003586, validation loss 0.068591\n",
      "Iter 3942, training loss 0.003218, validation loss 0.073181\n",
      "Iter 3943, training loss 0.002288, validation loss 0.067727\n",
      "Iter 3944, training loss 0.001197, validation loss 0.070006\n",
      "Iter 3945, training loss 0.000381, validation loss 0.066956\n",
      "Iter 3946, training loss 0.000069, validation loss 0.067402\n",
      "Iter 3947, training loss 0.000247, validation loss 0.068107\n",
      "Iter 3948, training loss 0.000713, validation loss 0.066888\n",
      "Iter 3949, training loss 0.001194, validation loss 0.069961\n",
      "Iter 3950, training loss 0.001486, validation loss 0.067145\n",
      "Iter 3951, training loss 0.001497, validation loss 0.070478\n",
      "Iter 3952, training loss 0.001254, validation loss 0.067118\n",
      "Iter 3953, training loss 0.000861, validation loss 0.069457\n",
      "Iter 3954, training loss 0.000460, validation loss 0.066974\n",
      "Iter 3955, training loss 0.000168, validation loss 0.067978\n",
      "Iter 3956, training loss 0.000047, validation loss 0.067272\n",
      "Iter 3957, training loss 0.000091, validation loss 0.067043\n",
      "Iter 3958, training loss 0.000239, validation loss 0.068045\n",
      "Iter 3959, training loss 0.000411, validation loss 0.066757\n",
      "Iter 3960, training loss 0.000538, validation loss 0.068647\n",
      "Iter 3961, training loss 0.000581, validation loss 0.066710\n",
      "Iter 3962, training loss 0.000537, validation loss 0.068628\n",
      "Iter 3963, training loss 0.000429, validation loss 0.066718\n",
      "Iter 3964, training loss 0.000294, validation loss 0.068120\n",
      "Iter 3965, training loss 0.000169, validation loss 0.066818\n",
      "Iter 3966, training loss 0.000079, validation loss 0.067492\n",
      "Iter 3967, training loss 0.000036, validation loss 0.067078\n",
      "Iter 3968, training loss 0.000039, validation loss 0.067038\n",
      "Iter 3969, training loss 0.000074, validation loss 0.067451\n",
      "Iter 3970, training loss 0.000123, validation loss 0.066803\n",
      "Iter 3971, training loss 0.000172, validation loss 0.067755\n",
      "Iter 3972, training loss 0.000206, validation loss 0.066706\n",
      "Iter 3973, training loss 0.000220, validation loss 0.067873\n",
      "Iter 3974, training loss 0.000215, validation loss 0.066696\n",
      "Iter 3975, training loss 0.000192, validation loss 0.067801\n",
      "Iter 3976, training loss 0.000159, validation loss 0.066737\n",
      "Iter 3977, training loss 0.000121, validation loss 0.067590\n",
      "Iter 3978, training loss 0.000086, validation loss 0.066814\n",
      "Iter 3979, training loss 0.000057, validation loss 0.067336\n",
      "Iter 3980, training loss 0.000038, validation loss 0.066928\n",
      "Iter 3981, training loss 0.000028, validation loss 0.067109\n",
      "Iter 3982, training loss 0.000026, validation loss 0.067069\n",
      "Iter 3983, training loss 0.000031, validation loss 0.066954\n",
      "Iter 3984, training loss 0.000039, validation loss 0.067219\n",
      "Iter 3985, training loss 0.000049, validation loss 0.066868\n",
      "Iter 3986, training loss 0.000059, validation loss 0.067343\n",
      "Iter 3987, training loss 0.000069, validation loss 0.066818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3988, training loss 0.000077, validation loss 0.067417\n",
      "Iter 3989, training loss 0.000082, validation loss 0.066785\n",
      "Iter 3990, training loss 0.000086, validation loss 0.067443\n",
      "Iter 3991, training loss 0.000087, validation loss 0.066762\n",
      "Iter 3992, training loss 0.000088, validation loss 0.067437\n",
      "Iter 3993, training loss 0.000087, validation loss 0.066749\n",
      "Iter 3994, training loss 0.000086, validation loss 0.067422\n",
      "Iter 3995, training loss 0.000085, validation loss 0.066750\n",
      "Iter 3996, training loss 0.000083, validation loss 0.067414\n",
      "Iter 3997, training loss 0.000082, validation loss 0.066755\n",
      "Iter 3998, training loss 0.000081, validation loss 0.067411\n",
      "Iter 3999, training loss 0.000082, validation loss 0.066756\n",
      "Iter 4000, training loss 0.000083, validation loss 0.067417\n",
      "Iter 4001, training loss 0.000085, validation loss 0.066749\n",
      "Iter 4002, training loss 0.000089, validation loss 0.067438\n",
      "Iter 4003, training loss 0.000094, validation loss 0.066733\n",
      "Iter 4004, training loss 0.000102, validation loss 0.067484\n",
      "Iter 4005, training loss 0.000112, validation loss 0.066709\n",
      "Iter 4006, training loss 0.000126, validation loss 0.067566\n",
      "Iter 4007, training loss 0.000145, validation loss 0.066676\n",
      "Iter 4008, training loss 0.000170, validation loss 0.067697\n",
      "Iter 4009, training loss 0.000204, validation loss 0.066637\n",
      "Iter 4010, training loss 0.000249, validation loss 0.067904\n",
      "Iter 4011, training loss 0.000309, validation loss 0.066601\n",
      "Iter 4012, training loss 0.000391, validation loss 0.068228\n",
      "Iter 4013, training loss 0.000503, validation loss 0.066591\n",
      "Iter 4014, training loss 0.000657, validation loss 0.068757\n",
      "Iter 4015, training loss 0.000869, validation loss 0.066673\n",
      "Iter 4016, training loss 0.001163, validation loss 0.069652\n",
      "Iter 4017, training loss 0.001574, validation loss 0.066984\n",
      "Iter 4018, training loss 0.002143, validation loss 0.071195\n",
      "Iter 4019, training loss 0.002941, validation loss 0.067800\n",
      "Iter 4020, training loss 0.004026, validation loss 0.073883\n",
      "Iter 4021, training loss 0.005552, validation loss 0.069682\n",
      "Iter 4022, training loss 0.007563, validation loss 0.078554\n",
      "Iter 4023, training loss 0.010263, validation loss 0.073509\n",
      "Iter 4024, training loss 0.013527, validation loss 0.085972\n",
      "Iter 4025, training loss 0.017394, validation loss 0.079724\n",
      "Iter 4026, training loss 0.021147, validation loss 0.095056\n",
      "Iter 4027, training loss 0.024352, validation loss 0.086072\n",
      "Iter 4028, training loss 0.025562, validation loss 0.100195\n",
      "Iter 4029, training loss 0.024380, validation loss 0.086368\n",
      "Iter 4030, training loss 0.019910, validation loss 0.093655\n",
      "Iter 4031, training loss 0.013520, validation loss 0.077078\n",
      "Iter 4032, training loss 0.006747, validation loss 0.077882\n",
      "Iter 4033, training loss 0.001913, validation loss 0.068167\n",
      "Iter 4034, training loss 0.000210, validation loss 0.068010\n",
      "Iter 4035, training loss 0.001463, validation loss 0.070650\n",
      "Iter 4036, training loss 0.004384, validation loss 0.069298\n",
      "Iter 4037, training loss 0.007378, validation loss 0.078595\n",
      "Iter 4038, training loss 0.009313, validation loss 0.072711\n",
      "Iter 4039, training loss 0.009555, validation loss 0.081201\n",
      "Iter 4040, training loss 0.008241, validation loss 0.071803\n",
      "Iter 4041, training loss 0.005782, validation loss 0.076363\n",
      "Iter 4042, training loss 0.003114, validation loss 0.068132\n",
      "Iter 4043, training loss 0.001047, validation loss 0.069637\n",
      "Iter 4044, training loss 0.000132, validation loss 0.067176\n",
      "Iter 4045, training loss 0.000391, validation loss 0.066928\n",
      "Iter 4046, training loss 0.001400, validation loss 0.070203\n",
      "Iter 4047, training loss 0.002549, validation loss 0.067637\n",
      "Iter 4048, training loss 0.003302, validation loss 0.072897\n",
      "Iter 4049, training loss 0.003407, validation loss 0.068020\n",
      "Iter 4050, training loss 0.002889, validation loss 0.072162\n",
      "Iter 4051, training loss 0.001999, validation loss 0.067078\n",
      "Iter 4052, training loss 0.001059, validation loss 0.069258\n",
      "Iter 4053, training loss 0.000367, validation loss 0.066449\n",
      "Iter 4054, training loss 0.000073, validation loss 0.066963\n",
      "Iter 4055, training loss 0.000165, validation loss 0.067337\n",
      "Iter 4056, training loss 0.000507, validation loss 0.066340\n",
      "Iter 4057, training loss 0.000913, validation loss 0.068908\n",
      "Iter 4058, training loss 0.001226, validation loss 0.066511\n",
      "Iter 4059, training loss 0.001347, validation loss 0.069661\n",
      "Iter 4060, training loss 0.001265, validation loss 0.066557\n",
      "Iter 4061, training loss 0.001022, validation loss 0.069156\n",
      "Iter 4062, training loss 0.000704, validation loss 0.066420\n",
      "Iter 4063, training loss 0.000395, validation loss 0.067976\n",
      "Iter 4064, training loss 0.000167, validation loss 0.066458\n",
      "Iter 4065, training loss 0.000055, validation loss 0.066917\n",
      "Iter 4066, training loss 0.000057, validation loss 0.066899\n",
      "Iter 4067, training loss 0.000141, validation loss 0.066368\n",
      "Iter 4068, training loss 0.000261, validation loss 0.067511\n",
      "Iter 4069, training loss 0.000376, validation loss 0.066186\n",
      "Iter 4070, training loss 0.000453, validation loss 0.067886\n",
      "Iter 4071, training loss 0.000480, validation loss 0.066141\n",
      "Iter 4072, training loss 0.000457, validation loss 0.067879\n",
      "Iter 4073, training loss 0.000394, validation loss 0.066152\n",
      "Iter 4074, training loss 0.000310, validation loss 0.067572\n",
      "Iter 4075, training loss 0.000221, validation loss 0.066211\n",
      "Iter 4076, training loss 0.000142, validation loss 0.067144\n",
      "Iter 4077, training loss 0.000082, validation loss 0.066356\n",
      "Iter 4078, training loss 0.000046, validation loss 0.066755\n",
      "Iter 4079, training loss 0.000033, validation loss 0.066566\n",
      "Iter 4080, training loss 0.000038, validation loss 0.066463\n",
      "Iter 4081, training loss 0.000057, validation loss 0.066788\n",
      "Iter 4082, training loss 0.000082, validation loss 0.066284\n",
      "Iter 4083, training loss 0.000109, validation loss 0.066979\n",
      "Iter 4084, training loss 0.000132, validation loss 0.066195\n",
      "Iter 4085, training loss 0.000150, validation loss 0.067098\n",
      "Iter 4086, training loss 0.000162, validation loss 0.066158\n",
      "Iter 4087, training loss 0.000168, validation loss 0.067143\n",
      "Iter 4088, training loss 0.000167, validation loss 0.066151\n",
      "Iter 4089, training loss 0.000162, validation loss 0.067125\n",
      "Iter 4090, training loss 0.000154, validation loss 0.066154\n",
      "Iter 4091, training loss 0.000143, validation loss 0.067061\n",
      "Iter 4092, training loss 0.000132, validation loss 0.066166\n",
      "Iter 4093, training loss 0.000120, validation loss 0.066990\n",
      "Iter 4094, training loss 0.000109, validation loss 0.066190\n",
      "Iter 4095, training loss 0.000100, validation loss 0.066924\n",
      "Iter 4096, training loss 0.000091, validation loss 0.066214\n",
      "Iter 4097, training loss 0.000084, validation loss 0.066868\n",
      "Iter 4098, training loss 0.000078, validation loss 0.066233\n",
      "Iter 4099, training loss 0.000074, validation loss 0.066823\n",
      "Iter 4100, training loss 0.000071, validation loss 0.066239\n",
      "Iter 4101, training loss 0.000068, validation loss 0.066791\n",
      "Iter 4102, training loss 0.000068, validation loss 0.066234\n",
      "Iter 4103, training loss 0.000068, validation loss 0.066783\n",
      "Iter 4104, training loss 0.000070, validation loss 0.066222\n",
      "Iter 4105, training loss 0.000074, validation loss 0.066802\n",
      "Iter 4106, training loss 0.000079, validation loss 0.066200\n",
      "Iter 4107, training loss 0.000087, validation loss 0.066855\n",
      "Iter 4108, training loss 0.000098, validation loss 0.066169\n",
      "Iter 4109, training loss 0.000114, validation loss 0.066949\n",
      "Iter 4110, training loss 0.000135, validation loss 0.066128\n",
      "Iter 4111, training loss 0.000163, validation loss 0.067100\n",
      "Iter 4112, training loss 0.000202, validation loss 0.066078\n",
      "Iter 4113, training loss 0.000257, validation loss 0.067341\n",
      "Iter 4114, training loss 0.000332, validation loss 0.066035\n",
      "Iter 4115, training loss 0.000438, validation loss 0.067740\n",
      "Iter 4116, training loss 0.000586, validation loss 0.066040\n",
      "Iter 4117, training loss 0.000797, validation loss 0.068425\n",
      "Iter 4118, training loss 0.001099, validation loss 0.066195\n",
      "Iter 4119, training loss 0.001531, validation loss 0.069653\n",
      "Iter 4120, training loss 0.002157, validation loss 0.066740\n",
      "Iter 4121, training loss 0.003051, validation loss 0.071919\n",
      "Iter 4122, training loss 0.004357, validation loss 0.068218\n",
      "Iter 4123, training loss 0.006221, validation loss 0.076206\n",
      "Iter 4124, training loss 0.008931, validation loss 0.071817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4125, training loss 0.012681, validation loss 0.084311\n",
      "Iter 4126, training loss 0.017911, validation loss 0.079587\n",
      "Iter 4127, training loss 0.024530, validation loss 0.098354\n",
      "Iter 4128, training loss 0.032706, validation loss 0.093176\n",
      "Iter 4129, training loss 0.041032, validation loss 0.117102\n",
      "Iter 4130, training loss 0.048475, validation loss 0.108370\n",
      "Iter 4131, training loss 0.051400, validation loss 0.128318\n",
      "Iter 4132, training loss 0.048357, validation loss 0.109035\n",
      "Iter 4133, training loss 0.037433, validation loss 0.112347\n",
      "Iter 4134, training loss 0.022356, validation loss 0.085553\n",
      "Iter 4135, training loss 0.008298, validation loss 0.078843\n",
      "Iter 4136, training loss 0.001007, validation loss 0.067913\n",
      "Iter 4137, training loss 0.002200, validation loss 0.068430\n",
      "Iter 4138, training loss 0.008820, validation loss 0.079537\n",
      "Iter 4139, training loss 0.015494, validation loss 0.078829\n",
      "Iter 4140, training loss 0.017846, validation loss 0.090152\n",
      "Iter 4141, training loss 0.015010, validation loss 0.077804\n",
      "Iter 4142, training loss 0.008887, validation loss 0.079290\n",
      "Iter 4143, training loss 0.003086, validation loss 0.067837\n",
      "Iter 4144, training loss 0.000339, validation loss 0.067268\n",
      "Iter 4145, training loss 0.001267, validation loss 0.069057\n",
      "Iter 4146, training loss 0.004317, validation loss 0.068675\n",
      "Iter 4147, training loss 0.007041, validation loss 0.076905\n",
      "Iter 4148, training loss 0.007726, validation loss 0.071317\n",
      "Iter 4149, training loss 0.006047, validation loss 0.075715\n",
      "Iter 4150, training loss 0.003230, validation loss 0.067821\n",
      "Iter 4151, training loss 0.000903, validation loss 0.068511\n",
      "Iter 4152, training loss 0.000171, validation loss 0.066785\n",
      "Iter 4153, training loss 0.001001, validation loss 0.066374\n",
      "Iter 4154, training loss 0.002460, validation loss 0.070775\n",
      "Iter 4155, training loss 0.003457, validation loss 0.067695\n",
      "Iter 4156, training loss 0.003389, validation loss 0.071956\n",
      "Iter 4157, training loss 0.002405, validation loss 0.066852\n",
      "Iter 4158, training loss 0.001136, validation loss 0.068506\n",
      "Iter 4159, training loss 0.000276, validation loss 0.065846\n",
      "Iter 4160, training loss 0.000149, validation loss 0.065910\n",
      "Iter 4161, training loss 0.000618, validation loss 0.067481\n",
      "Iter 4162, training loss 0.001261, validation loss 0.065955\n",
      "Iter 4163, training loss 0.001655, validation loss 0.069217\n",
      "Iter 4164, training loss 0.001602, validation loss 0.066128\n",
      "Iter 4165, training loss 0.001171, validation loss 0.068505\n",
      "Iter 4166, training loss 0.000617, validation loss 0.065752\n",
      "Iter 4167, training loss 0.000205, validation loss 0.066667\n",
      "Iter 4168, training loss 0.000080, validation loss 0.066137\n",
      "Iter 4169, training loss 0.000221, validation loss 0.065753\n",
      "Iter 4170, training loss 0.000485, validation loss 0.067275\n",
      "Iter 4171, training loss 0.000701, validation loss 0.065697\n",
      "Iter 4172, training loss 0.000757, validation loss 0.067751\n",
      "Iter 4173, training loss 0.000644, validation loss 0.065614\n",
      "Iter 4174, training loss 0.000429, validation loss 0.067051\n",
      "Iter 4175, training loss 0.000216, validation loss 0.065544\n",
      "Iter 4176, training loss 0.000086, validation loss 0.066053\n",
      "Iter 4177, training loss 0.000067, validation loss 0.065913\n",
      "Iter 4178, training loss 0.000137, validation loss 0.065514\n",
      "Iter 4179, training loss 0.000241, validation loss 0.066493\n",
      "Iter 4180, training loss 0.000324, validation loss 0.065385\n",
      "Iter 4181, training loss 0.000349, validation loss 0.066735\n",
      "Iter 4182, training loss 0.000314, validation loss 0.065388\n",
      "Iter 4183, training loss 0.000237, validation loss 0.066486\n",
      "Iter 4184, training loss 0.000151, validation loss 0.065471\n",
      "Iter 4185, training loss 0.000084, validation loss 0.066024\n",
      "Iter 4186, training loss 0.000052, validation loss 0.065708\n",
      "Iter 4187, training loss 0.000056, validation loss 0.065661\n",
      "Iter 4188, training loss 0.000085, validation loss 0.066031\n",
      "Iter 4189, training loss 0.000121, validation loss 0.065476\n",
      "Iter 4190, training loss 0.000149, validation loss 0.066242\n",
      "Iter 4191, training loss 0.000161, validation loss 0.065408\n",
      "Iter 4192, training loss 0.000153, validation loss 0.066235\n",
      "Iter 4193, training loss 0.000131, validation loss 0.065416\n",
      "Iter 4194, training loss 0.000102, validation loss 0.066053\n",
      "Iter 4195, training loss 0.000074, validation loss 0.065487\n",
      "Iter 4196, training loss 0.000053, validation loss 0.065808\n",
      "Iter 4197, training loss 0.000043, validation loss 0.065615\n",
      "Iter 4198, training loss 0.000043, validation loss 0.065607\n",
      "Iter 4199, training loss 0.000049, validation loss 0.065767\n",
      "Iter 4200, training loss 0.000059, validation loss 0.065485\n",
      "Iter 4201, training loss 0.000069, validation loss 0.065883\n",
      "Iter 4202, training loss 0.000076, validation loss 0.065428\n",
      "Iter 4203, training loss 0.000079, validation loss 0.065924\n",
      "Iter 4204, training loss 0.000077, validation loss 0.065416\n",
      "Iter 4205, training loss 0.000072, validation loss 0.065889\n",
      "Iter 4206, training loss 0.000064, validation loss 0.065436\n",
      "Iter 4207, training loss 0.000056, validation loss 0.065806\n",
      "Iter 4208, training loss 0.000049, validation loss 0.065481\n",
      "Iter 4209, training loss 0.000042, validation loss 0.065710\n",
      "Iter 4210, training loss 0.000038, validation loss 0.065541\n",
      "Iter 4211, training loss 0.000036, validation loss 0.065617\n",
      "Iter 4212, training loss 0.000035, validation loss 0.065598\n",
      "Iter 4213, training loss 0.000036, validation loss 0.065542\n",
      "Iter 4214, training loss 0.000037, validation loss 0.065648\n",
      "Iter 4215, training loss 0.000039, validation loss 0.065490\n",
      "Iter 4216, training loss 0.000041, validation loss 0.065685\n",
      "Iter 4217, training loss 0.000043, validation loss 0.065458\n",
      "Iter 4218, training loss 0.000044, validation loss 0.065708\n",
      "Iter 4219, training loss 0.000045, validation loss 0.065440\n",
      "Iter 4220, training loss 0.000045, validation loss 0.065717\n",
      "Iter 4221, training loss 0.000045, validation loss 0.065433\n",
      "Iter 4222, training loss 0.000045, validation loss 0.065715\n",
      "Iter 4223, training loss 0.000044, validation loss 0.065429\n",
      "Iter 4224, training loss 0.000044, validation loss 0.065707\n",
      "Iter 4225, training loss 0.000043, validation loss 0.065427\n",
      "Iter 4226, training loss 0.000042, validation loss 0.065696\n",
      "Iter 4227, training loss 0.000042, validation loss 0.065424\n",
      "Iter 4228, training loss 0.000041, validation loss 0.065686\n",
      "Iter 4229, training loss 0.000041, validation loss 0.065420\n",
      "Iter 4230, training loss 0.000041, validation loss 0.065682\n",
      "Iter 4231, training loss 0.000041, validation loss 0.065414\n",
      "Iter 4232, training loss 0.000041, validation loss 0.065684\n",
      "Iter 4233, training loss 0.000042, validation loss 0.065403\n",
      "Iter 4234, training loss 0.000043, validation loss 0.065694\n",
      "Iter 4235, training loss 0.000045, validation loss 0.065386\n",
      "Iter 4236, training loss 0.000047, validation loss 0.065715\n",
      "Iter 4237, training loss 0.000050, validation loss 0.065363\n",
      "Iter 4238, training loss 0.000054, validation loss 0.065751\n",
      "Iter 4239, training loss 0.000059, validation loss 0.065331\n",
      "Iter 4240, training loss 0.000066, validation loss 0.065808\n",
      "Iter 4241, training loss 0.000076, validation loss 0.065289\n",
      "Iter 4242, training loss 0.000090, validation loss 0.065898\n",
      "Iter 4243, training loss 0.000108, validation loss 0.065236\n",
      "Iter 4244, training loss 0.000134, validation loss 0.066041\n",
      "Iter 4245, training loss 0.000170, validation loss 0.065178\n",
      "Iter 4246, training loss 0.000221, validation loss 0.066276\n",
      "Iter 4247, training loss 0.000293, validation loss 0.065129\n",
      "Iter 4248, training loss 0.000397, validation loss 0.066673\n",
      "Iter 4249, training loss 0.000545, validation loss 0.065130\n",
      "Iter 4250, training loss 0.000760, validation loss 0.067371\n",
      "Iter 4251, training loss 0.001075, validation loss 0.065296\n",
      "Iter 4252, training loss 0.001536, validation loss 0.068658\n",
      "Iter 4253, training loss 0.002220, validation loss 0.065911\n",
      "Iter 4254, training loss 0.003233, validation loss 0.071144\n",
      "Iter 4255, training loss 0.004745, validation loss 0.067672\n",
      "Iter 4256, training loss 0.006983, validation loss 0.076119\n",
      "Iter 4257, training loss 0.010323, validation loss 0.072181\n",
      "Iter 4258, training loss 0.015168, validation loss 0.086203\n",
      "Iter 4259, training loss 0.022220, validation loss 0.082715\n",
      "Iter 4260, training loss 0.031854, validation loss 0.105618\n",
      "Iter 4261, training loss 0.044791, validation loss 0.103950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4262, training loss 0.059992, validation loss 0.136893\n",
      "Iter 4263, training loss 0.076453, validation loss 0.135052\n",
      "Iter 4264, training loss 0.088495, validation loss 0.167250\n",
      "Iter 4265, training loss 0.091857, validation loss 0.151446\n",
      "Iter 4266, training loss 0.080221, validation loss 0.156929\n",
      "Iter 4267, training loss 0.055594, validation loss 0.117412\n",
      "Iter 4268, training loss 0.026248, validation loss 0.097586\n",
      "Iter 4269, training loss 0.005438, validation loss 0.070577\n",
      "Iter 4270, training loss 0.001414, validation loss 0.067472\n",
      "Iter 4271, training loss 0.011722, validation loss 0.081251\n",
      "Iter 4272, training loss 0.025161, validation loss 0.088241\n",
      "Iter 4273, training loss 0.030215, validation loss 0.101861\n",
      "Iter 4274, training loss 0.023549, validation loss 0.086115\n",
      "Iter 4275, training loss 0.010832, validation loss 0.079644\n",
      "Iter 4276, training loss 0.001683, validation loss 0.066277\n",
      "Iter 4277, training loss 0.001454, validation loss 0.065950\n",
      "Iter 4278, training loss 0.007763, validation loss 0.075612\n",
      "Iter 4279, training loss 0.013609, validation loss 0.075945\n",
      "Iter 4280, training loss 0.013625, validation loss 0.082358\n",
      "Iter 4281, training loss 0.008269, validation loss 0.071101\n",
      "Iter 4282, training loss 0.002298, validation loss 0.068720\n",
      "Iter 4283, training loss 0.000312, validation loss 0.065590\n",
      "Iter 4284, training loss 0.002758, validation loss 0.066522\n",
      "Iter 4285, training loss 0.006245, validation loss 0.073722\n",
      "Iter 4286, training loss 0.007165, validation loss 0.070031\n",
      "Iter 4287, training loss 0.004832, validation loss 0.071933\n",
      "Iter 4288, training loss 0.001606, validation loss 0.065539\n",
      "Iter 4289, training loss 0.000219, validation loss 0.065148\n",
      "Iter 4290, training loss 0.001297, validation loss 0.067152\n",
      "Iter 4291, training loss 0.003191, validation loss 0.066467\n",
      "Iter 4292, training loss 0.003873, validation loss 0.070454\n",
      "Iter 4293, training loss 0.002801, validation loss 0.066021\n",
      "Iter 4294, training loss 0.001077, validation loss 0.066561\n",
      "Iter 4295, training loss 0.000179, validation loss 0.064603\n",
      "Iter 4296, training loss 0.000594, validation loss 0.064490\n",
      "Iter 4297, training loss 0.001598, validation loss 0.067259\n",
      "Iter 4298, training loss 0.002121, validation loss 0.065324\n",
      "Iter 4299, training loss 0.001715, validation loss 0.067469\n",
      "Iter 4300, training loss 0.000816, validation loss 0.064549\n",
      "Iter 4301, training loss 0.000191, validation loss 0.064957\n",
      "Iter 4302, training loss 0.000238, validation loss 0.065098\n",
      "Iter 4303, training loss 0.000729, validation loss 0.064494\n",
      "Iter 4304, training loss 0.001118, validation loss 0.066664\n",
      "Iter 4305, training loss 0.001059, validation loss 0.064613\n",
      "Iter 4306, training loss 0.000639, validation loss 0.065860\n",
      "Iter 4307, training loss 0.000226, validation loss 0.064326\n",
      "Iter 4308, training loss 0.000112, validation loss 0.064435\n",
      "Iter 4309, training loss 0.000299, validation loss 0.065123\n",
      "Iter 4310, training loss 0.000552, validation loss 0.064230\n",
      "Iter 4311, training loss 0.000637, validation loss 0.065722\n",
      "Iter 4312, training loss 0.000497, validation loss 0.064155\n",
      "Iter 4313, training loss 0.000261, validation loss 0.064949\n",
      "Iter 4314, training loss 0.000106, validation loss 0.064216\n",
      "Iter 4315, training loss 0.000114, validation loss 0.064168\n",
      "Iter 4316, training loss 0.000233, validation loss 0.064844\n",
      "Iter 4317, training loss 0.000342, validation loss 0.064034\n",
      "Iter 4318, training loss 0.000351, validation loss 0.065107\n",
      "Iter 4319, training loss 0.000263, validation loss 0.064039\n",
      "Iter 4320, training loss 0.000148, validation loss 0.064643\n",
      "Iter 4321, training loss 0.000082, validation loss 0.064248\n",
      "Iter 4322, training loss 0.000093, validation loss 0.064198\n",
      "Iter 4323, training loss 0.000151, validation loss 0.064686\n",
      "Iter 4324, training loss 0.000202, validation loss 0.064067\n",
      "Iter 4325, training loss 0.000207, validation loss 0.064831\n",
      "Iter 4326, training loss 0.000166, validation loss 0.064065\n",
      "Iter 4327, training loss 0.000110, validation loss 0.064539\n",
      "Iter 4328, training loss 0.000073, validation loss 0.064202\n",
      "Iter 4329, training loss 0.000071, validation loss 0.064194\n",
      "Iter 4330, training loss 0.000095, validation loss 0.064452\n",
      "Iter 4331, training loss 0.000122, validation loss 0.064031\n",
      "Iter 4332, training loss 0.000132, validation loss 0.064567\n",
      "Iter 4333, training loss 0.000120, validation loss 0.064011\n",
      "Iter 4334, training loss 0.000095, validation loss 0.064429\n",
      "Iter 4335, training loss 0.000072, validation loss 0.064095\n",
      "Iter 4336, training loss 0.000061, validation loss 0.064199\n",
      "Iter 4337, training loss 0.000065, validation loss 0.064261\n",
      "Iter 4338, training loss 0.000077, validation loss 0.064053\n",
      "Iter 4339, training loss 0.000087, validation loss 0.064387\n",
      "Iter 4340, training loss 0.000089, validation loss 0.064011\n",
      "Iter 4341, training loss 0.000083, validation loss 0.064369\n",
      "Iter 4342, training loss 0.000071, validation loss 0.064049\n",
      "Iter 4343, training loss 0.000061, validation loss 0.064246\n",
      "Iter 4344, training loss 0.000055, validation loss 0.064146\n",
      "Iter 4345, training loss 0.000056, validation loss 0.064117\n",
      "Iter 4346, training loss 0.000060, validation loss 0.064247\n",
      "Iter 4347, training loss 0.000065, validation loss 0.064041\n",
      "Iter 4348, training loss 0.000067, validation loss 0.064290\n",
      "Iter 4349, training loss 0.000065, validation loss 0.064030\n",
      "Iter 4350, training loss 0.000061, validation loss 0.064253\n",
      "Iter 4351, training loss 0.000056, validation loss 0.064065\n",
      "Iter 4352, training loss 0.000052, validation loss 0.064171\n",
      "Iter 4353, training loss 0.000050, validation loss 0.064127\n",
      "Iter 4354, training loss 0.000050, validation loss 0.064094\n",
      "Iter 4355, training loss 0.000052, validation loss 0.064186\n",
      "Iter 4356, training loss 0.000054, validation loss 0.064049\n",
      "Iter 4357, training loss 0.000054, validation loss 0.064211\n",
      "Iter 4358, training loss 0.000053, validation loss 0.064040\n",
      "Iter 4359, training loss 0.000052, validation loss 0.064194\n",
      "Iter 4360, training loss 0.000050, validation loss 0.064060\n",
      "Iter 4361, training loss 0.000048, validation loss 0.064149\n",
      "Iter 4362, training loss 0.000046, validation loss 0.064095\n",
      "Iter 4363, training loss 0.000046, validation loss 0.064100\n",
      "Iter 4364, training loss 0.000046, validation loss 0.064129\n",
      "Iter 4365, training loss 0.000046, validation loss 0.064063\n",
      "Iter 4366, training loss 0.000046, validation loss 0.064150\n",
      "Iter 4367, training loss 0.000046, validation loss 0.064046\n",
      "Iter 4368, training loss 0.000046, validation loss 0.064151\n",
      "Iter 4369, training loss 0.000045, validation loss 0.064047\n",
      "Iter 4370, training loss 0.000044, validation loss 0.064134\n",
      "Iter 4371, training loss 0.000044, validation loss 0.064061\n",
      "Iter 4372, training loss 0.000043, validation loss 0.064109\n",
      "Iter 4373, training loss 0.000042, validation loss 0.064080\n",
      "Iter 4374, training loss 0.000042, validation loss 0.064082\n",
      "Iter 4375, training loss 0.000042, validation loss 0.064096\n",
      "Iter 4376, training loss 0.000042, validation loss 0.064061\n",
      "Iter 4377, training loss 0.000041, validation loss 0.064108\n",
      "Iter 4378, training loss 0.000041, validation loss 0.064050\n",
      "Iter 4379, training loss 0.000041, validation loss 0.064111\n",
      "Iter 4380, training loss 0.000041, validation loss 0.064046\n",
      "Iter 4381, training loss 0.000040, validation loss 0.064106\n",
      "Iter 4382, training loss 0.000040, validation loss 0.064049\n",
      "Iter 4383, training loss 0.000040, validation loss 0.064095\n",
      "Iter 4384, training loss 0.000039, validation loss 0.064055\n",
      "Iter 4385, training loss 0.000039, validation loss 0.064083\n",
      "Iter 4386, training loss 0.000038, validation loss 0.064063\n",
      "Iter 4387, training loss 0.000038, validation loss 0.064071\n",
      "Iter 4388, training loss 0.000038, validation loss 0.064071\n",
      "Iter 4389, training loss 0.000038, validation loss 0.064062\n",
      "Iter 4390, training loss 0.000037, validation loss 0.064077\n",
      "Iter 4391, training loss 0.000037, validation loss 0.064054\n",
      "Iter 4392, training loss 0.000037, validation loss 0.064081\n",
      "Iter 4393, training loss 0.000037, validation loss 0.064050\n",
      "Iter 4394, training loss 0.000037, validation loss 0.064083\n",
      "Iter 4395, training loss 0.000037, validation loss 0.064048\n",
      "Iter 4396, training loss 0.000036, validation loss 0.064083\n",
      "Iter 4397, training loss 0.000036, validation loss 0.064047\n",
      "Iter 4398, training loss 0.000036, validation loss 0.064082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4399, training loss 0.000036, validation loss 0.064046\n",
      "Iter 4400, training loss 0.000035, validation loss 0.064079\n",
      "Iter 4401, training loss 0.000035, validation loss 0.064046\n",
      "Iter 4402, training loss 0.000035, validation loss 0.064075\n",
      "Iter 4403, training loss 0.000035, validation loss 0.064045\n",
      "Iter 4404, training loss 0.000034, validation loss 0.064072\n",
      "Iter 4405, training loss 0.000034, validation loss 0.064044\n",
      "Iter 4406, training loss 0.000034, validation loss 0.064070\n",
      "Iter 4407, training loss 0.000034, validation loss 0.064043\n",
      "Iter 4408, training loss 0.000034, validation loss 0.064068\n",
      "Iter 4409, training loss 0.000033, validation loss 0.064042\n",
      "Iter 4410, training loss 0.000033, validation loss 0.064067\n",
      "Iter 4411, training loss 0.000033, validation loss 0.064040\n",
      "Iter 4412, training loss 0.000033, validation loss 0.064066\n",
      "Iter 4413, training loss 0.000033, validation loss 0.064037\n",
      "Iter 4414, training loss 0.000033, validation loss 0.064067\n",
      "Iter 4415, training loss 0.000033, validation loss 0.064034\n",
      "Iter 4416, training loss 0.000032, validation loss 0.064068\n",
      "Iter 4417, training loss 0.000032, validation loss 0.064029\n",
      "Iter 4418, training loss 0.000032, validation loss 0.064071\n",
      "Iter 4419, training loss 0.000032, validation loss 0.064022\n",
      "Iter 4420, training loss 0.000032, validation loss 0.064076\n",
      "Iter 4421, training loss 0.000032, validation loss 0.064014\n",
      "Iter 4422, training loss 0.000032, validation loss 0.064084\n",
      "Iter 4423, training loss 0.000032, validation loss 0.064002\n",
      "Iter 4424, training loss 0.000033, validation loss 0.064097\n",
      "Iter 4425, training loss 0.000033, validation loss 0.063986\n",
      "Iter 4426, training loss 0.000034, validation loss 0.064117\n",
      "Iter 4427, training loss 0.000036, validation loss 0.063964\n",
      "Iter 4428, training loss 0.000038, validation loss 0.064148\n",
      "Iter 4429, training loss 0.000041, validation loss 0.063933\n",
      "Iter 4430, training loss 0.000045, validation loss 0.064199\n",
      "Iter 4431, training loss 0.000052, validation loss 0.063891\n",
      "Iter 4432, training loss 0.000062, validation loss 0.064282\n",
      "Iter 4433, training loss 0.000078, validation loss 0.063835\n",
      "Iter 4434, training loss 0.000102, validation loss 0.064424\n",
      "Iter 4435, training loss 0.000138, validation loss 0.063770\n",
      "Iter 4436, training loss 0.000194, validation loss 0.064679\n",
      "Iter 4437, training loss 0.000281, validation loss 0.063719\n",
      "Iter 4438, training loss 0.000418, validation loss 0.065164\n",
      "Iter 4439, training loss 0.000632, validation loss 0.063767\n",
      "Iter 4440, training loss 0.000972, validation loss 0.066142\n",
      "Iter 4441, training loss 0.001512, validation loss 0.064174\n",
      "Iter 4442, training loss 0.002373, validation loss 0.068246\n",
      "Iter 4443, training loss 0.003755, validation loss 0.065693\n",
      "Iter 4444, training loss 0.005966, validation loss 0.073025\n",
      "Iter 4445, training loss 0.009530, validation loss 0.070402\n",
      "Iter 4446, training loss 0.015178, validation loss 0.084237\n",
      "Iter 4447, training loss 0.024171, validation loss 0.083633\n",
      "Iter 4448, training loss 0.037953, validation loss 0.110297\n",
      "Iter 4449, training loss 0.058815, validation loss 0.116854\n",
      "Iter 4450, training loss 0.087666, validation loss 0.164727\n",
      "Iter 4451, training loss 0.125300, validation loss 0.183062\n",
      "Iter 4452, training loss 0.164762, validation loss 0.246101\n",
      "Iter 4453, training loss 0.195350, validation loss 0.254811\n",
      "Iter 4454, training loss 0.195879, validation loss 0.276162\n",
      "Iter 4455, training loss 0.155411, validation loss 0.217147\n",
      "Iter 4456, training loss 0.082929, validation loss 0.154838\n",
      "Iter 4457, training loss 0.018981, validation loss 0.082558\n",
      "Iter 4458, training loss 0.002523, validation loss 0.067102\n",
      "Iter 4459, training loss 0.032192, validation loss 0.100102\n",
      "Iter 4460, training loss 0.064141, validation loss 0.127921\n",
      "Iter 4461, training loss 0.058131, validation loss 0.127111\n",
      "Iter 4462, training loss 0.022890, validation loss 0.086401\n",
      "Iter 4463, training loss 0.001166, validation loss 0.065497\n",
      "Iter 4464, training loss 0.013537, validation loss 0.078969\n",
      "Iter 4465, training loss 0.033655, validation loss 0.096267\n",
      "Iter 4466, training loss 0.029563, validation loss 0.095440\n",
      "Iter 4467, training loss 0.008651, validation loss 0.070857\n",
      "Iter 4468, training loss 0.001236, validation loss 0.063663\n",
      "Iter 4469, training loss 0.013162, validation loss 0.077206\n",
      "Iter 4470, training loss 0.020914, validation loss 0.082268\n",
      "Iter 4471, training loss 0.011645, validation loss 0.075277\n",
      "Iter 4472, training loss 0.001135, validation loss 0.062930\n",
      "Iter 4473, training loss 0.004382, validation loss 0.065786\n",
      "Iter 4474, training loss 0.012237, validation loss 0.075749\n",
      "Iter 4475, training loss 0.010091, validation loss 0.071185\n",
      "Iter 4476, training loss 0.002173, validation loss 0.064548\n",
      "Iter 4477, training loss 0.001434, validation loss 0.063660\n",
      "Iter 4478, training loss 0.006807, validation loss 0.067915\n",
      "Iter 4479, training loss 0.007479, validation loss 0.070555\n",
      "Iter 4480, training loss 0.002518, validation loss 0.063857\n",
      "Iter 4481, training loss 0.000564, validation loss 0.062229\n",
      "Iter 4482, training loss 0.003709, validation loss 0.066326\n",
      "Iter 4483, training loss 0.005210, validation loss 0.066257\n",
      "Iter 4484, training loss 0.002391, validation loss 0.064769\n",
      "Iter 4485, training loss 0.000358, validation loss 0.062047\n",
      "Iter 4486, training loss 0.001973, validation loss 0.063131\n",
      "Iter 4487, training loss 0.003496, validation loss 0.065998\n",
      "Iter 4488, training loss 0.002089, validation loss 0.063124\n",
      "Iter 4489, training loss 0.000371, validation loss 0.062033\n",
      "Iter 4490, training loss 0.001008, validation loss 0.062952\n",
      "Iter 4491, training loss 0.002265, validation loss 0.063165\n",
      "Iter 4492, training loss 0.001729, validation loss 0.063857\n",
      "Iter 4493, training loss 0.000442, validation loss 0.061713\n",
      "Iter 4494, training loss 0.000506, validation loss 0.061721\n",
      "Iter 4495, training loss 0.001407, validation loss 0.063430\n",
      "Iter 4496, training loss 0.001365, validation loss 0.062285\n",
      "Iter 4497, training loss 0.000504, validation loss 0.062179\n",
      "Iter 4498, training loss 0.000278, validation loss 0.061774\n",
      "Iter 4499, training loss 0.000836, validation loss 0.061810\n",
      "Iter 4500, training loss 0.001029, validation loss 0.062869\n",
      "Iter 4501, training loss 0.000531, validation loss 0.061567\n",
      "Iter 4502, training loss 0.000204, validation loss 0.061503\n",
      "Iter 4503, training loss 0.000475, validation loss 0.062053\n",
      "Iter 4504, training loss 0.000735, validation loss 0.061631\n",
      "Iter 4505, training loss 0.000515, validation loss 0.062086\n",
      "Iter 4506, training loss 0.000206, validation loss 0.061362\n",
      "Iter 4507, training loss 0.000270, validation loss 0.061331\n",
      "Iter 4508, training loss 0.000494, validation loss 0.062024\n",
      "Iter 4509, training loss 0.000459, validation loss 0.061371\n",
      "Iter 4510, training loss 0.000235, validation loss 0.061572\n",
      "Iter 4511, training loss 0.000176, validation loss 0.061425\n",
      "Iter 4512, training loss 0.000315, validation loss 0.061282\n",
      "Iter 4513, training loss 0.000374, validation loss 0.061831\n",
      "Iter 4514, training loss 0.000255, validation loss 0.061259\n",
      "Iter 4515, training loss 0.000152, validation loss 0.061357\n",
      "Iter 4516, training loss 0.000201, validation loss 0.061516\n",
      "Iter 4517, training loss 0.000281, validation loss 0.061244\n",
      "Iter 4518, training loss 0.000249, validation loss 0.061614\n",
      "Iter 4519, training loss 0.000161, validation loss 0.061237\n",
      "Iter 4520, training loss 0.000145, validation loss 0.061246\n",
      "Iter 4521, training loss 0.000200, validation loss 0.061496\n",
      "Iter 4522, training loss 0.000219, validation loss 0.061177\n",
      "Iter 4523, training loss 0.000171, validation loss 0.061414\n",
      "Iter 4524, training loss 0.000130, validation loss 0.061223\n",
      "Iter 4525, training loss 0.000146, validation loss 0.061164\n",
      "Iter 4526, training loss 0.000177, validation loss 0.061410\n",
      "Iter 4527, training loss 0.000168, validation loss 0.061122\n",
      "Iter 4528, training loss 0.000132, validation loss 0.061272\n",
      "Iter 4529, training loss 0.000120, validation loss 0.061206\n",
      "Iter 4530, training loss 0.000138, validation loss 0.061108\n",
      "Iter 4531, training loss 0.000150, validation loss 0.061319\n",
      "Iter 4532, training loss 0.000135, validation loss 0.061094\n",
      "Iter 4533, training loss 0.000115, validation loss 0.061189\n",
      "Iter 4534, training loss 0.000114, validation loss 0.061193\n",
      "Iter 4535, training loss 0.000127, validation loss 0.061084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4536, training loss 0.000128, validation loss 0.061256\n",
      "Iter 4537, training loss 0.000116, validation loss 0.061091\n",
      "Iter 4538, training loss 0.000105, validation loss 0.061146\n",
      "Iter 4539, training loss 0.000108, validation loss 0.061179\n",
      "Iter 4540, training loss 0.000115, validation loss 0.061070\n",
      "Iter 4541, training loss 0.000113, validation loss 0.061204\n",
      "Iter 4542, training loss 0.000104, validation loss 0.061083\n",
      "Iter 4543, training loss 0.000099, validation loss 0.061112\n",
      "Iter 4544, training loss 0.000101, validation loss 0.061152\n",
      "Iter 4545, training loss 0.000104, validation loss 0.061056\n",
      "Iter 4546, training loss 0.000102, validation loss 0.061160\n",
      "Iter 4547, training loss 0.000096, validation loss 0.061074\n",
      "Iter 4548, training loss 0.000093, validation loss 0.061090\n",
      "Iter 4549, training loss 0.000095, validation loss 0.061128\n",
      "Iter 4550, training loss 0.000096, validation loss 0.061048\n",
      "Iter 4551, training loss 0.000094, validation loss 0.061128\n",
      "Iter 4552, training loss 0.000090, validation loss 0.061062\n",
      "Iter 4553, training loss 0.000089, validation loss 0.061071\n",
      "Iter 4554, training loss 0.000089, validation loss 0.061101\n",
      "Iter 4555, training loss 0.000090, validation loss 0.061036\n",
      "Iter 4556, training loss 0.000088, validation loss 0.061098\n",
      "Iter 4557, training loss 0.000086, validation loss 0.061046\n",
      "Iter 4558, training loss 0.000084, validation loss 0.061053\n",
      "Iter 4559, training loss 0.000084, validation loss 0.061076\n",
      "Iter 4560, training loss 0.000084, validation loss 0.061025\n",
      "Iter 4561, training loss 0.000083, validation loss 0.061074\n",
      "Iter 4562, training loss 0.000081, validation loss 0.061032\n",
      "Iter 4563, training loss 0.000080, validation loss 0.061041\n",
      "Iter 4564, training loss 0.000080, validation loss 0.061055\n",
      "Iter 4565, training loss 0.000080, validation loss 0.061017\n",
      "Iter 4566, training loss 0.000079, validation loss 0.061057\n",
      "Iter 4567, training loss 0.000078, validation loss 0.061022\n",
      "Iter 4568, training loss 0.000077, validation loss 0.061032\n",
      "Iter 4569, training loss 0.000076, validation loss 0.061039\n",
      "Iter 4570, training loss 0.000076, validation loss 0.061012\n",
      "Iter 4571, training loss 0.000075, validation loss 0.061042\n",
      "Iter 4572, training loss 0.000074, validation loss 0.061012\n",
      "Iter 4573, training loss 0.000073, validation loss 0.061024\n",
      "Iter 4574, training loss 0.000073, validation loss 0.061023\n",
      "Iter 4575, training loss 0.000072, validation loss 0.061005\n",
      "Iter 4576, training loss 0.000072, validation loss 0.061026\n",
      "Iter 4577, training loss 0.000071, validation loss 0.061000\n",
      "Iter 4578, training loss 0.000070, validation loss 0.061013\n",
      "Iter 4579, training loss 0.000070, validation loss 0.061007\n",
      "Iter 4580, training loss 0.000069, validation loss 0.060996\n",
      "Iter 4581, training loss 0.000069, validation loss 0.061010\n",
      "Iter 4582, training loss 0.000068, validation loss 0.060989\n",
      "Iter 4583, training loss 0.000068, validation loss 0.061001\n",
      "Iter 4584, training loss 0.000067, validation loss 0.060991\n",
      "Iter 4585, training loss 0.000066, validation loss 0.060987\n",
      "Iter 4586, training loss 0.000066, validation loss 0.060994\n",
      "Iter 4587, training loss 0.000066, validation loss 0.060977\n",
      "Iter 4588, training loss 0.000065, validation loss 0.060988\n",
      "Iter 4589, training loss 0.000064, validation loss 0.060976\n",
      "Iter 4590, training loss 0.000064, validation loss 0.060976\n",
      "Iter 4591, training loss 0.000063, validation loss 0.060978\n",
      "Iter 4592, training loss 0.000063, validation loss 0.060967\n",
      "Iter 4593, training loss 0.000063, validation loss 0.060976\n",
      "Iter 4594, training loss 0.000062, validation loss 0.060964\n",
      "Iter 4595, training loss 0.000062, validation loss 0.060968\n",
      "Iter 4596, training loss 0.000061, validation loss 0.060965\n",
      "Iter 4597, training loss 0.000061, validation loss 0.060959\n",
      "Iter 4598, training loss 0.000060, validation loss 0.060965\n",
      "Iter 4599, training loss 0.000060, validation loss 0.060955\n",
      "Iter 4600, training loss 0.000059, validation loss 0.060960\n",
      "Iter 4601, training loss 0.000059, validation loss 0.060955\n",
      "Iter 4602, training loss 0.000059, validation loss 0.060952\n",
      "Iter 4603, training loss 0.000058, validation loss 0.060954\n",
      "Iter 4604, training loss 0.000058, validation loss 0.060946\n",
      "Iter 4605, training loss 0.000057, validation loss 0.060951\n",
      "Iter 4606, training loss 0.000057, validation loss 0.060944\n",
      "Iter 4607, training loss 0.000057, validation loss 0.060945\n",
      "Iter 4608, training loss 0.000056, validation loss 0.060943\n",
      "Iter 4609, training loss 0.000056, validation loss 0.060939\n",
      "Iter 4610, training loss 0.000055, validation loss 0.060941\n",
      "Iter 4611, training loss 0.000055, validation loss 0.060935\n",
      "Iter 4612, training loss 0.000055, validation loss 0.060937\n",
      "Iter 4613, training loss 0.000054, validation loss 0.060933\n",
      "Iter 4614, training loss 0.000054, validation loss 0.060932\n",
      "Iter 4615, training loss 0.000054, validation loss 0.060931\n",
      "Iter 4616, training loss 0.000053, validation loss 0.060927\n",
      "Iter 4617, training loss 0.000053, validation loss 0.060928\n",
      "Iter 4618, training loss 0.000053, validation loss 0.060923\n",
      "Iter 4619, training loss 0.000052, validation loss 0.060924\n",
      "Iter 4620, training loss 0.000052, validation loss 0.060921\n",
      "Iter 4621, training loss 0.000052, validation loss 0.060919\n",
      "Iter 4622, training loss 0.000051, validation loss 0.060919\n",
      "Iter 4623, training loss 0.000051, validation loss 0.060915\n",
      "Iter 4624, training loss 0.000051, validation loss 0.060916\n",
      "Iter 4625, training loss 0.000050, validation loss 0.060912\n",
      "Iter 4626, training loss 0.000050, validation loss 0.060912\n",
      "Iter 4627, training loss 0.000050, validation loss 0.060910\n",
      "Iter 4628, training loss 0.000049, validation loss 0.060908\n",
      "Iter 4629, training loss 0.000049, validation loss 0.060908\n",
      "Iter 4630, training loss 0.000049, validation loss 0.060905\n",
      "Iter 4631, training loss 0.000048, validation loss 0.060905\n",
      "Iter 4632, training loss 0.000048, validation loss 0.060902\n",
      "Iter 4633, training loss 0.000048, validation loss 0.060901\n",
      "Iter 4634, training loss 0.000048, validation loss 0.060900\n",
      "Iter 4635, training loss 0.000047, validation loss 0.060898\n",
      "Iter 4636, training loss 0.000047, validation loss 0.060898\n",
      "Iter 4637, training loss 0.000047, validation loss 0.060896\n",
      "Iter 4638, training loss 0.000047, validation loss 0.060896\n",
      "Iter 4639, training loss 0.000046, validation loss 0.060893\n",
      "Iter 4640, training loss 0.000046, validation loss 0.060892\n",
      "Iter 4641, training loss 0.000046, validation loss 0.060891\n",
      "Iter 4642, training loss 0.000045, validation loss 0.060890\n",
      "Iter 4643, training loss 0.000045, validation loss 0.060890\n",
      "Iter 4644, training loss 0.000045, validation loss 0.060887\n",
      "Iter 4645, training loss 0.000045, validation loss 0.060887\n",
      "Iter 4646, training loss 0.000044, validation loss 0.060885\n",
      "Iter 4647, training loss 0.000044, validation loss 0.060884\n",
      "Iter 4648, training loss 0.000044, validation loss 0.060883\n",
      "Iter 4649, training loss 0.000044, validation loss 0.060882\n",
      "Iter 4650, training loss 0.000043, validation loss 0.060881\n",
      "Iter 4651, training loss 0.000043, validation loss 0.060879\n",
      "Iter 4652, training loss 0.000043, validation loss 0.060879\n",
      "Iter 4653, training loss 0.000043, validation loss 0.060877\n",
      "Iter 4654, training loss 0.000043, validation loss 0.060876\n",
      "Iter 4655, training loss 0.000042, validation loss 0.060875\n",
      "Iter 4656, training loss 0.000042, validation loss 0.060873\n",
      "Iter 4657, training loss 0.000042, validation loss 0.060873\n",
      "Iter 4658, training loss 0.000042, validation loss 0.060871\n",
      "Iter 4659, training loss 0.000041, validation loss 0.060870\n",
      "Iter 4660, training loss 0.000041, validation loss 0.060869\n",
      "Iter 4661, training loss 0.000041, validation loss 0.060868\n",
      "Iter 4662, training loss 0.000041, validation loss 0.060867\n",
      "Iter 4663, training loss 0.000041, validation loss 0.060866\n",
      "Iter 4664, training loss 0.000040, validation loss 0.060865\n",
      "Iter 4665, training loss 0.000040, validation loss 0.060863\n",
      "Iter 4666, training loss 0.000040, validation loss 0.060862\n",
      "Iter 4667, training loss 0.000040, validation loss 0.060861\n",
      "Iter 4668, training loss 0.000040, validation loss 0.060860\n",
      "Iter 4669, training loss 0.000039, validation loss 0.060859\n",
      "Iter 4670, training loss 0.000039, validation loss 0.060858\n",
      "Iter 4671, training loss 0.000039, validation loss 0.060857\n",
      "Iter 4672, training loss 0.000039, validation loss 0.060855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4673, training loss 0.000039, validation loss 0.060854\n",
      "Iter 4674, training loss 0.000038, validation loss 0.060853\n",
      "Iter 4675, training loss 0.000038, validation loss 0.060852\n",
      "Iter 4676, training loss 0.000038, validation loss 0.060851\n",
      "Iter 4677, training loss 0.000038, validation loss 0.060850\n",
      "Iter 4678, training loss 0.000038, validation loss 0.060849\n",
      "Iter 4679, training loss 0.000037, validation loss 0.060848\n",
      "Iter 4680, training loss 0.000037, validation loss 0.060847\n",
      "Iter 4681, training loss 0.000037, validation loss 0.060846\n",
      "Iter 4682, training loss 0.000037, validation loss 0.060845\n",
      "Iter 4683, training loss 0.000037, validation loss 0.060844\n",
      "Iter 4684, training loss 0.000037, validation loss 0.060843\n",
      "Iter 4685, training loss 0.000036, validation loss 0.060842\n",
      "Iter 4686, training loss 0.000036, validation loss 0.060842\n",
      "Iter 4687, training loss 0.000036, validation loss 0.060841\n",
      "Iter 4688, training loss 0.000036, validation loss 0.060840\n",
      "Iter 4689, training loss 0.000036, validation loss 0.060839\n",
      "Iter 4690, training loss 0.000035, validation loss 0.060838\n",
      "Iter 4691, training loss 0.000035, validation loss 0.060837\n",
      "Iter 4692, training loss 0.000035, validation loss 0.060836\n",
      "Iter 4693, training loss 0.000035, validation loss 0.060835\n",
      "Iter 4694, training loss 0.000035, validation loss 0.060834\n",
      "Iter 4695, training loss 0.000035, validation loss 0.060834\n",
      "Iter 4696, training loss 0.000035, validation loss 0.060833\n",
      "Iter 4697, training loss 0.000034, validation loss 0.060832\n",
      "Iter 4698, training loss 0.000034, validation loss 0.060831\n",
      "Iter 4699, training loss 0.000034, validation loss 0.060830\n",
      "Iter 4700, training loss 0.000034, validation loss 0.060829\n",
      "Iter 4701, training loss 0.000034, validation loss 0.060828\n",
      "Iter 4702, training loss 0.000034, validation loss 0.060827\n",
      "Iter 4703, training loss 0.000033, validation loss 0.060826\n",
      "Iter 4704, training loss 0.000033, validation loss 0.060825\n",
      "Iter 4705, training loss 0.000033, validation loss 0.060825\n",
      "Iter 4706, training loss 0.000033, validation loss 0.060824\n",
      "Iter 4707, training loss 0.000033, validation loss 0.060823\n",
      "Iter 4708, training loss 0.000033, validation loss 0.060822\n",
      "Iter 4709, training loss 0.000033, validation loss 0.060822\n",
      "Iter 4710, training loss 0.000032, validation loss 0.060821\n",
      "Iter 4711, training loss 0.000032, validation loss 0.060820\n",
      "Iter 4712, training loss 0.000032, validation loss 0.060819\n",
      "Iter 4713, training loss 0.000032, validation loss 0.060819\n",
      "Iter 4714, training loss 0.000032, validation loss 0.060818\n",
      "Iter 4715, training loss 0.000032, validation loss 0.060818\n",
      "Iter 4716, training loss 0.000032, validation loss 0.060817\n",
      "Iter 4717, training loss 0.000031, validation loss 0.060816\n",
      "Iter 4718, training loss 0.000031, validation loss 0.060816\n",
      "Iter 4719, training loss 0.000031, validation loss 0.060816\n",
      "Iter 4720, training loss 0.000031, validation loss 0.060815\n",
      "Iter 4721, training loss 0.000031, validation loss 0.060814\n",
      "Iter 4722, training loss 0.000031, validation loss 0.060814\n",
      "Iter 4723, training loss 0.000031, validation loss 0.060813\n",
      "Iter 4724, training loss 0.000031, validation loss 0.060813\n",
      "Iter 4725, training loss 0.000030, validation loss 0.060812\n",
      "Iter 4726, training loss 0.000030, validation loss 0.060812\n",
      "Iter 4727, training loss 0.000030, validation loss 0.060811\n",
      "Iter 4728, training loss 0.000030, validation loss 0.060811\n",
      "Iter 4729, training loss 0.000030, validation loss 0.060811\n",
      "Iter 4730, training loss 0.000030, validation loss 0.060810\n",
      "Iter 4731, training loss 0.000030, validation loss 0.060810\n",
      "Iter 4732, training loss 0.000030, validation loss 0.060809\n",
      "Iter 4733, training loss 0.000029, validation loss 0.060809\n",
      "Iter 4734, training loss 0.000029, validation loss 0.060809\n",
      "Iter 4735, training loss 0.000029, validation loss 0.060808\n",
      "Iter 4736, training loss 0.000029, validation loss 0.060808\n",
      "Iter 4737, training loss 0.000029, validation loss 0.060807\n",
      "Iter 4738, training loss 0.000029, validation loss 0.060807\n",
      "Iter 4739, training loss 0.000029, validation loss 0.060806\n",
      "Iter 4740, training loss 0.000029, validation loss 0.060807\n",
      "Iter 4741, training loss 0.000028, validation loss 0.060806\n",
      "Iter 4742, training loss 0.000028, validation loss 0.060806\n",
      "Iter 4743, training loss 0.000028, validation loss 0.060805\n",
      "Iter 4744, training loss 0.000028, validation loss 0.060806\n",
      "Iter 4745, training loss 0.000028, validation loss 0.060804\n",
      "Iter 4746, training loss 0.000028, validation loss 0.060805\n",
      "Iter 4747, training loss 0.000028, validation loss 0.060803\n",
      "Iter 4748, training loss 0.000028, validation loss 0.060804\n",
      "Iter 4749, training loss 0.000028, validation loss 0.060802\n",
      "Iter 4750, training loss 0.000027, validation loss 0.060804\n",
      "Iter 4751, training loss 0.000027, validation loss 0.060801\n",
      "Iter 4752, training loss 0.000027, validation loss 0.060803\n",
      "Iter 4753, training loss 0.000027, validation loss 0.060799\n",
      "Iter 4754, training loss 0.000027, validation loss 0.060804\n",
      "Iter 4755, training loss 0.000027, validation loss 0.060798\n",
      "Iter 4756, training loss 0.000027, validation loss 0.060804\n",
      "Iter 4757, training loss 0.000027, validation loss 0.060795\n",
      "Iter 4758, training loss 0.000027, validation loss 0.060805\n",
      "Iter 4759, training loss 0.000027, validation loss 0.060793\n",
      "Iter 4760, training loss 0.000027, validation loss 0.060807\n",
      "Iter 4761, training loss 0.000026, validation loss 0.060788\n",
      "Iter 4762, training loss 0.000026, validation loss 0.060810\n",
      "Iter 4763, training loss 0.000026, validation loss 0.060783\n",
      "Iter 4764, training loss 0.000027, validation loss 0.060816\n",
      "Iter 4765, training loss 0.000027, validation loss 0.060774\n",
      "Iter 4766, training loss 0.000027, validation loss 0.060827\n",
      "Iter 4767, training loss 0.000028, validation loss 0.060762\n",
      "Iter 4768, training loss 0.000029, validation loss 0.060844\n",
      "Iter 4769, training loss 0.000031, validation loss 0.060743\n",
      "Iter 4770, training loss 0.000034, validation loss 0.060875\n",
      "Iter 4771, training loss 0.000038, validation loss 0.060716\n",
      "Iter 4772, training loss 0.000046, validation loss 0.060932\n",
      "Iter 4773, training loss 0.000060, validation loss 0.060680\n",
      "Iter 4774, training loss 0.000081, validation loss 0.061042\n",
      "Iter 4775, training loss 0.000118, validation loss 0.060644\n",
      "Iter 4776, training loss 0.000179, validation loss 0.061267\n",
      "Iter 4777, training loss 0.000282, validation loss 0.060651\n",
      "Iter 4778, training loss 0.000456, validation loss 0.061768\n",
      "Iter 4779, training loss 0.000754, validation loss 0.060863\n",
      "Iter 4780, training loss 0.001264, validation loss 0.062973\n",
      "Iter 4781, training loss 0.002142, validation loss 0.061828\n",
      "Iter 4782, training loss 0.003659, validation loss 0.066102\n",
      "Iter 4783, training loss 0.006292, validation loss 0.065313\n",
      "Iter 4784, training loss 0.010848, validation loss 0.074685\n",
      "Iter 4785, training loss 0.018759, validation loss 0.076871\n",
      "Iter 4786, training loss 0.032276, validation loss 0.098800\n",
      "Iter 4787, training loss 0.055239, validation loss 0.112540\n",
      "Iter 4788, training loss 0.092434, validation loss 0.163993\n",
      "Iter 4789, training loss 0.150416, validation loss 0.208436\n",
      "Iter 4790, training loss 0.229412, validation loss 0.308604\n",
      "Iter 4791, training loss 0.319775, validation loss 0.382472\n",
      "Iter 4792, training loss 0.377207, validation loss 0.460146\n",
      "Iter 4793, training loss 0.345944, validation loss 0.413005\n",
      "Iter 4794, training loss 0.201839, validation loss 0.272720\n",
      "Iter 4795, training loss 0.042448, validation loss 0.104438\n",
      "Iter 4796, training loss 0.008736, validation loss 0.070134\n",
      "Iter 4797, training loss 0.094551, validation loss 0.158367\n",
      "Iter 4798, training loss 0.133191, validation loss 0.200104\n",
      "Iter 4799, training loss 0.054875, validation loss 0.116331\n",
      "Iter 4800, training loss 0.001957, validation loss 0.062420\n",
      "Iter 4801, training loss 0.051394, validation loss 0.115140\n",
      "Iter 4802, training loss 0.073049, validation loss 0.133142\n",
      "Iter 4803, training loss 0.017947, validation loss 0.079226\n",
      "Iter 4804, training loss 0.009038, validation loss 0.069222\n",
      "Iter 4805, training loss 0.047430, validation loss 0.105373\n",
      "Iter 4806, training loss 0.029035, validation loss 0.090148\n",
      "Iter 4807, training loss 0.001503, validation loss 0.059415\n",
      "Iter 4808, training loss 0.025839, validation loss 0.082521\n",
      "Iter 4809, training loss 0.027243, validation loss 0.087311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4810, training loss 0.002178, validation loss 0.058889\n",
      "Iter 4811, training loss 0.014213, validation loss 0.070296\n",
      "Iter 4812, training loss 0.021578, validation loss 0.080594\n",
      "Iter 4813, training loss 0.002955, validation loss 0.059126\n",
      "Iter 4814, training loss 0.008541, validation loss 0.064406\n",
      "Iter 4815, training loss 0.016160, validation loss 0.074368\n",
      "Iter 4816, training loss 0.002911, validation loss 0.058889\n",
      "Iter 4817, training loss 0.005724, validation loss 0.061554\n",
      "Iter 4818, training loss 0.011928, validation loss 0.069629\n",
      "Iter 4819, training loss 0.002447, validation loss 0.058421\n",
      "Iter 4820, training loss 0.004189, validation loss 0.060074\n",
      "Iter 4821, training loss 0.008791, validation loss 0.066200\n",
      "Iter 4822, training loss 0.001909, validation loss 0.057944\n",
      "Iter 4823, training loss 0.003250, validation loss 0.059200\n",
      "Iter 4824, training loss 0.006498, validation loss 0.063649\n",
      "Iter 4825, training loss 0.001456, validation loss 0.057512\n",
      "Iter 4826, training loss 0.002607, validation loss 0.058553\n",
      "Iter 4827, training loss 0.004825, validation loss 0.061695\n",
      "Iter 4828, training loss 0.001117, validation loss 0.057129\n",
      "Iter 4829, training loss 0.002123, validation loss 0.058020\n",
      "Iter 4830, training loss 0.003603, validation loss 0.060221\n",
      "Iter 4831, training loss 0.000878, validation loss 0.056838\n",
      "Iter 4832, training loss 0.001737, validation loss 0.057591\n",
      "Iter 4833, training loss 0.002711, validation loss 0.059141\n",
      "Iter 4834, training loss 0.000714, validation loss 0.056631\n",
      "Iter 4835, training loss 0.001421, validation loss 0.057231\n",
      "Iter 4836, training loss 0.002063, validation loss 0.058332\n",
      "Iter 4837, training loss 0.000603, validation loss 0.056454\n",
      "Iter 4838, training loss 0.001164, validation loss 0.056908\n",
      "Iter 4839, training loss 0.001592, validation loss 0.057690\n",
      "Iter 4840, training loss 0.000525, validation loss 0.056292\n",
      "Iter 4841, training loss 0.000955, validation loss 0.056629\n",
      "Iter 4842, training loss 0.001250, validation loss 0.057188\n",
      "Iter 4843, training loss 0.000468, validation loss 0.056158\n",
      "Iter 4844, training loss 0.000786, validation loss 0.056406\n",
      "Iter 4845, training loss 0.000998, validation loss 0.056821\n",
      "Iter 4846, training loss 0.000425, validation loss 0.056065\n",
      "Iter 4847, training loss 0.000652, validation loss 0.056244\n",
      "Iter 4848, training loss 0.000812, validation loss 0.056563\n",
      "Iter 4849, training loss 0.000391, validation loss 0.056006\n",
      "Iter 4850, training loss 0.000546, validation loss 0.056128\n",
      "Iter 4851, training loss 0.000673, validation loss 0.056383\n",
      "Iter 4852, training loss 0.000364, validation loss 0.055968\n",
      "Iter 4853, training loss 0.000463, validation loss 0.056046\n",
      "Iter 4854, training loss 0.000567, validation loss 0.056256\n",
      "Iter 4855, training loss 0.000341, validation loss 0.055944\n",
      "Iter 4856, training loss 0.000399, validation loss 0.055988\n",
      "Iter 4857, training loss 0.000486, validation loss 0.056158\n",
      "Iter 4858, training loss 0.000321, validation loss 0.055921\n",
      "Iter 4859, training loss 0.000349, validation loss 0.055938\n",
      "Iter 4860, training loss 0.000422, validation loss 0.056072\n",
      "Iter 4861, training loss 0.000304, validation loss 0.055892\n",
      "Iter 4862, training loss 0.000311, validation loss 0.055890\n",
      "Iter 4863, training loss 0.000371, validation loss 0.055994\n",
      "Iter 4864, training loss 0.000289, validation loss 0.055860\n",
      "Iter 4865, training loss 0.000281, validation loss 0.055848\n",
      "Iter 4866, training loss 0.000331, validation loss 0.055928\n",
      "Iter 4867, training loss 0.000274, validation loss 0.055831\n",
      "Iter 4868, training loss 0.000259, validation loss 0.055814\n",
      "Iter 4869, training loss 0.000298, validation loss 0.055876\n",
      "Iter 4870, training loss 0.000261, validation loss 0.055807\n",
      "Iter 4871, training loss 0.000241, validation loss 0.055789\n",
      "Iter 4872, training loss 0.000270, validation loss 0.055835\n",
      "Iter 4873, training loss 0.000249, validation loss 0.055789\n",
      "Iter 4874, training loss 0.000228, validation loss 0.055773\n",
      "Iter 4875, training loss 0.000248, validation loss 0.055806\n",
      "Iter 4876, training loss 0.000237, validation loss 0.055778\n",
      "Iter 4877, training loss 0.000217, validation loss 0.055766\n",
      "Iter 4878, training loss 0.000230, validation loss 0.055789\n",
      "Iter 4879, training loss 0.000225, validation loss 0.055774\n",
      "Iter 4880, training loss 0.000208, validation loss 0.055766\n",
      "Iter 4881, training loss 0.000215, validation loss 0.055782\n",
      "Iter 4882, training loss 0.000214, validation loss 0.055776\n",
      "Iter 4883, training loss 0.000200, validation loss 0.055771\n",
      "Iter 4884, training loss 0.000202, validation loss 0.055779\n",
      "Iter 4885, training loss 0.000204, validation loss 0.055777\n",
      "Iter 4886, training loss 0.000193, validation loss 0.055774\n",
      "Iter 4887, training loss 0.000192, validation loss 0.055776\n",
      "Iter 4888, training loss 0.000194, validation loss 0.055776\n",
      "Iter 4889, training loss 0.000187, validation loss 0.055774\n",
      "Iter 4890, training loss 0.000183, validation loss 0.055772\n",
      "Iter 4891, training loss 0.000185, validation loss 0.055772\n",
      "Iter 4892, training loss 0.000180, validation loss 0.055772\n",
      "Iter 4893, training loss 0.000176, validation loss 0.055767\n",
      "Iter 4894, training loss 0.000177, validation loss 0.055767\n",
      "Iter 4895, training loss 0.000174, validation loss 0.055768\n",
      "Iter 4896, training loss 0.000170, validation loss 0.055763\n",
      "Iter 4897, training loss 0.000170, validation loss 0.055763\n",
      "Iter 4898, training loss 0.000168, validation loss 0.055763\n",
      "Iter 4899, training loss 0.000164, validation loss 0.055759\n",
      "Iter 4900, training loss 0.000163, validation loss 0.055758\n",
      "Iter 4901, training loss 0.000162, validation loss 0.055758\n",
      "Iter 4902, training loss 0.000159, validation loss 0.055754\n",
      "Iter 4903, training loss 0.000158, validation loss 0.055752\n",
      "Iter 4904, training loss 0.000157, validation loss 0.055752\n",
      "Iter 4905, training loss 0.000154, validation loss 0.055749\n",
      "Iter 4906, training loss 0.000152, validation loss 0.055747\n",
      "Iter 4907, training loss 0.000152, validation loss 0.055746\n",
      "Iter 4908, training loss 0.000150, validation loss 0.055743\n",
      "Iter 4909, training loss 0.000148, validation loss 0.055741\n",
      "Iter 4910, training loss 0.000147, validation loss 0.055740\n",
      "Iter 4911, training loss 0.000145, validation loss 0.055738\n",
      "Iter 4912, training loss 0.000143, validation loss 0.055735\n",
      "Iter 4913, training loss 0.000142, validation loss 0.055734\n",
      "Iter 4914, training loss 0.000141, validation loss 0.055732\n",
      "Iter 4915, training loss 0.000139, validation loss 0.055730\n",
      "Iter 4916, training loss 0.000138, validation loss 0.055728\n",
      "Iter 4917, training loss 0.000137, validation loss 0.055726\n",
      "Iter 4918, training loss 0.000135, validation loss 0.055724\n",
      "Iter 4919, training loss 0.000134, validation loss 0.055722\n",
      "Iter 4920, training loss 0.000133, validation loss 0.055721\n",
      "Iter 4921, training loss 0.000131, validation loss 0.055718\n",
      "Iter 4922, training loss 0.000130, validation loss 0.055716\n",
      "Iter 4923, training loss 0.000129, validation loss 0.055715\n",
      "Iter 4924, training loss 0.000128, validation loss 0.055713\n",
      "Iter 4925, training loss 0.000127, validation loss 0.055711\n",
      "Iter 4926, training loss 0.000125, validation loss 0.055710\n",
      "Iter 4927, training loss 0.000124, validation loss 0.055709\n",
      "Iter 4928, training loss 0.000123, validation loss 0.055708\n",
      "Iter 4929, training loss 0.000122, validation loss 0.055706\n",
      "Iter 4930, training loss 0.000121, validation loss 0.055705\n",
      "Iter 4931, training loss 0.000120, validation loss 0.055704\n",
      "Iter 4932, training loss 0.000119, validation loss 0.055703\n",
      "Iter 4933, training loss 0.000118, validation loss 0.055702\n",
      "Iter 4934, training loss 0.000117, validation loss 0.055701\n",
      "Iter 4935, training loss 0.000116, validation loss 0.055700\n",
      "Iter 4936, training loss 0.000115, validation loss 0.055699\n",
      "Iter 4937, training loss 0.000114, validation loss 0.055698\n",
      "Iter 4938, training loss 0.000113, validation loss 0.055697\n",
      "Iter 4939, training loss 0.000112, validation loss 0.055697\n",
      "Iter 4940, training loss 0.000111, validation loss 0.055696\n",
      "Iter 4941, training loss 0.000110, validation loss 0.055694\n",
      "Iter 4942, training loss 0.000109, validation loss 0.055693\n",
      "Iter 4943, training loss 0.000109, validation loss 0.055692\n",
      "Iter 4944, training loss 0.000108, validation loss 0.055691\n",
      "Iter 4945, training loss 0.000107, validation loss 0.055690\n",
      "Iter 4946, training loss 0.000106, validation loss 0.055688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4947, training loss 0.000105, validation loss 0.055686\n",
      "Iter 4948, training loss 0.000104, validation loss 0.055685\n",
      "Iter 4949, training loss 0.000103, validation loss 0.055683\n",
      "Iter 4950, training loss 0.000103, validation loss 0.055681\n",
      "Iter 4951, training loss 0.000102, validation loss 0.055680\n",
      "Iter 4952, training loss 0.000101, validation loss 0.055678\n",
      "Iter 4953, training loss 0.000100, validation loss 0.055676\n",
      "Iter 4954, training loss 0.000100, validation loss 0.055675\n",
      "Iter 4955, training loss 0.000099, validation loss 0.055673\n",
      "Iter 4956, training loss 0.000098, validation loss 0.055672\n",
      "Iter 4957, training loss 0.000097, validation loss 0.055671\n",
      "Iter 4958, training loss 0.000097, validation loss 0.055669\n",
      "Iter 4959, training loss 0.000096, validation loss 0.055667\n",
      "Iter 4960, training loss 0.000095, validation loss 0.055665\n",
      "Iter 4961, training loss 0.000094, validation loss 0.055663\n",
      "Iter 4962, training loss 0.000094, validation loss 0.055662\n",
      "Iter 4963, training loss 0.000093, validation loss 0.055660\n",
      "Iter 4964, training loss 0.000092, validation loss 0.055659\n",
      "Iter 4965, training loss 0.000092, validation loss 0.055657\n",
      "Iter 4966, training loss 0.000091, validation loss 0.055656\n",
      "Iter 4967, training loss 0.000090, validation loss 0.055655\n",
      "Iter 4968, training loss 0.000090, validation loss 0.055653\n",
      "Iter 4969, training loss 0.000089, validation loss 0.055652\n",
      "Iter 4970, training loss 0.000088, validation loss 0.055651\n",
      "Iter 4971, training loss 0.000088, validation loss 0.055650\n",
      "Iter 4972, training loss 0.000087, validation loss 0.055648\n",
      "Iter 4973, training loss 0.000087, validation loss 0.055647\n",
      "Iter 4974, training loss 0.000086, validation loss 0.055645\n",
      "Iter 4975, training loss 0.000085, validation loss 0.055643\n",
      "Iter 4976, training loss 0.000085, validation loss 0.055642\n",
      "Iter 4977, training loss 0.000084, validation loss 0.055640\n",
      "Iter 4978, training loss 0.000084, validation loss 0.055639\n",
      "Iter 4979, training loss 0.000083, validation loss 0.055637\n",
      "Iter 4980, training loss 0.000083, validation loss 0.055636\n",
      "Iter 4981, training loss 0.000082, validation loss 0.055634\n",
      "Iter 4982, training loss 0.000081, validation loss 0.055633\n",
      "Iter 4983, training loss 0.000081, validation loss 0.055632\n",
      "Iter 4984, training loss 0.000080, validation loss 0.055631\n",
      "Iter 4985, training loss 0.000080, validation loss 0.055629\n",
      "Iter 4986, training loss 0.000079, validation loss 0.055628\n",
      "Iter 4987, training loss 0.000079, validation loss 0.055627\n",
      "Iter 4988, training loss 0.000078, validation loss 0.055625\n",
      "Iter 4989, training loss 0.000078, validation loss 0.055624\n",
      "Iter 4990, training loss 0.000077, validation loss 0.055623\n",
      "Iter 4991, training loss 0.000077, validation loss 0.055622\n",
      "Iter 4992, training loss 0.000076, validation loss 0.055621\n",
      "Iter 4993, training loss 0.000076, validation loss 0.055620\n",
      "Iter 4994, training loss 0.000075, validation loss 0.055618\n",
      "Iter 4995, training loss 0.000075, validation loss 0.055617\n",
      "Iter 4996, training loss 0.000074, validation loss 0.055616\n",
      "Iter 4997, training loss 0.000074, validation loss 0.055615\n",
      "Iter 4998, training loss 0.000073, validation loss 0.055614\n",
      "Iter 4999, training loss 0.000073, validation loss 0.055613\n",
      "Iter 5000, training loss 0.000072, validation loss 0.055612\n",
      "Iter 5001, training loss 0.000072, validation loss 0.055611\n",
      "Iter 5002, training loss 0.000072, validation loss 0.055610\n",
      "Iter 5003, training loss 0.000071, validation loss 0.055609\n",
      "Iter 5004, training loss 0.000071, validation loss 0.055608\n",
      "Iter 5005, training loss 0.000070, validation loss 0.055607\n",
      "Iter 5006, training loss 0.000070, validation loss 0.055605\n",
      "Iter 5007, training loss 0.000069, validation loss 0.055604\n",
      "Iter 5008, training loss 0.000069, validation loss 0.055603\n",
      "Iter 5009, training loss 0.000069, validation loss 0.055602\n",
      "Iter 5010, training loss 0.000068, validation loss 0.055601\n",
      "Iter 5011, training loss 0.000068, validation loss 0.055600\n",
      "Iter 5012, training loss 0.000067, validation loss 0.055599\n",
      "Iter 5013, training loss 0.000067, validation loss 0.055598\n",
      "Iter 5014, training loss 0.000067, validation loss 0.055597\n",
      "Iter 5015, training loss 0.000066, validation loss 0.055596\n",
      "Iter 5016, training loss 0.000066, validation loss 0.055595\n",
      "Iter 5017, training loss 0.000065, validation loss 0.055594\n",
      "Iter 5018, training loss 0.000065, validation loss 0.055593\n",
      "Iter 5019, training loss 0.000065, validation loss 0.055592\n",
      "Iter 5020, training loss 0.000064, validation loss 0.055592\n",
      "Iter 5021, training loss 0.000064, validation loss 0.055591\n",
      "Iter 5022, training loss 0.000064, validation loss 0.055590\n",
      "Iter 5023, training loss 0.000063, validation loss 0.055589\n",
      "Iter 5024, training loss 0.000063, validation loss 0.055588\n",
      "Iter 5025, training loss 0.000062, validation loss 0.055587\n",
      "Iter 5026, training loss 0.000062, validation loss 0.055586\n",
      "Iter 5027, training loss 0.000062, validation loss 0.055585\n",
      "Iter 5028, training loss 0.000061, validation loss 0.055584\n",
      "Iter 5029, training loss 0.000061, validation loss 0.055583\n",
      "Iter 5030, training loss 0.000061, validation loss 0.055582\n",
      "Iter 5031, training loss 0.000060, validation loss 0.055581\n",
      "Iter 5032, training loss 0.000060, validation loss 0.055580\n",
      "Iter 5033, training loss 0.000060, validation loss 0.055579\n",
      "Iter 5034, training loss 0.000059, validation loss 0.055578\n",
      "Iter 5035, training loss 0.000059, validation loss 0.055576\n",
      "Iter 5036, training loss 0.000059, validation loss 0.055575\n",
      "Iter 5037, training loss 0.000058, validation loss 0.055574\n",
      "Iter 5038, training loss 0.000058, validation loss 0.055574\n",
      "Iter 5039, training loss 0.000058, validation loss 0.055573\n",
      "Iter 5040, training loss 0.000057, validation loss 0.055572\n",
      "Iter 5041, training loss 0.000057, validation loss 0.055571\n",
      "Iter 5042, training loss 0.000057, validation loss 0.055570\n",
      "Iter 5043, training loss 0.000057, validation loss 0.055569\n",
      "Iter 5044, training loss 0.000056, validation loss 0.055568\n",
      "Iter 5045, training loss 0.000056, validation loss 0.055567\n",
      "Iter 5046, training loss 0.000056, validation loss 0.055567\n",
      "Iter 5047, training loss 0.000055, validation loss 0.055566\n",
      "Iter 5048, training loss 0.000055, validation loss 0.055565\n",
      "Iter 5049, training loss 0.000055, validation loss 0.055564\n",
      "Iter 5050, training loss 0.000055, validation loss 0.055564\n",
      "Iter 5051, training loss 0.000054, validation loss 0.055563\n",
      "Iter 5052, training loss 0.000054, validation loss 0.055562\n",
      "Iter 5053, training loss 0.000054, validation loss 0.055561\n",
      "Iter 5054, training loss 0.000053, validation loss 0.055560\n",
      "Iter 5055, training loss 0.000053, validation loss 0.055559\n",
      "Iter 5056, training loss 0.000053, validation loss 0.055558\n",
      "Iter 5057, training loss 0.000053, validation loss 0.055558\n",
      "Iter 5058, training loss 0.000052, validation loss 0.055557\n",
      "Iter 5059, training loss 0.000052, validation loss 0.055556\n",
      "Iter 5060, training loss 0.000052, validation loss 0.055556\n",
      "Iter 5061, training loss 0.000052, validation loss 0.055555\n",
      "Iter 5062, training loss 0.000051, validation loss 0.055554\n",
      "Iter 5063, training loss 0.000051, validation loss 0.055553\n",
      "Iter 5064, training loss 0.000051, validation loss 0.055552\n",
      "Iter 5065, training loss 0.000050, validation loss 0.055551\n",
      "Iter 5066, training loss 0.000050, validation loss 0.055550\n",
      "Iter 5067, training loss 0.000050, validation loss 0.055549\n",
      "Iter 5068, training loss 0.000050, validation loss 0.055548\n",
      "Iter 5069, training loss 0.000050, validation loss 0.055548\n",
      "Iter 5070, training loss 0.000049, validation loss 0.055547\n",
      "Iter 5071, training loss 0.000049, validation loss 0.055545\n",
      "Iter 5072, training loss 0.000049, validation loss 0.055544\n",
      "Iter 5073, training loss 0.000049, validation loss 0.055543\n",
      "Iter 5074, training loss 0.000048, validation loss 0.055542\n",
      "Iter 5075, training loss 0.000048, validation loss 0.055542\n",
      "Iter 5076, training loss 0.000048, validation loss 0.055541\n",
      "Iter 5077, training loss 0.000048, validation loss 0.055539\n",
      "Iter 5078, training loss 0.000047, validation loss 0.055538\n",
      "Iter 5079, training loss 0.000047, validation loss 0.055537\n",
      "Iter 5080, training loss 0.000047, validation loss 0.055536\n",
      "Iter 5081, training loss 0.000047, validation loss 0.055536\n",
      "Iter 5082, training loss 0.000046, validation loss 0.055535\n",
      "Iter 5083, training loss 0.000046, validation loss 0.055534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5084, training loss 0.000046, validation loss 0.055533\n",
      "Iter 5085, training loss 0.000046, validation loss 0.055532\n",
      "Iter 5086, training loss 0.000046, validation loss 0.055531\n",
      "Iter 5087, training loss 0.000045, validation loss 0.055530\n",
      "Iter 5088, training loss 0.000045, validation loss 0.055529\n",
      "Iter 5089, training loss 0.000045, validation loss 0.055528\n",
      "Iter 5090, training loss 0.000045, validation loss 0.055527\n",
      "Iter 5091, training loss 0.000045, validation loss 0.055526\n",
      "Iter 5092, training loss 0.000044, validation loss 0.055525\n",
      "Iter 5093, training loss 0.000044, validation loss 0.055524\n",
      "Iter 5094, training loss 0.000044, validation loss 0.055523\n",
      "Iter 5095, training loss 0.000044, validation loss 0.055523\n",
      "Iter 5096, training loss 0.000044, validation loss 0.055522\n",
      "Iter 5097, training loss 0.000043, validation loss 0.055521\n",
      "Iter 5098, training loss 0.000043, validation loss 0.055520\n",
      "Iter 5099, training loss 0.000043, validation loss 0.055519\n",
      "Iter 5100, training loss 0.000043, validation loss 0.055518\n",
      "Iter 5101, training loss 0.000043, validation loss 0.055517\n",
      "Iter 5102, training loss 0.000042, validation loss 0.055516\n",
      "Iter 5103, training loss 0.000042, validation loss 0.055515\n",
      "Iter 5104, training loss 0.000042, validation loss 0.055514\n",
      "Iter 5105, training loss 0.000042, validation loss 0.055513\n",
      "Iter 5106, training loss 0.000042, validation loss 0.055512\n",
      "Iter 5107, training loss 0.000041, validation loss 0.055512\n",
      "Iter 5108, training loss 0.000041, validation loss 0.055511\n",
      "Iter 5109, training loss 0.000041, validation loss 0.055510\n",
      "Iter 5110, training loss 0.000041, validation loss 0.055509\n",
      "Iter 5111, training loss 0.000041, validation loss 0.055508\n",
      "Iter 5112, training loss 0.000041, validation loss 0.055507\n",
      "Iter 5113, training loss 0.000040, validation loss 0.055507\n",
      "Iter 5114, training loss 0.000040, validation loss 0.055506\n",
      "Iter 5115, training loss 0.000040, validation loss 0.055505\n",
      "Iter 5116, training loss 0.000040, validation loss 0.055504\n",
      "Iter 5117, training loss 0.000040, validation loss 0.055503\n",
      "Iter 5118, training loss 0.000040, validation loss 0.055502\n",
      "Iter 5119, training loss 0.000039, validation loss 0.055501\n",
      "Iter 5120, training loss 0.000039, validation loss 0.055500\n",
      "Iter 5121, training loss 0.000039, validation loss 0.055499\n",
      "Iter 5122, training loss 0.000039, validation loss 0.055498\n",
      "Iter 5123, training loss 0.000039, validation loss 0.055497\n",
      "Iter 5124, training loss 0.000039, validation loss 0.055497\n",
      "Iter 5125, training loss 0.000038, validation loss 0.055496\n",
      "Iter 5126, training loss 0.000038, validation loss 0.055495\n",
      "Iter 5127, training loss 0.000038, validation loss 0.055494\n",
      "Iter 5128, training loss 0.000038, validation loss 0.055493\n",
      "Iter 5129, training loss 0.000038, validation loss 0.055492\n",
      "Iter 5130, training loss 0.000038, validation loss 0.055491\n",
      "Iter 5131, training loss 0.000037, validation loss 0.055490\n",
      "Iter 5132, training loss 0.000037, validation loss 0.055489\n",
      "Iter 5133, training loss 0.000037, validation loss 0.055488\n",
      "Iter 5134, training loss 0.000037, validation loss 0.055487\n",
      "Iter 5135, training loss 0.000037, validation loss 0.055487\n",
      "Iter 5136, training loss 0.000037, validation loss 0.055486\n",
      "Iter 5137, training loss 0.000036, validation loss 0.055485\n",
      "Iter 5138, training loss 0.000036, validation loss 0.055484\n",
      "Iter 5139, training loss 0.000036, validation loss 0.055483\n",
      "Iter 5140, training loss 0.000036, validation loss 0.055482\n",
      "Iter 5141, training loss 0.000036, validation loss 0.055481\n",
      "Iter 5142, training loss 0.000036, validation loss 0.055480\n",
      "Iter 5143, training loss 0.000036, validation loss 0.055479\n",
      "Iter 5144, training loss 0.000035, validation loss 0.055478\n",
      "Iter 5145, training loss 0.000035, validation loss 0.055477\n",
      "Iter 5146, training loss 0.000035, validation loss 0.055477\n",
      "Iter 5147, training loss 0.000035, validation loss 0.055476\n",
      "Iter 5148, training loss 0.000035, validation loss 0.055475\n",
      "Iter 5149, training loss 0.000035, validation loss 0.055474\n",
      "Iter 5150, training loss 0.000035, validation loss 0.055473\n",
      "Iter 5151, training loss 0.000034, validation loss 0.055472\n",
      "Iter 5152, training loss 0.000034, validation loss 0.055471\n",
      "Iter 5153, training loss 0.000034, validation loss 0.055470\n",
      "Iter 5154, training loss 0.000034, validation loss 0.055469\n",
      "Iter 5155, training loss 0.000034, validation loss 0.055469\n",
      "Iter 5156, training loss 0.000034, validation loss 0.055468\n",
      "Iter 5157, training loss 0.000034, validation loss 0.055467\n",
      "Iter 5158, training loss 0.000034, validation loss 0.055466\n",
      "Iter 5159, training loss 0.000033, validation loss 0.055465\n",
      "Iter 5160, training loss 0.000033, validation loss 0.055464\n",
      "Iter 5161, training loss 0.000033, validation loss 0.055464\n",
      "Iter 5162, training loss 0.000033, validation loss 0.055463\n",
      "Iter 5163, training loss 0.000033, validation loss 0.055462\n",
      "Iter 5164, training loss 0.000033, validation loss 0.055461\n",
      "Iter 5165, training loss 0.000033, validation loss 0.055460\n",
      "Iter 5166, training loss 0.000032, validation loss 0.055460\n",
      "Iter 5167, training loss 0.000032, validation loss 0.055459\n",
      "Iter 5168, training loss 0.000032, validation loss 0.055458\n",
      "Iter 5169, training loss 0.000032, validation loss 0.055457\n",
      "Iter 5170, training loss 0.000032, validation loss 0.055457\n",
      "Iter 5171, training loss 0.000032, validation loss 0.055456\n",
      "Iter 5172, training loss 0.000032, validation loss 0.055455\n",
      "Iter 5173, training loss 0.000032, validation loss 0.055454\n",
      "Iter 5174, training loss 0.000031, validation loss 0.055453\n",
      "Iter 5175, training loss 0.000031, validation loss 0.055453\n",
      "Iter 5176, training loss 0.000031, validation loss 0.055452\n",
      "Iter 5177, training loss 0.000031, validation loss 0.055451\n",
      "Iter 5178, training loss 0.000031, validation loss 0.055451\n",
      "Iter 5179, training loss 0.000031, validation loss 0.055450\n",
      "Iter 5180, training loss 0.000031, validation loss 0.055449\n",
      "Iter 5181, training loss 0.000031, validation loss 0.055448\n",
      "Iter 5182, training loss 0.000031, validation loss 0.055448\n",
      "Iter 5183, training loss 0.000030, validation loss 0.055447\n",
      "Iter 5184, training loss 0.000030, validation loss 0.055446\n",
      "Iter 5185, training loss 0.000030, validation loss 0.055445\n",
      "Iter 5186, training loss 0.000030, validation loss 0.055444\n",
      "Iter 5187, training loss 0.000030, validation loss 0.055443\n",
      "Iter 5188, training loss 0.000030, validation loss 0.055443\n",
      "Iter 5189, training loss 0.000030, validation loss 0.055442\n",
      "Iter 5190, training loss 0.000030, validation loss 0.055441\n",
      "Iter 5191, training loss 0.000030, validation loss 0.055441\n",
      "Iter 5192, training loss 0.000029, validation loss 0.055440\n",
      "Iter 5193, training loss 0.000029, validation loss 0.055439\n",
      "Iter 5194, training loss 0.000029, validation loss 0.055438\n",
      "Iter 5195, training loss 0.000029, validation loss 0.055437\n",
      "Iter 5196, training loss 0.000029, validation loss 0.055437\n",
      "Iter 5197, training loss 0.000029, validation loss 0.055436\n",
      "Iter 5198, training loss 0.000029, validation loss 0.055435\n",
      "Iter 5199, training loss 0.000029, validation loss 0.055434\n",
      "Iter 5200, training loss 0.000029, validation loss 0.055433\n",
      "Iter 5201, training loss 0.000028, validation loss 0.055432\n",
      "Iter 5202, training loss 0.000028, validation loss 0.055432\n",
      "Iter 5203, training loss 0.000028, validation loss 0.055431\n",
      "Iter 5204, training loss 0.000028, validation loss 0.055430\n",
      "Iter 5205, training loss 0.000028, validation loss 0.055430\n",
      "Iter 5206, training loss 0.000028, validation loss 0.055429\n",
      "Iter 5207, training loss 0.000028, validation loss 0.055428\n",
      "Iter 5208, training loss 0.000028, validation loss 0.055428\n",
      "Iter 5209, training loss 0.000028, validation loss 0.055427\n",
      "Iter 5210, training loss 0.000028, validation loss 0.055427\n",
      "Iter 5211, training loss 0.000027, validation loss 0.055426\n",
      "Iter 5212, training loss 0.000027, validation loss 0.055425\n",
      "Iter 5213, training loss 0.000027, validation loss 0.055425\n",
      "Iter 5214, training loss 0.000027, validation loss 0.055424\n",
      "Iter 5215, training loss 0.000027, validation loss 0.055423\n",
      "Iter 5216, training loss 0.000027, validation loss 0.055423\n",
      "Iter 5217, training loss 0.000027, validation loss 0.055422\n",
      "Iter 5218, training loss 0.000027, validation loss 0.055421\n",
      "Iter 5219, training loss 0.000027, validation loss 0.055421\n",
      "Iter 5220, training loss 0.000027, validation loss 0.055420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5221, training loss 0.000026, validation loss 0.055420\n",
      "Iter 5222, training loss 0.000026, validation loss 0.055419\n",
      "Iter 5223, training loss 0.000026, validation loss 0.055419\n",
      "Iter 5224, training loss 0.000026, validation loss 0.055418\n",
      "Iter 5225, training loss 0.000026, validation loss 0.055418\n",
      "Iter 5226, training loss 0.000026, validation loss 0.055417\n",
      "Iter 5227, training loss 0.000026, validation loss 0.055416\n",
      "Iter 5228, training loss 0.000026, validation loss 0.055416\n",
      "Iter 5229, training loss 0.000026, validation loss 0.055415\n",
      "Iter 5230, training loss 0.000026, validation loss 0.055415\n",
      "Iter 5231, training loss 0.000026, validation loss 0.055414\n",
      "Iter 5232, training loss 0.000025, validation loss 0.055414\n",
      "Iter 5233, training loss 0.000025, validation loss 0.055413\n",
      "Iter 5234, training loss 0.000025, validation loss 0.055412\n",
      "Iter 5235, training loss 0.000025, validation loss 0.055412\n",
      "Iter 5236, training loss 0.000025, validation loss 0.055411\n",
      "Iter 5237, training loss 0.000025, validation loss 0.055411\n",
      "Iter 5238, training loss 0.000025, validation loss 0.055411\n",
      "Iter 5239, training loss 0.000025, validation loss 0.055410\n",
      "Iter 5240, training loss 0.000025, validation loss 0.055409\n",
      "Iter 5241, training loss 0.000025, validation loss 0.055409\n",
      "Iter 5242, training loss 0.000025, validation loss 0.055408\n",
      "Iter 5243, training loss 0.000024, validation loss 0.055408\n",
      "Iter 5244, training loss 0.000024, validation loss 0.055407\n",
      "Iter 5245, training loss 0.000024, validation loss 0.055407\n",
      "Iter 5246, training loss 0.000024, validation loss 0.055406\n",
      "Iter 5247, training loss 0.000024, validation loss 0.055405\n",
      "Iter 5248, training loss 0.000024, validation loss 0.055405\n",
      "Iter 5249, training loss 0.000024, validation loss 0.055404\n",
      "Iter 5250, training loss 0.000024, validation loss 0.055404\n",
      "Iter 5251, training loss 0.000024, validation loss 0.055403\n",
      "Iter 5252, training loss 0.000024, validation loss 0.055403\n",
      "Iter 5253, training loss 0.000024, validation loss 0.055402\n",
      "Iter 5254, training loss 0.000024, validation loss 0.055401\n",
      "Iter 5255, training loss 0.000023, validation loss 0.055401\n",
      "Iter 5256, training loss 0.000023, validation loss 0.055400\n",
      "Iter 5257, training loss 0.000023, validation loss 0.055400\n",
      "Iter 5258, training loss 0.000023, validation loss 0.055399\n",
      "Iter 5259, training loss 0.000023, validation loss 0.055399\n",
      "Iter 5260, training loss 0.000023, validation loss 0.055398\n",
      "Iter 5261, training loss 0.000023, validation loss 0.055398\n",
      "Iter 5262, training loss 0.000023, validation loss 0.055397\n",
      "Iter 5263, training loss 0.000023, validation loss 0.055397\n",
      "Iter 5264, training loss 0.000023, validation loss 0.055396\n",
      "Iter 5265, training loss 0.000023, validation loss 0.055395\n",
      "Iter 5266, training loss 0.000023, validation loss 0.055395\n",
      "Iter 5267, training loss 0.000023, validation loss 0.055394\n",
      "Iter 5268, training loss 0.000022, validation loss 0.055394\n",
      "Iter 5269, training loss 0.000022, validation loss 0.055393\n",
      "Iter 5270, training loss 0.000022, validation loss 0.055393\n",
      "Iter 5271, training loss 0.000022, validation loss 0.055392\n",
      "Iter 5272, training loss 0.000022, validation loss 0.055392\n",
      "Iter 5273, training loss 0.000022, validation loss 0.055391\n",
      "Iter 5274, training loss 0.000022, validation loss 0.055391\n",
      "Iter 5275, training loss 0.000022, validation loss 0.055390\n",
      "Iter 5276, training loss 0.000022, validation loss 0.055390\n",
      "Iter 5277, training loss 0.000022, validation loss 0.055389\n",
      "Iter 5278, training loss 0.000022, validation loss 0.055389\n",
      "Iter 5279, training loss 0.000022, validation loss 0.055388\n",
      "Iter 5280, training loss 0.000022, validation loss 0.055387\n",
      "Iter 5281, training loss 0.000022, validation loss 0.055387\n",
      "Iter 5282, training loss 0.000021, validation loss 0.055386\n",
      "Iter 5283, training loss 0.000021, validation loss 0.055385\n",
      "Iter 5284, training loss 0.000021, validation loss 0.055385\n",
      "Iter 5285, training loss 0.000021, validation loss 0.055384\n",
      "Iter 5286, training loss 0.000021, validation loss 0.055383\n",
      "Iter 5287, training loss 0.000021, validation loss 0.055383\n",
      "Iter 5288, training loss 0.000021, validation loss 0.055383\n",
      "Iter 5289, training loss 0.000021, validation loss 0.055382\n",
      "Iter 5290, training loss 0.000021, validation loss 0.055381\n",
      "Iter 5291, training loss 0.000021, validation loss 0.055381\n",
      "Iter 5292, training loss 0.000021, validation loss 0.055380\n",
      "Iter 5293, training loss 0.000021, validation loss 0.055380\n",
      "Iter 5294, training loss 0.000021, validation loss 0.055379\n",
      "Iter 5295, training loss 0.000021, validation loss 0.055378\n",
      "Iter 5296, training loss 0.000020, validation loss 0.055378\n",
      "Iter 5297, training loss 0.000020, validation loss 0.055377\n",
      "Iter 5298, training loss 0.000020, validation loss 0.055377\n",
      "Iter 5299, training loss 0.000020, validation loss 0.055376\n",
      "Iter 5300, training loss 0.000020, validation loss 0.055375\n",
      "Iter 5301, training loss 0.000020, validation loss 0.055375\n",
      "Iter 5302, training loss 0.000020, validation loss 0.055374\n",
      "Iter 5303, training loss 0.000020, validation loss 0.055374\n",
      "Iter 5304, training loss 0.000020, validation loss 0.055373\n",
      "Iter 5305, training loss 0.000020, validation loss 0.055373\n",
      "Iter 5306, training loss 0.000020, validation loss 0.055372\n",
      "Iter 5307, training loss 0.000020, validation loss 0.055371\n",
      "Iter 5308, training loss 0.000020, validation loss 0.055371\n",
      "Iter 5309, training loss 0.000020, validation loss 0.055370\n",
      "Iter 5310, training loss 0.000020, validation loss 0.055370\n",
      "Iter 5311, training loss 0.000020, validation loss 0.055369\n",
      "Iter 5312, training loss 0.000019, validation loss 0.055368\n",
      "Iter 5313, training loss 0.000019, validation loss 0.055367\n",
      "Iter 5314, training loss 0.000019, validation loss 0.055367\n",
      "Iter 5315, training loss 0.000019, validation loss 0.055366\n",
      "Iter 5316, training loss 0.000019, validation loss 0.055366\n",
      "Iter 5317, training loss 0.000019, validation loss 0.055365\n",
      "Iter 5318, training loss 0.000019, validation loss 0.055364\n",
      "Iter 5319, training loss 0.000019, validation loss 0.055364\n",
      "Iter 5320, training loss 0.000019, validation loss 0.055363\n",
      "Iter 5321, training loss 0.000019, validation loss 0.055363\n",
      "Iter 5322, training loss 0.000019, validation loss 0.055362\n",
      "Iter 5323, training loss 0.000019, validation loss 0.055362\n",
      "Iter 5324, training loss 0.000019, validation loss 0.055361\n",
      "Iter 5325, training loss 0.000019, validation loss 0.055360\n",
      "Iter 5326, training loss 0.000019, validation loss 0.055360\n",
      "Iter 5327, training loss 0.000019, validation loss 0.055359\n",
      "Iter 5328, training loss 0.000018, validation loss 0.055359\n",
      "Iter 5329, training loss 0.000018, validation loss 0.055358\n",
      "Iter 5330, training loss 0.000018, validation loss 0.055357\n",
      "Iter 5331, training loss 0.000018, validation loss 0.055357\n",
      "Iter 5332, training loss 0.000018, validation loss 0.055356\n",
      "Iter 5333, training loss 0.000018, validation loss 0.055355\n",
      "Iter 5334, training loss 0.000018, validation loss 0.055355\n",
      "Iter 5335, training loss 0.000018, validation loss 0.055354\n",
      "Iter 5336, training loss 0.000018, validation loss 0.055353\n",
      "Iter 5337, training loss 0.000018, validation loss 0.055353\n",
      "Iter 5338, training loss 0.000018, validation loss 0.055352\n",
      "Iter 5339, training loss 0.000018, validation loss 0.055352\n",
      "Iter 5340, training loss 0.000018, validation loss 0.055351\n",
      "Iter 5341, training loss 0.000018, validation loss 0.055350\n",
      "Iter 5342, training loss 0.000018, validation loss 0.055350\n",
      "Iter 5343, training loss 0.000018, validation loss 0.055349\n",
      "Iter 5344, training loss 0.000018, validation loss 0.055349\n",
      "Iter 5345, training loss 0.000018, validation loss 0.055348\n",
      "Iter 5346, training loss 0.000017, validation loss 0.055347\n",
      "Iter 5347, training loss 0.000017, validation loss 0.055347\n",
      "Iter 5348, training loss 0.000017, validation loss 0.055346\n",
      "Iter 5349, training loss 0.000017, validation loss 0.055345\n",
      "Iter 5350, training loss 0.000017, validation loss 0.055345\n",
      "Iter 5351, training loss 0.000017, validation loss 0.055345\n",
      "Iter 5352, training loss 0.000017, validation loss 0.055344\n",
      "Iter 5353, training loss 0.000017, validation loss 0.055343\n",
      "Iter 5354, training loss 0.000017, validation loss 0.055342\n",
      "Iter 5355, training loss 0.000017, validation loss 0.055342\n",
      "Iter 5356, training loss 0.000017, validation loss 0.055341\n",
      "Iter 5357, training loss 0.000017, validation loss 0.055341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5358, training loss 0.000017, validation loss 0.055340\n",
      "Iter 5359, training loss 0.000017, validation loss 0.055340\n",
      "Iter 5360, training loss 0.000017, validation loss 0.055339\n",
      "Iter 5361, training loss 0.000017, validation loss 0.055338\n",
      "Iter 5362, training loss 0.000017, validation loss 0.055338\n",
      "Iter 5363, training loss 0.000017, validation loss 0.055337\n",
      "Iter 5364, training loss 0.000017, validation loss 0.055337\n",
      "Iter 5365, training loss 0.000016, validation loss 0.055336\n",
      "Iter 5366, training loss 0.000016, validation loss 0.055336\n",
      "Iter 5367, training loss 0.000016, validation loss 0.055335\n",
      "Iter 5368, training loss 0.000016, validation loss 0.055335\n",
      "Iter 5369, training loss 0.000016, validation loss 0.055334\n",
      "Iter 5370, training loss 0.000016, validation loss 0.055334\n",
      "Iter 5371, training loss 0.000016, validation loss 0.055333\n",
      "Iter 5372, training loss 0.000016, validation loss 0.055333\n",
      "Iter 5373, training loss 0.000016, validation loss 0.055332\n",
      "Iter 5374, training loss 0.000016, validation loss 0.055332\n",
      "Iter 5375, training loss 0.000016, validation loss 0.055331\n",
      "Iter 5376, training loss 0.000016, validation loss 0.055331\n",
      "Iter 5377, training loss 0.000016, validation loss 0.055330\n",
      "Iter 5378, training loss 0.000016, validation loss 0.055331\n",
      "Iter 5379, training loss 0.000016, validation loss 0.055328\n",
      "Iter 5380, training loss 0.000016, validation loss 0.055330\n",
      "Iter 5381, training loss 0.000016, validation loss 0.055327\n",
      "Iter 5382, training loss 0.000016, validation loss 0.055330\n",
      "Iter 5383, training loss 0.000016, validation loss 0.055325\n",
      "Iter 5384, training loss 0.000016, validation loss 0.055331\n",
      "Iter 5385, training loss 0.000016, validation loss 0.055323\n",
      "Iter 5386, training loss 0.000017, validation loss 0.055332\n",
      "Iter 5387, training loss 0.000017, validation loss 0.055320\n",
      "Iter 5388, training loss 0.000018, validation loss 0.055336\n",
      "Iter 5389, training loss 0.000019, validation loss 0.055318\n",
      "Iter 5390, training loss 0.000022, validation loss 0.055343\n",
      "Iter 5391, training loss 0.000025, validation loss 0.055317\n",
      "Iter 5392, training loss 0.000031, validation loss 0.055360\n",
      "Iter 5393, training loss 0.000041, validation loss 0.055323\n",
      "Iter 5394, training loss 0.000056, validation loss 0.055398\n",
      "Iter 5395, training loss 0.000082, validation loss 0.055349\n",
      "Iter 5396, training loss 0.000124, validation loss 0.055488\n",
      "Iter 5397, training loss 0.000194, validation loss 0.055439\n",
      "Iter 5398, training loss 0.000312, validation loss 0.055718\n",
      "Iter 5399, training loss 0.000508, validation loss 0.055721\n",
      "Iter 5400, training loss 0.000841, validation loss 0.056328\n",
      "Iter 5401, training loss 0.001405, validation loss 0.056580\n",
      "Iter 5402, training loss 0.002368, validation loss 0.058020\n",
      "Iter 5403, training loss 0.004018, validation loss 0.059177\n",
      "Iter 5404, training loss 0.006848, validation loss 0.062868\n",
      "Iter 5405, training loss 0.011712, validation loss 0.067001\n",
      "Iter 5406, training loss 0.020012, validation loss 0.076899\n",
      "Iter 5407, training loss 0.034104, validation loss 0.090066\n",
      "Iter 5408, training loss 0.057367, validation loss 0.116325\n",
      "Iter 5409, training loss 0.094597, validation loss 0.152867\n",
      "Iter 5410, training loss 0.149185, validation loss 0.212425\n",
      "Iter 5411, training loss 0.219774, validation loss 0.283932\n",
      "Iter 5412, training loss 0.285473, validation loss 0.353394\n",
      "Iter 5413, training loss 0.304763, validation loss 0.375168\n",
      "Iter 5414, training loss 0.230744, validation loss 0.293571\n",
      "Iter 5415, training loss 0.091053, validation loss 0.152780\n",
      "Iter 5416, training loss 0.002912, validation loss 0.057554\n",
      "Iter 5417, training loss 0.040725, validation loss 0.094851\n",
      "Iter 5418, training loss 0.106536, validation loss 0.170817\n",
      "Iter 5419, training loss 0.073692, validation loss 0.127056\n",
      "Iter 5420, training loss 0.005767, validation loss 0.061338\n",
      "Iter 5421, training loss 0.024749, validation loss 0.082607\n",
      "Iter 5422, training loss 0.062530, validation loss 0.114107\n",
      "Iter 5423, training loss 0.025885, validation loss 0.083441\n",
      "Iter 5424, training loss 0.002869, validation loss 0.056674\n",
      "Iter 5425, training loss 0.035405, validation loss 0.085819\n",
      "Iter 5426, training loss 0.028116, validation loss 0.085136\n",
      "Iter 5427, training loss 0.001257, validation loss 0.053157\n",
      "Iter 5428, training loss 0.020317, validation loss 0.070261\n",
      "Iter 5429, training loss 0.022653, validation loss 0.078348\n",
      "Iter 5430, training loss 0.001573, validation loss 0.052656\n",
      "Iter 5431, training loss 0.013270, validation loss 0.062951\n",
      "Iter 5432, training loss 0.016492, validation loss 0.071067\n",
      "Iter 5433, training loss 0.001332, validation loss 0.052144\n",
      "Iter 5434, training loss 0.010035, validation loss 0.059633\n",
      "Iter 5435, training loss 0.011478, validation loss 0.065113\n",
      "Iter 5436, training loss 0.000972, validation loss 0.051794\n",
      "Iter 5437, training loss 0.008218, validation loss 0.057781\n",
      "Iter 5438, training loss 0.007625, validation loss 0.060553\n",
      "Iter 5439, training loss 0.000840, validation loss 0.051837\n",
      "Iter 5440, training loss 0.006866, validation loss 0.056491\n",
      "Iter 5441, training loss 0.004782, validation loss 0.057171\n",
      "Iter 5442, training loss 0.000960, validation loss 0.052165\n",
      "Iter 5443, training loss 0.005630, validation loss 0.055302\n",
      "Iter 5444, training loss 0.002818, validation loss 0.054744\n",
      "Iter 5445, training loss 0.001203, validation loss 0.052560\n",
      "Iter 5446, training loss 0.004439, validation loss 0.054164\n",
      "Iter 5447, training loss 0.001575, validation loss 0.053072\n",
      "Iter 5448, training loss 0.001419, validation loss 0.052838\n",
      "Iter 5449, training loss 0.003339, validation loss 0.053104\n",
      "Iter 5450, training loss 0.000880, validation loss 0.052005\n",
      "Iter 5451, training loss 0.001522, validation loss 0.052938\n",
      "Iter 5452, training loss 0.002401, validation loss 0.052256\n",
      "Iter 5453, training loss 0.000552, validation loss 0.051388\n",
      "Iter 5454, training loss 0.001495, validation loss 0.052857\n",
      "Iter 5455, training loss 0.001668, validation loss 0.051614\n",
      "Iter 5456, training loss 0.000437, validation loss 0.051048\n",
      "Iter 5457, training loss 0.001368, validation loss 0.052630\n",
      "Iter 5458, training loss 0.001136, validation loss 0.051173\n",
      "Iter 5459, training loss 0.000420, validation loss 0.050859\n",
      "Iter 5460, training loss 0.001185, validation loss 0.052320\n",
      "Iter 5461, training loss 0.000777, validation loss 0.050889\n",
      "Iter 5462, training loss 0.000433, validation loss 0.050757\n",
      "Iter 5463, training loss 0.000987, validation loss 0.052005\n",
      "Iter 5464, training loss 0.000549, validation loss 0.050743\n",
      "Iter 5465, training loss 0.000443, validation loss 0.050708\n",
      "Iter 5466, training loss 0.000802, validation loss 0.051730\n",
      "Iter 5467, training loss 0.000413, validation loss 0.050692\n",
      "Iter 5468, training loss 0.000438, validation loss 0.050691\n",
      "Iter 5469, training loss 0.000645, validation loss 0.051507\n",
      "Iter 5470, training loss 0.000334, validation loss 0.050693\n",
      "Iter 5471, training loss 0.000419, validation loss 0.050679\n",
      "Iter 5472, training loss 0.000521, validation loss 0.051322\n",
      "Iter 5473, training loss 0.000288, validation loss 0.050705\n",
      "Iter 5474, training loss 0.000390, validation loss 0.050662\n",
      "Iter 5475, training loss 0.000426, validation loss 0.051171\n",
      "Iter 5476, training loss 0.000262, validation loss 0.050716\n",
      "Iter 5477, training loss 0.000357, validation loss 0.050641\n",
      "Iter 5478, training loss 0.000356, validation loss 0.051044\n",
      "Iter 5479, training loss 0.000244, validation loss 0.050717\n",
      "Iter 5480, training loss 0.000324, validation loss 0.050618\n",
      "Iter 5481, training loss 0.000305, validation loss 0.050942\n",
      "Iter 5482, training loss 0.000231, validation loss 0.050713\n",
      "Iter 5483, training loss 0.000294, validation loss 0.050600\n",
      "Iter 5484, training loss 0.000267, validation loss 0.050865\n",
      "Iter 5485, training loss 0.000219, validation loss 0.050708\n",
      "Iter 5486, training loss 0.000266, validation loss 0.050592\n",
      "Iter 5487, training loss 0.000240, validation loss 0.050814\n",
      "Iter 5488, training loss 0.000209, validation loss 0.050708\n",
      "Iter 5489, training loss 0.000243, validation loss 0.050593\n",
      "Iter 5490, training loss 0.000219, validation loss 0.050782\n",
      "Iter 5491, training loss 0.000199, validation loss 0.050710\n",
      "Iter 5492, training loss 0.000223, validation loss 0.050600\n",
      "Iter 5493, training loss 0.000202, validation loss 0.050760\n",
      "Iter 5494, training loss 0.000189, validation loss 0.050710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5495, training loss 0.000206, validation loss 0.050606\n",
      "Iter 5496, training loss 0.000189, validation loss 0.050742\n",
      "Iter 5497, training loss 0.000180, validation loss 0.050708\n",
      "Iter 5498, training loss 0.000192, validation loss 0.050614\n",
      "Iter 5499, training loss 0.000179, validation loss 0.050731\n",
      "Iter 5500, training loss 0.000172, validation loss 0.050707\n",
      "Iter 5501, training loss 0.000180, validation loss 0.050623\n",
      "Iter 5502, training loss 0.000170, validation loss 0.050724\n",
      "Iter 5503, training loss 0.000164, validation loss 0.050705\n",
      "Iter 5504, training loss 0.000170, validation loss 0.050631\n",
      "Iter 5505, training loss 0.000162, validation loss 0.050718\n",
      "Iter 5506, training loss 0.000157, validation loss 0.050702\n",
      "Iter 5507, training loss 0.000161, validation loss 0.050636\n",
      "Iter 5508, training loss 0.000155, validation loss 0.050711\n",
      "Iter 5509, training loss 0.000151, validation loss 0.050695\n",
      "Iter 5510, training loss 0.000153, validation loss 0.050638\n",
      "Iter 5511, training loss 0.000148, validation loss 0.050703\n",
      "Iter 5512, training loss 0.000145, validation loss 0.050687\n",
      "Iter 5513, training loss 0.000146, validation loss 0.050639\n",
      "Iter 5514, training loss 0.000142, validation loss 0.050696\n",
      "Iter 5515, training loss 0.000140, validation loss 0.050681\n",
      "Iter 5516, training loss 0.000140, validation loss 0.050641\n",
      "Iter 5517, training loss 0.000137, validation loss 0.050690\n",
      "Iter 5518, training loss 0.000134, validation loss 0.050675\n",
      "Iter 5519, training loss 0.000135, validation loss 0.050641\n",
      "Iter 5520, training loss 0.000132, validation loss 0.050684\n",
      "Iter 5521, training loss 0.000130, validation loss 0.050669\n",
      "Iter 5522, training loss 0.000129, validation loss 0.050641\n",
      "Iter 5523, training loss 0.000128, validation loss 0.050679\n",
      "Iter 5524, training loss 0.000125, validation loss 0.050663\n",
      "Iter 5525, training loss 0.000125, validation loss 0.050642\n",
      "Iter 5526, training loss 0.000123, validation loss 0.050675\n",
      "Iter 5527, training loss 0.000121, validation loss 0.050659\n",
      "Iter 5528, training loss 0.000120, validation loss 0.050643\n",
      "Iter 5529, training loss 0.000119, validation loss 0.050671\n",
      "Iter 5530, training loss 0.000117, validation loss 0.050655\n",
      "Iter 5531, training loss 0.000116, validation loss 0.050642\n",
      "Iter 5532, training loss 0.000115, validation loss 0.050666\n",
      "Iter 5533, training loss 0.000114, validation loss 0.050650\n",
      "Iter 5534, training loss 0.000113, validation loss 0.050641\n",
      "Iter 5535, training loss 0.000112, validation loss 0.050660\n",
      "Iter 5536, training loss 0.000110, validation loss 0.050644\n",
      "Iter 5537, training loss 0.000109, validation loss 0.050638\n",
      "Iter 5538, training loss 0.000108, validation loss 0.050654\n",
      "Iter 5539, training loss 0.000107, validation loss 0.050639\n",
      "Iter 5540, training loss 0.000106, validation loss 0.050635\n",
      "Iter 5541, training loss 0.000105, validation loss 0.050648\n",
      "Iter 5542, training loss 0.000104, validation loss 0.050634\n",
      "Iter 5543, training loss 0.000103, validation loss 0.050632\n",
      "Iter 5544, training loss 0.000102, validation loss 0.050643\n",
      "Iter 5545, training loss 0.000101, validation loss 0.050631\n",
      "Iter 5546, training loss 0.000100, validation loss 0.050632\n",
      "Iter 5547, training loss 0.000099, validation loss 0.050640\n",
      "Iter 5548, training loss 0.000098, validation loss 0.050629\n",
      "Iter 5549, training loss 0.000097, validation loss 0.050631\n",
      "Iter 5550, training loss 0.000096, validation loss 0.050636\n",
      "Iter 5551, training loss 0.000096, validation loss 0.050625\n",
      "Iter 5552, training loss 0.000095, validation loss 0.050628\n",
      "Iter 5553, training loss 0.000094, validation loss 0.050631\n",
      "Iter 5554, training loss 0.000093, validation loss 0.050621\n",
      "Iter 5555, training loss 0.000092, validation loss 0.050625\n",
      "Iter 5556, training loss 0.000091, validation loss 0.050626\n",
      "Iter 5557, training loss 0.000091, validation loss 0.050618\n",
      "Iter 5558, training loss 0.000090, validation loss 0.050621\n",
      "Iter 5559, training loss 0.000089, validation loss 0.050621\n",
      "Iter 5560, training loss 0.000088, validation loss 0.050614\n",
      "Iter 5561, training loss 0.000087, validation loss 0.050618\n",
      "Iter 5562, training loss 0.000087, validation loss 0.050616\n",
      "Iter 5563, training loss 0.000086, validation loss 0.050611\n",
      "Iter 5564, training loss 0.000085, validation loss 0.050614\n",
      "Iter 5565, training loss 0.000085, validation loss 0.050612\n",
      "Iter 5566, training loss 0.000084, validation loss 0.050608\n",
      "Iter 5567, training loss 0.000083, validation loss 0.050610\n",
      "Iter 5568, training loss 0.000083, validation loss 0.050607\n",
      "Iter 5569, training loss 0.000082, validation loss 0.050604\n",
      "Iter 5570, training loss 0.000081, validation loss 0.050607\n",
      "Iter 5571, training loss 0.000081, validation loss 0.050603\n",
      "Iter 5572, training loss 0.000080, validation loss 0.050601\n",
      "Iter 5573, training loss 0.000079, validation loss 0.050603\n",
      "Iter 5574, training loss 0.000079, validation loss 0.050599\n",
      "Iter 5575, training loss 0.000078, validation loss 0.050598\n",
      "Iter 5576, training loss 0.000077, validation loss 0.050599\n",
      "Iter 5577, training loss 0.000077, validation loss 0.050596\n",
      "Iter 5578, training loss 0.000076, validation loss 0.050595\n",
      "Iter 5579, training loss 0.000076, validation loss 0.050596\n",
      "Iter 5580, training loss 0.000075, validation loss 0.050592\n",
      "Iter 5581, training loss 0.000074, validation loss 0.050592\n",
      "Iter 5582, training loss 0.000074, validation loss 0.050592\n",
      "Iter 5583, training loss 0.000073, validation loss 0.050589\n",
      "Iter 5584, training loss 0.000073, validation loss 0.050589\n",
      "Iter 5585, training loss 0.000072, validation loss 0.050589\n",
      "Iter 5586, training loss 0.000072, validation loss 0.050586\n",
      "Iter 5587, training loss 0.000071, validation loss 0.050587\n",
      "Iter 5588, training loss 0.000071, validation loss 0.050586\n",
      "Iter 5589, training loss 0.000070, validation loss 0.050584\n",
      "Iter 5590, training loss 0.000070, validation loss 0.050584\n",
      "Iter 5591, training loss 0.000069, validation loss 0.050583\n",
      "Iter 5592, training loss 0.000069, validation loss 0.050582\n",
      "Iter 5593, training loss 0.000068, validation loss 0.050582\n",
      "Iter 5594, training loss 0.000068, validation loss 0.050581\n",
      "Iter 5595, training loss 0.000067, validation loss 0.050580\n",
      "Iter 5596, training loss 0.000067, validation loss 0.050580\n",
      "Iter 5597, training loss 0.000066, validation loss 0.050578\n",
      "Iter 5598, training loss 0.000066, validation loss 0.050577\n",
      "Iter 5599, training loss 0.000065, validation loss 0.050577\n",
      "Iter 5600, training loss 0.000065, validation loss 0.050575\n",
      "Iter 5601, training loss 0.000064, validation loss 0.050575\n",
      "Iter 5602, training loss 0.000064, validation loss 0.050574\n",
      "Iter 5603, training loss 0.000063, validation loss 0.050573\n",
      "Iter 5604, training loss 0.000063, validation loss 0.050573\n",
      "Iter 5605, training loss 0.000063, validation loss 0.050572\n",
      "Iter 5606, training loss 0.000062, validation loss 0.050571\n",
      "Iter 5607, training loss 0.000062, validation loss 0.050570\n",
      "Iter 5608, training loss 0.000061, validation loss 0.050569\n",
      "Iter 5609, training loss 0.000061, validation loss 0.050568\n",
      "Iter 5610, training loss 0.000060, validation loss 0.050568\n",
      "Iter 5611, training loss 0.000060, validation loss 0.050567\n",
      "Iter 5612, training loss 0.000060, validation loss 0.050566\n",
      "Iter 5613, training loss 0.000059, validation loss 0.050566\n",
      "Iter 5614, training loss 0.000059, validation loss 0.050566\n",
      "Iter 5615, training loss 0.000058, validation loss 0.050565\n",
      "Iter 5616, training loss 0.000058, validation loss 0.050565\n",
      "Iter 5617, training loss 0.000058, validation loss 0.050564\n",
      "Iter 5618, training loss 0.000057, validation loss 0.050564\n",
      "Iter 5619, training loss 0.000057, validation loss 0.050563\n",
      "Iter 5620, training loss 0.000057, validation loss 0.050563\n",
      "Iter 5621, training loss 0.000056, validation loss 0.050563\n",
      "Iter 5622, training loss 0.000056, validation loss 0.050562\n",
      "Iter 5623, training loss 0.000055, validation loss 0.050562\n",
      "Iter 5624, training loss 0.000055, validation loss 0.050562\n",
      "Iter 5625, training loss 0.000055, validation loss 0.050561\n",
      "Iter 5626, training loss 0.000054, validation loss 0.050561\n",
      "Iter 5627, training loss 0.000054, validation loss 0.050560\n",
      "Iter 5628, training loss 0.000054, validation loss 0.050560\n",
      "Iter 5629, training loss 0.000053, validation loss 0.050560\n",
      "Iter 5630, training loss 0.000053, validation loss 0.050559\n",
      "Iter 5631, training loss 0.000053, validation loss 0.050559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5632, training loss 0.000052, validation loss 0.050558\n",
      "Iter 5633, training loss 0.000052, validation loss 0.050558\n",
      "Iter 5634, training loss 0.000052, validation loss 0.050558\n",
      "Iter 5635, training loss 0.000051, validation loss 0.050557\n",
      "Iter 5636, training loss 0.000051, validation loss 0.050557\n",
      "Iter 5637, training loss 0.000051, validation loss 0.050557\n",
      "Iter 5638, training loss 0.000050, validation loss 0.050557\n",
      "Iter 5639, training loss 0.000050, validation loss 0.050556\n",
      "Iter 5640, training loss 0.000050, validation loss 0.050556\n",
      "Iter 5641, training loss 0.000050, validation loss 0.050555\n",
      "Iter 5642, training loss 0.000049, validation loss 0.050555\n",
      "Iter 5643, training loss 0.000049, validation loss 0.050555\n",
      "Iter 5644, training loss 0.000049, validation loss 0.050554\n",
      "Iter 5645, training loss 0.000048, validation loss 0.050554\n",
      "Iter 5646, training loss 0.000048, validation loss 0.050554\n",
      "Iter 5647, training loss 0.000048, validation loss 0.050554\n",
      "Iter 5648, training loss 0.000047, validation loss 0.050553\n",
      "Iter 5649, training loss 0.000047, validation loss 0.050553\n",
      "Iter 5650, training loss 0.000047, validation loss 0.050552\n",
      "Iter 5651, training loss 0.000047, validation loss 0.050552\n",
      "Iter 5652, training loss 0.000046, validation loss 0.050552\n",
      "Iter 5653, training loss 0.000046, validation loss 0.050552\n",
      "Iter 5654, training loss 0.000046, validation loss 0.050551\n",
      "Iter 5655, training loss 0.000046, validation loss 0.050551\n",
      "Iter 5656, training loss 0.000045, validation loss 0.050550\n",
      "Iter 5657, training loss 0.000045, validation loss 0.050550\n",
      "Iter 5658, training loss 0.000045, validation loss 0.050549\n",
      "Iter 5659, training loss 0.000045, validation loss 0.050548\n",
      "Iter 5660, training loss 0.000044, validation loss 0.050548\n",
      "Iter 5661, training loss 0.000044, validation loss 0.050547\n",
      "Iter 5662, training loss 0.000044, validation loss 0.050546\n",
      "Iter 5663, training loss 0.000044, validation loss 0.050546\n",
      "Iter 5664, training loss 0.000043, validation loss 0.050545\n",
      "Iter 5665, training loss 0.000043, validation loss 0.050545\n",
      "Iter 5666, training loss 0.000043, validation loss 0.050544\n",
      "Iter 5667, training loss 0.000043, validation loss 0.050544\n",
      "Iter 5668, training loss 0.000042, validation loss 0.050543\n",
      "Iter 5669, training loss 0.000042, validation loss 0.050542\n",
      "Iter 5670, training loss 0.000042, validation loss 0.050542\n",
      "Iter 5671, training loss 0.000042, validation loss 0.050541\n",
      "Iter 5672, training loss 0.000041, validation loss 0.050540\n",
      "Iter 5673, training loss 0.000041, validation loss 0.050539\n",
      "Iter 5674, training loss 0.000041, validation loss 0.050539\n",
      "Iter 5675, training loss 0.000041, validation loss 0.050538\n",
      "Iter 5676, training loss 0.000041, validation loss 0.050538\n",
      "Iter 5677, training loss 0.000040, validation loss 0.050537\n",
      "Iter 5678, training loss 0.000040, validation loss 0.050536\n",
      "Iter 5679, training loss 0.000040, validation loss 0.050535\n",
      "Iter 5680, training loss 0.000040, validation loss 0.050535\n",
      "Iter 5681, training loss 0.000039, validation loss 0.050534\n",
      "Iter 5682, training loss 0.000039, validation loss 0.050533\n",
      "Iter 5683, training loss 0.000039, validation loss 0.050533\n",
      "Iter 5684, training loss 0.000039, validation loss 0.050532\n",
      "Iter 5685, training loss 0.000039, validation loss 0.050531\n",
      "Iter 5686, training loss 0.000038, validation loss 0.050530\n",
      "Iter 5687, training loss 0.000038, validation loss 0.050530\n",
      "Iter 5688, training loss 0.000038, validation loss 0.050529\n",
      "Iter 5689, training loss 0.000038, validation loss 0.050528\n",
      "Iter 5690, training loss 0.000038, validation loss 0.050528\n",
      "Iter 5691, training loss 0.000037, validation loss 0.050527\n",
      "Iter 5692, training loss 0.000037, validation loss 0.050526\n",
      "Iter 5693, training loss 0.000037, validation loss 0.050526\n",
      "Iter 5694, training loss 0.000037, validation loss 0.050525\n",
      "Iter 5695, training loss 0.000037, validation loss 0.050524\n",
      "Iter 5696, training loss 0.000037, validation loss 0.050523\n",
      "Iter 5697, training loss 0.000036, validation loss 0.050522\n",
      "Iter 5698, training loss 0.000036, validation loss 0.050522\n",
      "Iter 5699, training loss 0.000036, validation loss 0.050521\n",
      "Iter 5700, training loss 0.000036, validation loss 0.050520\n",
      "Iter 5701, training loss 0.000036, validation loss 0.050519\n",
      "Iter 5702, training loss 0.000035, validation loss 0.050519\n",
      "Iter 5703, training loss 0.000035, validation loss 0.050518\n",
      "Iter 5704, training loss 0.000035, validation loss 0.050517\n",
      "Iter 5705, training loss 0.000035, validation loss 0.050516\n",
      "Iter 5706, training loss 0.000035, validation loss 0.050515\n",
      "Iter 5707, training loss 0.000035, validation loss 0.050515\n",
      "Iter 5708, training loss 0.000034, validation loss 0.050514\n",
      "Iter 5709, training loss 0.000034, validation loss 0.050513\n",
      "Iter 5710, training loss 0.000034, validation loss 0.050512\n",
      "Iter 5711, training loss 0.000034, validation loss 0.050512\n",
      "Iter 5712, training loss 0.000034, validation loss 0.050511\n",
      "Iter 5713, training loss 0.000034, validation loss 0.050510\n",
      "Iter 5714, training loss 0.000033, validation loss 0.050509\n",
      "Iter 5715, training loss 0.000033, validation loss 0.050509\n",
      "Iter 5716, training loss 0.000033, validation loss 0.050508\n",
      "Iter 5717, training loss 0.000033, validation loss 0.050507\n",
      "Iter 5718, training loss 0.000033, validation loss 0.050506\n",
      "Iter 5719, training loss 0.000033, validation loss 0.050506\n",
      "Iter 5720, training loss 0.000032, validation loss 0.050505\n",
      "Iter 5721, training loss 0.000032, validation loss 0.050504\n",
      "Iter 5722, training loss 0.000032, validation loss 0.050504\n",
      "Iter 5723, training loss 0.000032, validation loss 0.050503\n",
      "Iter 5724, training loss 0.000032, validation loss 0.050502\n",
      "Iter 5725, training loss 0.000032, validation loss 0.050502\n",
      "Iter 5726, training loss 0.000032, validation loss 0.050502\n",
      "Iter 5727, training loss 0.000031, validation loss 0.050501\n",
      "Iter 5728, training loss 0.000031, validation loss 0.050500\n",
      "Iter 5729, training loss 0.000031, validation loss 0.050500\n",
      "Iter 5730, training loss 0.000031, validation loss 0.050499\n",
      "Iter 5731, training loss 0.000031, validation loss 0.050498\n",
      "Iter 5732, training loss 0.000031, validation loss 0.050498\n",
      "Iter 5733, training loss 0.000031, validation loss 0.050497\n",
      "Iter 5734, training loss 0.000030, validation loss 0.050496\n",
      "Iter 5735, training loss 0.000030, validation loss 0.050496\n",
      "Iter 5736, training loss 0.000030, validation loss 0.050495\n",
      "Iter 5737, training loss 0.000030, validation loss 0.050495\n",
      "Iter 5738, training loss 0.000030, validation loss 0.050494\n",
      "Iter 5739, training loss 0.000030, validation loss 0.050494\n",
      "Iter 5740, training loss 0.000030, validation loss 0.050493\n",
      "Iter 5741, training loss 0.000029, validation loss 0.050493\n",
      "Iter 5742, training loss 0.000029, validation loss 0.050492\n",
      "Iter 5743, training loss 0.000029, validation loss 0.050492\n",
      "Iter 5744, training loss 0.000029, validation loss 0.050491\n",
      "Iter 5745, training loss 0.000029, validation loss 0.050491\n",
      "Iter 5746, training loss 0.000029, validation loss 0.050490\n",
      "Iter 5747, training loss 0.000029, validation loss 0.050490\n",
      "Iter 5748, training loss 0.000029, validation loss 0.050489\n",
      "Iter 5749, training loss 0.000028, validation loss 0.050488\n",
      "Iter 5750, training loss 0.000028, validation loss 0.050488\n",
      "Iter 5751, training loss 0.000028, validation loss 0.050488\n",
      "Iter 5752, training loss 0.000028, validation loss 0.050487\n",
      "Iter 5753, training loss 0.000028, validation loss 0.050487\n",
      "Iter 5754, training loss 0.000028, validation loss 0.050486\n",
      "Iter 5755, training loss 0.000028, validation loss 0.050486\n",
      "Iter 5756, training loss 0.000028, validation loss 0.050485\n",
      "Iter 5757, training loss 0.000027, validation loss 0.050485\n",
      "Iter 5758, training loss 0.000027, validation loss 0.050484\n",
      "Iter 5759, training loss 0.000027, validation loss 0.050484\n",
      "Iter 5760, training loss 0.000027, validation loss 0.050483\n",
      "Iter 5761, training loss 0.000027, validation loss 0.050482\n",
      "Iter 5762, training loss 0.000027, validation loss 0.050482\n",
      "Iter 5763, training loss 0.000027, validation loss 0.050482\n",
      "Iter 5764, training loss 0.000027, validation loss 0.050481\n",
      "Iter 5765, training loss 0.000027, validation loss 0.050481\n",
      "Iter 5766, training loss 0.000026, validation loss 0.050480\n",
      "Iter 5767, training loss 0.000026, validation loss 0.050480\n",
      "Iter 5768, training loss 0.000026, validation loss 0.050479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5769, training loss 0.000026, validation loss 0.050479\n",
      "Iter 5770, training loss 0.000026, validation loss 0.050479\n",
      "Iter 5771, training loss 0.000026, validation loss 0.050478\n",
      "Iter 5772, training loss 0.000026, validation loss 0.050478\n",
      "Iter 5773, training loss 0.000026, validation loss 0.050477\n",
      "Iter 5774, training loss 0.000026, validation loss 0.050477\n",
      "Iter 5775, training loss 0.000025, validation loss 0.050477\n",
      "Iter 5776, training loss 0.000025, validation loss 0.050476\n",
      "Iter 5777, training loss 0.000025, validation loss 0.050476\n",
      "Iter 5778, training loss 0.000025, validation loss 0.050475\n",
      "Iter 5779, training loss 0.000025, validation loss 0.050475\n",
      "Iter 5780, training loss 0.000025, validation loss 0.050475\n",
      "Iter 5781, training loss 0.000025, validation loss 0.050474\n",
      "Iter 5782, training loss 0.000025, validation loss 0.050474\n",
      "Iter 5783, training loss 0.000025, validation loss 0.050473\n",
      "Iter 5784, training loss 0.000025, validation loss 0.050473\n",
      "Iter 5785, training loss 0.000024, validation loss 0.050473\n",
      "Iter 5786, training loss 0.000024, validation loss 0.050472\n",
      "Iter 5787, training loss 0.000024, validation loss 0.050472\n",
      "Iter 5788, training loss 0.000024, validation loss 0.050472\n",
      "Iter 5789, training loss 0.000024, validation loss 0.050472\n",
      "Iter 5790, training loss 0.000024, validation loss 0.050471\n",
      "Iter 5791, training loss 0.000024, validation loss 0.050471\n",
      "Iter 5792, training loss 0.000024, validation loss 0.050471\n",
      "Iter 5793, training loss 0.000024, validation loss 0.050470\n",
      "Iter 5794, training loss 0.000024, validation loss 0.050470\n",
      "Iter 5795, training loss 0.000023, validation loss 0.050470\n",
      "Iter 5796, training loss 0.000023, validation loss 0.050470\n",
      "Iter 5797, training loss 0.000023, validation loss 0.050469\n",
      "Iter 5798, training loss 0.000023, validation loss 0.050469\n",
      "Iter 5799, training loss 0.000023, validation loss 0.050469\n",
      "Iter 5800, training loss 0.000023, validation loss 0.050469\n",
      "Iter 5801, training loss 0.000023, validation loss 0.050468\n",
      "Iter 5802, training loss 0.000023, validation loss 0.050468\n",
      "Iter 5803, training loss 0.000023, validation loss 0.050468\n",
      "Iter 5804, training loss 0.000023, validation loss 0.050468\n",
      "Iter 5805, training loss 0.000023, validation loss 0.050468\n",
      "Iter 5806, training loss 0.000022, validation loss 0.050467\n",
      "Iter 5807, training loss 0.000022, validation loss 0.050467\n",
      "Iter 5808, training loss 0.000022, validation loss 0.050467\n",
      "Iter 5809, training loss 0.000022, validation loss 0.050467\n",
      "Iter 5810, training loss 0.000022, validation loss 0.050467\n",
      "Iter 5811, training loss 0.000022, validation loss 0.050466\n",
      "Iter 5812, training loss 0.000022, validation loss 0.050466\n",
      "Iter 5813, training loss 0.000022, validation loss 0.050466\n",
      "Iter 5814, training loss 0.000022, validation loss 0.050466\n",
      "Iter 5815, training loss 0.000022, validation loss 0.050466\n",
      "Iter 5816, training loss 0.000022, validation loss 0.050466\n",
      "Iter 5817, training loss 0.000021, validation loss 0.050465\n",
      "Iter 5818, training loss 0.000021, validation loss 0.050465\n",
      "Iter 5819, training loss 0.000021, validation loss 0.050465\n",
      "Iter 5820, training loss 0.000021, validation loss 0.050465\n",
      "Iter 5821, training loss 0.000021, validation loss 0.050465\n",
      "Iter 5822, training loss 0.000021, validation loss 0.050464\n",
      "Iter 5823, training loss 0.000021, validation loss 0.050464\n",
      "Iter 5824, training loss 0.000021, validation loss 0.050464\n",
      "Iter 5825, training loss 0.000021, validation loss 0.050464\n",
      "Iter 5826, training loss 0.000021, validation loss 0.050464\n",
      "Iter 5827, training loss 0.000021, validation loss 0.050463\n",
      "Iter 5828, training loss 0.000021, validation loss 0.050463\n",
      "Iter 5829, training loss 0.000021, validation loss 0.050463\n",
      "Iter 5830, training loss 0.000020, validation loss 0.050463\n",
      "Iter 5831, training loss 0.000020, validation loss 0.050462\n",
      "Iter 5832, training loss 0.000020, validation loss 0.050462\n",
      "Iter 5833, training loss 0.000020, validation loss 0.050462\n",
      "Iter 5834, training loss 0.000020, validation loss 0.050462\n",
      "Iter 5835, training loss 0.000020, validation loss 0.050461\n",
      "Iter 5836, training loss 0.000020, validation loss 0.050461\n",
      "Iter 5837, training loss 0.000020, validation loss 0.050461\n",
      "Iter 5838, training loss 0.000020, validation loss 0.050461\n",
      "Iter 5839, training loss 0.000020, validation loss 0.050460\n",
      "Iter 5840, training loss 0.000020, validation loss 0.050460\n",
      "Iter 5841, training loss 0.000020, validation loss 0.050460\n",
      "Iter 5842, training loss 0.000020, validation loss 0.050460\n",
      "Iter 5843, training loss 0.000019, validation loss 0.050460\n",
      "Iter 5844, training loss 0.000019, validation loss 0.050460\n",
      "Iter 5845, training loss 0.000019, validation loss 0.050460\n",
      "Iter 5846, training loss 0.000019, validation loss 0.050460\n",
      "Iter 5847, training loss 0.000019, validation loss 0.050460\n",
      "Iter 5848, training loss 0.000019, validation loss 0.050459\n",
      "Iter 5849, training loss 0.000019, validation loss 0.050459\n",
      "Iter 5850, training loss 0.000019, validation loss 0.050459\n",
      "Iter 5851, training loss 0.000019, validation loss 0.050458\n",
      "Iter 5852, training loss 0.000019, validation loss 0.050458\n",
      "Iter 5853, training loss 0.000019, validation loss 0.050458\n",
      "Iter 5854, training loss 0.000019, validation loss 0.050458\n",
      "Iter 5855, training loss 0.000019, validation loss 0.050458\n",
      "Iter 5856, training loss 0.000019, validation loss 0.050457\n",
      "Iter 5857, training loss 0.000018, validation loss 0.050457\n",
      "Iter 5858, training loss 0.000018, validation loss 0.050457\n",
      "Iter 5859, training loss 0.000018, validation loss 0.050456\n",
      "Iter 5860, training loss 0.000018, validation loss 0.050456\n",
      "Iter 5861, training loss 0.000018, validation loss 0.050456\n",
      "Iter 5862, training loss 0.000018, validation loss 0.050456\n",
      "Iter 5863, training loss 0.000018, validation loss 0.050456\n",
      "Iter 5864, training loss 0.000018, validation loss 0.050456\n",
      "Iter 5865, training loss 0.000018, validation loss 0.050456\n",
      "Iter 5866, training loss 0.000018, validation loss 0.050455\n",
      "Iter 5867, training loss 0.000018, validation loss 0.050455\n",
      "Iter 5868, training loss 0.000018, validation loss 0.050455\n",
      "Iter 5869, training loss 0.000018, validation loss 0.050455\n",
      "Iter 5870, training loss 0.000018, validation loss 0.050455\n",
      "Iter 5871, training loss 0.000018, validation loss 0.050454\n",
      "Iter 5872, training loss 0.000017, validation loss 0.050454\n",
      "Iter 5873, training loss 0.000017, validation loss 0.050454\n",
      "Iter 5874, training loss 0.000017, validation loss 0.050454\n",
      "Iter 5875, training loss 0.000017, validation loss 0.050454\n",
      "Iter 5876, training loss 0.000017, validation loss 0.050453\n",
      "Iter 5877, training loss 0.000017, validation loss 0.050453\n",
      "Iter 5878, training loss 0.000017, validation loss 0.050453\n",
      "Iter 5879, training loss 0.000017, validation loss 0.050453\n",
      "Iter 5880, training loss 0.000017, validation loss 0.050453\n",
      "Iter 5881, training loss 0.000017, validation loss 0.050453\n",
      "Iter 5882, training loss 0.000017, validation loss 0.050453\n",
      "Iter 5883, training loss 0.000017, validation loss 0.050453\n",
      "Iter 5884, training loss 0.000017, validation loss 0.050452\n",
      "Iter 5885, training loss 0.000017, validation loss 0.050452\n",
      "Iter 5886, training loss 0.000017, validation loss 0.050452\n",
      "Iter 5887, training loss 0.000017, validation loss 0.050452\n",
      "Iter 5888, training loss 0.000017, validation loss 0.050452\n",
      "Iter 5889, training loss 0.000016, validation loss 0.050452\n",
      "Iter 5890, training loss 0.000016, validation loss 0.050451\n",
      "Iter 5891, training loss 0.000016, validation loss 0.050451\n",
      "Iter 5892, training loss 0.000016, validation loss 0.050451\n",
      "Iter 5893, training loss 0.000016, validation loss 0.050451\n",
      "Iter 5894, training loss 0.000016, validation loss 0.050451\n",
      "Iter 5895, training loss 0.000016, validation loss 0.050451\n",
      "Iter 5896, training loss 0.000016, validation loss 0.050451\n",
      "Iter 5897, training loss 0.000016, validation loss 0.050451\n",
      "Iter 5898, training loss 0.000016, validation loss 0.050450\n",
      "Iter 5899, training loss 0.000016, validation loss 0.050450\n",
      "Iter 5900, training loss 0.000016, validation loss 0.050450\n",
      "Iter 5901, training loss 0.000016, validation loss 0.050450\n",
      "Iter 5902, training loss 0.000016, validation loss 0.050450\n",
      "Iter 5903, training loss 0.000016, validation loss 0.050450\n",
      "Iter 5904, training loss 0.000016, validation loss 0.050449\n",
      "Iter 5905, training loss 0.000016, validation loss 0.050449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5906, training loss 0.000016, validation loss 0.050449\n",
      "Iter 5907, training loss 0.000015, validation loss 0.050449\n",
      "Iter 5908, training loss 0.000015, validation loss 0.050449\n",
      "Iter 5909, training loss 0.000015, validation loss 0.050449\n",
      "Iter 5910, training loss 0.000015, validation loss 0.050449\n",
      "Iter 5911, training loss 0.000015, validation loss 0.050448\n",
      "Iter 5912, training loss 0.000015, validation loss 0.050448\n",
      "Iter 5913, training loss 0.000015, validation loss 0.050448\n",
      "Iter 5914, training loss 0.000015, validation loss 0.050448\n",
      "Iter 5915, training loss 0.000015, validation loss 0.050448\n",
      "Iter 5916, training loss 0.000015, validation loss 0.050448\n",
      "Iter 5917, training loss 0.000015, validation loss 0.050448\n",
      "Iter 5918, training loss 0.000015, validation loss 0.050448\n",
      "Iter 5919, training loss 0.000015, validation loss 0.050447\n",
      "Iter 5920, training loss 0.000015, validation loss 0.050447\n",
      "Iter 5921, training loss 0.000015, validation loss 0.050447\n",
      "Iter 5922, training loss 0.000015, validation loss 0.050447\n",
      "Iter 5923, training loss 0.000015, validation loss 0.050447\n",
      "Iter 5924, training loss 0.000015, validation loss 0.050447\n",
      "Iter 5925, training loss 0.000015, validation loss 0.050447\n",
      "Iter 5926, training loss 0.000015, validation loss 0.050446\n",
      "Iter 5927, training loss 0.000014, validation loss 0.050446\n",
      "Iter 5928, training loss 0.000014, validation loss 0.050446\n",
      "Iter 5929, training loss 0.000014, validation loss 0.050446\n",
      "Iter 5930, training loss 0.000014, validation loss 0.050446\n",
      "Iter 5931, training loss 0.000014, validation loss 0.050446\n",
      "Iter 5932, training loss 0.000014, validation loss 0.050446\n",
      "Iter 5933, training loss 0.000014, validation loss 0.050445\n",
      "Iter 5934, training loss 0.000014, validation loss 0.050445\n",
      "Iter 5935, training loss 0.000014, validation loss 0.050445\n",
      "Iter 5936, training loss 0.000014, validation loss 0.050445\n",
      "Iter 5937, training loss 0.000014, validation loss 0.050445\n",
      "Iter 5938, training loss 0.000014, validation loss 0.050445\n",
      "Iter 5939, training loss 0.000014, validation loss 0.050445\n",
      "Iter 5940, training loss 0.000014, validation loss 0.050444\n",
      "Iter 5941, training loss 0.000014, validation loss 0.050444\n",
      "Iter 5942, training loss 0.000014, validation loss 0.050444\n",
      "Iter 5943, training loss 0.000014, validation loss 0.050444\n",
      "Iter 5944, training loss 0.000014, validation loss 0.050444\n",
      "Iter 5945, training loss 0.000014, validation loss 0.050444\n",
      "Iter 5946, training loss 0.000014, validation loss 0.050444\n",
      "Iter 5947, training loss 0.000014, validation loss 0.050444\n",
      "Iter 5948, training loss 0.000014, validation loss 0.050443\n",
      "Iter 5949, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5950, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5951, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5952, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5953, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5954, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5955, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5956, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5957, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5958, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5959, training loss 0.000013, validation loss 0.050443\n",
      "Iter 5960, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5961, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5962, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5963, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5964, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5965, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5966, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5967, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5968, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5969, training loss 0.000013, validation loss 0.050442\n",
      "Iter 5970, training loss 0.000013, validation loss 0.050441\n",
      "Iter 5971, training loss 0.000013, validation loss 0.050441\n",
      "Iter 5972, training loss 0.000013, validation loss 0.050441\n",
      "Iter 5973, training loss 0.000012, validation loss 0.050441\n",
      "Iter 5974, training loss 0.000012, validation loss 0.050441\n",
      "Iter 5975, training loss 0.000012, validation loss 0.050441\n",
      "Iter 5976, training loss 0.000012, validation loss 0.050441\n",
      "Iter 5977, training loss 0.000012, validation loss 0.050441\n",
      "Iter 5978, training loss 0.000012, validation loss 0.050441\n",
      "Iter 5979, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5980, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5981, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5982, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5983, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5984, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5985, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5986, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5987, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5988, training loss 0.000012, validation loss 0.050440\n",
      "Iter 5989, training loss 0.000012, validation loss 0.050439\n",
      "Iter 5990, training loss 0.000012, validation loss 0.050439\n",
      "Iter 5991, training loss 0.000012, validation loss 0.050439\n",
      "Iter 5992, training loss 0.000012, validation loss 0.050439\n",
      "Iter 5993, training loss 0.000012, validation loss 0.050439\n",
      "Iter 5994, training loss 0.000012, validation loss 0.050439\n",
      "Iter 5995, training loss 0.000012, validation loss 0.050439\n",
      "Iter 5996, training loss 0.000012, validation loss 0.050438\n",
      "Iter 5997, training loss 0.000012, validation loss 0.050438\n",
      "Iter 5998, training loss 0.000012, validation loss 0.050438\n",
      "Iter 5999, training loss 0.000011, validation loss 0.050438\n",
      "Iter 6000, training loss 0.000011, validation loss 0.050438\n",
      "Iter 6001, training loss 0.000011, validation loss 0.050438\n",
      "Iter 6002, training loss 0.000011, validation loss 0.050438\n",
      "Iter 6003, training loss 0.000011, validation loss 0.050438\n",
      "Iter 6004, training loss 0.000011, validation loss 0.050438\n",
      "Iter 6005, training loss 0.000011, validation loss 0.050437\n",
      "Iter 6006, training loss 0.000011, validation loss 0.050437\n",
      "Iter 6007, training loss 0.000011, validation loss 0.050437\n",
      "Iter 6008, training loss 0.000011, validation loss 0.050437\n",
      "Iter 6009, training loss 0.000011, validation loss 0.050437\n",
      "Iter 6010, training loss 0.000011, validation loss 0.050437\n",
      "Iter 6011, training loss 0.000011, validation loss 0.050437\n",
      "Iter 6012, training loss 0.000011, validation loss 0.050437\n",
      "Iter 6013, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6014, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6015, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6016, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6017, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6018, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6019, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6020, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6021, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6022, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6023, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6024, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6025, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6026, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6027, training loss 0.000011, validation loss 0.050436\n",
      "Iter 6028, training loss 0.000011, validation loss 0.050435\n",
      "Iter 6029, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6030, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6031, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6032, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6033, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6034, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6035, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6036, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6037, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6038, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6039, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6040, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6041, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6042, training loss 0.000010, validation loss 0.050435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6043, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6044, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6045, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6046, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6047, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6048, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6049, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6050, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6051, training loss 0.000010, validation loss 0.050433\n",
      "Iter 6052, training loss 0.000010, validation loss 0.050434\n",
      "Iter 6053, training loss 0.000010, validation loss 0.050433\n",
      "Iter 6054, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6055, training loss 0.000010, validation loss 0.050432\n",
      "Iter 6056, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6057, training loss 0.000010, validation loss 0.050432\n",
      "Iter 6058, training loss 0.000010, validation loss 0.050435\n",
      "Iter 6059, training loss 0.000010, validation loss 0.050431\n",
      "Iter 6060, training loss 0.000010, validation loss 0.050436\n",
      "Iter 6061, training loss 0.000010, validation loss 0.050429\n",
      "Iter 6062, training loss 0.000010, validation loss 0.050437\n",
      "Iter 6063, training loss 0.000010, validation loss 0.050427\n",
      "Iter 6064, training loss 0.000010, validation loss 0.050440\n",
      "Iter 6065, training loss 0.000010, validation loss 0.050424\n",
      "Iter 6066, training loss 0.000010, validation loss 0.050444\n",
      "Iter 6067, training loss 0.000010, validation loss 0.050419\n",
      "Iter 6068, training loss 0.000010, validation loss 0.050450\n",
      "Iter 6069, training loss 0.000010, validation loss 0.050411\n",
      "Iter 6070, training loss 0.000011, validation loss 0.050460\n",
      "Iter 6071, training loss 0.000012, validation loss 0.050400\n",
      "Iter 6072, training loss 0.000013, validation loss 0.050478\n",
      "Iter 6073, training loss 0.000015, validation loss 0.050384\n",
      "Iter 6074, training loss 0.000019, validation loss 0.050510\n",
      "Iter 6075, training loss 0.000025, validation loss 0.050361\n",
      "Iter 6076, training loss 0.000035, validation loss 0.050569\n",
      "Iter 6077, training loss 0.000051, validation loss 0.050333\n",
      "Iter 6078, training loss 0.000078, validation loss 0.050687\n",
      "Iter 6079, training loss 0.000123, validation loss 0.050316\n",
      "Iter 6080, training loss 0.000199, validation loss 0.050936\n",
      "Iter 6081, training loss 0.000328, validation loss 0.050375\n",
      "Iter 6082, training loss 0.000548, validation loss 0.051514\n",
      "Iter 6083, training loss 0.000925, validation loss 0.050731\n",
      "Iter 6084, training loss 0.001573, validation loss 0.052958\n",
      "Iter 6085, training loss 0.002693, validation loss 0.052119\n",
      "Iter 6086, training loss 0.004633, validation loss 0.056825\n",
      "Iter 6087, training loss 0.007993, validation loss 0.056871\n",
      "Iter 6088, training loss 0.013803, validation loss 0.067624\n",
      "Iter 6089, training loss 0.023734, validation loss 0.072008\n",
      "Iter 6090, training loss 0.040480, validation loss 0.097725\n",
      "Iter 6091, training loss 0.067622, validation loss 0.115824\n",
      "Iter 6092, training loss 0.109090, validation loss 0.173292\n",
      "Iter 6093, training loss 0.164389, validation loss 0.214241\n",
      "Iter 6094, training loss 0.221287, validation loss 0.295819\n",
      "Iter 6095, training loss 0.244065, validation loss 0.295108\n",
      "Iter 6096, training loss 0.192783, validation loss 0.265999\n",
      "Iter 6097, training loss 0.078810, validation loss 0.125038\n",
      "Iter 6098, training loss 0.002641, validation loss 0.053850\n",
      "Iter 6099, training loss 0.033675, validation loss 0.090785\n",
      "Iter 6100, training loss 0.086116, validation loss 0.130702\n",
      "Iter 6101, training loss 0.052853, validation loss 0.112460\n",
      "Iter 6102, training loss 0.001921, validation loss 0.049703\n",
      "Iter 6103, training loss 0.028459, validation loss 0.072753\n",
      "Iter 6104, training loss 0.048921, validation loss 0.107488\n",
      "Iter 6105, training loss 0.010500, validation loss 0.055525\n",
      "Iter 6106, training loss 0.009285, validation loss 0.054203\n",
      "Iter 6107, training loss 0.033433, validation loss 0.088960\n",
      "Iter 6108, training loss 0.011042, validation loss 0.055291\n",
      "Iter 6109, training loss 0.005124, validation loss 0.050095\n",
      "Iter 6110, training loss 0.023189, validation loss 0.076399\n",
      "Iter 6111, training loss 0.007457, validation loss 0.051686\n",
      "Iter 6112, training loss 0.004747, validation loss 0.049353\n",
      "Iter 6113, training loss 0.016628, validation loss 0.068298\n",
      "Iter 6114, training loss 0.003743, validation loss 0.048414\n",
      "Iter 6115, training loss 0.005455, validation loss 0.049640\n",
      "Iter 6116, training loss 0.011526, validation loss 0.061875\n",
      "Iter 6117, training loss 0.001415, validation loss 0.046740\n",
      "Iter 6118, training loss 0.006184, validation loss 0.050092\n",
      "Iter 6119, training loss 0.007142, validation loss 0.056255\n",
      "Iter 6120, training loss 0.000756, validation loss 0.046866\n",
      "Iter 6121, training loss 0.006182, validation loss 0.050061\n",
      "Iter 6122, training loss 0.003666, validation loss 0.051650\n",
      "Iter 6123, training loss 0.001256, validation loss 0.048085\n",
      "Iter 6124, training loss 0.005220, validation loss 0.049249\n",
      "Iter 6125, training loss 0.001479, validation loss 0.048472\n",
      "Iter 6126, training loss 0.002066, validation loss 0.049376\n",
      "Iter 6127, training loss 0.003655, validation loss 0.047939\n",
      "Iter 6128, training loss 0.000588, validation loss 0.046731\n",
      "Iter 6129, training loss 0.002506, validation loss 0.049980\n",
      "Iter 6130, training loss 0.002094, validation loss 0.046718\n",
      "Iter 6131, training loss 0.000581, validation loss 0.046071\n",
      "Iter 6132, training loss 0.002361, validation loss 0.049723\n",
      "Iter 6133, training loss 0.000993, validation loss 0.046025\n",
      "Iter 6134, training loss 0.000898, validation loss 0.045966\n",
      "Iter 6135, training loss 0.001803, validation loss 0.048848\n",
      "Iter 6136, training loss 0.000468, validation loss 0.045914\n",
      "Iter 6137, training loss 0.001132, validation loss 0.045987\n",
      "Iter 6138, training loss 0.001156, validation loss 0.047788\n",
      "Iter 6139, training loss 0.000374, validation loss 0.046150\n",
      "Iter 6140, training loss 0.001129, validation loss 0.045928\n",
      "Iter 6141, training loss 0.000654, validation loss 0.046869\n",
      "Iter 6142, training loss 0.000466, validation loss 0.046460\n",
      "Iter 6143, training loss 0.000937, validation loss 0.045788\n",
      "Iter 6144, training loss 0.000378, validation loss 0.046233\n",
      "Iter 6145, training loss 0.000559, validation loss 0.046663\n",
      "Iter 6146, training loss 0.000681, validation loss 0.045650\n",
      "Iter 6147, training loss 0.000284, validation loss 0.045858\n",
      "Iter 6148, training loss 0.000577, validation loss 0.046690\n",
      "Iter 6149, training loss 0.000459, validation loss 0.045572\n",
      "Iter 6150, training loss 0.000287, validation loss 0.045655\n",
      "Iter 6151, training loss 0.000520, validation loss 0.046565\n",
      "Iter 6152, training loss 0.000313, validation loss 0.045568\n",
      "Iter 6153, training loss 0.000313, validation loss 0.045548\n",
      "Iter 6154, training loss 0.000427, validation loss 0.046361\n",
      "Iter 6155, training loss 0.000241, validation loss 0.045621\n",
      "Iter 6156, training loss 0.000325, validation loss 0.045489\n",
      "Iter 6157, training loss 0.000334, validation loss 0.046140\n",
      "Iter 6158, training loss 0.000216, validation loss 0.045693\n",
      "Iter 6159, training loss 0.000312, validation loss 0.045454\n",
      "Iter 6160, training loss 0.000261, validation loss 0.045941\n",
      "Iter 6161, training loss 0.000212, validation loss 0.045753\n",
      "Iter 6162, training loss 0.000283, validation loss 0.045433\n",
      "Iter 6163, training loss 0.000213, validation loss 0.045781\n",
      "Iter 6164, training loss 0.000213, validation loss 0.045787\n",
      "Iter 6165, training loss 0.000247, validation loss 0.045428\n",
      "Iter 6166, training loss 0.000186, validation loss 0.045666\n",
      "Iter 6167, training loss 0.000209, validation loss 0.045797\n",
      "Iter 6168, training loss 0.000213, validation loss 0.045438\n",
      "Iter 6169, training loss 0.000172, validation loss 0.045588\n",
      "Iter 6170, training loss 0.000199, validation loss 0.045786\n",
      "Iter 6171, training loss 0.000187, validation loss 0.045458\n",
      "Iter 6172, training loss 0.000165, validation loss 0.045536\n",
      "Iter 6173, training loss 0.000186, validation loss 0.045758\n",
      "Iter 6174, training loss 0.000167, validation loss 0.045481\n",
      "Iter 6175, training loss 0.000159, validation loss 0.045501\n",
      "Iter 6176, training loss 0.000172, validation loss 0.045720\n",
      "Iter 6177, training loss 0.000153, validation loss 0.045502\n",
      "Iter 6178, training loss 0.000154, validation loss 0.045479\n",
      "Iter 6179, training loss 0.000159, validation loss 0.045680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6180, training loss 0.000143, validation loss 0.045522\n",
      "Iter 6181, training loss 0.000148, validation loss 0.045467\n",
      "Iter 6182, training loss 0.000147, validation loss 0.045643\n",
      "Iter 6183, training loss 0.000136, validation loss 0.045537\n",
      "Iter 6184, training loss 0.000141, validation loss 0.045461\n",
      "Iter 6185, training loss 0.000137, validation loss 0.045610\n",
      "Iter 6186, training loss 0.000130, validation loss 0.045546\n",
      "Iter 6187, training loss 0.000134, validation loss 0.045459\n",
      "Iter 6188, training loss 0.000129, validation loss 0.045581\n",
      "Iter 6189, training loss 0.000125, validation loss 0.045550\n",
      "Iter 6190, training loss 0.000127, validation loss 0.045460\n",
      "Iter 6191, training loss 0.000122, validation loss 0.045557\n",
      "Iter 6192, training loss 0.000120, validation loss 0.045551\n",
      "Iter 6193, training loss 0.000121, validation loss 0.045463\n",
      "Iter 6194, training loss 0.000117, validation loss 0.045538\n",
      "Iter 6195, training loss 0.000116, validation loss 0.045548\n",
      "Iter 6196, training loss 0.000116, validation loss 0.045466\n",
      "Iter 6197, training loss 0.000112, validation loss 0.045522\n",
      "Iter 6198, training loss 0.000111, validation loss 0.045543\n",
      "Iter 6199, training loss 0.000110, validation loss 0.045469\n",
      "Iter 6200, training loss 0.000107, validation loss 0.045510\n",
      "Iter 6201, training loss 0.000107, validation loss 0.045538\n",
      "Iter 6202, training loss 0.000106, validation loss 0.045472\n",
      "Iter 6203, training loss 0.000103, validation loss 0.045501\n",
      "Iter 6204, training loss 0.000103, validation loss 0.045531\n",
      "Iter 6205, training loss 0.000101, validation loss 0.045475\n",
      "Iter 6206, training loss 0.000100, validation loss 0.045493\n",
      "Iter 6207, training loss 0.000099, validation loss 0.045523\n",
      "Iter 6208, training loss 0.000098, validation loss 0.045474\n",
      "Iter 6209, training loss 0.000096, validation loss 0.045485\n",
      "Iter 6210, training loss 0.000096, validation loss 0.045513\n",
      "Iter 6211, training loss 0.000094, validation loss 0.045472\n",
      "Iter 6212, training loss 0.000093, validation loss 0.045477\n",
      "Iter 6213, training loss 0.000092, validation loss 0.045504\n",
      "Iter 6214, training loss 0.000091, validation loss 0.045470\n",
      "Iter 6215, training loss 0.000090, validation loss 0.045471\n",
      "Iter 6216, training loss 0.000089, validation loss 0.045496\n",
      "Iter 6217, training loss 0.000088, validation loss 0.045468\n",
      "Iter 6218, training loss 0.000087, validation loss 0.045467\n",
      "Iter 6219, training loss 0.000086, validation loss 0.045489\n",
      "Iter 6220, training loss 0.000085, validation loss 0.045467\n",
      "Iter 6221, training loss 0.000084, validation loss 0.045464\n",
      "Iter 6222, training loss 0.000083, validation loss 0.045483\n",
      "Iter 6223, training loss 0.000082, validation loss 0.045465\n",
      "Iter 6224, training loss 0.000081, validation loss 0.045461\n",
      "Iter 6225, training loss 0.000080, validation loss 0.045478\n",
      "Iter 6226, training loss 0.000079, validation loss 0.045463\n",
      "Iter 6227, training loss 0.000079, validation loss 0.045459\n",
      "Iter 6228, training loss 0.000078, validation loss 0.045474\n",
      "Iter 6229, training loss 0.000077, validation loss 0.045461\n",
      "Iter 6230, training loss 0.000076, validation loss 0.045457\n",
      "Iter 6231, training loss 0.000075, validation loss 0.045470\n",
      "Iter 6232, training loss 0.000075, validation loss 0.045459\n",
      "Iter 6233, training loss 0.000074, validation loss 0.045455\n",
      "Iter 6234, training loss 0.000073, validation loss 0.045465\n",
      "Iter 6235, training loss 0.000072, validation loss 0.045456\n",
      "Iter 6236, training loss 0.000072, validation loss 0.045452\n",
      "Iter 6237, training loss 0.000071, validation loss 0.045462\n",
      "Iter 6238, training loss 0.000070, validation loss 0.045454\n",
      "Iter 6239, training loss 0.000070, validation loss 0.045450\n",
      "Iter 6240, training loss 0.000069, validation loss 0.045459\n",
      "Iter 6241, training loss 0.000068, validation loss 0.045453\n",
      "Iter 6242, training loss 0.000068, validation loss 0.045449\n",
      "Iter 6243, training loss 0.000067, validation loss 0.045456\n",
      "Iter 6244, training loss 0.000066, validation loss 0.045451\n",
      "Iter 6245, training loss 0.000066, validation loss 0.045447\n",
      "Iter 6246, training loss 0.000065, validation loss 0.045453\n",
      "Iter 6247, training loss 0.000065, validation loss 0.045448\n",
      "Iter 6248, training loss 0.000064, validation loss 0.045446\n",
      "Iter 6249, training loss 0.000063, validation loss 0.045451\n",
      "Iter 6250, training loss 0.000063, validation loss 0.045446\n",
      "Iter 6251, training loss 0.000062, validation loss 0.045444\n",
      "Iter 6252, training loss 0.000062, validation loss 0.045448\n",
      "Iter 6253, training loss 0.000061, validation loss 0.045444\n",
      "Iter 6254, training loss 0.000061, validation loss 0.045442\n",
      "Iter 6255, training loss 0.000060, validation loss 0.045446\n",
      "Iter 6256, training loss 0.000060, validation loss 0.045442\n",
      "Iter 6257, training loss 0.000059, validation loss 0.045440\n",
      "Iter 6258, training loss 0.000059, validation loss 0.045443\n",
      "Iter 6259, training loss 0.000058, validation loss 0.045440\n",
      "Iter 6260, training loss 0.000057, validation loss 0.045439\n",
      "Iter 6261, training loss 0.000057, validation loss 0.045441\n",
      "Iter 6262, training loss 0.000057, validation loss 0.045438\n",
      "Iter 6263, training loss 0.000056, validation loss 0.045437\n",
      "Iter 6264, training loss 0.000056, validation loss 0.045439\n",
      "Iter 6265, training loss 0.000055, validation loss 0.045437\n",
      "Iter 6266, training loss 0.000055, validation loss 0.045436\n",
      "Iter 6267, training loss 0.000054, validation loss 0.045437\n",
      "Iter 6268, training loss 0.000054, validation loss 0.045435\n",
      "Iter 6269, training loss 0.000053, validation loss 0.045434\n",
      "Iter 6270, training loss 0.000053, validation loss 0.045435\n",
      "Iter 6271, training loss 0.000052, validation loss 0.045433\n",
      "Iter 6272, training loss 0.000052, validation loss 0.045433\n",
      "Iter 6273, training loss 0.000052, validation loss 0.045433\n",
      "Iter 6274, training loss 0.000051, validation loss 0.045431\n",
      "Iter 6275, training loss 0.000051, validation loss 0.045431\n",
      "Iter 6276, training loss 0.000050, validation loss 0.045431\n",
      "Iter 6277, training loss 0.000050, validation loss 0.045429\n",
      "Iter 6278, training loss 0.000050, validation loss 0.045429\n",
      "Iter 6279, training loss 0.000049, validation loss 0.045429\n",
      "Iter 6280, training loss 0.000049, validation loss 0.045427\n",
      "Iter 6281, training loss 0.000048, validation loss 0.045426\n",
      "Iter 6282, training loss 0.000048, validation loss 0.045426\n",
      "Iter 6283, training loss 0.000048, validation loss 0.045424\n",
      "Iter 6284, training loss 0.000047, validation loss 0.045424\n",
      "Iter 6285, training loss 0.000047, validation loss 0.045424\n",
      "Iter 6286, training loss 0.000047, validation loss 0.045422\n",
      "Iter 6287, training loss 0.000046, validation loss 0.045422\n",
      "Iter 6288, training loss 0.000046, validation loss 0.045421\n",
      "Iter 6289, training loss 0.000045, validation loss 0.045420\n",
      "Iter 6290, training loss 0.000045, validation loss 0.045419\n",
      "Iter 6291, training loss 0.000045, validation loss 0.045419\n",
      "Iter 6292, training loss 0.000044, validation loss 0.045417\n",
      "Iter 6293, training loss 0.000044, validation loss 0.045417\n",
      "Iter 6294, training loss 0.000044, validation loss 0.045416\n",
      "Iter 6295, training loss 0.000043, validation loss 0.045415\n",
      "Iter 6296, training loss 0.000043, validation loss 0.045415\n",
      "Iter 6297, training loss 0.000043, validation loss 0.045414\n",
      "Iter 6298, training loss 0.000043, validation loss 0.045413\n",
      "Iter 6299, training loss 0.000042, validation loss 0.045413\n",
      "Iter 6300, training loss 0.000042, validation loss 0.045412\n",
      "Iter 6301, training loss 0.000042, validation loss 0.045411\n",
      "Iter 6302, training loss 0.000041, validation loss 0.045411\n",
      "Iter 6303, training loss 0.000041, validation loss 0.045410\n",
      "Iter 6304, training loss 0.000041, validation loss 0.045409\n",
      "Iter 6305, training loss 0.000040, validation loss 0.045409\n",
      "Iter 6306, training loss 0.000040, validation loss 0.045408\n",
      "Iter 6307, training loss 0.000040, validation loss 0.045407\n",
      "Iter 6308, training loss 0.000040, validation loss 0.045407\n",
      "Iter 6309, training loss 0.000039, validation loss 0.045406\n",
      "Iter 6310, training loss 0.000039, validation loss 0.045406\n",
      "Iter 6311, training loss 0.000039, validation loss 0.045405\n",
      "Iter 6312, training loss 0.000039, validation loss 0.045405\n",
      "Iter 6313, training loss 0.000038, validation loss 0.045404\n",
      "Iter 6314, training loss 0.000038, validation loss 0.045404\n",
      "Iter 6315, training loss 0.000038, validation loss 0.045403\n",
      "Iter 6316, training loss 0.000037, validation loss 0.045403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6317, training loss 0.000037, validation loss 0.045402\n",
      "Iter 6318, training loss 0.000037, validation loss 0.045402\n",
      "Iter 6319, training loss 0.000037, validation loss 0.045401\n",
      "Iter 6320, training loss 0.000036, validation loss 0.045401\n",
      "Iter 6321, training loss 0.000036, validation loss 0.045400\n",
      "Iter 6322, training loss 0.000036, validation loss 0.045400\n",
      "Iter 6323, training loss 0.000036, validation loss 0.045400\n",
      "Iter 6324, training loss 0.000035, validation loss 0.045399\n",
      "Iter 6325, training loss 0.000035, validation loss 0.045399\n",
      "Iter 6326, training loss 0.000035, validation loss 0.045398\n",
      "Iter 6327, training loss 0.000035, validation loss 0.045398\n",
      "Iter 6328, training loss 0.000035, validation loss 0.045398\n",
      "Iter 6329, training loss 0.000034, validation loss 0.045397\n",
      "Iter 6330, training loss 0.000034, validation loss 0.045397\n",
      "Iter 6331, training loss 0.000034, validation loss 0.045397\n",
      "Iter 6332, training loss 0.000034, validation loss 0.045396\n",
      "Iter 6333, training loss 0.000033, validation loss 0.045396\n",
      "Iter 6334, training loss 0.000033, validation loss 0.045396\n",
      "Iter 6335, training loss 0.000033, validation loss 0.045396\n",
      "Iter 6336, training loss 0.000033, validation loss 0.045396\n",
      "Iter 6337, training loss 0.000033, validation loss 0.045396\n",
      "Iter 6338, training loss 0.000032, validation loss 0.045395\n",
      "Iter 6339, training loss 0.000032, validation loss 0.045395\n",
      "Iter 6340, training loss 0.000032, validation loss 0.045395\n",
      "Iter 6341, training loss 0.000032, validation loss 0.045395\n",
      "Iter 6342, training loss 0.000032, validation loss 0.045395\n",
      "Iter 6343, training loss 0.000031, validation loss 0.045395\n",
      "Iter 6344, training loss 0.000031, validation loss 0.045395\n",
      "Iter 6345, training loss 0.000031, validation loss 0.045395\n",
      "Iter 6346, training loss 0.000031, validation loss 0.045394\n",
      "Iter 6347, training loss 0.000031, validation loss 0.045394\n",
      "Iter 6348, training loss 0.000030, validation loss 0.045394\n",
      "Iter 6349, training loss 0.000030, validation loss 0.045394\n",
      "Iter 6350, training loss 0.000030, validation loss 0.045394\n",
      "Iter 6351, training loss 0.000030, validation loss 0.045394\n",
      "Iter 6352, training loss 0.000030, validation loss 0.045394\n",
      "Iter 6353, training loss 0.000029, validation loss 0.045393\n",
      "Iter 6354, training loss 0.000029, validation loss 0.045393\n",
      "Iter 6355, training loss 0.000029, validation loss 0.045393\n",
      "Iter 6356, training loss 0.000029, validation loss 0.045393\n",
      "Iter 6357, training loss 0.000029, validation loss 0.045392\n",
      "Iter 6358, training loss 0.000029, validation loss 0.045392\n",
      "Iter 6359, training loss 0.000028, validation loss 0.045392\n",
      "Iter 6360, training loss 0.000028, validation loss 0.045391\n",
      "Iter 6361, training loss 0.000028, validation loss 0.045391\n",
      "Iter 6362, training loss 0.000028, validation loss 0.045391\n",
      "Iter 6363, training loss 0.000028, validation loss 0.045390\n",
      "Iter 6364, training loss 0.000028, validation loss 0.045390\n",
      "Iter 6365, training loss 0.000027, validation loss 0.045390\n",
      "Iter 6366, training loss 0.000027, validation loss 0.045390\n",
      "Iter 6367, training loss 0.000027, validation loss 0.045390\n",
      "Iter 6368, training loss 0.000027, validation loss 0.045389\n",
      "Iter 6369, training loss 0.000027, validation loss 0.045389\n",
      "Iter 6370, training loss 0.000027, validation loss 0.045389\n",
      "Iter 6371, training loss 0.000026, validation loss 0.045389\n",
      "Iter 6372, training loss 0.000026, validation loss 0.045389\n",
      "Iter 6373, training loss 0.000026, validation loss 0.045389\n",
      "Iter 6374, training loss 0.000026, validation loss 0.045389\n",
      "Iter 6375, training loss 0.000026, validation loss 0.045389\n",
      "Iter 6376, training loss 0.000026, validation loss 0.045389\n",
      "Iter 6377, training loss 0.000025, validation loss 0.045389\n",
      "Iter 6378, training loss 0.000025, validation loss 0.045389\n",
      "Iter 6379, training loss 0.000025, validation loss 0.045389\n",
      "Iter 6380, training loss 0.000025, validation loss 0.045388\n",
      "Iter 6381, training loss 0.000025, validation loss 0.045388\n",
      "Iter 6382, training loss 0.000025, validation loss 0.045388\n",
      "Iter 6383, training loss 0.000025, validation loss 0.045388\n",
      "Iter 6384, training loss 0.000024, validation loss 0.045388\n",
      "Iter 6385, training loss 0.000024, validation loss 0.045388\n",
      "Iter 6386, training loss 0.000024, validation loss 0.045388\n",
      "Iter 6387, training loss 0.000024, validation loss 0.045388\n",
      "Iter 6388, training loss 0.000024, validation loss 0.045388\n",
      "Iter 6389, training loss 0.000024, validation loss 0.045388\n",
      "Iter 6390, training loss 0.000024, validation loss 0.045388\n",
      "Iter 6391, training loss 0.000024, validation loss 0.045388\n",
      "Iter 6392, training loss 0.000023, validation loss 0.045388\n",
      "Iter 6393, training loss 0.000023, validation loss 0.045388\n",
      "Iter 6394, training loss 0.000023, validation loss 0.045388\n",
      "Iter 6395, training loss 0.000023, validation loss 0.045388\n",
      "Iter 6396, training loss 0.000023, validation loss 0.045387\n",
      "Iter 6397, training loss 0.000023, validation loss 0.045387\n",
      "Iter 6398, training loss 0.000023, validation loss 0.045387\n",
      "Iter 6399, training loss 0.000023, validation loss 0.045387\n",
      "Iter 6400, training loss 0.000022, validation loss 0.045387\n",
      "Iter 6401, training loss 0.000022, validation loss 0.045386\n",
      "Iter 6402, training loss 0.000022, validation loss 0.045386\n",
      "Iter 6403, training loss 0.000022, validation loss 0.045386\n",
      "Iter 6404, training loss 0.000022, validation loss 0.045386\n",
      "Iter 6405, training loss 0.000022, validation loss 0.045386\n",
      "Iter 6406, training loss 0.000022, validation loss 0.045385\n",
      "Iter 6407, training loss 0.000022, validation loss 0.045385\n",
      "Iter 6408, training loss 0.000021, validation loss 0.045385\n",
      "Iter 6409, training loss 0.000021, validation loss 0.045385\n",
      "Iter 6410, training loss 0.000021, validation loss 0.045385\n",
      "Iter 6411, training loss 0.000021, validation loss 0.045385\n",
      "Iter 6412, training loss 0.000021, validation loss 0.045385\n",
      "Iter 6413, training loss 0.000021, validation loss 0.045385\n",
      "Iter 6414, training loss 0.000021, validation loss 0.045384\n",
      "Iter 6415, training loss 0.000021, validation loss 0.045384\n",
      "Iter 6416, training loss 0.000021, validation loss 0.045384\n",
      "Iter 6417, training loss 0.000020, validation loss 0.045384\n",
      "Iter 6418, training loss 0.000020, validation loss 0.045384\n",
      "Iter 6419, training loss 0.000020, validation loss 0.045383\n",
      "Iter 6420, training loss 0.000020, validation loss 0.045383\n",
      "Iter 6421, training loss 0.000020, validation loss 0.045383\n",
      "Iter 6422, training loss 0.000020, validation loss 0.045383\n",
      "Iter 6423, training loss 0.000020, validation loss 0.045383\n",
      "Iter 6424, training loss 0.000020, validation loss 0.045382\n",
      "Iter 6425, training loss 0.000020, validation loss 0.045382\n",
      "Iter 6426, training loss 0.000020, validation loss 0.045382\n",
      "Iter 6427, training loss 0.000019, validation loss 0.045382\n",
      "Iter 6428, training loss 0.000019, validation loss 0.045382\n",
      "Iter 6429, training loss 0.000019, validation loss 0.045382\n",
      "Iter 6430, training loss 0.000019, validation loss 0.045382\n",
      "Iter 6431, training loss 0.000019, validation loss 0.045381\n",
      "Iter 6432, training loss 0.000019, validation loss 0.045381\n",
      "Iter 6433, training loss 0.000019, validation loss 0.045381\n",
      "Iter 6434, training loss 0.000019, validation loss 0.045381\n",
      "Iter 6435, training loss 0.000019, validation loss 0.045381\n",
      "Iter 6436, training loss 0.000019, validation loss 0.045381\n",
      "Iter 6437, training loss 0.000018, validation loss 0.045381\n",
      "Iter 6438, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6439, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6440, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6441, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6442, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6443, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6444, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6445, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6446, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6447, training loss 0.000018, validation loss 0.045380\n",
      "Iter 6448, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6449, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6450, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6451, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6452, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6453, training loss 0.000017, validation loss 0.045379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6454, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6455, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6456, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6457, training loss 0.000017, validation loss 0.045379\n",
      "Iter 6458, training loss 0.000017, validation loss 0.045378\n",
      "Iter 6459, training loss 0.000017, validation loss 0.045378\n",
      "Iter 6460, training loss 0.000016, validation loss 0.045378\n",
      "Iter 6461, training loss 0.000016, validation loss 0.045378\n",
      "Iter 6462, training loss 0.000016, validation loss 0.045378\n",
      "Iter 6463, training loss 0.000016, validation loss 0.045378\n",
      "Iter 6464, training loss 0.000016, validation loss 0.045378\n",
      "Iter 6465, training loss 0.000016, validation loss 0.045378\n",
      "Iter 6466, training loss 0.000016, validation loss 0.045378\n",
      "Iter 6467, training loss 0.000016, validation loss 0.045377\n",
      "Iter 6468, training loss 0.000016, validation loss 0.045377\n",
      "Iter 6469, training loss 0.000016, validation loss 0.045377\n",
      "Iter 6470, training loss 0.000016, validation loss 0.045377\n",
      "Iter 6471, training loss 0.000016, validation loss 0.045377\n",
      "Iter 6472, training loss 0.000016, validation loss 0.045377\n",
      "Iter 6473, training loss 0.000015, validation loss 0.045377\n",
      "Iter 6474, training loss 0.000015, validation loss 0.045377\n",
      "Iter 6475, training loss 0.000015, validation loss 0.045377\n",
      "Iter 6476, training loss 0.000015, validation loss 0.045377\n",
      "Iter 6477, training loss 0.000015, validation loss 0.045377\n",
      "Iter 6478, training loss 0.000015, validation loss 0.045376\n",
      "Iter 6479, training loss 0.000015, validation loss 0.045376\n",
      "Iter 6480, training loss 0.000015, validation loss 0.045376\n",
      "Iter 6481, training loss 0.000015, validation loss 0.045376\n",
      "Iter 6482, training loss 0.000015, validation loss 0.045376\n",
      "Iter 6483, training loss 0.000015, validation loss 0.045376\n",
      "Iter 6484, training loss 0.000015, validation loss 0.045375\n",
      "Iter 6485, training loss 0.000015, validation loss 0.045375\n",
      "Iter 6486, training loss 0.000015, validation loss 0.045375\n",
      "Iter 6487, training loss 0.000014, validation loss 0.045375\n",
      "Iter 6488, training loss 0.000014, validation loss 0.045375\n",
      "Iter 6489, training loss 0.000014, validation loss 0.045375\n",
      "Iter 6490, training loss 0.000014, validation loss 0.045375\n",
      "Iter 6491, training loss 0.000014, validation loss 0.045374\n",
      "Iter 6492, training loss 0.000014, validation loss 0.045374\n",
      "Iter 6493, training loss 0.000014, validation loss 0.045374\n",
      "Iter 6494, training loss 0.000014, validation loss 0.045374\n",
      "Iter 6495, training loss 0.000014, validation loss 0.045374\n",
      "Iter 6496, training loss 0.000014, validation loss 0.045374\n",
      "Iter 6497, training loss 0.000014, validation loss 0.045374\n",
      "Iter 6498, training loss 0.000014, validation loss 0.045373\n",
      "Iter 6499, training loss 0.000014, validation loss 0.045373\n",
      "Iter 6500, training loss 0.000014, validation loss 0.045373\n",
      "Iter 6501, training loss 0.000014, validation loss 0.045373\n",
      "Iter 6502, training loss 0.000014, validation loss 0.045373\n",
      "Iter 6503, training loss 0.000013, validation loss 0.045373\n",
      "Iter 6504, training loss 0.000013, validation loss 0.045372\n",
      "Iter 6505, training loss 0.000013, validation loss 0.045372\n",
      "Iter 6506, training loss 0.000013, validation loss 0.045372\n",
      "Iter 6507, training loss 0.000013, validation loss 0.045372\n",
      "Iter 6508, training loss 0.000013, validation loss 0.045372\n",
      "Iter 6509, training loss 0.000013, validation loss 0.045371\n",
      "Iter 6510, training loss 0.000013, validation loss 0.045371\n",
      "Iter 6511, training loss 0.000013, validation loss 0.045371\n",
      "Iter 6512, training loss 0.000013, validation loss 0.045371\n",
      "Iter 6513, training loss 0.000013, validation loss 0.045371\n",
      "Iter 6514, training loss 0.000013, validation loss 0.045370\n",
      "Iter 6515, training loss 0.000013, validation loss 0.045370\n",
      "Iter 6516, training loss 0.000013, validation loss 0.045370\n",
      "Iter 6517, training loss 0.000013, validation loss 0.045370\n",
      "Iter 6518, training loss 0.000013, validation loss 0.045369\n",
      "Iter 6519, training loss 0.000013, validation loss 0.045369\n",
      "Iter 6520, training loss 0.000013, validation loss 0.045369\n",
      "Iter 6521, training loss 0.000012, validation loss 0.045369\n",
      "Iter 6522, training loss 0.000012, validation loss 0.045368\n",
      "Iter 6523, training loss 0.000012, validation loss 0.045368\n",
      "Iter 6524, training loss 0.000012, validation loss 0.045368\n",
      "Iter 6525, training loss 0.000012, validation loss 0.045368\n",
      "Iter 6526, training loss 0.000012, validation loss 0.045367\n",
      "Iter 6527, training loss 0.000012, validation loss 0.045367\n",
      "Iter 6528, training loss 0.000012, validation loss 0.045367\n",
      "Iter 6529, training loss 0.000012, validation loss 0.045367\n",
      "Iter 6530, training loss 0.000012, validation loss 0.045367\n",
      "Iter 6531, training loss 0.000012, validation loss 0.045366\n",
      "Iter 6532, training loss 0.000012, validation loss 0.045366\n",
      "Iter 6533, training loss 0.000012, validation loss 0.045366\n",
      "Iter 6534, training loss 0.000012, validation loss 0.045366\n",
      "Iter 6535, training loss 0.000012, validation loss 0.045365\n",
      "Iter 6536, training loss 0.000012, validation loss 0.045365\n",
      "Iter 6537, training loss 0.000012, validation loss 0.045365\n",
      "Iter 6538, training loss 0.000012, validation loss 0.045365\n",
      "Iter 6539, training loss 0.000012, validation loss 0.045364\n",
      "Iter 6540, training loss 0.000012, validation loss 0.045364\n",
      "Iter 6541, training loss 0.000011, validation loss 0.045364\n",
      "Iter 6542, training loss 0.000011, validation loss 0.045364\n",
      "Iter 6543, training loss 0.000011, validation loss 0.045364\n",
      "Iter 6544, training loss 0.000011, validation loss 0.045363\n",
      "Iter 6545, training loss 0.000011, validation loss 0.045363\n",
      "Iter 6546, training loss 0.000011, validation loss 0.045363\n",
      "Iter 6547, training loss 0.000011, validation loss 0.045363\n",
      "Iter 6548, training loss 0.000011, validation loss 0.045362\n",
      "Iter 6549, training loss 0.000011, validation loss 0.045362\n",
      "Iter 6550, training loss 0.000011, validation loss 0.045362\n",
      "Iter 6551, training loss 0.000011, validation loss 0.045362\n",
      "Iter 6552, training loss 0.000011, validation loss 0.045361\n",
      "Iter 6553, training loss 0.000011, validation loss 0.045361\n",
      "Iter 6554, training loss 0.000011, validation loss 0.045361\n",
      "Iter 6555, training loss 0.000011, validation loss 0.045361\n",
      "Iter 6556, training loss 0.000011, validation loss 0.045360\n",
      "Iter 6557, training loss 0.000011, validation loss 0.045360\n",
      "Iter 6558, training loss 0.000011, validation loss 0.045360\n",
      "Iter 6559, training loss 0.000011, validation loss 0.045360\n",
      "Iter 6560, training loss 0.000011, validation loss 0.045359\n",
      "Iter 6561, training loss 0.000011, validation loss 0.045359\n",
      "Iter 6562, training loss 0.000011, validation loss 0.045359\n",
      "Iter 6563, training loss 0.000010, validation loss 0.045359\n",
      "Iter 6564, training loss 0.000010, validation loss 0.045359\n",
      "Iter 6565, training loss 0.000010, validation loss 0.045358\n",
      "Iter 6566, training loss 0.000010, validation loss 0.045358\n",
      "Iter 6567, training loss 0.000010, validation loss 0.045358\n",
      "Iter 6568, training loss 0.000010, validation loss 0.045358\n",
      "Iter 6569, training loss 0.000010, validation loss 0.045357\n",
      "Iter 6570, training loss 0.000010, validation loss 0.045357\n",
      "Iter 6571, training loss 0.000010, validation loss 0.045357\n",
      "Iter 6572, training loss 0.000010, validation loss 0.045357\n",
      "Iter 6573, training loss 0.000010, validation loss 0.045356\n",
      "Iter 6574, training loss 0.000010, validation loss 0.045356\n",
      "Iter 6575, training loss 0.000010, validation loss 0.045356\n",
      "Iter 6576, training loss 0.000010, validation loss 0.045355\n",
      "Iter 6577, training loss 0.000010, validation loss 0.045355\n",
      "Iter 6578, training loss 0.000010, validation loss 0.045355\n",
      "Iter 6579, training loss 0.000010, validation loss 0.045354\n",
      "Iter 6580, training loss 0.000010, validation loss 0.045354\n",
      "Iter 6581, training loss 0.000010, validation loss 0.045354\n",
      "Iter 6582, training loss 0.000010, validation loss 0.045354\n",
      "Iter 6583, training loss 0.000010, validation loss 0.045353\n",
      "Iter 6584, training loss 0.000010, validation loss 0.045353\n",
      "Iter 6585, training loss 0.000010, validation loss 0.045353\n",
      "Iter 6586, training loss 0.000010, validation loss 0.045352\n",
      "Iter 6587, training loss 0.000010, validation loss 0.045352\n",
      "Iter 6588, training loss 0.000009, validation loss 0.045352\n",
      "Iter 6589, training loss 0.000009, validation loss 0.045352\n",
      "Iter 6590, training loss 0.000009, validation loss 0.045351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6591, training loss 0.000009, validation loss 0.045351\n",
      "Iter 6592, training loss 0.000009, validation loss 0.045351\n",
      "Iter 6593, training loss 0.000009, validation loss 0.045351\n",
      "Iter 6594, training loss 0.000009, validation loss 0.045350\n",
      "Iter 6595, training loss 0.000009, validation loss 0.045350\n",
      "Iter 6596, training loss 0.000009, validation loss 0.045350\n",
      "Iter 6597, training loss 0.000009, validation loss 0.045350\n",
      "Iter 6598, training loss 0.000009, validation loss 0.045349\n",
      "Iter 6599, training loss 0.000009, validation loss 0.045349\n",
      "Iter 6600, training loss 0.000009, validation loss 0.045349\n",
      "Iter 6601, training loss 0.000009, validation loss 0.045349\n",
      "Iter 6602, training loss 0.000009, validation loss 0.045348\n",
      "Iter 6603, training loss 0.000009, validation loss 0.045348\n",
      "Iter 6604, training loss 0.000009, validation loss 0.045348\n",
      "Iter 6605, training loss 0.000009, validation loss 0.045347\n",
      "Iter 6606, training loss 0.000009, validation loss 0.045347\n",
      "Iter 6607, training loss 0.000009, validation loss 0.045347\n",
      "Iter 6608, training loss 0.000009, validation loss 0.045346\n",
      "Iter 6609, training loss 0.000009, validation loss 0.045346\n",
      "Iter 6610, training loss 0.000009, validation loss 0.045346\n",
      "Iter 6611, training loss 0.000009, validation loss 0.045346\n",
      "Iter 6612, training loss 0.000009, validation loss 0.045345\n",
      "Iter 6613, training loss 0.000009, validation loss 0.045345\n",
      "Iter 6614, training loss 0.000009, validation loss 0.045345\n",
      "Iter 6615, training loss 0.000009, validation loss 0.045345\n",
      "Iter 6616, training loss 0.000009, validation loss 0.045344\n",
      "Iter 6617, training loss 0.000008, validation loss 0.045344\n",
      "Iter 6618, training loss 0.000008, validation loss 0.045344\n",
      "Iter 6619, training loss 0.000008, validation loss 0.045343\n",
      "Iter 6620, training loss 0.000008, validation loss 0.045343\n",
      "Iter 6621, training loss 0.000008, validation loss 0.045343\n",
      "Iter 6622, training loss 0.000008, validation loss 0.045343\n",
      "Iter 6623, training loss 0.000008, validation loss 0.045342\n",
      "Iter 6624, training loss 0.000008, validation loss 0.045342\n",
      "Iter 6625, training loss 0.000008, validation loss 0.045342\n",
      "Iter 6626, training loss 0.000008, validation loss 0.045341\n",
      "Iter 6627, training loss 0.000008, validation loss 0.045341\n",
      "Iter 6628, training loss 0.000008, validation loss 0.045341\n",
      "Iter 6629, training loss 0.000008, validation loss 0.045340\n",
      "Iter 6630, training loss 0.000008, validation loss 0.045340\n",
      "Iter 6631, training loss 0.000008, validation loss 0.045340\n",
      "Iter 6632, training loss 0.000008, validation loss 0.045340\n",
      "Iter 6633, training loss 0.000008, validation loss 0.045339\n",
      "Iter 6634, training loss 0.000008, validation loss 0.045339\n",
      "Iter 6635, training loss 0.000008, validation loss 0.045339\n",
      "Iter 6636, training loss 0.000008, validation loss 0.045339\n",
      "Iter 6637, training loss 0.000008, validation loss 0.045338\n",
      "Iter 6638, training loss 0.000008, validation loss 0.045338\n",
      "Iter 6639, training loss 0.000008, validation loss 0.045338\n",
      "Iter 6640, training loss 0.000008, validation loss 0.045338\n",
      "Iter 6641, training loss 0.000008, validation loss 0.045337\n",
      "Iter 6642, training loss 0.000008, validation loss 0.045337\n",
      "Iter 6643, training loss 0.000008, validation loss 0.045337\n",
      "Iter 6644, training loss 0.000008, validation loss 0.045337\n",
      "Iter 6645, training loss 0.000008, validation loss 0.045336\n",
      "Iter 6646, training loss 0.000008, validation loss 0.045336\n",
      "Iter 6647, training loss 0.000008, validation loss 0.045336\n",
      "Iter 6648, training loss 0.000008, validation loss 0.045336\n",
      "Iter 6649, training loss 0.000008, validation loss 0.045335\n",
      "Iter 6650, training loss 0.000007, validation loss 0.045335\n",
      "Iter 6651, training loss 0.000007, validation loss 0.045335\n",
      "Iter 6652, training loss 0.000007, validation loss 0.045335\n",
      "Iter 6653, training loss 0.000007, validation loss 0.045334\n",
      "Iter 6654, training loss 0.000007, validation loss 0.045334\n",
      "Iter 6655, training loss 0.000007, validation loss 0.045334\n",
      "Iter 6656, training loss 0.000007, validation loss 0.045334\n",
      "Iter 6657, training loss 0.000007, validation loss 0.045333\n",
      "Iter 6658, training loss 0.000007, validation loss 0.045333\n",
      "Iter 6659, training loss 0.000007, validation loss 0.045333\n",
      "Iter 6660, training loss 0.000007, validation loss 0.045333\n",
      "Iter 6661, training loss 0.000007, validation loss 0.045332\n",
      "Iter 6662, training loss 0.000007, validation loss 0.045332\n",
      "Iter 6663, training loss 0.000007, validation loss 0.045332\n",
      "Iter 6664, training loss 0.000007, validation loss 0.045331\n",
      "Iter 6665, training loss 0.000007, validation loss 0.045331\n",
      "Iter 6666, training loss 0.000007, validation loss 0.045331\n",
      "Iter 6667, training loss 0.000007, validation loss 0.045330\n",
      "Iter 6668, training loss 0.000007, validation loss 0.045330\n",
      "Iter 6669, training loss 0.000007, validation loss 0.045330\n",
      "Iter 6670, training loss 0.000007, validation loss 0.045330\n",
      "Iter 6671, training loss 0.000007, validation loss 0.045329\n",
      "Iter 6672, training loss 0.000007, validation loss 0.045329\n",
      "Iter 6673, training loss 0.000007, validation loss 0.045329\n",
      "Iter 6674, training loss 0.000007, validation loss 0.045328\n",
      "Iter 6675, training loss 0.000007, validation loss 0.045328\n",
      "Iter 6676, training loss 0.000007, validation loss 0.045328\n",
      "Iter 6677, training loss 0.000007, validation loss 0.045328\n",
      "Iter 6678, training loss 0.000007, validation loss 0.045327\n",
      "Iter 6679, training loss 0.000007, validation loss 0.045327\n",
      "Iter 6680, training loss 0.000007, validation loss 0.045327\n",
      "Iter 6681, training loss 0.000007, validation loss 0.045326\n",
      "Iter 6682, training loss 0.000007, validation loss 0.045326\n",
      "Iter 6683, training loss 0.000007, validation loss 0.045326\n",
      "Iter 6684, training loss 0.000007, validation loss 0.045326\n",
      "Iter 6685, training loss 0.000007, validation loss 0.045325\n",
      "Iter 6686, training loss 0.000007, validation loss 0.045325\n",
      "Iter 6687, training loss 0.000007, validation loss 0.045325\n",
      "Iter 6688, training loss 0.000007, validation loss 0.045325\n",
      "Iter 6689, training loss 0.000007, validation loss 0.045324\n",
      "Iter 6690, training loss 0.000007, validation loss 0.045324\n",
      "Iter 6691, training loss 0.000006, validation loss 0.045324\n",
      "Iter 6692, training loss 0.000006, validation loss 0.045323\n",
      "Iter 6693, training loss 0.000006, validation loss 0.045323\n",
      "Iter 6694, training loss 0.000006, validation loss 0.045323\n",
      "Iter 6695, training loss 0.000006, validation loss 0.045323\n",
      "Iter 6696, training loss 0.000006, validation loss 0.045322\n",
      "Iter 6697, training loss 0.000006, validation loss 0.045322\n",
      "Iter 6698, training loss 0.000006, validation loss 0.045322\n",
      "Iter 6699, training loss 0.000006, validation loss 0.045321\n",
      "Iter 6700, training loss 0.000006, validation loss 0.045321\n",
      "Iter 6701, training loss 0.000006, validation loss 0.045321\n",
      "Iter 6702, training loss 0.000006, validation loss 0.045321\n",
      "Iter 6703, training loss 0.000006, validation loss 0.045320\n",
      "Iter 6704, training loss 0.000006, validation loss 0.045320\n",
      "Iter 6705, training loss 0.000006, validation loss 0.045320\n",
      "Iter 6706, training loss 0.000006, validation loss 0.045320\n",
      "Iter 6707, training loss 0.000006, validation loss 0.045319\n",
      "Iter 6708, training loss 0.000006, validation loss 0.045319\n",
      "Iter 6709, training loss 0.000006, validation loss 0.045319\n",
      "Iter 6710, training loss 0.000006, validation loss 0.045319\n",
      "Iter 6711, training loss 0.000006, validation loss 0.045318\n",
      "Iter 6712, training loss 0.000006, validation loss 0.045318\n",
      "Iter 6713, training loss 0.000006, validation loss 0.045318\n",
      "Iter 6714, training loss 0.000006, validation loss 0.045318\n",
      "Iter 6715, training loss 0.000006, validation loss 0.045317\n",
      "Iter 6716, training loss 0.000006, validation loss 0.045317\n",
      "Iter 6717, training loss 0.000006, validation loss 0.045317\n",
      "Iter 6718, training loss 0.000006, validation loss 0.045317\n",
      "Iter 6719, training loss 0.000006, validation loss 0.045316\n",
      "Iter 6720, training loss 0.000006, validation loss 0.045316\n",
      "Iter 6721, training loss 0.000006, validation loss 0.045316\n",
      "Iter 6722, training loss 0.000006, validation loss 0.045315\n",
      "Iter 6723, training loss 0.000006, validation loss 0.045315\n",
      "Iter 6724, training loss 0.000006, validation loss 0.045315\n",
      "Iter 6725, training loss 0.000006, validation loss 0.045314\n",
      "Iter 6726, training loss 0.000006, validation loss 0.045314\n",
      "Iter 6727, training loss 0.000006, validation loss 0.045314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6728, training loss 0.000006, validation loss 0.045314\n",
      "Iter 6729, training loss 0.000006, validation loss 0.045313\n",
      "Iter 6730, training loss 0.000006, validation loss 0.045313\n",
      "Iter 6731, training loss 0.000006, validation loss 0.045313\n",
      "Iter 6732, training loss 0.000006, validation loss 0.045312\n",
      "Iter 6733, training loss 0.000006, validation loss 0.045312\n",
      "Iter 6734, training loss 0.000006, validation loss 0.045312\n",
      "Iter 6735, training loss 0.000006, validation loss 0.045311\n",
      "Iter 6736, training loss 0.000006, validation loss 0.045311\n",
      "Iter 6737, training loss 0.000006, validation loss 0.045311\n",
      "Iter 6738, training loss 0.000006, validation loss 0.045310\n",
      "Iter 6739, training loss 0.000006, validation loss 0.045310\n",
      "Iter 6740, training loss 0.000006, validation loss 0.045310\n",
      "Iter 6741, training loss 0.000005, validation loss 0.045310\n",
      "Iter 6742, training loss 0.000005, validation loss 0.045309\n",
      "Iter 6743, training loss 0.000005, validation loss 0.045309\n",
      "Iter 6744, training loss 0.000005, validation loss 0.045309\n",
      "Iter 6745, training loss 0.000005, validation loss 0.045308\n",
      "Iter 6746, training loss 0.000005, validation loss 0.045308\n",
      "Iter 6747, training loss 0.000005, validation loss 0.045308\n",
      "Iter 6748, training loss 0.000005, validation loss 0.045307\n",
      "Iter 6749, training loss 0.000005, validation loss 0.045307\n",
      "Iter 6750, training loss 0.000005, validation loss 0.045307\n",
      "Iter 6751, training loss 0.000005, validation loss 0.045306\n",
      "Iter 6752, training loss 0.000005, validation loss 0.045306\n",
      "Iter 6753, training loss 0.000005, validation loss 0.045306\n",
      "Iter 6754, training loss 0.000005, validation loss 0.045306\n",
      "Iter 6755, training loss 0.000005, validation loss 0.045305\n",
      "Iter 6756, training loss 0.000005, validation loss 0.045305\n",
      "Iter 6757, training loss 0.000005, validation loss 0.045305\n",
      "Iter 6758, training loss 0.000005, validation loss 0.045304\n",
      "Iter 6759, training loss 0.000005, validation loss 0.045304\n",
      "Iter 6760, training loss 0.000005, validation loss 0.045304\n",
      "Iter 6761, training loss 0.000005, validation loss 0.045304\n",
      "Iter 6762, training loss 0.000005, validation loss 0.045303\n",
      "Iter 6763, training loss 0.000005, validation loss 0.045303\n",
      "Iter 6764, training loss 0.000005, validation loss 0.045303\n",
      "Iter 6765, training loss 0.000005, validation loss 0.045302\n",
      "Iter 6766, training loss 0.000005, validation loss 0.045302\n",
      "Iter 6767, training loss 0.000005, validation loss 0.045302\n",
      "Iter 6768, training loss 0.000005, validation loss 0.045301\n",
      "Iter 6769, training loss 0.000005, validation loss 0.045301\n",
      "Iter 6770, training loss 0.000005, validation loss 0.045301\n",
      "Iter 6771, training loss 0.000005, validation loss 0.045300\n",
      "Iter 6772, training loss 0.000005, validation loss 0.045300\n",
      "Iter 6773, training loss 0.000005, validation loss 0.045300\n",
      "Iter 6774, training loss 0.000005, validation loss 0.045299\n",
      "Iter 6775, training loss 0.000005, validation loss 0.045299\n",
      "Iter 6776, training loss 0.000005, validation loss 0.045299\n",
      "Iter 6777, training loss 0.000005, validation loss 0.045298\n",
      "Iter 6778, training loss 0.000005, validation loss 0.045298\n",
      "Iter 6779, training loss 0.000005, validation loss 0.045297\n",
      "Iter 6780, training loss 0.000005, validation loss 0.045297\n",
      "Iter 6781, training loss 0.000005, validation loss 0.045297\n",
      "Iter 6782, training loss 0.000005, validation loss 0.045297\n",
      "Iter 6783, training loss 0.000005, validation loss 0.045296\n",
      "Iter 6784, training loss 0.000005, validation loss 0.045296\n",
      "Iter 6785, training loss 0.000005, validation loss 0.045295\n",
      "Iter 6786, training loss 0.000005, validation loss 0.045295\n",
      "Iter 6787, training loss 0.000005, validation loss 0.045295\n",
      "Iter 6788, training loss 0.000005, validation loss 0.045295\n",
      "Iter 6789, training loss 0.000005, validation loss 0.045294\n",
      "Iter 6790, training loss 0.000005, validation loss 0.045294\n",
      "Iter 6791, training loss 0.000005, validation loss 0.045293\n",
      "Iter 6792, training loss 0.000005, validation loss 0.045293\n",
      "Iter 6793, training loss 0.000005, validation loss 0.045292\n",
      "Iter 6794, training loss 0.000005, validation loss 0.045293\n",
      "Iter 6795, training loss 0.000005, validation loss 0.045292\n",
      "Iter 6796, training loss 0.000005, validation loss 0.045292\n",
      "Iter 6797, training loss 0.000005, validation loss 0.045291\n",
      "Iter 6798, training loss 0.000005, validation loss 0.045291\n",
      "Iter 6799, training loss 0.000005, validation loss 0.045290\n",
      "Iter 6800, training loss 0.000005, validation loss 0.045291\n",
      "Iter 6801, training loss 0.000005, validation loss 0.045289\n",
      "Iter 6802, training loss 0.000005, validation loss 0.045290\n",
      "Iter 6803, training loss 0.000005, validation loss 0.045288\n",
      "Iter 6804, training loss 0.000005, validation loss 0.045290\n",
      "Iter 6805, training loss 0.000004, validation loss 0.045287\n",
      "Iter 6806, training loss 0.000004, validation loss 0.045289\n",
      "Iter 6807, training loss 0.000004, validation loss 0.045286\n",
      "Iter 6808, training loss 0.000004, validation loss 0.045289\n",
      "Iter 6809, training loss 0.000004, validation loss 0.045285\n",
      "Iter 6810, training loss 0.000004, validation loss 0.045289\n",
      "Iter 6811, training loss 0.000004, validation loss 0.045283\n",
      "Iter 6812, training loss 0.000004, validation loss 0.045290\n",
      "Iter 6813, training loss 0.000004, validation loss 0.045281\n",
      "Iter 6814, training loss 0.000004, validation loss 0.045291\n",
      "Iter 6815, training loss 0.000004, validation loss 0.045278\n",
      "Iter 6816, training loss 0.000004, validation loss 0.045293\n",
      "Iter 6817, training loss 0.000004, validation loss 0.045274\n",
      "Iter 6818, training loss 0.000004, validation loss 0.045296\n",
      "Iter 6819, training loss 0.000005, validation loss 0.045269\n",
      "Iter 6820, training loss 0.000005, validation loss 0.045302\n",
      "Iter 6821, training loss 0.000005, validation loss 0.045261\n",
      "Iter 6822, training loss 0.000005, validation loss 0.045311\n",
      "Iter 6823, training loss 0.000006, validation loss 0.045249\n",
      "Iter 6824, training loss 0.000006, validation loss 0.045327\n",
      "Iter 6825, training loss 0.000007, validation loss 0.045231\n",
      "Iter 6826, training loss 0.000009, validation loss 0.045353\n",
      "Iter 6827, training loss 0.000012, validation loss 0.045205\n",
      "Iter 6828, training loss 0.000016, validation loss 0.045399\n",
      "Iter 6829, training loss 0.000023, validation loss 0.045167\n",
      "Iter 6830, training loss 0.000034, validation loss 0.045480\n",
      "Iter 6831, training loss 0.000052, validation loss 0.045117\n",
      "Iter 6832, training loss 0.000082, validation loss 0.045635\n",
      "Iter 6833, training loss 0.000132, validation loss 0.045068\n",
      "Iter 6834, training loss 0.000213, validation loss 0.045946\n",
      "Iter 6835, training loss 0.000349, validation loss 0.045074\n",
      "Iter 6836, training loss 0.000576, validation loss 0.046622\n",
      "Iter 6837, training loss 0.000958, validation loss 0.045342\n",
      "Iter 6838, training loss 0.001601, validation loss 0.048208\n",
      "Iter 6839, training loss 0.002690, validation loss 0.046538\n",
      "Iter 6840, training loss 0.004534, validation loss 0.052172\n",
      "Iter 6841, training loss 0.007650, validation loss 0.050712\n",
      "Iter 6842, training loss 0.012887, validation loss 0.062501\n",
      "Iter 6843, training loss 0.021547, validation loss 0.063643\n",
      "Iter 6844, training loss 0.035503, validation loss 0.088941\n",
      "Iter 6845, training loss 0.056730, validation loss 0.098185\n",
      "Iter 6846, training loss 0.086132, validation loss 0.146244\n",
      "Iter 6847, training loss 0.119322, validation loss 0.161371\n",
      "Iter 6848, training loss 0.142554, validation loss 0.209129\n",
      "Iter 6849, training loss 0.132146, validation loss 0.173980\n",
      "Iter 6850, training loss 0.078756, validation loss 0.137775\n",
      "Iter 6851, training loss 0.016690, validation loss 0.057746\n",
      "Iter 6852, training loss 0.003255, validation loss 0.045623\n",
      "Iter 6853, training loss 0.036576, validation loss 0.089145\n",
      "Iter 6854, training loss 0.048372, validation loss 0.087558\n",
      "Iter 6855, training loss 0.016237, validation loss 0.064689\n",
      "Iter 6856, training loss 0.001795, validation loss 0.045868\n",
      "Iter 6857, training loss 0.023821, validation loss 0.062910\n",
      "Iter 6858, training loss 0.023961, validation loss 0.073017\n",
      "Iter 6859, training loss 0.002433, validation loss 0.043377\n",
      "Iter 6860, training loss 0.008737, validation loss 0.048385\n",
      "Iter 6861, training loss 0.018788, validation loss 0.066276\n",
      "Iter 6862, training loss 0.004685, validation loss 0.044664\n",
      "Iter 6863, training loss 0.003663, validation loss 0.043744\n",
      "Iter 6864, training loss 0.013154, validation loss 0.059142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6865, training loss 0.004540, validation loss 0.044264\n",
      "Iter 6866, training loss 0.002139, validation loss 0.042363\n",
      "Iter 6867, training loss 0.009311, validation loss 0.054113\n",
      "Iter 6868, training loss 0.003433, validation loss 0.043179\n",
      "Iter 6869, training loss 0.001683, validation loss 0.041858\n",
      "Iter 6870, training loss 0.006778, validation loss 0.050640\n",
      "Iter 6871, training loss 0.002271, validation loss 0.042175\n",
      "Iter 6872, training loss 0.001562, validation loss 0.041664\n",
      "Iter 6873, training loss 0.004988, validation loss 0.048173\n",
      "Iter 6874, training loss 0.001370, validation loss 0.041509\n",
      "Iter 6875, training loss 0.001535, validation loss 0.041600\n",
      "Iter 6876, training loss 0.003639, validation loss 0.046354\n",
      "Iter 6877, training loss 0.000783, validation loss 0.041144\n",
      "Iter 6878, training loss 0.001498, validation loss 0.041524\n",
      "Iter 6879, training loss 0.002593, validation loss 0.044925\n",
      "Iter 6880, training loss 0.000456, validation loss 0.041003\n",
      "Iter 6881, training loss 0.001413, validation loss 0.041413\n",
      "Iter 6882, training loss 0.001798, validation loss 0.043782\n",
      "Iter 6883, training loss 0.000309, validation loss 0.041007\n",
      "Iter 6884, training loss 0.001274, validation loss 0.041283\n",
      "Iter 6885, training loss 0.001214, validation loss 0.042882\n",
      "Iter 6886, training loss 0.000265, validation loss 0.041069\n",
      "Iter 6887, training loss 0.001099, validation loss 0.041137\n",
      "Iter 6888, training loss 0.000806, validation loss 0.042211\n",
      "Iter 6889, training loss 0.000264, validation loss 0.041136\n",
      "Iter 6890, training loss 0.000912, validation loss 0.040990\n",
      "Iter 6891, training loss 0.000535, validation loss 0.041733\n",
      "Iter 6892, training loss 0.000273, validation loss 0.041188\n",
      "Iter 6893, training loss 0.000735, validation loss 0.040864\n",
      "Iter 6894, training loss 0.000364, validation loss 0.041411\n",
      "Iter 6895, training loss 0.000276, validation loss 0.041220\n",
      "Iter 6896, training loss 0.000581, validation loss 0.040771\n",
      "Iter 6897, training loss 0.000260, validation loss 0.041194\n",
      "Iter 6898, training loss 0.000269, validation loss 0.041220\n",
      "Iter 6899, training loss 0.000455, validation loss 0.040706\n",
      "Iter 6900, training loss 0.000199, validation loss 0.041042\n",
      "Iter 6901, training loss 0.000252, validation loss 0.041196\n",
      "Iter 6902, training loss 0.000356, validation loss 0.040665\n",
      "Iter 6903, training loss 0.000162, validation loss 0.040943\n",
      "Iter 6904, training loss 0.000231, validation loss 0.041163\n",
      "Iter 6905, training loss 0.000282, validation loss 0.040650\n",
      "Iter 6906, training loss 0.000141, validation loss 0.040881\n",
      "Iter 6907, training loss 0.000207, validation loss 0.041124\n",
      "Iter 6908, training loss 0.000227, validation loss 0.040648\n",
      "Iter 6909, training loss 0.000126, validation loss 0.040838\n",
      "Iter 6910, training loss 0.000184, validation loss 0.041079\n",
      "Iter 6911, training loss 0.000187, validation loss 0.040650\n",
      "Iter 6912, training loss 0.000116, validation loss 0.040807\n",
      "Iter 6913, training loss 0.000163, validation loss 0.041033\n",
      "Iter 6914, training loss 0.000157, validation loss 0.040657\n",
      "Iter 6915, training loss 0.000108, validation loss 0.040788\n",
      "Iter 6916, training loss 0.000144, validation loss 0.040996\n",
      "Iter 6917, training loss 0.000136, validation loss 0.040671\n",
      "Iter 6918, training loss 0.000101, validation loss 0.040780\n",
      "Iter 6919, training loss 0.000128, validation loss 0.040964\n",
      "Iter 6920, training loss 0.000120, validation loss 0.040685\n",
      "Iter 6921, training loss 0.000095, validation loss 0.040776\n",
      "Iter 6922, training loss 0.000115, validation loss 0.040936\n",
      "Iter 6923, training loss 0.000107, validation loss 0.040697\n",
      "Iter 6924, training loss 0.000090, validation loss 0.040775\n",
      "Iter 6925, training loss 0.000104, validation loss 0.040911\n",
      "Iter 6926, training loss 0.000098, validation loss 0.040708\n",
      "Iter 6927, training loss 0.000085, validation loss 0.040775\n",
      "Iter 6928, training loss 0.000095, validation loss 0.040890\n",
      "Iter 6929, training loss 0.000090, validation loss 0.040716\n",
      "Iter 6930, training loss 0.000080, validation loss 0.040775\n",
      "Iter 6931, training loss 0.000088, validation loss 0.040871\n",
      "Iter 6932, training loss 0.000084, validation loss 0.040721\n",
      "Iter 6933, training loss 0.000076, validation loss 0.040774\n",
      "Iter 6934, training loss 0.000081, validation loss 0.040853\n",
      "Iter 6935, training loss 0.000078, validation loss 0.040725\n",
      "Iter 6936, training loss 0.000072, validation loss 0.040773\n",
      "Iter 6937, training loss 0.000076, validation loss 0.040837\n",
      "Iter 6938, training loss 0.000074, validation loss 0.040728\n",
      "Iter 6939, training loss 0.000069, validation loss 0.040773\n",
      "Iter 6940, training loss 0.000071, validation loss 0.040824\n",
      "Iter 6941, training loss 0.000070, validation loss 0.040730\n",
      "Iter 6942, training loss 0.000066, validation loss 0.040772\n",
      "Iter 6943, training loss 0.000067, validation loss 0.040810\n",
      "Iter 6944, training loss 0.000066, validation loss 0.040730\n",
      "Iter 6945, training loss 0.000063, validation loss 0.040770\n",
      "Iter 6946, training loss 0.000063, validation loss 0.040799\n",
      "Iter 6947, training loss 0.000063, validation loss 0.040731\n",
      "Iter 6948, training loss 0.000060, validation loss 0.040770\n",
      "Iter 6949, training loss 0.000060, validation loss 0.040789\n",
      "Iter 6950, training loss 0.000060, validation loss 0.040732\n",
      "Iter 6951, training loss 0.000058, validation loss 0.040768\n",
      "Iter 6952, training loss 0.000058, validation loss 0.040781\n",
      "Iter 6953, training loss 0.000057, validation loss 0.040734\n",
      "Iter 6954, training loss 0.000056, validation loss 0.040768\n",
      "Iter 6955, training loss 0.000055, validation loss 0.040774\n",
      "Iter 6956, training loss 0.000055, validation loss 0.040734\n",
      "Iter 6957, training loss 0.000053, validation loss 0.040766\n",
      "Iter 6958, training loss 0.000053, validation loss 0.040768\n",
      "Iter 6959, training loss 0.000053, validation loss 0.040736\n",
      "Iter 6960, training loss 0.000051, validation loss 0.040766\n",
      "Iter 6961, training loss 0.000051, validation loss 0.040764\n",
      "Iter 6962, training loss 0.000050, validation loss 0.040739\n",
      "Iter 6963, training loss 0.000050, validation loss 0.040766\n",
      "Iter 6964, training loss 0.000049, validation loss 0.040760\n",
      "Iter 6965, training loss 0.000048, validation loss 0.040740\n",
      "Iter 6966, training loss 0.000048, validation loss 0.040764\n",
      "Iter 6967, training loss 0.000047, validation loss 0.040756\n",
      "Iter 6968, training loss 0.000047, validation loss 0.040742\n",
      "Iter 6969, training loss 0.000046, validation loss 0.040764\n",
      "Iter 6970, training loss 0.000045, validation loss 0.040754\n",
      "Iter 6971, training loss 0.000045, validation loss 0.040744\n",
      "Iter 6972, training loss 0.000044, validation loss 0.040762\n",
      "Iter 6973, training loss 0.000044, validation loss 0.040751\n",
      "Iter 6974, training loss 0.000043, validation loss 0.040744\n",
      "Iter 6975, training loss 0.000043, validation loss 0.040760\n",
      "Iter 6976, training loss 0.000042, validation loss 0.040748\n",
      "Iter 6977, training loss 0.000042, validation loss 0.040745\n",
      "Iter 6978, training loss 0.000041, validation loss 0.040757\n",
      "Iter 6979, training loss 0.000041, validation loss 0.040746\n",
      "Iter 6980, training loss 0.000040, validation loss 0.040746\n",
      "Iter 6981, training loss 0.000040, validation loss 0.040755\n",
      "Iter 6982, training loss 0.000040, validation loss 0.040744\n",
      "Iter 6983, training loss 0.000039, validation loss 0.040746\n",
      "Iter 6984, training loss 0.000039, validation loss 0.040754\n",
      "Iter 6985, training loss 0.000038, validation loss 0.040744\n",
      "Iter 6986, training loss 0.000038, validation loss 0.040748\n",
      "Iter 6987, training loss 0.000037, validation loss 0.040752\n",
      "Iter 6988, training loss 0.000037, validation loss 0.040744\n",
      "Iter 6989, training loss 0.000037, validation loss 0.040749\n",
      "Iter 6990, training loss 0.000036, validation loss 0.040751\n",
      "Iter 6991, training loss 0.000036, validation loss 0.040745\n",
      "Iter 6992, training loss 0.000036, validation loss 0.040750\n",
      "Iter 6993, training loss 0.000035, validation loss 0.040750\n",
      "Iter 6994, training loss 0.000035, validation loss 0.040745\n",
      "Iter 6995, training loss 0.000034, validation loss 0.040750\n",
      "Iter 6996, training loss 0.000034, validation loss 0.040749\n",
      "Iter 6997, training loss 0.000034, validation loss 0.040746\n",
      "Iter 6998, training loss 0.000033, validation loss 0.040751\n",
      "Iter 6999, training loss 0.000033, validation loss 0.040750\n",
      "Iter 7000, training loss 0.000033, validation loss 0.040747\n",
      "Iter 7001, training loss 0.000032, validation loss 0.040751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7002, training loss 0.000032, validation loss 0.040749\n",
      "Iter 7003, training loss 0.000032, validation loss 0.040748\n",
      "Iter 7004, training loss 0.000032, validation loss 0.040752\n",
      "Iter 7005, training loss 0.000031, validation loss 0.040749\n",
      "Iter 7006, training loss 0.000031, validation loss 0.040749\n",
      "Iter 7007, training loss 0.000031, validation loss 0.040752\n",
      "Iter 7008, training loss 0.000030, validation loss 0.040749\n",
      "Iter 7009, training loss 0.000030, validation loss 0.040750\n",
      "Iter 7010, training loss 0.000030, validation loss 0.040752\n",
      "Iter 7011, training loss 0.000030, validation loss 0.040750\n",
      "Iter 7012, training loss 0.000029, validation loss 0.040751\n",
      "Iter 7013, training loss 0.000029, validation loss 0.040752\n",
      "Iter 7014, training loss 0.000029, validation loss 0.040750\n",
      "Iter 7015, training loss 0.000028, validation loss 0.040752\n",
      "Iter 7016, training loss 0.000028, validation loss 0.040752\n",
      "Iter 7017, training loss 0.000028, validation loss 0.040751\n",
      "Iter 7018, training loss 0.000028, validation loss 0.040752\n",
      "Iter 7019, training loss 0.000028, validation loss 0.040752\n",
      "Iter 7020, training loss 0.000027, validation loss 0.040751\n",
      "Iter 7021, training loss 0.000027, validation loss 0.040752\n",
      "Iter 7022, training loss 0.000027, validation loss 0.040751\n",
      "Iter 7023, training loss 0.000027, validation loss 0.040750\n",
      "Iter 7024, training loss 0.000026, validation loss 0.040751\n",
      "Iter 7025, training loss 0.000026, validation loss 0.040750\n",
      "Iter 7026, training loss 0.000026, validation loss 0.040749\n",
      "Iter 7027, training loss 0.000026, validation loss 0.040750\n",
      "Iter 7028, training loss 0.000025, validation loss 0.040749\n",
      "Iter 7029, training loss 0.000025, validation loss 0.040748\n",
      "Iter 7030, training loss 0.000025, validation loss 0.040749\n",
      "Iter 7031, training loss 0.000025, validation loss 0.040748\n",
      "Iter 7032, training loss 0.000025, validation loss 0.040748\n",
      "Iter 7033, training loss 0.000024, validation loss 0.040748\n",
      "Iter 7034, training loss 0.000024, validation loss 0.040747\n",
      "Iter 7035, training loss 0.000024, validation loss 0.040747\n",
      "Iter 7036, training loss 0.000024, validation loss 0.040746\n",
      "Iter 7037, training loss 0.000024, validation loss 0.040746\n",
      "Iter 7038, training loss 0.000023, validation loss 0.040746\n",
      "Iter 7039, training loss 0.000023, validation loss 0.040746\n",
      "Iter 7040, training loss 0.000023, validation loss 0.040745\n",
      "Iter 7041, training loss 0.000023, validation loss 0.040745\n",
      "Iter 7042, training loss 0.000023, validation loss 0.040745\n",
      "Iter 7043, training loss 0.000023, validation loss 0.040744\n",
      "Iter 7044, training loss 0.000022, validation loss 0.040744\n",
      "Iter 7045, training loss 0.000022, validation loss 0.040744\n",
      "Iter 7046, training loss 0.000022, validation loss 0.040744\n",
      "Iter 7047, training loss 0.000022, validation loss 0.040743\n",
      "Iter 7048, training loss 0.000022, validation loss 0.040743\n",
      "Iter 7049, training loss 0.000021, validation loss 0.040743\n",
      "Iter 7050, training loss 0.000021, validation loss 0.040743\n",
      "Iter 7051, training loss 0.000021, validation loss 0.040743\n",
      "Iter 7052, training loss 0.000021, validation loss 0.040743\n",
      "Iter 7053, training loss 0.000021, validation loss 0.040743\n",
      "Iter 7054, training loss 0.000021, validation loss 0.040743\n",
      "Iter 7055, training loss 0.000021, validation loss 0.040743\n",
      "Iter 7056, training loss 0.000020, validation loss 0.040742\n",
      "Iter 7057, training loss 0.000020, validation loss 0.040742\n",
      "Iter 7058, training loss 0.000020, validation loss 0.040742\n",
      "Iter 7059, training loss 0.000020, validation loss 0.040742\n",
      "Iter 7060, training loss 0.000020, validation loss 0.040742\n",
      "Iter 7061, training loss 0.000020, validation loss 0.040742\n",
      "Iter 7062, training loss 0.000019, validation loss 0.040741\n",
      "Iter 7063, training loss 0.000019, validation loss 0.040742\n",
      "Iter 7064, training loss 0.000019, validation loss 0.040742\n",
      "Iter 7065, training loss 0.000019, validation loss 0.040742\n",
      "Iter 7066, training loss 0.000019, validation loss 0.040742\n",
      "Iter 7067, training loss 0.000019, validation loss 0.040742\n",
      "Iter 7068, training loss 0.000019, validation loss 0.040741\n",
      "Iter 7069, training loss 0.000019, validation loss 0.040741\n",
      "Iter 7070, training loss 0.000018, validation loss 0.040741\n",
      "Iter 7071, training loss 0.000018, validation loss 0.040741\n",
      "Iter 7072, training loss 0.000018, validation loss 0.040741\n",
      "Iter 7073, training loss 0.000018, validation loss 0.040741\n",
      "Iter 7074, training loss 0.000018, validation loss 0.040741\n",
      "Iter 7075, training loss 0.000018, validation loss 0.040741\n",
      "Iter 7076, training loss 0.000018, validation loss 0.040741\n",
      "Iter 7077, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7078, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7079, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7080, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7081, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7082, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7083, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7084, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7085, training loss 0.000017, validation loss 0.040741\n",
      "Iter 7086, training loss 0.000016, validation loss 0.040741\n",
      "Iter 7087, training loss 0.000016, validation loss 0.040741\n",
      "Iter 7088, training loss 0.000016, validation loss 0.040741\n",
      "Iter 7089, training loss 0.000016, validation loss 0.040741\n",
      "Iter 7090, training loss 0.000016, validation loss 0.040741\n",
      "Iter 7091, training loss 0.000016, validation loss 0.040740\n",
      "Iter 7092, training loss 0.000016, validation loss 0.040740\n",
      "Iter 7093, training loss 0.000016, validation loss 0.040740\n",
      "Iter 7094, training loss 0.000016, validation loss 0.040740\n",
      "Iter 7095, training loss 0.000015, validation loss 0.040740\n",
      "Iter 7096, training loss 0.000015, validation loss 0.040740\n",
      "Iter 7097, training loss 0.000015, validation loss 0.040740\n",
      "Iter 7098, training loss 0.000015, validation loss 0.040740\n",
      "Iter 7099, training loss 0.000015, validation loss 0.040740\n",
      "Iter 7100, training loss 0.000015, validation loss 0.040740\n",
      "Iter 7101, training loss 0.000015, validation loss 0.040739\n",
      "Iter 7102, training loss 0.000015, validation loss 0.040739\n",
      "Iter 7103, training loss 0.000015, validation loss 0.040739\n",
      "Iter 7104, training loss 0.000015, validation loss 0.040739\n",
      "Iter 7105, training loss 0.000015, validation loss 0.040739\n",
      "Iter 7106, training loss 0.000014, validation loss 0.040738\n",
      "Iter 7107, training loss 0.000014, validation loss 0.040738\n",
      "Iter 7108, training loss 0.000014, validation loss 0.040738\n",
      "Iter 7109, training loss 0.000014, validation loss 0.040738\n",
      "Iter 7110, training loss 0.000014, validation loss 0.040738\n",
      "Iter 7111, training loss 0.000014, validation loss 0.040738\n",
      "Iter 7112, training loss 0.000014, validation loss 0.040738\n",
      "Iter 7113, training loss 0.000014, validation loss 0.040737\n",
      "Iter 7114, training loss 0.000014, validation loss 0.040737\n",
      "Iter 7115, training loss 0.000014, validation loss 0.040737\n",
      "Iter 7116, training loss 0.000014, validation loss 0.040737\n",
      "Iter 7117, training loss 0.000013, validation loss 0.040737\n",
      "Iter 7118, training loss 0.000013, validation loss 0.040737\n",
      "Iter 7119, training loss 0.000013, validation loss 0.040737\n",
      "Iter 7120, training loss 0.000013, validation loss 0.040737\n",
      "Iter 7121, training loss 0.000013, validation loss 0.040736\n",
      "Iter 7122, training loss 0.000013, validation loss 0.040736\n",
      "Iter 7123, training loss 0.000013, validation loss 0.040736\n",
      "Iter 7124, training loss 0.000013, validation loss 0.040736\n",
      "Iter 7125, training loss 0.000013, validation loss 0.040735\n",
      "Iter 7126, training loss 0.000013, validation loss 0.040735\n",
      "Iter 7127, training loss 0.000013, validation loss 0.040735\n",
      "Iter 7128, training loss 0.000013, validation loss 0.040735\n",
      "Iter 7129, training loss 0.000013, validation loss 0.040735\n",
      "Iter 7130, training loss 0.000012, validation loss 0.040735\n",
      "Iter 7131, training loss 0.000012, validation loss 0.040735\n",
      "Iter 7132, training loss 0.000012, validation loss 0.040734\n",
      "Iter 7133, training loss 0.000012, validation loss 0.040734\n",
      "Iter 7134, training loss 0.000012, validation loss 0.040734\n",
      "Iter 7135, training loss 0.000012, validation loss 0.040734\n",
      "Iter 7136, training loss 0.000012, validation loss 0.040734\n",
      "Iter 7137, training loss 0.000012, validation loss 0.040734\n",
      "Iter 7138, training loss 0.000012, validation loss 0.040733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7139, training loss 0.000012, validation loss 0.040733\n",
      "Iter 7140, training loss 0.000012, validation loss 0.040733\n",
      "Iter 7141, training loss 0.000012, validation loss 0.040733\n",
      "Iter 7142, training loss 0.000012, validation loss 0.040733\n",
      "Iter 7143, training loss 0.000012, validation loss 0.040733\n",
      "Iter 7144, training loss 0.000011, validation loss 0.040733\n",
      "Iter 7145, training loss 0.000011, validation loss 0.040732\n",
      "Iter 7146, training loss 0.000011, validation loss 0.040732\n",
      "Iter 7147, training loss 0.000011, validation loss 0.040732\n",
      "Iter 7148, training loss 0.000011, validation loss 0.040732\n",
      "Iter 7149, training loss 0.000011, validation loss 0.040732\n",
      "Iter 7150, training loss 0.000011, validation loss 0.040731\n",
      "Iter 7151, training loss 0.000011, validation loss 0.040731\n",
      "Iter 7152, training loss 0.000011, validation loss 0.040731\n",
      "Iter 7153, training loss 0.000011, validation loss 0.040731\n",
      "Iter 7154, training loss 0.000011, validation loss 0.040731\n",
      "Iter 7155, training loss 0.000011, validation loss 0.040731\n",
      "Iter 7156, training loss 0.000011, validation loss 0.040730\n",
      "Iter 7157, training loss 0.000011, validation loss 0.040730\n",
      "Iter 7158, training loss 0.000011, validation loss 0.040730\n",
      "Iter 7159, training loss 0.000011, validation loss 0.040730\n",
      "Iter 7160, training loss 0.000010, validation loss 0.040730\n",
      "Iter 7161, training loss 0.000010, validation loss 0.040729\n",
      "Iter 7162, training loss 0.000010, validation loss 0.040729\n",
      "Iter 7163, training loss 0.000010, validation loss 0.040729\n",
      "Iter 7164, training loss 0.000010, validation loss 0.040729\n",
      "Iter 7165, training loss 0.000010, validation loss 0.040729\n",
      "Iter 7166, training loss 0.000010, validation loss 0.040728\n",
      "Iter 7167, training loss 0.000010, validation loss 0.040728\n",
      "Iter 7168, training loss 0.000010, validation loss 0.040728\n",
      "Iter 7169, training loss 0.000010, validation loss 0.040728\n",
      "Iter 7170, training loss 0.000010, validation loss 0.040728\n",
      "Iter 7171, training loss 0.000010, validation loss 0.040728\n",
      "Iter 7172, training loss 0.000010, validation loss 0.040727\n",
      "Iter 7173, training loss 0.000010, validation loss 0.040727\n",
      "Iter 7174, training loss 0.000010, validation loss 0.040727\n",
      "Iter 7175, training loss 0.000010, validation loss 0.040727\n",
      "Iter 7176, training loss 0.000010, validation loss 0.040727\n",
      "Iter 7177, training loss 0.000010, validation loss 0.040726\n",
      "Iter 7178, training loss 0.000010, validation loss 0.040726\n",
      "Iter 7179, training loss 0.000009, validation loss 0.040726\n",
      "Iter 7180, training loss 0.000009, validation loss 0.040726\n",
      "Iter 7181, training loss 0.000009, validation loss 0.040726\n",
      "Iter 7182, training loss 0.000009, validation loss 0.040725\n",
      "Iter 7183, training loss 0.000009, validation loss 0.040725\n",
      "Iter 7184, training loss 0.000009, validation loss 0.040725\n",
      "Iter 7185, training loss 0.000009, validation loss 0.040725\n",
      "Iter 7186, training loss 0.000009, validation loss 0.040725\n",
      "Iter 7187, training loss 0.000009, validation loss 0.040725\n",
      "Iter 7188, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7189, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7190, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7191, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7192, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7193, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7194, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7195, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7196, training loss 0.000009, validation loss 0.040724\n",
      "Iter 7197, training loss 0.000009, validation loss 0.040723\n",
      "Iter 7198, training loss 0.000009, validation loss 0.040723\n",
      "Iter 7199, training loss 0.000009, validation loss 0.040723\n",
      "Iter 7200, training loss 0.000008, validation loss 0.040723\n",
      "Iter 7201, training loss 0.000008, validation loss 0.040723\n",
      "Iter 7202, training loss 0.000008, validation loss 0.040723\n",
      "Iter 7203, training loss 0.000008, validation loss 0.040723\n",
      "Iter 7204, training loss 0.000008, validation loss 0.040723\n",
      "Iter 7205, training loss 0.000008, validation loss 0.040723\n",
      "Iter 7206, training loss 0.000008, validation loss 0.040722\n",
      "Iter 7207, training loss 0.000008, validation loss 0.040722\n",
      "Iter 7208, training loss 0.000008, validation loss 0.040722\n",
      "Iter 7209, training loss 0.000008, validation loss 0.040722\n",
      "Iter 7210, training loss 0.000008, validation loss 0.040722\n",
      "Iter 7211, training loss 0.000008, validation loss 0.040721\n",
      "Iter 7212, training loss 0.000008, validation loss 0.040721\n",
      "Iter 7213, training loss 0.000008, validation loss 0.040721\n",
      "Iter 7214, training loss 0.000008, validation loss 0.040721\n",
      "Iter 7215, training loss 0.000008, validation loss 0.040721\n",
      "Iter 7216, training loss 0.000008, validation loss 0.040720\n",
      "Iter 7217, training loss 0.000008, validation loss 0.040720\n",
      "Iter 7218, training loss 0.000008, validation loss 0.040720\n",
      "Iter 7219, training loss 0.000008, validation loss 0.040720\n",
      "Iter 7220, training loss 0.000008, validation loss 0.040719\n",
      "Iter 7221, training loss 0.000008, validation loss 0.040719\n",
      "Iter 7222, training loss 0.000008, validation loss 0.040719\n",
      "Iter 7223, training loss 0.000008, validation loss 0.040719\n",
      "Iter 7224, training loss 0.000008, validation loss 0.040718\n",
      "Iter 7225, training loss 0.000008, validation loss 0.040718\n",
      "Iter 7226, training loss 0.000007, validation loss 0.040718\n",
      "Iter 7227, training loss 0.000007, validation loss 0.040718\n",
      "Iter 7228, training loss 0.000007, validation loss 0.040717\n",
      "Iter 7229, training loss 0.000007, validation loss 0.040717\n",
      "Iter 7230, training loss 0.000007, validation loss 0.040717\n",
      "Iter 7231, training loss 0.000007, validation loss 0.040716\n",
      "Iter 7232, training loss 0.000007, validation loss 0.040716\n",
      "Iter 7233, training loss 0.000007, validation loss 0.040716\n",
      "Iter 7234, training loss 0.000007, validation loss 0.040716\n",
      "Iter 7235, training loss 0.000007, validation loss 0.040715\n",
      "Iter 7236, training loss 0.000007, validation loss 0.040715\n",
      "Iter 7237, training loss 0.000007, validation loss 0.040715\n",
      "Iter 7238, training loss 0.000007, validation loss 0.040715\n",
      "Iter 7239, training loss 0.000007, validation loss 0.040714\n",
      "Iter 7240, training loss 0.000007, validation loss 0.040714\n",
      "Iter 7241, training loss 0.000007, validation loss 0.040714\n",
      "Iter 7242, training loss 0.000007, validation loss 0.040713\n",
      "Iter 7243, training loss 0.000007, validation loss 0.040713\n",
      "Iter 7244, training loss 0.000007, validation loss 0.040713\n",
      "Iter 7245, training loss 0.000007, validation loss 0.040713\n",
      "Iter 7246, training loss 0.000007, validation loss 0.040713\n",
      "Iter 7247, training loss 0.000007, validation loss 0.040712\n",
      "Iter 7248, training loss 0.000007, validation loss 0.040712\n",
      "Iter 7249, training loss 0.000007, validation loss 0.040712\n",
      "Iter 7250, training loss 0.000007, validation loss 0.040711\n",
      "Iter 7251, training loss 0.000007, validation loss 0.040711\n",
      "Iter 7252, training loss 0.000007, validation loss 0.040711\n",
      "Iter 7253, training loss 0.000007, validation loss 0.040711\n",
      "Iter 7254, training loss 0.000007, validation loss 0.040711\n",
      "Iter 7255, training loss 0.000007, validation loss 0.040710\n",
      "Iter 7256, training loss 0.000006, validation loss 0.040710\n",
      "Iter 7257, training loss 0.000006, validation loss 0.040710\n",
      "Iter 7258, training loss 0.000006, validation loss 0.040710\n",
      "Iter 7259, training loss 0.000006, validation loss 0.040709\n",
      "Iter 7260, training loss 0.000006, validation loss 0.040709\n",
      "Iter 7261, training loss 0.000006, validation loss 0.040709\n",
      "Iter 7262, training loss 0.000006, validation loss 0.040709\n",
      "Iter 7263, training loss 0.000006, validation loss 0.040708\n",
      "Iter 7264, training loss 0.000006, validation loss 0.040708\n",
      "Iter 7265, training loss 0.000006, validation loss 0.040708\n",
      "Iter 7266, training loss 0.000006, validation loss 0.040708\n",
      "Iter 7267, training loss 0.000006, validation loss 0.040708\n",
      "Iter 7268, training loss 0.000006, validation loss 0.040707\n",
      "Iter 7269, training loss 0.000006, validation loss 0.040707\n",
      "Iter 7270, training loss 0.000006, validation loss 0.040707\n",
      "Iter 7271, training loss 0.000006, validation loss 0.040707\n",
      "Iter 7272, training loss 0.000006, validation loss 0.040706\n",
      "Iter 7273, training loss 0.000006, validation loss 0.040706\n",
      "Iter 7274, training loss 0.000006, validation loss 0.040706\n",
      "Iter 7275, training loss 0.000006, validation loss 0.040706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7276, training loss 0.000006, validation loss 0.040705\n",
      "Iter 7277, training loss 0.000006, validation loss 0.040705\n",
      "Iter 7278, training loss 0.000006, validation loss 0.040705\n",
      "Iter 7279, training loss 0.000006, validation loss 0.040705\n",
      "Iter 7280, training loss 0.000006, validation loss 0.040705\n",
      "Iter 7281, training loss 0.000006, validation loss 0.040704\n",
      "Iter 7282, training loss 0.000006, validation loss 0.040704\n",
      "Iter 7283, training loss 0.000006, validation loss 0.040704\n",
      "Iter 7284, training loss 0.000006, validation loss 0.040704\n",
      "Iter 7285, training loss 0.000006, validation loss 0.040703\n",
      "Iter 7286, training loss 0.000006, validation loss 0.040703\n",
      "Iter 7287, training loss 0.000006, validation loss 0.040703\n",
      "Iter 7288, training loss 0.000006, validation loss 0.040703\n",
      "Iter 7289, training loss 0.000006, validation loss 0.040702\n",
      "Iter 7290, training loss 0.000006, validation loss 0.040702\n",
      "Iter 7291, training loss 0.000006, validation loss 0.040702\n",
      "Iter 7292, training loss 0.000006, validation loss 0.040702\n",
      "Iter 7293, training loss 0.000006, validation loss 0.040701\n",
      "Iter 7294, training loss 0.000005, validation loss 0.040701\n",
      "Iter 7295, training loss 0.000005, validation loss 0.040701\n",
      "Iter 7296, training loss 0.000005, validation loss 0.040701\n",
      "Iter 7297, training loss 0.000005, validation loss 0.040701\n",
      "Iter 7298, training loss 0.000005, validation loss 0.040700\n",
      "Iter 7299, training loss 0.000005, validation loss 0.040700\n",
      "Iter 7300, training loss 0.000005, validation loss 0.040700\n",
      "Iter 7301, training loss 0.000005, validation loss 0.040700\n",
      "Iter 7302, training loss 0.000005, validation loss 0.040699\n",
      "Iter 7303, training loss 0.000005, validation loss 0.040699\n",
      "Iter 7304, training loss 0.000005, validation loss 0.040699\n",
      "Iter 7305, training loss 0.000005, validation loss 0.040699\n",
      "Iter 7306, training loss 0.000005, validation loss 0.040698\n",
      "Iter 7307, training loss 0.000005, validation loss 0.040698\n",
      "Iter 7308, training loss 0.000005, validation loss 0.040698\n",
      "Iter 7309, training loss 0.000005, validation loss 0.040698\n",
      "Iter 7310, training loss 0.000005, validation loss 0.040697\n",
      "Iter 7311, training loss 0.000005, validation loss 0.040697\n",
      "Iter 7312, training loss 0.000005, validation loss 0.040697\n",
      "Iter 7313, training loss 0.000005, validation loss 0.040697\n",
      "Iter 7314, training loss 0.000005, validation loss 0.040697\n",
      "Iter 7315, training loss 0.000005, validation loss 0.040696\n",
      "Iter 7316, training loss 0.000005, validation loss 0.040696\n",
      "Iter 7317, training loss 0.000005, validation loss 0.040696\n",
      "Iter 7318, training loss 0.000005, validation loss 0.040695\n",
      "Iter 7319, training loss 0.000005, validation loss 0.040695\n",
      "Iter 7320, training loss 0.000005, validation loss 0.040695\n",
      "Iter 7321, training loss 0.000005, validation loss 0.040694\n",
      "Iter 7322, training loss 0.000005, validation loss 0.040694\n",
      "Iter 7323, training loss 0.000005, validation loss 0.040694\n",
      "Iter 7324, training loss 0.000005, validation loss 0.040694\n",
      "Iter 7325, training loss 0.000005, validation loss 0.040693\n",
      "Iter 7326, training loss 0.000005, validation loss 0.040693\n",
      "Iter 7327, training loss 0.000005, validation loss 0.040693\n",
      "Iter 7328, training loss 0.000005, validation loss 0.040693\n",
      "Iter 7329, training loss 0.000005, validation loss 0.040692\n",
      "Iter 7330, training loss 0.000005, validation loss 0.040692\n",
      "Iter 7331, training loss 0.000005, validation loss 0.040692\n",
      "Iter 7332, training loss 0.000005, validation loss 0.040692\n",
      "Iter 7333, training loss 0.000005, validation loss 0.040692\n",
      "Iter 7334, training loss 0.000005, validation loss 0.040691\n",
      "Iter 7335, training loss 0.000005, validation loss 0.040691\n",
      "Iter 7336, training loss 0.000005, validation loss 0.040691\n",
      "Iter 7337, training loss 0.000005, validation loss 0.040691\n",
      "Iter 7338, training loss 0.000005, validation loss 0.040690\n",
      "Iter 7339, training loss 0.000005, validation loss 0.040690\n",
      "Iter 7340, training loss 0.000005, validation loss 0.040690\n",
      "Iter 7341, training loss 0.000005, validation loss 0.040690\n",
      "Iter 7342, training loss 0.000004, validation loss 0.040690\n",
      "Iter 7343, training loss 0.000004, validation loss 0.040689\n",
      "Iter 7344, training loss 0.000004, validation loss 0.040689\n",
      "Iter 7345, training loss 0.000004, validation loss 0.040689\n",
      "Iter 7346, training loss 0.000004, validation loss 0.040689\n",
      "Iter 7347, training loss 0.000004, validation loss 0.040689\n",
      "Iter 7348, training loss 0.000004, validation loss 0.040689\n",
      "Iter 7349, training loss 0.000004, validation loss 0.040688\n",
      "Iter 7350, training loss 0.000004, validation loss 0.040688\n",
      "Iter 7351, training loss 0.000004, validation loss 0.040688\n",
      "Iter 7352, training loss 0.000004, validation loss 0.040688\n",
      "Iter 7353, training loss 0.000004, validation loss 0.040688\n",
      "Iter 7354, training loss 0.000004, validation loss 0.040687\n",
      "Iter 7355, training loss 0.000004, validation loss 0.040687\n",
      "Iter 7356, training loss 0.000004, validation loss 0.040687\n",
      "Iter 7357, training loss 0.000004, validation loss 0.040687\n",
      "Iter 7358, training loss 0.000004, validation loss 0.040686\n",
      "Iter 7359, training loss 0.000004, validation loss 0.040686\n",
      "Iter 7360, training loss 0.000004, validation loss 0.040686\n",
      "Iter 7361, training loss 0.000004, validation loss 0.040686\n",
      "Iter 7362, training loss 0.000004, validation loss 0.040686\n",
      "Iter 7363, training loss 0.000004, validation loss 0.040685\n",
      "Iter 7364, training loss 0.000004, validation loss 0.040685\n",
      "Iter 7365, training loss 0.000004, validation loss 0.040685\n",
      "Iter 7366, training loss 0.000004, validation loss 0.040685\n",
      "Iter 7367, training loss 0.000004, validation loss 0.040685\n",
      "Iter 7368, training loss 0.000004, validation loss 0.040684\n",
      "Iter 7369, training loss 0.000004, validation loss 0.040684\n",
      "Iter 7370, training loss 0.000004, validation loss 0.040684\n",
      "Iter 7371, training loss 0.000004, validation loss 0.040684\n",
      "Iter 7372, training loss 0.000004, validation loss 0.040684\n",
      "Iter 7373, training loss 0.000004, validation loss 0.040683\n",
      "Iter 7374, training loss 0.000004, validation loss 0.040683\n",
      "Iter 7375, training loss 0.000004, validation loss 0.040683\n",
      "Iter 7376, training loss 0.000004, validation loss 0.040683\n",
      "Iter 7377, training loss 0.000004, validation loss 0.040682\n",
      "Iter 7378, training loss 0.000004, validation loss 0.040682\n",
      "Iter 7379, training loss 0.000004, validation loss 0.040682\n",
      "Iter 7380, training loss 0.000004, validation loss 0.040682\n",
      "Iter 7381, training loss 0.000004, validation loss 0.040682\n",
      "Iter 7382, training loss 0.000004, validation loss 0.040681\n",
      "Iter 7383, training loss 0.000004, validation loss 0.040681\n",
      "Iter 7384, training loss 0.000004, validation loss 0.040681\n",
      "Iter 7385, training loss 0.000004, validation loss 0.040681\n",
      "Iter 7386, training loss 0.000004, validation loss 0.040681\n",
      "Iter 7387, training loss 0.000004, validation loss 0.040680\n",
      "Iter 7388, training loss 0.000004, validation loss 0.040680\n",
      "Iter 7389, training loss 0.000004, validation loss 0.040680\n",
      "Iter 7390, training loss 0.000004, validation loss 0.040680\n",
      "Iter 7391, training loss 0.000004, validation loss 0.040680\n",
      "Iter 7392, training loss 0.000004, validation loss 0.040680\n",
      "Iter 7393, training loss 0.000004, validation loss 0.040679\n",
      "Iter 7394, training loss 0.000004, validation loss 0.040679\n",
      "Iter 7395, training loss 0.000004, validation loss 0.040679\n",
      "Iter 7396, training loss 0.000004, validation loss 0.040679\n",
      "Iter 7397, training loss 0.000004, validation loss 0.040679\n",
      "Iter 7398, training loss 0.000004, validation loss 0.040678\n",
      "Iter 7399, training loss 0.000004, validation loss 0.040678\n",
      "Iter 7400, training loss 0.000004, validation loss 0.040678\n",
      "Iter 7401, training loss 0.000004, validation loss 0.040678\n",
      "Iter 7402, training loss 0.000004, validation loss 0.040678\n",
      "Iter 7403, training loss 0.000004, validation loss 0.040677\n",
      "Iter 7404, training loss 0.000004, validation loss 0.040677\n",
      "Iter 7405, training loss 0.000004, validation loss 0.040677\n",
      "Iter 7406, training loss 0.000004, validation loss 0.040677\n",
      "Iter 7407, training loss 0.000004, validation loss 0.040677\n",
      "Iter 7408, training loss 0.000003, validation loss 0.040677\n",
      "Iter 7409, training loss 0.000003, validation loss 0.040676\n",
      "Iter 7410, training loss 0.000003, validation loss 0.040676\n",
      "Iter 7411, training loss 0.000003, validation loss 0.040676\n",
      "Iter 7412, training loss 0.000003, validation loss 0.040676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7413, training loss 0.000003, validation loss 0.040676\n",
      "Iter 7414, training loss 0.000003, validation loss 0.040676\n",
      "Iter 7415, training loss 0.000003, validation loss 0.040675\n",
      "Iter 7416, training loss 0.000003, validation loss 0.040675\n",
      "Iter 7417, training loss 0.000003, validation loss 0.040675\n",
      "Iter 7418, training loss 0.000003, validation loss 0.040675\n",
      "Iter 7419, training loss 0.000003, validation loss 0.040674\n",
      "Iter 7420, training loss 0.000003, validation loss 0.040674\n",
      "Iter 7421, training loss 0.000003, validation loss 0.040674\n",
      "Iter 7422, training loss 0.000003, validation loss 0.040674\n",
      "Iter 7423, training loss 0.000003, validation loss 0.040674\n",
      "Iter 7424, training loss 0.000003, validation loss 0.040674\n",
      "Iter 7425, training loss 0.000003, validation loss 0.040673\n",
      "Iter 7426, training loss 0.000003, validation loss 0.040673\n",
      "Iter 7427, training loss 0.000003, validation loss 0.040673\n",
      "Iter 7428, training loss 0.000003, validation loss 0.040673\n",
      "Iter 7429, training loss 0.000003, validation loss 0.040673\n",
      "Iter 7430, training loss 0.000003, validation loss 0.040672\n",
      "Iter 7431, training loss 0.000003, validation loss 0.040672\n",
      "Iter 7432, training loss 0.000003, validation loss 0.040672\n",
      "Iter 7433, training loss 0.000003, validation loss 0.040672\n",
      "Iter 7434, training loss 0.000003, validation loss 0.040672\n",
      "Iter 7435, training loss 0.000003, validation loss 0.040671\n",
      "Iter 7436, training loss 0.000003, validation loss 0.040671\n",
      "Iter 7437, training loss 0.000003, validation loss 0.040671\n",
      "Iter 7438, training loss 0.000003, validation loss 0.040671\n",
      "Iter 7439, training loss 0.000003, validation loss 0.040670\n",
      "Iter 7440, training loss 0.000003, validation loss 0.040670\n",
      "Iter 7441, training loss 0.000003, validation loss 0.040670\n",
      "Iter 7442, training loss 0.000003, validation loss 0.040670\n",
      "Iter 7443, training loss 0.000003, validation loss 0.040670\n",
      "Iter 7444, training loss 0.000003, validation loss 0.040670\n",
      "Iter 7445, training loss 0.000003, validation loss 0.040669\n",
      "Iter 7446, training loss 0.000003, validation loss 0.040669\n",
      "Iter 7447, training loss 0.000003, validation loss 0.040669\n",
      "Iter 7448, training loss 0.000003, validation loss 0.040669\n",
      "Iter 7449, training loss 0.000003, validation loss 0.040668\n",
      "Iter 7450, training loss 0.000003, validation loss 0.040668\n",
      "Iter 7451, training loss 0.000003, validation loss 0.040668\n",
      "Iter 7452, training loss 0.000003, validation loss 0.040668\n",
      "Iter 7453, training loss 0.000003, validation loss 0.040668\n",
      "Iter 7454, training loss 0.000003, validation loss 0.040668\n",
      "Iter 7455, training loss 0.000003, validation loss 0.040667\n",
      "Iter 7456, training loss 0.000003, validation loss 0.040667\n",
      "Iter 7457, training loss 0.000003, validation loss 0.040667\n",
      "Iter 7458, training loss 0.000003, validation loss 0.040667\n",
      "Iter 7459, training loss 0.000003, validation loss 0.040667\n",
      "Iter 7460, training loss 0.000003, validation loss 0.040667\n",
      "Iter 7461, training loss 0.000003, validation loss 0.040666\n",
      "Iter 7462, training loss 0.000003, validation loss 0.040666\n",
      "Iter 7463, training loss 0.000003, validation loss 0.040666\n",
      "Iter 7464, training loss 0.000003, validation loss 0.040666\n",
      "Iter 7465, training loss 0.000003, validation loss 0.040666\n",
      "Iter 7466, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7467, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7468, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7469, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7470, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7471, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7472, training loss 0.000003, validation loss 0.040664\n",
      "Iter 7473, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7474, training loss 0.000003, validation loss 0.040664\n",
      "Iter 7475, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7476, training loss 0.000003, validation loss 0.040664\n",
      "Iter 7477, training loss 0.000003, validation loss 0.040664\n",
      "Iter 7478, training loss 0.000003, validation loss 0.040663\n",
      "Iter 7479, training loss 0.000003, validation loss 0.040664\n",
      "Iter 7480, training loss 0.000003, validation loss 0.040662\n",
      "Iter 7481, training loss 0.000003, validation loss 0.040664\n",
      "Iter 7482, training loss 0.000003, validation loss 0.040662\n",
      "Iter 7483, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7484, training loss 0.000003, validation loss 0.040661\n",
      "Iter 7485, training loss 0.000003, validation loss 0.040665\n",
      "Iter 7486, training loss 0.000003, validation loss 0.040659\n",
      "Iter 7487, training loss 0.000003, validation loss 0.040666\n",
      "Iter 7488, training loss 0.000003, validation loss 0.040658\n",
      "Iter 7489, training loss 0.000003, validation loss 0.040668\n",
      "Iter 7490, training loss 0.000003, validation loss 0.040655\n",
      "Iter 7491, training loss 0.000003, validation loss 0.040670\n",
      "Iter 7492, training loss 0.000003, validation loss 0.040651\n",
      "Iter 7493, training loss 0.000003, validation loss 0.040675\n",
      "Iter 7494, training loss 0.000003, validation loss 0.040644\n",
      "Iter 7495, training loss 0.000003, validation loss 0.040683\n",
      "Iter 7496, training loss 0.000003, validation loss 0.040635\n",
      "Iter 7497, training loss 0.000004, validation loss 0.040696\n",
      "Iter 7498, training loss 0.000005, validation loss 0.040621\n",
      "Iter 7499, training loss 0.000006, validation loss 0.040719\n",
      "Iter 7500, training loss 0.000008, validation loss 0.040598\n",
      "Iter 7501, training loss 0.000012, validation loss 0.040759\n",
      "Iter 7502, training loss 0.000018, validation loss 0.040564\n",
      "Iter 7503, training loss 0.000029, validation loss 0.040833\n",
      "Iter 7504, training loss 0.000046, validation loss 0.040520\n",
      "Iter 7505, training loss 0.000075, validation loss 0.040979\n",
      "Iter 7506, training loss 0.000124, validation loss 0.040478\n",
      "Iter 7507, training loss 0.000209, validation loss 0.041287\n",
      "Iter 7508, training loss 0.000353, validation loss 0.040507\n",
      "Iter 7509, training loss 0.000601, validation loss 0.041996\n",
      "Iter 7510, training loss 0.001031, validation loss 0.040857\n",
      "Iter 7511, training loss 0.001775, validation loss 0.043759\n",
      "Iter 7512, training loss 0.003067, validation loss 0.042388\n",
      "Iter 7513, training loss 0.005312, validation loss 0.048444\n",
      "Iter 7514, training loss 0.009189, validation loss 0.047825\n",
      "Iter 7515, training loss 0.015808, validation loss 0.061256\n",
      "Iter 7516, training loss 0.026792, validation loss 0.064814\n",
      "Iter 7517, training loss 0.044139, validation loss 0.094202\n",
      "Iter 7518, training loss 0.068931, validation loss 0.107278\n",
      "Iter 7519, training loss 0.098008, validation loss 0.155337\n",
      "Iter 7520, training loss 0.118407, validation loss 0.157833\n",
      "Iter 7521, training loss 0.108334, validation loss 0.167023\n",
      "Iter 7522, training loss 0.059562, validation loss 0.096332\n",
      "Iter 7523, training loss 0.008693, validation loss 0.051549\n",
      "Iter 7524, training loss 0.006954, validation loss 0.049043\n",
      "Iter 7525, training loss 0.037638, validation loss 0.072966\n",
      "Iter 7526, training loss 0.035146, validation loss 0.082391\n",
      "Iter 7527, training loss 0.004973, validation loss 0.041245\n",
      "Iter 7528, training loss 0.008074, validation loss 0.043689\n",
      "Iter 7529, training loss 0.025280, validation loss 0.069790\n",
      "Iter 7530, training loss 0.010297, validation loss 0.045365\n",
      "Iter 7531, training loss 0.001997, validation loss 0.038161\n",
      "Iter 7532, training loss 0.015837, validation loss 0.057729\n",
      "Iter 7533, training loss 0.008954, validation loss 0.043846\n",
      "Iter 7534, training loss 0.001192, validation loss 0.037312\n",
      "Iter 7535, training loss 0.010993, validation loss 0.051318\n",
      "Iter 7536, training loss 0.005935, validation loss 0.040905\n",
      "Iter 7537, training loss 0.001297, validation loss 0.037075\n",
      "Iter 7538, training loss 0.008286, validation loss 0.047594\n",
      "Iter 7539, training loss 0.003206, validation loss 0.038386\n",
      "Iter 7540, training loss 0.001790, validation loss 0.037214\n",
      "Iter 7541, training loss 0.006180, validation loss 0.044648\n",
      "Iter 7542, training loss 0.001344, validation loss 0.036824\n",
      "Iter 7543, training loss 0.002389, validation loss 0.037610\n",
      "Iter 7544, training loss 0.004187, validation loss 0.041983\n",
      "Iter 7545, training loss 0.000486, validation loss 0.036343\n",
      "Iter 7546, training loss 0.002721, validation loss 0.037885\n",
      "Iter 7547, training loss 0.002409, validation loss 0.039637\n",
      "Iter 7548, training loss 0.000429, validation loss 0.036649\n",
      "Iter 7549, training loss 0.002565, validation loss 0.037707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7550, training loss 0.001112, validation loss 0.037793\n",
      "Iter 7551, training loss 0.000752, validation loss 0.037221\n",
      "Iter 7552, training loss 0.001993, validation loss 0.037161\n",
      "Iter 7553, training loss 0.000425, validation loss 0.036618\n",
      "Iter 7554, training loss 0.001052, validation loss 0.037631\n",
      "Iter 7555, training loss 0.001273, validation loss 0.036544\n",
      "Iter 7556, training loss 0.000246, validation loss 0.036070\n",
      "Iter 7557, training loss 0.001115, validation loss 0.037663\n",
      "Iter 7558, training loss 0.000669, validation loss 0.036078\n",
      "Iter 7559, training loss 0.000334, validation loss 0.035915\n",
      "Iter 7560, training loss 0.000944, validation loss 0.037370\n",
      "Iter 7561, training loss 0.000314, validation loss 0.035868\n",
      "Iter 7562, training loss 0.000453, validation loss 0.035894\n",
      "Iter 7563, training loss 0.000653, validation loss 0.036907\n",
      "Iter 7564, training loss 0.000180, validation loss 0.035885\n",
      "Iter 7565, training loss 0.000501, validation loss 0.035878\n",
      "Iter 7566, training loss 0.000399, validation loss 0.036476\n",
      "Iter 7567, training loss 0.000181, validation loss 0.036006\n",
      "Iter 7568, training loss 0.000456, validation loss 0.035823\n",
      "Iter 7569, training loss 0.000228, validation loss 0.036134\n",
      "Iter 7570, training loss 0.000221, validation loss 0.036118\n",
      "Iter 7571, training loss 0.000358, validation loss 0.035749\n",
      "Iter 7572, training loss 0.000145, validation loss 0.035903\n",
      "Iter 7573, training loss 0.000245, validation loss 0.036169\n",
      "Iter 7574, training loss 0.000253, validation loss 0.035697\n",
      "Iter 7575, training loss 0.000122, validation loss 0.035777\n",
      "Iter 7576, training loss 0.000237, validation loss 0.036161\n",
      "Iter 7577, training loss 0.000172, validation loss 0.035689\n",
      "Iter 7578, training loss 0.000126, validation loss 0.035719\n",
      "Iter 7579, training loss 0.000206, validation loss 0.036104\n",
      "Iter 7580, training loss 0.000122, validation loss 0.035707\n",
      "Iter 7581, training loss 0.000133, validation loss 0.035687\n",
      "Iter 7582, training loss 0.000167, validation loss 0.036020\n",
      "Iter 7583, training loss 0.000098, validation loss 0.035740\n",
      "Iter 7584, training loss 0.000133, validation loss 0.035676\n",
      "Iter 7585, training loss 0.000131, validation loss 0.035941\n",
      "Iter 7586, training loss 0.000089, validation loss 0.035779\n",
      "Iter 7587, training loss 0.000125, validation loss 0.035674\n",
      "Iter 7588, training loss 0.000103, validation loss 0.035876\n",
      "Iter 7589, training loss 0.000087, validation loss 0.035814\n",
      "Iter 7590, training loss 0.000112, validation loss 0.035680\n",
      "Iter 7591, training loss 0.000086, validation loss 0.035828\n",
      "Iter 7592, training loss 0.000086, validation loss 0.035838\n",
      "Iter 7593, training loss 0.000098, validation loss 0.035691\n",
      "Iter 7594, training loss 0.000075, validation loss 0.035794\n",
      "Iter 7595, training loss 0.000083, validation loss 0.035851\n",
      "Iter 7596, training loss 0.000085, validation loss 0.035707\n",
      "Iter 7597, training loss 0.000069, validation loss 0.035775\n",
      "Iter 7598, training loss 0.000079, validation loss 0.035857\n",
      "Iter 7599, training loss 0.000074, validation loss 0.035725\n",
      "Iter 7600, training loss 0.000065, validation loss 0.035764\n",
      "Iter 7601, training loss 0.000073, validation loss 0.035854\n",
      "Iter 7602, training loss 0.000066, validation loss 0.035741\n",
      "Iter 7603, training loss 0.000063, validation loss 0.035755\n",
      "Iter 7604, training loss 0.000067, validation loss 0.035845\n",
      "Iter 7605, training loss 0.000060, validation loss 0.035755\n",
      "Iter 7606, training loss 0.000060, validation loss 0.035752\n",
      "Iter 7607, training loss 0.000062, validation loss 0.035836\n",
      "Iter 7608, training loss 0.000056, validation loss 0.035769\n",
      "Iter 7609, training loss 0.000057, validation loss 0.035752\n",
      "Iter 7610, training loss 0.000057, validation loss 0.035826\n",
      "Iter 7611, training loss 0.000053, validation loss 0.035779\n",
      "Iter 7612, training loss 0.000054, validation loss 0.035754\n",
      "Iter 7613, training loss 0.000053, validation loss 0.035817\n",
      "Iter 7614, training loss 0.000050, validation loss 0.035786\n",
      "Iter 7615, training loss 0.000051, validation loss 0.035756\n",
      "Iter 7616, training loss 0.000050, validation loss 0.035810\n",
      "Iter 7617, training loss 0.000048, validation loss 0.035792\n",
      "Iter 7618, training loss 0.000049, validation loss 0.035761\n",
      "Iter 7619, training loss 0.000047, validation loss 0.035805\n",
      "Iter 7620, training loss 0.000046, validation loss 0.035798\n",
      "Iter 7621, training loss 0.000046, validation loss 0.035766\n",
      "Iter 7622, training loss 0.000045, validation loss 0.035802\n",
      "Iter 7623, training loss 0.000044, validation loss 0.035801\n",
      "Iter 7624, training loss 0.000044, validation loss 0.035771\n",
      "Iter 7625, training loss 0.000042, validation loss 0.035799\n",
      "Iter 7626, training loss 0.000042, validation loss 0.035803\n",
      "Iter 7627, training loss 0.000042, validation loss 0.035776\n",
      "Iter 7628, training loss 0.000041, validation loss 0.035798\n",
      "Iter 7629, training loss 0.000040, validation loss 0.035806\n",
      "Iter 7630, training loss 0.000040, validation loss 0.035781\n",
      "Iter 7631, training loss 0.000039, validation loss 0.035798\n",
      "Iter 7632, training loss 0.000039, validation loss 0.035807\n",
      "Iter 7633, training loss 0.000038, validation loss 0.035785\n",
      "Iter 7634, training loss 0.000037, validation loss 0.035798\n",
      "Iter 7635, training loss 0.000037, validation loss 0.035808\n",
      "Iter 7636, training loss 0.000036, validation loss 0.035789\n",
      "Iter 7637, training loss 0.000036, validation loss 0.035798\n",
      "Iter 7638, training loss 0.000036, validation loss 0.035808\n",
      "Iter 7639, training loss 0.000035, validation loss 0.035792\n",
      "Iter 7640, training loss 0.000034, validation loss 0.035799\n",
      "Iter 7641, training loss 0.000034, validation loss 0.035809\n",
      "Iter 7642, training loss 0.000034, validation loss 0.035795\n",
      "Iter 7643, training loss 0.000033, validation loss 0.035800\n",
      "Iter 7644, training loss 0.000033, validation loss 0.035809\n",
      "Iter 7645, training loss 0.000032, validation loss 0.035797\n",
      "Iter 7646, training loss 0.000032, validation loss 0.035800\n",
      "Iter 7647, training loss 0.000032, validation loss 0.035809\n",
      "Iter 7648, training loss 0.000031, validation loss 0.035799\n",
      "Iter 7649, training loss 0.000031, validation loss 0.035800\n",
      "Iter 7650, training loss 0.000030, validation loss 0.035808\n",
      "Iter 7651, training loss 0.000030, validation loss 0.035799\n",
      "Iter 7652, training loss 0.000030, validation loss 0.035800\n",
      "Iter 7653, training loss 0.000029, validation loss 0.035806\n",
      "Iter 7654, training loss 0.000029, validation loss 0.035799\n",
      "Iter 7655, training loss 0.000029, validation loss 0.035800\n",
      "Iter 7656, training loss 0.000028, validation loss 0.035805\n",
      "Iter 7657, training loss 0.000028, validation loss 0.035799\n",
      "Iter 7658, training loss 0.000028, validation loss 0.035799\n",
      "Iter 7659, training loss 0.000027, validation loss 0.035804\n",
      "Iter 7660, training loss 0.000027, validation loss 0.035799\n",
      "Iter 7661, training loss 0.000027, validation loss 0.035799\n",
      "Iter 7662, training loss 0.000027, validation loss 0.035803\n",
      "Iter 7663, training loss 0.000026, validation loss 0.035799\n",
      "Iter 7664, training loss 0.000026, validation loss 0.035798\n",
      "Iter 7665, training loss 0.000026, validation loss 0.035802\n",
      "Iter 7666, training loss 0.000025, validation loss 0.035798\n",
      "Iter 7667, training loss 0.000025, validation loss 0.035798\n",
      "Iter 7668, training loss 0.000025, validation loss 0.035800\n",
      "Iter 7669, training loss 0.000025, validation loss 0.035797\n",
      "Iter 7670, training loss 0.000024, validation loss 0.035797\n",
      "Iter 7671, training loss 0.000024, validation loss 0.035799\n",
      "Iter 7672, training loss 0.000024, validation loss 0.035796\n",
      "Iter 7673, training loss 0.000024, validation loss 0.035796\n",
      "Iter 7674, training loss 0.000023, validation loss 0.035798\n",
      "Iter 7675, training loss 0.000023, validation loss 0.035795\n",
      "Iter 7676, training loss 0.000023, validation loss 0.035795\n",
      "Iter 7677, training loss 0.000023, validation loss 0.035797\n",
      "Iter 7678, training loss 0.000022, validation loss 0.035795\n",
      "Iter 7679, training loss 0.000022, validation loss 0.035795\n",
      "Iter 7680, training loss 0.000022, validation loss 0.035796\n",
      "Iter 7681, training loss 0.000022, validation loss 0.035794\n",
      "Iter 7682, training loss 0.000022, validation loss 0.035794\n",
      "Iter 7683, training loss 0.000021, validation loss 0.035794\n",
      "Iter 7684, training loss 0.000021, validation loss 0.035792\n",
      "Iter 7685, training loss 0.000021, validation loss 0.035792\n",
      "Iter 7686, training loss 0.000021, validation loss 0.035793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7687, training loss 0.000020, validation loss 0.035791\n",
      "Iter 7688, training loss 0.000020, validation loss 0.035791\n",
      "Iter 7689, training loss 0.000020, validation loss 0.035791\n",
      "Iter 7690, training loss 0.000020, validation loss 0.035790\n",
      "Iter 7691, training loss 0.000020, validation loss 0.035790\n",
      "Iter 7692, training loss 0.000020, validation loss 0.035790\n",
      "Iter 7693, training loss 0.000019, validation loss 0.035789\n",
      "Iter 7694, training loss 0.000019, validation loss 0.035789\n",
      "Iter 7695, training loss 0.000019, validation loss 0.035789\n",
      "Iter 7696, training loss 0.000019, validation loss 0.035788\n",
      "Iter 7697, training loss 0.000019, validation loss 0.035788\n",
      "Iter 7698, training loss 0.000018, validation loss 0.035787\n",
      "Iter 7699, training loss 0.000018, validation loss 0.035786\n",
      "Iter 7700, training loss 0.000018, validation loss 0.035786\n",
      "Iter 7701, training loss 0.000018, validation loss 0.035786\n",
      "Iter 7702, training loss 0.000018, validation loss 0.035785\n",
      "Iter 7703, training loss 0.000018, validation loss 0.035785\n",
      "Iter 7704, training loss 0.000018, validation loss 0.035785\n",
      "Iter 7705, training loss 0.000017, validation loss 0.035784\n",
      "Iter 7706, training loss 0.000017, validation loss 0.035784\n",
      "Iter 7707, training loss 0.000017, validation loss 0.035784\n",
      "Iter 7708, training loss 0.000017, validation loss 0.035783\n",
      "Iter 7709, training loss 0.000017, validation loss 0.035783\n",
      "Iter 7710, training loss 0.000017, validation loss 0.035783\n",
      "Iter 7711, training loss 0.000016, validation loss 0.035782\n",
      "Iter 7712, training loss 0.000016, validation loss 0.035782\n",
      "Iter 7713, training loss 0.000016, validation loss 0.035782\n",
      "Iter 7714, training loss 0.000016, validation loss 0.035781\n",
      "Iter 7715, training loss 0.000016, validation loss 0.035781\n",
      "Iter 7716, training loss 0.000016, validation loss 0.035780\n",
      "Iter 7717, training loss 0.000016, validation loss 0.035780\n",
      "Iter 7718, training loss 0.000016, validation loss 0.035780\n",
      "Iter 7719, training loss 0.000015, validation loss 0.035779\n",
      "Iter 7720, training loss 0.000015, validation loss 0.035778\n",
      "Iter 7721, training loss 0.000015, validation loss 0.035778\n",
      "Iter 7722, training loss 0.000015, validation loss 0.035778\n",
      "Iter 7723, training loss 0.000015, validation loss 0.035777\n",
      "Iter 7724, training loss 0.000015, validation loss 0.035777\n",
      "Iter 7725, training loss 0.000015, validation loss 0.035776\n",
      "Iter 7726, training loss 0.000015, validation loss 0.035776\n",
      "Iter 7727, training loss 0.000014, validation loss 0.035776\n",
      "Iter 7728, training loss 0.000014, validation loss 0.035775\n",
      "Iter 7729, training loss 0.000014, validation loss 0.035775\n",
      "Iter 7730, training loss 0.000014, validation loss 0.035774\n",
      "Iter 7731, training loss 0.000014, validation loss 0.035774\n",
      "Iter 7732, training loss 0.000014, validation loss 0.035773\n",
      "Iter 7733, training loss 0.000014, validation loss 0.035773\n",
      "Iter 7734, training loss 0.000014, validation loss 0.035773\n",
      "Iter 7735, training loss 0.000014, validation loss 0.035772\n",
      "Iter 7736, training loss 0.000013, validation loss 0.035772\n",
      "Iter 7737, training loss 0.000013, validation loss 0.035772\n",
      "Iter 7738, training loss 0.000013, validation loss 0.035771\n",
      "Iter 7739, training loss 0.000013, validation loss 0.035771\n",
      "Iter 7740, training loss 0.000013, validation loss 0.035771\n",
      "Iter 7741, training loss 0.000013, validation loss 0.035770\n",
      "Iter 7742, training loss 0.000013, validation loss 0.035770\n",
      "Iter 7743, training loss 0.000013, validation loss 0.035770\n",
      "Iter 7744, training loss 0.000013, validation loss 0.035770\n",
      "Iter 7745, training loss 0.000013, validation loss 0.035769\n",
      "Iter 7746, training loss 0.000012, validation loss 0.035769\n",
      "Iter 7747, training loss 0.000012, validation loss 0.035769\n",
      "Iter 7748, training loss 0.000012, validation loss 0.035769\n",
      "Iter 7749, training loss 0.000012, validation loss 0.035768\n",
      "Iter 7750, training loss 0.000012, validation loss 0.035768\n",
      "Iter 7751, training loss 0.000012, validation loss 0.035768\n",
      "Iter 7752, training loss 0.000012, validation loss 0.035768\n",
      "Iter 7753, training loss 0.000012, validation loss 0.035767\n",
      "Iter 7754, training loss 0.000012, validation loss 0.035767\n",
      "Iter 7755, training loss 0.000012, validation loss 0.035767\n",
      "Iter 7756, training loss 0.000012, validation loss 0.035767\n",
      "Iter 7757, training loss 0.000012, validation loss 0.035767\n",
      "Iter 7758, training loss 0.000011, validation loss 0.035767\n",
      "Iter 7759, training loss 0.000011, validation loss 0.035767\n",
      "Iter 7760, training loss 0.000011, validation loss 0.035766\n",
      "Iter 7761, training loss 0.000011, validation loss 0.035766\n",
      "Iter 7762, training loss 0.000011, validation loss 0.035766\n",
      "Iter 7763, training loss 0.000011, validation loss 0.035766\n",
      "Iter 7764, training loss 0.000011, validation loss 0.035766\n",
      "Iter 7765, training loss 0.000011, validation loss 0.035765\n",
      "Iter 7766, training loss 0.000011, validation loss 0.035765\n",
      "Iter 7767, training loss 0.000011, validation loss 0.035765\n",
      "Iter 7768, training loss 0.000011, validation loss 0.035765\n",
      "Iter 7769, training loss 0.000011, validation loss 0.035765\n",
      "Iter 7770, training loss 0.000011, validation loss 0.035765\n",
      "Iter 7771, training loss 0.000010, validation loss 0.035764\n",
      "Iter 7772, training loss 0.000010, validation loss 0.035764\n",
      "Iter 7773, training loss 0.000010, validation loss 0.035764\n",
      "Iter 7774, training loss 0.000010, validation loss 0.035764\n",
      "Iter 7775, training loss 0.000010, validation loss 0.035764\n",
      "Iter 7776, training loss 0.000010, validation loss 0.035763\n",
      "Iter 7777, training loss 0.000010, validation loss 0.035763\n",
      "Iter 7778, training loss 0.000010, validation loss 0.035763\n",
      "Iter 7779, training loss 0.000010, validation loss 0.035763\n",
      "Iter 7780, training loss 0.000010, validation loss 0.035763\n",
      "Iter 7781, training loss 0.000010, validation loss 0.035763\n",
      "Iter 7782, training loss 0.000010, validation loss 0.035763\n",
      "Iter 7783, training loss 0.000010, validation loss 0.035763\n",
      "Iter 7784, training loss 0.000010, validation loss 0.035762\n",
      "Iter 7785, training loss 0.000010, validation loss 0.035762\n",
      "Iter 7786, training loss 0.000009, validation loss 0.035762\n",
      "Iter 7787, training loss 0.000009, validation loss 0.035762\n",
      "Iter 7788, training loss 0.000009, validation loss 0.035762\n",
      "Iter 7789, training loss 0.000009, validation loss 0.035762\n",
      "Iter 7790, training loss 0.000009, validation loss 0.035762\n",
      "Iter 7791, training loss 0.000009, validation loss 0.035762\n",
      "Iter 7792, training loss 0.000009, validation loss 0.035761\n",
      "Iter 7793, training loss 0.000009, validation loss 0.035761\n",
      "Iter 7794, training loss 0.000009, validation loss 0.035761\n",
      "Iter 7795, training loss 0.000009, validation loss 0.035761\n",
      "Iter 7796, training loss 0.000009, validation loss 0.035761\n",
      "Iter 7797, training loss 0.000009, validation loss 0.035761\n",
      "Iter 7798, training loss 0.000009, validation loss 0.035760\n",
      "Iter 7799, training loss 0.000009, validation loss 0.035760\n",
      "Iter 7800, training loss 0.000009, validation loss 0.035760\n",
      "Iter 7801, training loss 0.000009, validation loss 0.035760\n",
      "Iter 7802, training loss 0.000009, validation loss 0.035760\n",
      "Iter 7803, training loss 0.000008, validation loss 0.035759\n",
      "Iter 7804, training loss 0.000008, validation loss 0.035759\n",
      "Iter 7805, training loss 0.000008, validation loss 0.035758\n",
      "Iter 7806, training loss 0.000008, validation loss 0.035758\n",
      "Iter 7807, training loss 0.000008, validation loss 0.035758\n",
      "Iter 7808, training loss 0.000008, validation loss 0.035758\n",
      "Iter 7809, training loss 0.000008, validation loss 0.035758\n",
      "Iter 7810, training loss 0.000008, validation loss 0.035757\n",
      "Iter 7811, training loss 0.000008, validation loss 0.035757\n",
      "Iter 7812, training loss 0.000008, validation loss 0.035757\n",
      "Iter 7813, training loss 0.000008, validation loss 0.035756\n",
      "Iter 7814, training loss 0.000008, validation loss 0.035756\n",
      "Iter 7815, training loss 0.000008, validation loss 0.035756\n",
      "Iter 7816, training loss 0.000008, validation loss 0.035755\n",
      "Iter 7817, training loss 0.000008, validation loss 0.035755\n",
      "Iter 7818, training loss 0.000008, validation loss 0.035755\n",
      "Iter 7819, training loss 0.000008, validation loss 0.035754\n",
      "Iter 7820, training loss 0.000008, validation loss 0.035754\n",
      "Iter 7821, training loss 0.000008, validation loss 0.035754\n",
      "Iter 7822, training loss 0.000008, validation loss 0.035754\n",
      "Iter 7823, training loss 0.000007, validation loss 0.035753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7824, training loss 0.000007, validation loss 0.035753\n",
      "Iter 7825, training loss 0.000007, validation loss 0.035753\n",
      "Iter 7826, training loss 0.000007, validation loss 0.035753\n",
      "Iter 7827, training loss 0.000007, validation loss 0.035752\n",
      "Iter 7828, training loss 0.000007, validation loss 0.035752\n",
      "Iter 7829, training loss 0.000007, validation loss 0.035752\n",
      "Iter 7830, training loss 0.000007, validation loss 0.035752\n",
      "Iter 7831, training loss 0.000007, validation loss 0.035751\n",
      "Iter 7832, training loss 0.000007, validation loss 0.035751\n",
      "Iter 7833, training loss 0.000007, validation loss 0.035751\n",
      "Iter 7834, training loss 0.000007, validation loss 0.035750\n",
      "Iter 7835, training loss 0.000007, validation loss 0.035750\n",
      "Iter 7836, training loss 0.000007, validation loss 0.035750\n",
      "Iter 7837, training loss 0.000007, validation loss 0.035749\n",
      "Iter 7838, training loss 0.000007, validation loss 0.035749\n",
      "Iter 7839, training loss 0.000007, validation loss 0.035749\n",
      "Iter 7840, training loss 0.000007, validation loss 0.035749\n",
      "Iter 7841, training loss 0.000007, validation loss 0.035748\n",
      "Iter 7842, training loss 0.000007, validation loss 0.035748\n",
      "Iter 7843, training loss 0.000007, validation loss 0.035748\n",
      "Iter 7844, training loss 0.000007, validation loss 0.035748\n",
      "Iter 7845, training loss 0.000007, validation loss 0.035747\n",
      "Iter 7846, training loss 0.000007, validation loss 0.035747\n",
      "Iter 7847, training loss 0.000006, validation loss 0.035747\n",
      "Iter 7848, training loss 0.000006, validation loss 0.035747\n",
      "Iter 7849, training loss 0.000006, validation loss 0.035746\n",
      "Iter 7850, training loss 0.000006, validation loss 0.035746\n",
      "Iter 7851, training loss 0.000006, validation loss 0.035746\n",
      "Iter 7852, training loss 0.000006, validation loss 0.035746\n",
      "Iter 7853, training loss 0.000006, validation loss 0.035745\n",
      "Iter 7854, training loss 0.000006, validation loss 0.035745\n",
      "Iter 7855, training loss 0.000006, validation loss 0.035745\n",
      "Iter 7856, training loss 0.000006, validation loss 0.035745\n",
      "Iter 7857, training loss 0.000006, validation loss 0.035745\n",
      "Iter 7858, training loss 0.000006, validation loss 0.035745\n",
      "Iter 7859, training loss 0.000006, validation loss 0.035745\n",
      "Iter 7860, training loss 0.000006, validation loss 0.035744\n",
      "Iter 7861, training loss 0.000006, validation loss 0.035744\n",
      "Iter 7862, training loss 0.000006, validation loss 0.035744\n",
      "Iter 7863, training loss 0.000006, validation loss 0.035744\n",
      "Iter 7864, training loss 0.000006, validation loss 0.035744\n",
      "Iter 7865, training loss 0.000006, validation loss 0.035744\n",
      "Iter 7866, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7867, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7868, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7869, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7870, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7871, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7872, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7873, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7874, training loss 0.000006, validation loss 0.035743\n",
      "Iter 7875, training loss 0.000006, validation loss 0.035742\n",
      "Iter 7876, training loss 0.000006, validation loss 0.035742\n",
      "Iter 7877, training loss 0.000005, validation loss 0.035742\n",
      "Iter 7878, training loss 0.000005, validation loss 0.035742\n",
      "Iter 7879, training loss 0.000005, validation loss 0.035742\n",
      "Iter 7880, training loss 0.000005, validation loss 0.035742\n",
      "Iter 7881, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7882, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7883, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7884, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7885, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7886, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7887, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7888, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7889, training loss 0.000005, validation loss 0.035741\n",
      "Iter 7890, training loss 0.000005, validation loss 0.035740\n",
      "Iter 7891, training loss 0.000005, validation loss 0.035740\n",
      "Iter 7892, training loss 0.000005, validation loss 0.035740\n",
      "Iter 7893, training loss 0.000005, validation loss 0.035740\n",
      "Iter 7894, training loss 0.000005, validation loss 0.035740\n",
      "Iter 7895, training loss 0.000005, validation loss 0.035740\n",
      "Iter 7896, training loss 0.000005, validation loss 0.035740\n",
      "Iter 7897, training loss 0.000005, validation loss 0.035740\n",
      "Iter 7898, training loss 0.000005, validation loss 0.035739\n",
      "Iter 7899, training loss 0.000005, validation loss 0.035739\n",
      "Iter 7900, training loss 0.000005, validation loss 0.035739\n",
      "Iter 7901, training loss 0.000005, validation loss 0.035739\n",
      "Iter 7902, training loss 0.000005, validation loss 0.035739\n",
      "Iter 7903, training loss 0.000005, validation loss 0.035739\n",
      "Iter 7904, training loss 0.000005, validation loss 0.035739\n",
      "Iter 7905, training loss 0.000005, validation loss 0.035739\n",
      "Iter 7906, training loss 0.000005, validation loss 0.035738\n",
      "Iter 7907, training loss 0.000005, validation loss 0.035738\n",
      "Iter 7908, training loss 0.000005, validation loss 0.035738\n",
      "Iter 7909, training loss 0.000005, validation loss 0.035738\n",
      "Iter 7910, training loss 0.000005, validation loss 0.035738\n",
      "Iter 7911, training loss 0.000005, validation loss 0.035738\n",
      "Iter 7912, training loss 0.000005, validation loss 0.035738\n",
      "Iter 7913, training loss 0.000005, validation loss 0.035738\n",
      "Iter 7914, training loss 0.000005, validation loss 0.035737\n",
      "Iter 7915, training loss 0.000004, validation loss 0.035737\n",
      "Iter 7916, training loss 0.000004, validation loss 0.035737\n",
      "Iter 7917, training loss 0.000004, validation loss 0.035737\n",
      "Iter 7918, training loss 0.000004, validation loss 0.035737\n",
      "Iter 7919, training loss 0.000004, validation loss 0.035737\n",
      "Iter 7920, training loss 0.000004, validation loss 0.035737\n",
      "Iter 7921, training loss 0.000004, validation loss 0.035736\n",
      "Iter 7922, training loss 0.000004, validation loss 0.035736\n",
      "Iter 7923, training loss 0.000004, validation loss 0.035736\n",
      "Iter 7924, training loss 0.000004, validation loss 0.035736\n",
      "Iter 7925, training loss 0.000004, validation loss 0.035736\n",
      "Iter 7926, training loss 0.000004, validation loss 0.035735\n",
      "Iter 7927, training loss 0.000004, validation loss 0.035735\n",
      "Iter 7928, training loss 0.000004, validation loss 0.035735\n",
      "Iter 7929, training loss 0.000004, validation loss 0.035735\n",
      "Iter 7930, training loss 0.000004, validation loss 0.035735\n",
      "Iter 7931, training loss 0.000004, validation loss 0.035735\n",
      "Iter 7932, training loss 0.000004, validation loss 0.035734\n",
      "Iter 7933, training loss 0.000004, validation loss 0.035734\n",
      "Iter 7934, training loss 0.000004, validation loss 0.035734\n",
      "Iter 7935, training loss 0.000004, validation loss 0.035734\n",
      "Iter 7936, training loss 0.000004, validation loss 0.035734\n",
      "Iter 7937, training loss 0.000004, validation loss 0.035734\n",
      "Iter 7938, training loss 0.000004, validation loss 0.035733\n",
      "Iter 7939, training loss 0.000004, validation loss 0.035733\n",
      "Iter 7940, training loss 0.000004, validation loss 0.035733\n",
      "Iter 7941, training loss 0.000004, validation loss 0.035733\n",
      "Iter 7942, training loss 0.000004, validation loss 0.035733\n",
      "Iter 7943, training loss 0.000004, validation loss 0.035732\n",
      "Iter 7944, training loss 0.000004, validation loss 0.035732\n",
      "Iter 7945, training loss 0.000004, validation loss 0.035732\n",
      "Iter 7946, training loss 0.000004, validation loss 0.035732\n",
      "Iter 7947, training loss 0.000004, validation loss 0.035732\n",
      "Iter 7948, training loss 0.000004, validation loss 0.035731\n",
      "Iter 7949, training loss 0.000004, validation loss 0.035731\n",
      "Iter 7950, training loss 0.000004, validation loss 0.035731\n",
      "Iter 7951, training loss 0.000004, validation loss 0.035731\n",
      "Iter 7952, training loss 0.000004, validation loss 0.035730\n",
      "Iter 7953, training loss 0.000004, validation loss 0.035730\n",
      "Iter 7954, training loss 0.000004, validation loss 0.035730\n",
      "Iter 7955, training loss 0.000004, validation loss 0.035730\n",
      "Iter 7956, training loss 0.000004, validation loss 0.035730\n",
      "Iter 7957, training loss 0.000004, validation loss 0.035730\n",
      "Iter 7958, training loss 0.000004, validation loss 0.035730\n",
      "Iter 7959, training loss 0.000004, validation loss 0.035729\n",
      "Iter 7960, training loss 0.000004, validation loss 0.035729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7961, training loss 0.000004, validation loss 0.035729\n",
      "Iter 7962, training loss 0.000004, validation loss 0.035729\n",
      "Iter 7963, training loss 0.000004, validation loss 0.035729\n",
      "Iter 7964, training loss 0.000004, validation loss 0.035728\n",
      "Iter 7965, training loss 0.000004, validation loss 0.035728\n",
      "Iter 7966, training loss 0.000004, validation loss 0.035728\n",
      "Iter 7967, training loss 0.000004, validation loss 0.035728\n",
      "Iter 7968, training loss 0.000003, validation loss 0.035728\n",
      "Iter 7969, training loss 0.000003, validation loss 0.035728\n",
      "Iter 7970, training loss 0.000003, validation loss 0.035727\n",
      "Iter 7971, training loss 0.000003, validation loss 0.035727\n",
      "Iter 7972, training loss 0.000003, validation loss 0.035727\n",
      "Iter 7973, training loss 0.000003, validation loss 0.035727\n",
      "Iter 7974, training loss 0.000003, validation loss 0.035727\n",
      "Iter 7975, training loss 0.000003, validation loss 0.035726\n",
      "Iter 7976, training loss 0.000003, validation loss 0.035726\n",
      "Iter 7977, training loss 0.000003, validation loss 0.035726\n",
      "Iter 7978, training loss 0.000003, validation loss 0.035726\n",
      "Iter 7979, training loss 0.000003, validation loss 0.035726\n",
      "Iter 7980, training loss 0.000003, validation loss 0.035726\n",
      "Iter 7981, training loss 0.000003, validation loss 0.035725\n",
      "Iter 7982, training loss 0.000003, validation loss 0.035725\n",
      "Iter 7983, training loss 0.000003, validation loss 0.035725\n",
      "Iter 7984, training loss 0.000003, validation loss 0.035725\n",
      "Iter 7985, training loss 0.000003, validation loss 0.035725\n",
      "Iter 7986, training loss 0.000003, validation loss 0.035724\n",
      "Iter 7987, training loss 0.000003, validation loss 0.035724\n",
      "Iter 7988, training loss 0.000003, validation loss 0.035724\n",
      "Iter 7989, training loss 0.000003, validation loss 0.035724\n",
      "Iter 7990, training loss 0.000003, validation loss 0.035724\n",
      "Iter 7991, training loss 0.000003, validation loss 0.035724\n",
      "Iter 7992, training loss 0.000003, validation loss 0.035723\n",
      "Iter 7993, training loss 0.000003, validation loss 0.035723\n",
      "Iter 7994, training loss 0.000003, validation loss 0.035723\n",
      "Iter 7995, training loss 0.000003, validation loss 0.035723\n",
      "Iter 7996, training loss 0.000003, validation loss 0.035722\n",
      "Iter 7997, training loss 0.000003, validation loss 0.035722\n",
      "Iter 7998, training loss 0.000003, validation loss 0.035722\n",
      "Iter 7999, training loss 0.000003, validation loss 0.035722\n",
      "Iter 8000, training loss 0.000003, validation loss 0.035722\n",
      "Iter 8001, training loss 0.000003, validation loss 0.035721\n",
      "Iter 8002, training loss 0.000003, validation loss 0.035721\n",
      "Iter 8003, training loss 0.000003, validation loss 0.035721\n",
      "Iter 8004, training loss 0.000003, validation loss 0.035721\n",
      "Iter 8005, training loss 0.000003, validation loss 0.035720\n",
      "Iter 8006, training loss 0.000003, validation loss 0.035720\n",
      "Iter 8007, training loss 0.000003, validation loss 0.035720\n",
      "Iter 8008, training loss 0.000003, validation loss 0.035720\n",
      "Iter 8009, training loss 0.000003, validation loss 0.035719\n",
      "Iter 8010, training loss 0.000003, validation loss 0.035719\n",
      "Iter 8011, training loss 0.000003, validation loss 0.035719\n",
      "Iter 8012, training loss 0.000003, validation loss 0.035719\n",
      "Iter 8013, training loss 0.000003, validation loss 0.035719\n",
      "Iter 8014, training loss 0.000003, validation loss 0.035718\n",
      "Iter 8015, training loss 0.000003, validation loss 0.035718\n",
      "Iter 8016, training loss 0.000003, validation loss 0.035718\n",
      "Iter 8017, training loss 0.000003, validation loss 0.035718\n",
      "Iter 8018, training loss 0.000003, validation loss 0.035717\n",
      "Iter 8019, training loss 0.000003, validation loss 0.035717\n",
      "Iter 8020, training loss 0.000003, validation loss 0.035717\n",
      "Iter 8021, training loss 0.000003, validation loss 0.035717\n",
      "Iter 8022, training loss 0.000003, validation loss 0.035716\n",
      "Iter 8023, training loss 0.000003, validation loss 0.035716\n",
      "Iter 8024, training loss 0.000003, validation loss 0.035716\n",
      "Iter 8025, training loss 0.000003, validation loss 0.035716\n",
      "Iter 8026, training loss 0.000003, validation loss 0.035715\n",
      "Iter 8027, training loss 0.000003, validation loss 0.035715\n",
      "Iter 8028, training loss 0.000003, validation loss 0.035715\n",
      "Iter 8029, training loss 0.000003, validation loss 0.035715\n",
      "Iter 8030, training loss 0.000003, validation loss 0.035714\n",
      "Iter 8031, training loss 0.000003, validation loss 0.035714\n",
      "Iter 8032, training loss 0.000003, validation loss 0.035714\n",
      "Iter 8033, training loss 0.000003, validation loss 0.035713\n",
      "Iter 8034, training loss 0.000003, validation loss 0.035713\n",
      "Iter 8035, training loss 0.000003, validation loss 0.035713\n",
      "Iter 8036, training loss 0.000003, validation loss 0.035713\n",
      "Iter 8037, training loss 0.000003, validation loss 0.035712\n",
      "Iter 8038, training loss 0.000003, validation loss 0.035712\n",
      "Iter 8039, training loss 0.000003, validation loss 0.035712\n",
      "Iter 8040, training loss 0.000003, validation loss 0.035712\n",
      "Iter 8041, training loss 0.000003, validation loss 0.035711\n",
      "Iter 8042, training loss 0.000003, validation loss 0.035711\n",
      "Iter 8043, training loss 0.000003, validation loss 0.035711\n",
      "Iter 8044, training loss 0.000003, validation loss 0.035711\n",
      "Iter 8045, training loss 0.000003, validation loss 0.035711\n",
      "Iter 8046, training loss 0.000002, validation loss 0.035710\n",
      "Iter 8047, training loss 0.000002, validation loss 0.035710\n",
      "Iter 8048, training loss 0.000002, validation loss 0.035710\n",
      "Iter 8049, training loss 0.000002, validation loss 0.035710\n",
      "Iter 8050, training loss 0.000002, validation loss 0.035710\n",
      "Iter 8051, training loss 0.000002, validation loss 0.035709\n",
      "Iter 8052, training loss 0.000002, validation loss 0.035709\n",
      "Iter 8053, training loss 0.000002, validation loss 0.035709\n",
      "Iter 8054, training loss 0.000002, validation loss 0.035709\n",
      "Iter 8055, training loss 0.000002, validation loss 0.035708\n",
      "Iter 8056, training loss 0.000002, validation loss 0.035708\n",
      "Iter 8057, training loss 0.000002, validation loss 0.035708\n",
      "Iter 8058, training loss 0.000002, validation loss 0.035708\n",
      "Iter 8059, training loss 0.000002, validation loss 0.035708\n",
      "Iter 8060, training loss 0.000002, validation loss 0.035707\n",
      "Iter 8061, training loss 0.000002, validation loss 0.035707\n",
      "Iter 8062, training loss 0.000002, validation loss 0.035707\n",
      "Iter 8063, training loss 0.000002, validation loss 0.035707\n",
      "Iter 8064, training loss 0.000002, validation loss 0.035707\n",
      "Iter 8065, training loss 0.000002, validation loss 0.035706\n",
      "Iter 8066, training loss 0.000002, validation loss 0.035706\n",
      "Iter 8067, training loss 0.000002, validation loss 0.035706\n",
      "Iter 8068, training loss 0.000002, validation loss 0.035706\n",
      "Iter 8069, training loss 0.000002, validation loss 0.035705\n",
      "Iter 8070, training loss 0.000002, validation loss 0.035705\n",
      "Iter 8071, training loss 0.000002, validation loss 0.035705\n",
      "Iter 8072, training loss 0.000002, validation loss 0.035705\n",
      "Iter 8073, training loss 0.000002, validation loss 0.035704\n",
      "Iter 8074, training loss 0.000002, validation loss 0.035704\n",
      "Iter 8075, training loss 0.000002, validation loss 0.035704\n",
      "Iter 8076, training loss 0.000002, validation loss 0.035704\n",
      "Iter 8077, training loss 0.000002, validation loss 0.035704\n",
      "Iter 8078, training loss 0.000002, validation loss 0.035703\n",
      "Iter 8079, training loss 0.000002, validation loss 0.035703\n",
      "Iter 8080, training loss 0.000002, validation loss 0.035703\n",
      "Iter 8081, training loss 0.000002, validation loss 0.035703\n",
      "Iter 8082, training loss 0.000002, validation loss 0.035703\n",
      "Iter 8083, training loss 0.000002, validation loss 0.035702\n",
      "Iter 8084, training loss 0.000002, validation loss 0.035702\n",
      "Iter 8085, training loss 0.000002, validation loss 0.035702\n",
      "Iter 8086, training loss 0.000002, validation loss 0.035702\n",
      "Iter 8087, training loss 0.000002, validation loss 0.035701\n",
      "Iter 8088, training loss 0.000002, validation loss 0.035701\n",
      "Iter 8089, training loss 0.000002, validation loss 0.035701\n",
      "Iter 8090, training loss 0.000002, validation loss 0.035701\n",
      "Iter 8091, training loss 0.000002, validation loss 0.035701\n",
      "Iter 8092, training loss 0.000002, validation loss 0.035700\n",
      "Iter 8093, training loss 0.000002, validation loss 0.035700\n",
      "Iter 8094, training loss 0.000002, validation loss 0.035700\n",
      "Iter 8095, training loss 0.000002, validation loss 0.035700\n",
      "Iter 8096, training loss 0.000002, validation loss 0.035699\n",
      "Iter 8097, training loss 0.000002, validation loss 0.035699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8098, training loss 0.000002, validation loss 0.035699\n",
      "Iter 8099, training loss 0.000002, validation loss 0.035699\n",
      "Iter 8100, training loss 0.000002, validation loss 0.035698\n",
      "Iter 8101, training loss 0.000002, validation loss 0.035698\n",
      "Iter 8102, training loss 0.000002, validation loss 0.035698\n",
      "Iter 8103, training loss 0.000002, validation loss 0.035698\n",
      "Iter 8104, training loss 0.000002, validation loss 0.035698\n",
      "Iter 8105, training loss 0.000002, validation loss 0.035698\n",
      "Iter 8106, training loss 0.000002, validation loss 0.035697\n",
      "Iter 8107, training loss 0.000002, validation loss 0.035697\n",
      "Iter 8108, training loss 0.000002, validation loss 0.035697\n",
      "Iter 8109, training loss 0.000002, validation loss 0.035697\n",
      "Iter 8110, training loss 0.000002, validation loss 0.035697\n",
      "Iter 8111, training loss 0.000002, validation loss 0.035696\n",
      "Iter 8112, training loss 0.000002, validation loss 0.035696\n",
      "Iter 8113, training loss 0.000002, validation loss 0.035696\n",
      "Iter 8114, training loss 0.000002, validation loss 0.035696\n",
      "Iter 8115, training loss 0.000002, validation loss 0.035695\n",
      "Iter 8116, training loss 0.000002, validation loss 0.035695\n",
      "Iter 8117, training loss 0.000002, validation loss 0.035695\n",
      "Iter 8118, training loss 0.000002, validation loss 0.035695\n",
      "Iter 8119, training loss 0.000002, validation loss 0.035695\n",
      "Iter 8120, training loss 0.000002, validation loss 0.035694\n",
      "Iter 8121, training loss 0.000002, validation loss 0.035694\n",
      "Iter 8122, training loss 0.000002, validation loss 0.035694\n",
      "Iter 8123, training loss 0.000002, validation loss 0.035694\n",
      "Iter 8124, training loss 0.000002, validation loss 0.035694\n",
      "Iter 8125, training loss 0.000002, validation loss 0.035693\n",
      "Iter 8126, training loss 0.000002, validation loss 0.035693\n",
      "Iter 8127, training loss 0.000002, validation loss 0.035693\n",
      "Iter 8128, training loss 0.000002, validation loss 0.035693\n",
      "Iter 8129, training loss 0.000002, validation loss 0.035693\n",
      "Iter 8130, training loss 0.000002, validation loss 0.035692\n",
      "Iter 8131, training loss 0.000002, validation loss 0.035692\n",
      "Iter 8132, training loss 0.000002, validation loss 0.035692\n",
      "Iter 8133, training loss 0.000002, validation loss 0.035692\n",
      "Iter 8134, training loss 0.000002, validation loss 0.035691\n",
      "Iter 8135, training loss 0.000002, validation loss 0.035691\n",
      "Iter 8136, training loss 0.000002, validation loss 0.035691\n",
      "Iter 8137, training loss 0.000002, validation loss 0.035691\n",
      "Iter 8138, training loss 0.000002, validation loss 0.035690\n",
      "Iter 8139, training loss 0.000002, validation loss 0.035690\n",
      "Iter 8140, training loss 0.000002, validation loss 0.035690\n",
      "Iter 8141, training loss 0.000002, validation loss 0.035690\n",
      "Iter 8142, training loss 0.000002, validation loss 0.035689\n",
      "Iter 8143, training loss 0.000002, validation loss 0.035689\n",
      "Iter 8144, training loss 0.000002, validation loss 0.035689\n",
      "Iter 8145, training loss 0.000002, validation loss 0.035689\n",
      "Iter 8146, training loss 0.000002, validation loss 0.035688\n",
      "Iter 8147, training loss 0.000002, validation loss 0.035688\n",
      "Iter 8148, training loss 0.000002, validation loss 0.035688\n",
      "Iter 8149, training loss 0.000002, validation loss 0.035688\n",
      "Iter 8150, training loss 0.000002, validation loss 0.035687\n",
      "Iter 8151, training loss 0.000002, validation loss 0.035687\n",
      "Iter 8152, training loss 0.000002, validation loss 0.035687\n",
      "Iter 8153, training loss 0.000002, validation loss 0.035687\n",
      "Iter 8154, training loss 0.000002, validation loss 0.035686\n",
      "Iter 8155, training loss 0.000002, validation loss 0.035686\n",
      "Iter 8156, training loss 0.000002, validation loss 0.035686\n",
      "Iter 8157, training loss 0.000002, validation loss 0.035685\n",
      "Iter 8158, training loss 0.000002, validation loss 0.035685\n",
      "Iter 8159, training loss 0.000002, validation loss 0.035685\n",
      "Iter 8160, training loss 0.000002, validation loss 0.035685\n",
      "Iter 8161, training loss 0.000002, validation loss 0.035684\n",
      "Iter 8162, training loss 0.000002, validation loss 0.035684\n",
      "Iter 8163, training loss 0.000002, validation loss 0.035684\n",
      "Iter 8164, training loss 0.000002, validation loss 0.035684\n",
      "Iter 8165, training loss 0.000002, validation loss 0.035683\n",
      "Iter 8166, training loss 0.000002, validation loss 0.035683\n",
      "Iter 8167, training loss 0.000002, validation loss 0.035683\n",
      "Iter 8168, training loss 0.000002, validation loss 0.035683\n",
      "Iter 8169, training loss 0.000002, validation loss 0.035682\n",
      "Iter 8170, training loss 0.000002, validation loss 0.035682\n",
      "Iter 8171, training loss 0.000002, validation loss 0.035682\n",
      "Iter 8172, training loss 0.000002, validation loss 0.035682\n",
      "Iter 8173, training loss 0.000002, validation loss 0.035681\n",
      "Iter 8174, training loss 0.000002, validation loss 0.035681\n",
      "Iter 8175, training loss 0.000002, validation loss 0.035681\n",
      "Iter 8176, training loss 0.000002, validation loss 0.035681\n",
      "Iter 8177, training loss 0.000002, validation loss 0.035680\n",
      "Iter 8178, training loss 0.000002, validation loss 0.035680\n",
      "Iter 8179, training loss 0.000002, validation loss 0.035680\n",
      "Iter 8180, training loss 0.000001, validation loss 0.035679\n",
      "Iter 8181, training loss 0.000001, validation loss 0.035679\n",
      "Iter 8182, training loss 0.000001, validation loss 0.035679\n",
      "Iter 8183, training loss 0.000001, validation loss 0.035679\n",
      "Iter 8184, training loss 0.000001, validation loss 0.035678\n",
      "Iter 8185, training loss 0.000001, validation loss 0.035678\n",
      "Iter 8186, training loss 0.000001, validation loss 0.035678\n",
      "Iter 8187, training loss 0.000001, validation loss 0.035678\n",
      "Iter 8188, training loss 0.000001, validation loss 0.035677\n",
      "Iter 8189, training loss 0.000001, validation loss 0.035677\n",
      "Iter 8190, training loss 0.000001, validation loss 0.035677\n",
      "Iter 8191, training loss 0.000001, validation loss 0.035677\n",
      "Iter 8192, training loss 0.000001, validation loss 0.035677\n",
      "Iter 8193, training loss 0.000001, validation loss 0.035676\n",
      "Iter 8194, training loss 0.000001, validation loss 0.035676\n",
      "Iter 8195, training loss 0.000001, validation loss 0.035676\n",
      "Iter 8196, training loss 0.000001, validation loss 0.035676\n",
      "Iter 8197, training loss 0.000001, validation loss 0.035675\n",
      "Iter 8198, training loss 0.000001, validation loss 0.035675\n",
      "Iter 8199, training loss 0.000001, validation loss 0.035675\n",
      "Iter 8200, training loss 0.000001, validation loss 0.035675\n",
      "Iter 8201, training loss 0.000001, validation loss 0.035674\n",
      "Iter 8202, training loss 0.000001, validation loss 0.035674\n",
      "Iter 8203, training loss 0.000001, validation loss 0.035674\n",
      "Iter 8204, training loss 0.000001, validation loss 0.035674\n",
      "Iter 8205, training loss 0.000001, validation loss 0.035673\n",
      "Iter 8206, training loss 0.000001, validation loss 0.035673\n",
      "Iter 8207, training loss 0.000001, validation loss 0.035673\n",
      "Iter 8208, training loss 0.000001, validation loss 0.035673\n",
      "Iter 8209, training loss 0.000001, validation loss 0.035672\n",
      "Iter 8210, training loss 0.000001, validation loss 0.035672\n",
      "Iter 8211, training loss 0.000001, validation loss 0.035672\n",
      "Iter 8212, training loss 0.000001, validation loss 0.035672\n",
      "Iter 8213, training loss 0.000001, validation loss 0.035672\n",
      "Iter 8214, training loss 0.000001, validation loss 0.035671\n",
      "Iter 8215, training loss 0.000001, validation loss 0.035671\n",
      "Iter 8216, training loss 0.000001, validation loss 0.035671\n",
      "Iter 8217, training loss 0.000001, validation loss 0.035671\n",
      "Iter 8218, training loss 0.000001, validation loss 0.035670\n",
      "Iter 8219, training loss 0.000001, validation loss 0.035670\n",
      "Iter 8220, training loss 0.000001, validation loss 0.035670\n",
      "Iter 8221, training loss 0.000001, validation loss 0.035670\n",
      "Iter 8222, training loss 0.000001, validation loss 0.035669\n",
      "Iter 8223, training loss 0.000001, validation loss 0.035669\n",
      "Iter 8224, training loss 0.000001, validation loss 0.035669\n",
      "Iter 8225, training loss 0.000001, validation loss 0.035669\n",
      "Iter 8226, training loss 0.000001, validation loss 0.035669\n",
      "Iter 8227, training loss 0.000001, validation loss 0.035669\n",
      "Iter 8228, training loss 0.000001, validation loss 0.035668\n",
      "Iter 8229, training loss 0.000001, validation loss 0.035668\n",
      "Iter 8230, training loss 0.000001, validation loss 0.035668\n",
      "Iter 8231, training loss 0.000001, validation loss 0.035668\n",
      "Iter 8232, training loss 0.000001, validation loss 0.035667\n",
      "Iter 8233, training loss 0.000001, validation loss 0.035667\n",
      "Iter 8234, training loss 0.000001, validation loss 0.035667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8235, training loss 0.000001, validation loss 0.035667\n",
      "Iter 8236, training loss 0.000001, validation loss 0.035666\n",
      "Iter 8237, training loss 0.000001, validation loss 0.035666\n",
      "Iter 8238, training loss 0.000001, validation loss 0.035666\n",
      "Iter 8239, training loss 0.000001, validation loss 0.035666\n",
      "Iter 8240, training loss 0.000001, validation loss 0.035665\n",
      "Iter 8241, training loss 0.000001, validation loss 0.035665\n",
      "Iter 8242, training loss 0.000001, validation loss 0.035665\n",
      "Iter 8243, training loss 0.000001, validation loss 0.035665\n",
      "Iter 8244, training loss 0.000001, validation loss 0.035664\n",
      "Iter 8245, training loss 0.000001, validation loss 0.035665\n",
      "Iter 8246, training loss 0.000001, validation loss 0.035664\n",
      "Iter 8247, training loss 0.000001, validation loss 0.035664\n",
      "Iter 8248, training loss 0.000001, validation loss 0.035663\n",
      "Iter 8249, training loss 0.000001, validation loss 0.035664\n",
      "Iter 8250, training loss 0.000001, validation loss 0.035663\n",
      "Iter 8251, training loss 0.000001, validation loss 0.035664\n",
      "Iter 8252, training loss 0.000001, validation loss 0.035662\n",
      "Iter 8253, training loss 0.000001, validation loss 0.035664\n",
      "Iter 8254, training loss 0.000001, validation loss 0.035661\n",
      "Iter 8255, training loss 0.000001, validation loss 0.035664\n",
      "Iter 8256, training loss 0.000001, validation loss 0.035660\n",
      "Iter 8257, training loss 0.000001, validation loss 0.035664\n",
      "Iter 8258, training loss 0.000001, validation loss 0.035659\n",
      "Iter 8259, training loss 0.000001, validation loss 0.035665\n",
      "Iter 8260, training loss 0.000001, validation loss 0.035657\n",
      "Iter 8261, training loss 0.000001, validation loss 0.035667\n",
      "Iter 8262, training loss 0.000001, validation loss 0.035653\n",
      "Iter 8263, training loss 0.000001, validation loss 0.035671\n",
      "Iter 8264, training loss 0.000001, validation loss 0.035648\n",
      "Iter 8265, training loss 0.000002, validation loss 0.035677\n",
      "Iter 8266, training loss 0.000002, validation loss 0.035641\n",
      "Iter 8267, training loss 0.000002, validation loss 0.035687\n",
      "Iter 8268, training loss 0.000003, validation loss 0.035629\n",
      "Iter 8269, training loss 0.000005, validation loss 0.035707\n",
      "Iter 8270, training loss 0.000007, validation loss 0.035610\n",
      "Iter 8271, training loss 0.000011, validation loss 0.035743\n",
      "Iter 8272, training loss 0.000018, validation loss 0.035584\n",
      "Iter 8273, training loss 0.000031, validation loss 0.035814\n",
      "Iter 8274, training loss 0.000052, validation loss 0.035552\n",
      "Iter 8275, training loss 0.000088, validation loss 0.035965\n",
      "Iter 8276, training loss 0.000152, validation loss 0.035544\n",
      "Iter 8277, training loss 0.000265, validation loss 0.036314\n",
      "Iter 8278, training loss 0.000464, validation loss 0.035673\n",
      "Iter 8279, training loss 0.000817, validation loss 0.037196\n",
      "Iter 8280, training loss 0.001444, validation loss 0.036367\n",
      "Iter 8281, training loss 0.002561, validation loss 0.039595\n",
      "Iter 8282, training loss 0.004545, validation loss 0.039074\n",
      "Iter 8283, training loss 0.008048, validation loss 0.046459\n",
      "Iter 8284, training loss 0.014116, validation loss 0.048325\n",
      "Iter 8285, training loss 0.024275, validation loss 0.065669\n",
      "Iter 8286, training loss 0.040124, validation loss 0.074801\n",
      "Iter 8287, training loss 0.061648, validation loss 0.108604\n",
      "Iter 8288, training loss 0.083190, validation loss 0.119651\n",
      "Iter 8289, training loss 0.089333, validation loss 0.139756\n",
      "Iter 8290, training loss 0.063957, validation loss 0.098665\n",
      "Iter 8291, training loss 0.019065, validation loss 0.058134\n",
      "Iter 8292, training loss 0.000868, validation loss 0.035016\n",
      "Iter 8293, training loss 0.022276, validation loss 0.054091\n",
      "Iter 8294, training loss 0.031414, validation loss 0.071682\n",
      "Iter 8295, training loss 0.008262, validation loss 0.039494\n",
      "Iter 8296, training loss 0.003518, validation loss 0.034875\n",
      "Iter 8297, training loss 0.019149, validation loss 0.056478\n",
      "Iter 8298, training loss 0.009970, validation loss 0.040527\n",
      "Iter 8299, training loss 0.001230, validation loss 0.032528\n",
      "Iter 8300, training loss 0.012352, validation loss 0.047627\n",
      "Iter 8301, training loss 0.006926, validation loss 0.037344\n",
      "Iter 8302, training loss 0.001264, validation loss 0.032188\n",
      "Iter 8303, training loss 0.009165, validation loss 0.043225\n",
      "Iter 8304, training loss 0.003532, validation loss 0.033971\n",
      "Iter 8305, training loss 0.002013, validation loss 0.032563\n",
      "Iter 8306, training loss 0.006754, validation loss 0.039916\n",
      "Iter 8307, training loss 0.001174, validation loss 0.031778\n",
      "Iter 8308, training loss 0.003040, validation loss 0.033382\n",
      "Iter 8309, training loss 0.004165, validation loss 0.036485\n",
      "Iter 8310, training loss 0.000406, validation loss 0.031289\n",
      "Iter 8311, training loss 0.003487, validation loss 0.033755\n",
      "Iter 8312, training loss 0.001799, validation loss 0.033388\n",
      "Iter 8313, training loss 0.000902, validation loss 0.032144\n",
      "Iter 8314, training loss 0.002852, validation loss 0.033106\n",
      "Iter 8315, training loss 0.000472, validation loss 0.031452\n",
      "Iter 8316, training loss 0.001622, validation loss 0.033081\n",
      "Iter 8317, training loss 0.001569, validation loss 0.031828\n",
      "Iter 8318, training loss 0.000340, validation loss 0.030867\n",
      "Iter 8319, training loss 0.001708, validation loss 0.033106\n",
      "Iter 8320, training loss 0.000529, validation loss 0.030886\n",
      "Iter 8321, training loss 0.000760, validation loss 0.031024\n",
      "Iter 8322, training loss 0.001144, validation loss 0.032255\n",
      "Iter 8323, training loss 0.000204, validation loss 0.030729\n",
      "Iter 8324, training loss 0.000981, validation loss 0.031134\n",
      "Iter 8325, training loss 0.000486, validation loss 0.031233\n",
      "Iter 8326, training loss 0.000373, validation loss 0.031035\n",
      "Iter 8327, training loss 0.000778, validation loss 0.030911\n",
      "Iter 8328, training loss 0.000173, validation loss 0.030627\n",
      "Iter 8329, training loss 0.000555, validation loss 0.031277\n",
      "Iter 8330, training loss 0.000405, validation loss 0.030580\n",
      "Iter 8331, training loss 0.000200, validation loss 0.030461\n",
      "Iter 8332, training loss 0.000512, validation loss 0.031186\n",
      "Iter 8333, training loss 0.000164, validation loss 0.030427\n",
      "Iter 8334, training loss 0.000313, validation loss 0.030471\n",
      "Iter 8335, training loss 0.000320, validation loss 0.030870\n",
      "Iter 8336, training loss 0.000125, validation loss 0.030486\n",
      "Iter 8337, training loss 0.000329, validation loss 0.030452\n",
      "Iter 8338, training loss 0.000155, validation loss 0.030560\n",
      "Iter 8339, training loss 0.000180, validation loss 0.030608\n",
      "Iter 8340, training loss 0.000244, validation loss 0.030386\n",
      "Iter 8341, training loss 0.000096, validation loss 0.030390\n",
      "Iter 8342, training loss 0.000209, validation loss 0.030663\n",
      "Iter 8343, training loss 0.000142, validation loss 0.030340\n",
      "Iter 8344, training loss 0.000111, validation loss 0.030337\n",
      "Iter 8345, training loss 0.000180, validation loss 0.030615\n",
      "Iter 8346, training loss 0.000086, validation loss 0.030343\n",
      "Iter 8347, training loss 0.000133, validation loss 0.030320\n",
      "Iter 8348, training loss 0.000125, validation loss 0.030505\n",
      "Iter 8349, training loss 0.000078, validation loss 0.030382\n",
      "Iter 8350, training loss 0.000129, validation loss 0.030307\n",
      "Iter 8351, training loss 0.000082, validation loss 0.030404\n",
      "Iter 8352, training loss 0.000088, validation loss 0.030423\n",
      "Iter 8353, training loss 0.000104, validation loss 0.030293\n",
      "Iter 8354, training loss 0.000065, validation loss 0.030334\n",
      "Iter 8355, training loss 0.000092, validation loss 0.030435\n",
      "Iter 8356, training loss 0.000076, validation loss 0.030289\n",
      "Iter 8357, training loss 0.000064, validation loss 0.030299\n",
      "Iter 8358, training loss 0.000083, validation loss 0.030418\n",
      "Iter 8359, training loss 0.000059, validation loss 0.030301\n",
      "Iter 8360, training loss 0.000067, validation loss 0.030285\n",
      "Iter 8361, training loss 0.000068, validation loss 0.030387\n",
      "Iter 8362, training loss 0.000053, validation loss 0.030323\n",
      "Iter 8363, training loss 0.000065, validation loss 0.030281\n",
      "Iter 8364, training loss 0.000055, validation loss 0.030352\n",
      "Iter 8365, training loss 0.000052, validation loss 0.030341\n",
      "Iter 8366, training loss 0.000059, validation loss 0.030279\n",
      "Iter 8367, training loss 0.000047, validation loss 0.030322\n",
      "Iter 8368, training loss 0.000052, validation loss 0.030349\n",
      "Iter 8369, training loss 0.000051, validation loss 0.030281\n",
      "Iter 8370, training loss 0.000044, validation loss 0.030302\n",
      "Iter 8371, training loss 0.000049, validation loss 0.030349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8372, training loss 0.000044, validation loss 0.030290\n",
      "Iter 8373, training loss 0.000043, validation loss 0.030292\n",
      "Iter 8374, training loss 0.000045, validation loss 0.030342\n",
      "Iter 8375, training loss 0.000040, validation loss 0.030300\n",
      "Iter 8376, training loss 0.000041, validation loss 0.030287\n",
      "Iter 8377, training loss 0.000041, validation loss 0.030331\n",
      "Iter 8378, training loss 0.000038, validation loss 0.030310\n",
      "Iter 8379, training loss 0.000039, validation loss 0.030288\n",
      "Iter 8380, training loss 0.000037, validation loss 0.030322\n",
      "Iter 8381, training loss 0.000036, validation loss 0.030318\n",
      "Iter 8382, training loss 0.000037, validation loss 0.030290\n",
      "Iter 8383, training loss 0.000034, validation loss 0.030313\n",
      "Iter 8384, training loss 0.000035, validation loss 0.030323\n",
      "Iter 8385, training loss 0.000034, validation loss 0.030294\n",
      "Iter 8386, training loss 0.000032, validation loss 0.030307\n",
      "Iter 8387, training loss 0.000033, validation loss 0.030323\n",
      "Iter 8388, training loss 0.000032, validation loss 0.030298\n",
      "Iter 8389, training loss 0.000031, validation loss 0.030302\n",
      "Iter 8390, training loss 0.000031, validation loss 0.030322\n",
      "Iter 8391, training loss 0.000030, validation loss 0.030303\n",
      "Iter 8392, training loss 0.000030, validation loss 0.030300\n",
      "Iter 8393, training loss 0.000029, validation loss 0.030319\n",
      "Iter 8394, training loss 0.000028, validation loss 0.030306\n",
      "Iter 8395, training loss 0.000028, validation loss 0.030298\n",
      "Iter 8396, training loss 0.000028, validation loss 0.030314\n",
      "Iter 8397, training loss 0.000027, validation loss 0.030308\n",
      "Iter 8398, training loss 0.000027, validation loss 0.030297\n",
      "Iter 8399, training loss 0.000026, validation loss 0.030310\n",
      "Iter 8400, training loss 0.000026, validation loss 0.030310\n",
      "Iter 8401, training loss 0.000026, validation loss 0.030298\n",
      "Iter 8402, training loss 0.000025, validation loss 0.030306\n",
      "Iter 8403, training loss 0.000025, validation loss 0.030308\n",
      "Iter 8404, training loss 0.000025, validation loss 0.030296\n",
      "Iter 8405, training loss 0.000024, validation loss 0.030301\n",
      "Iter 8406, training loss 0.000024, validation loss 0.030306\n",
      "Iter 8407, training loss 0.000023, validation loss 0.030296\n",
      "Iter 8408, training loss 0.000023, validation loss 0.030298\n",
      "Iter 8409, training loss 0.000023, validation loss 0.030304\n",
      "Iter 8410, training loss 0.000022, validation loss 0.030295\n",
      "Iter 8411, training loss 0.000022, validation loss 0.030295\n",
      "Iter 8412, training loss 0.000022, validation loss 0.030301\n",
      "Iter 8413, training loss 0.000021, validation loss 0.030295\n",
      "Iter 8414, training loss 0.000021, validation loss 0.030292\n",
      "Iter 8415, training loss 0.000021, validation loss 0.030298\n",
      "Iter 8416, training loss 0.000021, validation loss 0.030294\n",
      "Iter 8417, training loss 0.000020, validation loss 0.030290\n",
      "Iter 8418, training loss 0.000020, validation loss 0.030295\n",
      "Iter 8419, training loss 0.000020, validation loss 0.030292\n",
      "Iter 8420, training loss 0.000020, validation loss 0.030288\n",
      "Iter 8421, training loss 0.000019, validation loss 0.030292\n",
      "Iter 8422, training loss 0.000019, validation loss 0.030291\n",
      "Iter 8423, training loss 0.000019, validation loss 0.030287\n",
      "Iter 8424, training loss 0.000019, validation loss 0.030290\n",
      "Iter 8425, training loss 0.000018, validation loss 0.030289\n",
      "Iter 8426, training loss 0.000018, validation loss 0.030285\n",
      "Iter 8427, training loss 0.000018, validation loss 0.030287\n",
      "Iter 8428, training loss 0.000018, validation loss 0.030287\n",
      "Iter 8429, training loss 0.000017, validation loss 0.030284\n",
      "Iter 8430, training loss 0.000017, validation loss 0.030285\n",
      "Iter 8431, training loss 0.000017, validation loss 0.030286\n",
      "Iter 8432, training loss 0.000017, validation loss 0.030283\n",
      "Iter 8433, training loss 0.000017, validation loss 0.030283\n",
      "Iter 8434, training loss 0.000016, validation loss 0.030285\n",
      "Iter 8435, training loss 0.000016, validation loss 0.030282\n",
      "Iter 8436, training loss 0.000016, validation loss 0.030282\n",
      "Iter 8437, training loss 0.000016, validation loss 0.030283\n",
      "Iter 8438, training loss 0.000016, validation loss 0.030281\n",
      "Iter 8439, training loss 0.000015, validation loss 0.030281\n",
      "Iter 8440, training loss 0.000015, validation loss 0.030282\n",
      "Iter 8441, training loss 0.000015, validation loss 0.030281\n",
      "Iter 8442, training loss 0.000015, validation loss 0.030280\n",
      "Iter 8443, training loss 0.000015, validation loss 0.030281\n",
      "Iter 8444, training loss 0.000015, validation loss 0.030280\n",
      "Iter 8445, training loss 0.000014, validation loss 0.030279\n",
      "Iter 8446, training loss 0.000014, validation loss 0.030280\n",
      "Iter 8447, training loss 0.000014, validation loss 0.030279\n",
      "Iter 8448, training loss 0.000014, validation loss 0.030278\n",
      "Iter 8449, training loss 0.000014, validation loss 0.030278\n",
      "Iter 8450, training loss 0.000014, validation loss 0.030278\n",
      "Iter 8451, training loss 0.000013, validation loss 0.030277\n",
      "Iter 8452, training loss 0.000013, validation loss 0.030277\n",
      "Iter 8453, training loss 0.000013, validation loss 0.030277\n",
      "Iter 8454, training loss 0.000013, validation loss 0.030276\n",
      "Iter 8455, training loss 0.000013, validation loss 0.030276\n",
      "Iter 8456, training loss 0.000013, validation loss 0.030276\n",
      "Iter 8457, training loss 0.000013, validation loss 0.030274\n",
      "Iter 8458, training loss 0.000012, validation loss 0.030275\n",
      "Iter 8459, training loss 0.000012, validation loss 0.030274\n",
      "Iter 8460, training loss 0.000012, validation loss 0.030273\n",
      "Iter 8461, training loss 0.000012, validation loss 0.030273\n",
      "Iter 8462, training loss 0.000012, validation loss 0.030273\n",
      "Iter 8463, training loss 0.000012, validation loss 0.030272\n",
      "Iter 8464, training loss 0.000012, validation loss 0.030272\n",
      "Iter 8465, training loss 0.000012, validation loss 0.030272\n",
      "Iter 8466, training loss 0.000012, validation loss 0.030271\n",
      "Iter 8467, training loss 0.000011, validation loss 0.030271\n",
      "Iter 8468, training loss 0.000011, validation loss 0.030270\n",
      "Iter 8469, training loss 0.000011, validation loss 0.030270\n",
      "Iter 8470, training loss 0.000011, validation loss 0.030269\n",
      "Iter 8471, training loss 0.000011, validation loss 0.030269\n",
      "Iter 8472, training loss 0.000011, validation loss 0.030268\n",
      "Iter 8473, training loss 0.000011, validation loss 0.030268\n",
      "Iter 8474, training loss 0.000011, validation loss 0.030268\n",
      "Iter 8475, training loss 0.000011, validation loss 0.030268\n",
      "Iter 8476, training loss 0.000010, validation loss 0.030267\n",
      "Iter 8477, training loss 0.000010, validation loss 0.030267\n",
      "Iter 8478, training loss 0.000010, validation loss 0.030267\n",
      "Iter 8479, training loss 0.000010, validation loss 0.030266\n",
      "Iter 8480, training loss 0.000010, validation loss 0.030266\n",
      "Iter 8481, training loss 0.000010, validation loss 0.030266\n",
      "Iter 8482, training loss 0.000010, validation loss 0.030265\n",
      "Iter 8483, training loss 0.000010, validation loss 0.030265\n",
      "Iter 8484, training loss 0.000010, validation loss 0.030264\n",
      "Iter 8485, training loss 0.000010, validation loss 0.030264\n",
      "Iter 8486, training loss 0.000009, validation loss 0.030264\n",
      "Iter 8487, training loss 0.000009, validation loss 0.030263\n",
      "Iter 8488, training loss 0.000009, validation loss 0.030263\n",
      "Iter 8489, training loss 0.000009, validation loss 0.030263\n",
      "Iter 8490, training loss 0.000009, validation loss 0.030262\n",
      "Iter 8491, training loss 0.000009, validation loss 0.030262\n",
      "Iter 8492, training loss 0.000009, validation loss 0.030262\n",
      "Iter 8493, training loss 0.000009, validation loss 0.030261\n",
      "Iter 8494, training loss 0.000009, validation loss 0.030261\n",
      "Iter 8495, training loss 0.000009, validation loss 0.030261\n",
      "Iter 8496, training loss 0.000009, validation loss 0.030260\n",
      "Iter 8497, training loss 0.000009, validation loss 0.030260\n",
      "Iter 8498, training loss 0.000009, validation loss 0.030259\n",
      "Iter 8499, training loss 0.000008, validation loss 0.030259\n",
      "Iter 8500, training loss 0.000008, validation loss 0.030259\n",
      "Iter 8501, training loss 0.000008, validation loss 0.030258\n",
      "Iter 8502, training loss 0.000008, validation loss 0.030258\n",
      "Iter 8503, training loss 0.000008, validation loss 0.030257\n",
      "Iter 8504, training loss 0.000008, validation loss 0.030257\n",
      "Iter 8505, training loss 0.000008, validation loss 0.030257\n",
      "Iter 8506, training loss 0.000008, validation loss 0.030256\n",
      "Iter 8507, training loss 0.000008, validation loss 0.030256\n",
      "Iter 8508, training loss 0.000008, validation loss 0.030256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8509, training loss 0.000008, validation loss 0.030256\n",
      "Iter 8510, training loss 0.000008, validation loss 0.030255\n",
      "Iter 8511, training loss 0.000008, validation loss 0.030255\n",
      "Iter 8512, training loss 0.000008, validation loss 0.030255\n",
      "Iter 8513, training loss 0.000008, validation loss 0.030255\n",
      "Iter 8514, training loss 0.000007, validation loss 0.030254\n",
      "Iter 8515, training loss 0.000007, validation loss 0.030254\n",
      "Iter 8516, training loss 0.000007, validation loss 0.030254\n",
      "Iter 8517, training loss 0.000007, validation loss 0.030253\n",
      "Iter 8518, training loss 0.000007, validation loss 0.030253\n",
      "Iter 8519, training loss 0.000007, validation loss 0.030253\n",
      "Iter 8520, training loss 0.000007, validation loss 0.030253\n",
      "Iter 8521, training loss 0.000007, validation loss 0.030253\n",
      "Iter 8522, training loss 0.000007, validation loss 0.030252\n",
      "Iter 8523, training loss 0.000007, validation loss 0.030252\n",
      "Iter 8524, training loss 0.000007, validation loss 0.030252\n",
      "Iter 8525, training loss 0.000007, validation loss 0.030251\n",
      "Iter 8526, training loss 0.000007, validation loss 0.030251\n",
      "Iter 8527, training loss 0.000007, validation loss 0.030251\n",
      "Iter 8528, training loss 0.000007, validation loss 0.030251\n",
      "Iter 8529, training loss 0.000007, validation loss 0.030250\n",
      "Iter 8530, training loss 0.000007, validation loss 0.030250\n",
      "Iter 8531, training loss 0.000007, validation loss 0.030250\n",
      "Iter 8532, training loss 0.000006, validation loss 0.030250\n",
      "Iter 8533, training loss 0.000006, validation loss 0.030249\n",
      "Iter 8534, training loss 0.000006, validation loss 0.030249\n",
      "Iter 8535, training loss 0.000006, validation loss 0.030249\n",
      "Iter 8536, training loss 0.000006, validation loss 0.030249\n",
      "Iter 8537, training loss 0.000006, validation loss 0.030248\n",
      "Iter 8538, training loss 0.000006, validation loss 0.030248\n",
      "Iter 8539, training loss 0.000006, validation loss 0.030248\n",
      "Iter 8540, training loss 0.000006, validation loss 0.030248\n",
      "Iter 8541, training loss 0.000006, validation loss 0.030247\n",
      "Iter 8542, training loss 0.000006, validation loss 0.030247\n",
      "Iter 8543, training loss 0.000006, validation loss 0.030247\n",
      "Iter 8544, training loss 0.000006, validation loss 0.030247\n",
      "Iter 8545, training loss 0.000006, validation loss 0.030247\n",
      "Iter 8546, training loss 0.000006, validation loss 0.030246\n",
      "Iter 8547, training loss 0.000006, validation loss 0.030246\n",
      "Iter 8548, training loss 0.000006, validation loss 0.030246\n",
      "Iter 8549, training loss 0.000006, validation loss 0.030246\n",
      "Iter 8550, training loss 0.000006, validation loss 0.030245\n",
      "Iter 8551, training loss 0.000006, validation loss 0.030245\n",
      "Iter 8552, training loss 0.000006, validation loss 0.030245\n",
      "Iter 8553, training loss 0.000006, validation loss 0.030245\n",
      "Iter 8554, training loss 0.000005, validation loss 0.030244\n",
      "Iter 8555, training loss 0.000005, validation loss 0.030244\n",
      "Iter 8556, training loss 0.000005, validation loss 0.030244\n",
      "Iter 8557, training loss 0.000005, validation loss 0.030244\n",
      "Iter 8558, training loss 0.000005, validation loss 0.030243\n",
      "Iter 8559, training loss 0.000005, validation loss 0.030243\n",
      "Iter 8560, training loss 0.000005, validation loss 0.030243\n",
      "Iter 8561, training loss 0.000005, validation loss 0.030242\n",
      "Iter 8562, training loss 0.000005, validation loss 0.030242\n",
      "Iter 8563, training loss 0.000005, validation loss 0.030242\n",
      "Iter 8564, training loss 0.000005, validation loss 0.030242\n",
      "Iter 8565, training loss 0.000005, validation loss 0.030241\n",
      "Iter 8566, training loss 0.000005, validation loss 0.030241\n",
      "Iter 8567, training loss 0.000005, validation loss 0.030241\n",
      "Iter 8568, training loss 0.000005, validation loss 0.030240\n",
      "Iter 8569, training loss 0.000005, validation loss 0.030240\n",
      "Iter 8570, training loss 0.000005, validation loss 0.030240\n",
      "Iter 8571, training loss 0.000005, validation loss 0.030239\n",
      "Iter 8572, training loss 0.000005, validation loss 0.030239\n",
      "Iter 8573, training loss 0.000005, validation loss 0.030239\n",
      "Iter 8574, training loss 0.000005, validation loss 0.030239\n",
      "Iter 8575, training loss 0.000005, validation loss 0.030238\n",
      "Iter 8576, training loss 0.000005, validation loss 0.030238\n",
      "Iter 8577, training loss 0.000005, validation loss 0.030238\n",
      "Iter 8578, training loss 0.000005, validation loss 0.030237\n",
      "Iter 8579, training loss 0.000005, validation loss 0.030237\n",
      "Iter 8580, training loss 0.000005, validation loss 0.030237\n",
      "Iter 8581, training loss 0.000005, validation loss 0.030236\n",
      "Iter 8582, training loss 0.000005, validation loss 0.030236\n",
      "Iter 8583, training loss 0.000004, validation loss 0.030236\n",
      "Iter 8584, training loss 0.000004, validation loss 0.030235\n",
      "Iter 8585, training loss 0.000004, validation loss 0.030235\n",
      "Iter 8586, training loss 0.000004, validation loss 0.030235\n",
      "Iter 8587, training loss 0.000004, validation loss 0.030235\n",
      "Iter 8588, training loss 0.000004, validation loss 0.030234\n",
      "Iter 8589, training loss 0.000004, validation loss 0.030234\n",
      "Iter 8590, training loss 0.000004, validation loss 0.030233\n",
      "Iter 8591, training loss 0.000004, validation loss 0.030233\n",
      "Iter 8592, training loss 0.000004, validation loss 0.030233\n",
      "Iter 8593, training loss 0.000004, validation loss 0.030232\n",
      "Iter 8594, training loss 0.000004, validation loss 0.030232\n",
      "Iter 8595, training loss 0.000004, validation loss 0.030232\n",
      "Iter 8596, training loss 0.000004, validation loss 0.030231\n",
      "Iter 8597, training loss 0.000004, validation loss 0.030231\n",
      "Iter 8598, training loss 0.000004, validation loss 0.030231\n",
      "Iter 8599, training loss 0.000004, validation loss 0.030230\n",
      "Iter 8600, training loss 0.000004, validation loss 0.030230\n",
      "Iter 8601, training loss 0.000004, validation loss 0.030230\n",
      "Iter 8602, training loss 0.000004, validation loss 0.030229\n",
      "Iter 8603, training loss 0.000004, validation loss 0.030229\n",
      "Iter 8604, training loss 0.000004, validation loss 0.030229\n",
      "Iter 8605, training loss 0.000004, validation loss 0.030228\n",
      "Iter 8606, training loss 0.000004, validation loss 0.030228\n",
      "Iter 8607, training loss 0.000004, validation loss 0.030227\n",
      "Iter 8608, training loss 0.000004, validation loss 0.030227\n",
      "Iter 8609, training loss 0.000004, validation loss 0.030226\n",
      "Iter 8610, training loss 0.000004, validation loss 0.030226\n",
      "Iter 8611, training loss 0.000004, validation loss 0.030226\n",
      "Iter 8612, training loss 0.000004, validation loss 0.030225\n",
      "Iter 8613, training loss 0.000004, validation loss 0.030225\n",
      "Iter 8614, training loss 0.000004, validation loss 0.030225\n",
      "Iter 8615, training loss 0.000004, validation loss 0.030224\n",
      "Iter 8616, training loss 0.000004, validation loss 0.030224\n",
      "Iter 8617, training loss 0.000004, validation loss 0.030224\n",
      "Iter 8618, training loss 0.000004, validation loss 0.030223\n",
      "Iter 8619, training loss 0.000004, validation loss 0.030223\n",
      "Iter 8620, training loss 0.000004, validation loss 0.030223\n",
      "Iter 8621, training loss 0.000004, validation loss 0.030222\n",
      "Iter 8622, training loss 0.000004, validation loss 0.030222\n",
      "Iter 8623, training loss 0.000003, validation loss 0.030222\n",
      "Iter 8624, training loss 0.000003, validation loss 0.030221\n",
      "Iter 8625, training loss 0.000003, validation loss 0.030221\n",
      "Iter 8626, training loss 0.000003, validation loss 0.030221\n",
      "Iter 8627, training loss 0.000003, validation loss 0.030220\n",
      "Iter 8628, training loss 0.000003, validation loss 0.030220\n",
      "Iter 8629, training loss 0.000003, validation loss 0.030220\n",
      "Iter 8630, training loss 0.000003, validation loss 0.030219\n",
      "Iter 8631, training loss 0.000003, validation loss 0.030219\n",
      "Iter 8632, training loss 0.000003, validation loss 0.030219\n",
      "Iter 8633, training loss 0.000003, validation loss 0.030218\n",
      "Iter 8634, training loss 0.000003, validation loss 0.030218\n",
      "Iter 8635, training loss 0.000003, validation loss 0.030218\n",
      "Iter 8636, training loss 0.000003, validation loss 0.030218\n",
      "Iter 8637, training loss 0.000003, validation loss 0.030217\n",
      "Iter 8638, training loss 0.000003, validation loss 0.030217\n",
      "Iter 8639, training loss 0.000003, validation loss 0.030217\n",
      "Iter 8640, training loss 0.000003, validation loss 0.030216\n",
      "Iter 8641, training loss 0.000003, validation loss 0.030216\n",
      "Iter 8642, training loss 0.000003, validation loss 0.030216\n",
      "Iter 8643, training loss 0.000003, validation loss 0.030216\n",
      "Iter 8644, training loss 0.000003, validation loss 0.030215\n",
      "Iter 8645, training loss 0.000003, validation loss 0.030215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8646, training loss 0.000003, validation loss 0.030215\n",
      "Iter 8647, training loss 0.000003, validation loss 0.030215\n",
      "Iter 8648, training loss 0.000003, validation loss 0.030214\n",
      "Iter 8649, training loss 0.000003, validation loss 0.030214\n",
      "Iter 8650, training loss 0.000003, validation loss 0.030214\n",
      "Iter 8651, training loss 0.000003, validation loss 0.030214\n",
      "Iter 8652, training loss 0.000003, validation loss 0.030213\n",
      "Iter 8653, training loss 0.000003, validation loss 0.030213\n",
      "Iter 8654, training loss 0.000003, validation loss 0.030213\n",
      "Iter 8655, training loss 0.000003, validation loss 0.030213\n",
      "Iter 8656, training loss 0.000003, validation loss 0.030212\n",
      "Iter 8657, training loss 0.000003, validation loss 0.030212\n",
      "Iter 8658, training loss 0.000003, validation loss 0.030212\n",
      "Iter 8659, training loss 0.000003, validation loss 0.030212\n",
      "Iter 8660, training loss 0.000003, validation loss 0.030211\n",
      "Iter 8661, training loss 0.000003, validation loss 0.030211\n",
      "Iter 8662, training loss 0.000003, validation loss 0.030211\n",
      "Iter 8663, training loss 0.000003, validation loss 0.030211\n",
      "Iter 8664, training loss 0.000003, validation loss 0.030210\n",
      "Iter 8665, training loss 0.000003, validation loss 0.030210\n",
      "Iter 8666, training loss 0.000003, validation loss 0.030210\n",
      "Iter 8667, training loss 0.000003, validation loss 0.030210\n",
      "Iter 8668, training loss 0.000003, validation loss 0.030209\n",
      "Iter 8669, training loss 0.000003, validation loss 0.030209\n",
      "Iter 8670, training loss 0.000003, validation loss 0.030209\n",
      "Iter 8671, training loss 0.000003, validation loss 0.030209\n",
      "Iter 8672, training loss 0.000003, validation loss 0.030208\n",
      "Iter 8673, training loss 0.000003, validation loss 0.030208\n",
      "Iter 8674, training loss 0.000003, validation loss 0.030208\n",
      "Iter 8675, training loss 0.000003, validation loss 0.030208\n",
      "Iter 8676, training loss 0.000003, validation loss 0.030208\n",
      "Iter 8677, training loss 0.000003, validation loss 0.030207\n",
      "Iter 8678, training loss 0.000003, validation loss 0.030207\n",
      "Iter 8679, training loss 0.000003, validation loss 0.030207\n",
      "Iter 8680, training loss 0.000003, validation loss 0.030207\n",
      "Iter 8681, training loss 0.000003, validation loss 0.030207\n",
      "Iter 8682, training loss 0.000003, validation loss 0.030207\n",
      "Iter 8683, training loss 0.000002, validation loss 0.030206\n",
      "Iter 8684, training loss 0.000002, validation loss 0.030206\n",
      "Iter 8685, training loss 0.000002, validation loss 0.030206\n",
      "Iter 8686, training loss 0.000002, validation loss 0.030206\n",
      "Iter 8687, training loss 0.000002, validation loss 0.030205\n",
      "Iter 8688, training loss 0.000002, validation loss 0.030205\n",
      "Iter 8689, training loss 0.000002, validation loss 0.030205\n",
      "Iter 8690, training loss 0.000002, validation loss 0.030205\n",
      "Iter 8691, training loss 0.000002, validation loss 0.030205\n",
      "Iter 8692, training loss 0.000002, validation loss 0.030204\n",
      "Iter 8693, training loss 0.000002, validation loss 0.030204\n",
      "Iter 8694, training loss 0.000002, validation loss 0.030204\n",
      "Iter 8695, training loss 0.000002, validation loss 0.030204\n",
      "Iter 8696, training loss 0.000002, validation loss 0.030204\n",
      "Iter 8697, training loss 0.000002, validation loss 0.030203\n",
      "Iter 8698, training loss 0.000002, validation loss 0.030203\n",
      "Iter 8699, training loss 0.000002, validation loss 0.030203\n",
      "Iter 8700, training loss 0.000002, validation loss 0.030203\n",
      "Iter 8701, training loss 0.000002, validation loss 0.030202\n",
      "Iter 8702, training loss 0.000002, validation loss 0.030202\n",
      "Iter 8703, training loss 0.000002, validation loss 0.030202\n",
      "Iter 8704, training loss 0.000002, validation loss 0.030202\n",
      "Iter 8705, training loss 0.000002, validation loss 0.030201\n",
      "Iter 8706, training loss 0.000002, validation loss 0.030201\n",
      "Iter 8707, training loss 0.000002, validation loss 0.030201\n",
      "Iter 8708, training loss 0.000002, validation loss 0.030201\n",
      "Iter 8709, training loss 0.000002, validation loss 0.030201\n",
      "Iter 8710, training loss 0.000002, validation loss 0.030200\n",
      "Iter 8711, training loss 0.000002, validation loss 0.030200\n",
      "Iter 8712, training loss 0.000002, validation loss 0.030200\n",
      "Iter 8713, training loss 0.000002, validation loss 0.030200\n",
      "Iter 8714, training loss 0.000002, validation loss 0.030199\n",
      "Iter 8715, training loss 0.000002, validation loss 0.030199\n",
      "Iter 8716, training loss 0.000002, validation loss 0.030199\n",
      "Iter 8717, training loss 0.000002, validation loss 0.030199\n",
      "Iter 8718, training loss 0.000002, validation loss 0.030198\n",
      "Iter 8719, training loss 0.000002, validation loss 0.030198\n",
      "Iter 8720, training loss 0.000002, validation loss 0.030198\n",
      "Iter 8721, training loss 0.000002, validation loss 0.030198\n",
      "Iter 8722, training loss 0.000002, validation loss 0.030197\n",
      "Iter 8723, training loss 0.000002, validation loss 0.030197\n",
      "Iter 8724, training loss 0.000002, validation loss 0.030197\n",
      "Iter 8725, training loss 0.000002, validation loss 0.030197\n",
      "Iter 8726, training loss 0.000002, validation loss 0.030196\n",
      "Iter 8727, training loss 0.000002, validation loss 0.030196\n",
      "Iter 8728, training loss 0.000002, validation loss 0.030196\n",
      "Iter 8729, training loss 0.000002, validation loss 0.030196\n",
      "Iter 8730, training loss 0.000002, validation loss 0.030196\n",
      "Iter 8731, training loss 0.000002, validation loss 0.030195\n",
      "Iter 8732, training loss 0.000002, validation loss 0.030195\n",
      "Iter 8733, training loss 0.000002, validation loss 0.030195\n",
      "Iter 8734, training loss 0.000002, validation loss 0.030195\n",
      "Iter 8735, training loss 0.000002, validation loss 0.030194\n",
      "Iter 8736, training loss 0.000002, validation loss 0.030194\n",
      "Iter 8737, training loss 0.000002, validation loss 0.030194\n",
      "Iter 8738, training loss 0.000002, validation loss 0.030194\n",
      "Iter 8739, training loss 0.000002, validation loss 0.030194\n",
      "Iter 8740, training loss 0.000002, validation loss 0.030193\n",
      "Iter 8741, training loss 0.000002, validation loss 0.030193\n",
      "Iter 8742, training loss 0.000002, validation loss 0.030193\n",
      "Iter 8743, training loss 0.000002, validation loss 0.030193\n",
      "Iter 8744, training loss 0.000002, validation loss 0.030193\n",
      "Iter 8745, training loss 0.000002, validation loss 0.030192\n",
      "Iter 8746, training loss 0.000002, validation loss 0.030192\n",
      "Iter 8747, training loss 0.000002, validation loss 0.030192\n",
      "Iter 8748, training loss 0.000002, validation loss 0.030192\n",
      "Iter 8749, training loss 0.000002, validation loss 0.030192\n",
      "Iter 8750, training loss 0.000002, validation loss 0.030191\n",
      "Iter 8751, training loss 0.000002, validation loss 0.030191\n",
      "Iter 8752, training loss 0.000002, validation loss 0.030191\n",
      "Iter 8753, training loss 0.000002, validation loss 0.030191\n",
      "Iter 8754, training loss 0.000002, validation loss 0.030191\n",
      "Iter 8755, training loss 0.000002, validation loss 0.030191\n",
      "Iter 8756, training loss 0.000002, validation loss 0.030190\n",
      "Iter 8757, training loss 0.000002, validation loss 0.030190\n",
      "Iter 8758, training loss 0.000002, validation loss 0.030190\n",
      "Iter 8759, training loss 0.000002, validation loss 0.030190\n",
      "Iter 8760, training loss 0.000002, validation loss 0.030190\n",
      "Iter 8761, training loss 0.000002, validation loss 0.030190\n",
      "Iter 8762, training loss 0.000002, validation loss 0.030189\n",
      "Iter 8763, training loss 0.000002, validation loss 0.030189\n",
      "Iter 8764, training loss 0.000002, validation loss 0.030189\n",
      "Iter 8765, training loss 0.000002, validation loss 0.030189\n",
      "Iter 8766, training loss 0.000002, validation loss 0.030189\n",
      "Iter 8767, training loss 0.000002, validation loss 0.030188\n",
      "Iter 8768, training loss 0.000002, validation loss 0.030188\n",
      "Iter 8769, training loss 0.000002, validation loss 0.030188\n",
      "Iter 8770, training loss 0.000002, validation loss 0.030188\n",
      "Iter 8771, training loss 0.000002, validation loss 0.030188\n",
      "Iter 8772, training loss 0.000002, validation loss 0.030187\n",
      "Iter 8773, training loss 0.000002, validation loss 0.030187\n",
      "Iter 8774, training loss 0.000002, validation loss 0.030187\n",
      "Iter 8775, training loss 0.000002, validation loss 0.030187\n",
      "Iter 8776, training loss 0.000002, validation loss 0.030187\n",
      "Iter 8777, training loss 0.000002, validation loss 0.030186\n",
      "Iter 8778, training loss 0.000002, validation loss 0.030186\n",
      "Iter 8779, training loss 0.000002, validation loss 0.030186\n",
      "Iter 8780, training loss 0.000002, validation loss 0.030186\n",
      "Iter 8781, training loss 0.000002, validation loss 0.030186\n",
      "Iter 8782, training loss 0.000002, validation loss 0.030185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8783, training loss 0.000002, validation loss 0.030185\n",
      "Iter 8784, training loss 0.000002, validation loss 0.030185\n",
      "Iter 8785, training loss 0.000002, validation loss 0.030185\n",
      "Iter 8786, training loss 0.000002, validation loss 0.030184\n",
      "Iter 8787, training loss 0.000002, validation loss 0.030184\n",
      "Iter 8788, training loss 0.000002, validation loss 0.030184\n",
      "Iter 8789, training loss 0.000002, validation loss 0.030184\n",
      "Iter 8790, training loss 0.000001, validation loss 0.030184\n",
      "Iter 8791, training loss 0.000001, validation loss 0.030184\n",
      "Iter 8792, training loss 0.000001, validation loss 0.030183\n",
      "Iter 8793, training loss 0.000001, validation loss 0.030183\n",
      "Iter 8794, training loss 0.000001, validation loss 0.030183\n",
      "Iter 8795, training loss 0.000001, validation loss 0.030183\n",
      "Iter 8796, training loss 0.000001, validation loss 0.030183\n",
      "Iter 8797, training loss 0.000001, validation loss 0.030182\n",
      "Iter 8798, training loss 0.000001, validation loss 0.030182\n",
      "Iter 8799, training loss 0.000001, validation loss 0.030182\n",
      "Iter 8800, training loss 0.000001, validation loss 0.030182\n",
      "Iter 8801, training loss 0.000001, validation loss 0.030182\n",
      "Iter 8802, training loss 0.000001, validation loss 0.030181\n",
      "Iter 8803, training loss 0.000001, validation loss 0.030181\n",
      "Iter 8804, training loss 0.000001, validation loss 0.030181\n",
      "Iter 8805, training loss 0.000001, validation loss 0.030181\n",
      "Iter 8806, training loss 0.000001, validation loss 0.030180\n",
      "Iter 8807, training loss 0.000001, validation loss 0.030180\n",
      "Iter 8808, training loss 0.000001, validation loss 0.030180\n",
      "Iter 8809, training loss 0.000001, validation loss 0.030180\n",
      "Iter 8810, training loss 0.000001, validation loss 0.030180\n",
      "Iter 8811, training loss 0.000001, validation loss 0.030180\n",
      "Iter 8812, training loss 0.000001, validation loss 0.030179\n",
      "Iter 8813, training loss 0.000001, validation loss 0.030179\n",
      "Iter 8814, training loss 0.000001, validation loss 0.030179\n",
      "Iter 8815, training loss 0.000001, validation loss 0.030179\n",
      "Iter 8816, training loss 0.000001, validation loss 0.030179\n",
      "Iter 8817, training loss 0.000001, validation loss 0.030178\n",
      "Iter 8818, training loss 0.000001, validation loss 0.030178\n",
      "Iter 8819, training loss 0.000001, validation loss 0.030178\n",
      "Iter 8820, training loss 0.000001, validation loss 0.030178\n",
      "Iter 8821, training loss 0.000001, validation loss 0.030178\n",
      "Iter 8822, training loss 0.000001, validation loss 0.030178\n",
      "Iter 8823, training loss 0.000001, validation loss 0.030177\n",
      "Iter 8824, training loss 0.000001, validation loss 0.030177\n",
      "Iter 8825, training loss 0.000001, validation loss 0.030177\n",
      "Iter 8826, training loss 0.000001, validation loss 0.030177\n",
      "Iter 8827, training loss 0.000001, validation loss 0.030176\n",
      "Iter 8828, training loss 0.000001, validation loss 0.030176\n",
      "Iter 8829, training loss 0.000001, validation loss 0.030176\n",
      "Iter 8830, training loss 0.000001, validation loss 0.030176\n",
      "Iter 8831, training loss 0.000001, validation loss 0.030176\n",
      "Iter 8832, training loss 0.000001, validation loss 0.030176\n",
      "Iter 8833, training loss 0.000001, validation loss 0.030175\n",
      "Iter 8834, training loss 0.000001, validation loss 0.030175\n",
      "Iter 8835, training loss 0.000001, validation loss 0.030175\n",
      "Iter 8836, training loss 0.000001, validation loss 0.030175\n",
      "Iter 8837, training loss 0.000001, validation loss 0.030175\n",
      "Iter 8838, training loss 0.000001, validation loss 0.030174\n",
      "Iter 8839, training loss 0.000001, validation loss 0.030174\n",
      "Iter 8840, training loss 0.000001, validation loss 0.030174\n",
      "Iter 8841, training loss 0.000001, validation loss 0.030174\n",
      "Iter 8842, training loss 0.000001, validation loss 0.030174\n",
      "Iter 8843, training loss 0.000001, validation loss 0.030173\n",
      "Iter 8844, training loss 0.000001, validation loss 0.030173\n",
      "Iter 8845, training loss 0.000001, validation loss 0.030173\n",
      "Iter 8846, training loss 0.000001, validation loss 0.030173\n",
      "Iter 8847, training loss 0.000001, validation loss 0.030173\n",
      "Iter 8848, training loss 0.000001, validation loss 0.030173\n",
      "Iter 8849, training loss 0.000001, validation loss 0.030173\n",
      "Iter 8850, training loss 0.000001, validation loss 0.030172\n",
      "Iter 8851, training loss 0.000001, validation loss 0.030172\n",
      "Iter 8852, training loss 0.000001, validation loss 0.030172\n",
      "Iter 8853, training loss 0.000001, validation loss 0.030172\n",
      "Iter 8854, training loss 0.000001, validation loss 0.030172\n",
      "Iter 8855, training loss 0.000001, validation loss 0.030172\n",
      "Iter 8856, training loss 0.000001, validation loss 0.030171\n",
      "Iter 8857, training loss 0.000001, validation loss 0.030171\n",
      "Iter 8858, training loss 0.000001, validation loss 0.030171\n",
      "Iter 8859, training loss 0.000001, validation loss 0.030171\n",
      "Iter 8860, training loss 0.000001, validation loss 0.030171\n",
      "Iter 8861, training loss 0.000001, validation loss 0.030170\n",
      "Iter 8862, training loss 0.000001, validation loss 0.030170\n",
      "Iter 8863, training loss 0.000001, validation loss 0.030170\n",
      "Iter 8864, training loss 0.000001, validation loss 0.030170\n",
      "Iter 8865, training loss 0.000001, validation loss 0.030170\n",
      "Iter 8866, training loss 0.000001, validation loss 0.030170\n",
      "Iter 8867, training loss 0.000001, validation loss 0.030169\n",
      "Iter 8868, training loss 0.000001, validation loss 0.030169\n",
      "Iter 8869, training loss 0.000001, validation loss 0.030169\n",
      "Iter 8870, training loss 0.000001, validation loss 0.030169\n",
      "Iter 8871, training loss 0.000001, validation loss 0.030169\n",
      "Iter 8872, training loss 0.000001, validation loss 0.030169\n",
      "Iter 8873, training loss 0.000001, validation loss 0.030168\n",
      "Iter 8874, training loss 0.000001, validation loss 0.030168\n",
      "Iter 8875, training loss 0.000001, validation loss 0.030168\n",
      "Iter 8876, training loss 0.000001, validation loss 0.030168\n",
      "Iter 8877, training loss 0.000001, validation loss 0.030168\n",
      "Iter 8878, training loss 0.000001, validation loss 0.030167\n",
      "Iter 8879, training loss 0.000001, validation loss 0.030167\n",
      "Iter 8880, training loss 0.000001, validation loss 0.030167\n",
      "Iter 8881, training loss 0.000001, validation loss 0.030167\n",
      "Iter 8882, training loss 0.000001, validation loss 0.030167\n",
      "Iter 8883, training loss 0.000001, validation loss 0.030166\n",
      "Iter 8884, training loss 0.000001, validation loss 0.030166\n",
      "Iter 8885, training loss 0.000001, validation loss 0.030166\n",
      "Iter 8886, training loss 0.000001, validation loss 0.030166\n",
      "Iter 8887, training loss 0.000001, validation loss 0.030166\n",
      "Iter 8888, training loss 0.000001, validation loss 0.030165\n",
      "Iter 8889, training loss 0.000001, validation loss 0.030165\n",
      "Iter 8890, training loss 0.000001, validation loss 0.030165\n",
      "Iter 8891, training loss 0.000001, validation loss 0.030165\n",
      "Iter 8892, training loss 0.000001, validation loss 0.030165\n",
      "Iter 8893, training loss 0.000001, validation loss 0.030165\n",
      "Iter 8894, training loss 0.000001, validation loss 0.030164\n",
      "Iter 8895, training loss 0.000001, validation loss 0.030164\n",
      "Iter 8896, training loss 0.000001, validation loss 0.030164\n",
      "Iter 8897, training loss 0.000001, validation loss 0.030164\n",
      "Iter 8898, training loss 0.000001, validation loss 0.030163\n",
      "Iter 8899, training loss 0.000001, validation loss 0.030163\n",
      "Iter 8900, training loss 0.000001, validation loss 0.030163\n",
      "Iter 8901, training loss 0.000001, validation loss 0.030163\n",
      "Iter 8902, training loss 0.000001, validation loss 0.030163\n",
      "Iter 8903, training loss 0.000001, validation loss 0.030163\n",
      "Iter 8904, training loss 0.000001, validation loss 0.030162\n",
      "Iter 8905, training loss 0.000001, validation loss 0.030162\n",
      "Iter 8906, training loss 0.000001, validation loss 0.030162\n",
      "Iter 8907, training loss 0.000001, validation loss 0.030162\n",
      "Iter 8908, training loss 0.000001, validation loss 0.030162\n",
      "Iter 8909, training loss 0.000001, validation loss 0.030161\n",
      "Iter 8910, training loss 0.000001, validation loss 0.030161\n",
      "Iter 8911, training loss 0.000001, validation loss 0.030161\n",
      "Iter 8912, training loss 0.000001, validation loss 0.030161\n",
      "Iter 8913, training loss 0.000001, validation loss 0.030161\n",
      "Iter 8914, training loss 0.000001, validation loss 0.030160\n",
      "Iter 8915, training loss 0.000001, validation loss 0.030160\n",
      "Iter 8916, training loss 0.000001, validation loss 0.030160\n",
      "Iter 8917, training loss 0.000001, validation loss 0.030160\n",
      "Iter 8918, training loss 0.000001, validation loss 0.030160\n",
      "Iter 8919, training loss 0.000001, validation loss 0.030160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8920, training loss 0.000001, validation loss 0.030159\n",
      "Iter 8921, training loss 0.000001, validation loss 0.030159\n",
      "Iter 8922, training loss 0.000001, validation loss 0.030159\n",
      "Iter 8923, training loss 0.000001, validation loss 0.030159\n",
      "Iter 8924, training loss 0.000001, validation loss 0.030159\n",
      "Iter 8925, training loss 0.000001, validation loss 0.030159\n",
      "Iter 8926, training loss 0.000001, validation loss 0.030158\n",
      "Iter 8927, training loss 0.000001, validation loss 0.030158\n",
      "Iter 8928, training loss 0.000001, validation loss 0.030158\n",
      "Iter 8929, training loss 0.000001, validation loss 0.030158\n",
      "Iter 8930, training loss 0.000001, validation loss 0.030158\n",
      "Iter 8931, training loss 0.000001, validation loss 0.030157\n",
      "Iter 8932, training loss 0.000001, validation loss 0.030157\n",
      "Iter 8933, training loss 0.000001, validation loss 0.030157\n",
      "Iter 8934, training loss 0.000001, validation loss 0.030157\n",
      "Iter 8935, training loss 0.000001, validation loss 0.030157\n",
      "Iter 8936, training loss 0.000001, validation loss 0.030157\n",
      "Iter 8937, training loss 0.000001, validation loss 0.030156\n",
      "Iter 8938, training loss 0.000001, validation loss 0.030156\n",
      "Iter 8939, training loss 0.000001, validation loss 0.030156\n",
      "Iter 8940, training loss 0.000001, validation loss 0.030156\n",
      "Iter 8941, training loss 0.000001, validation loss 0.030156\n",
      "Iter 8942, training loss 0.000001, validation loss 0.030155\n",
      "Iter 8943, training loss 0.000001, validation loss 0.030155\n",
      "Iter 8944, training loss 0.000001, validation loss 0.030155\n",
      "Iter 8945, training loss 0.000001, validation loss 0.030155\n",
      "Iter 8946, training loss 0.000001, validation loss 0.030155\n",
      "Iter 8947, training loss 0.000001, validation loss 0.030154\n",
      "Iter 8948, training loss 0.000001, validation loss 0.030154\n",
      "Iter 8949, training loss 0.000001, validation loss 0.030154\n",
      "Iter 8950, training loss 0.000001, validation loss 0.030154\n",
      "Iter 8951, training loss 0.000001, validation loss 0.030154\n",
      "Iter 8952, training loss 0.000001, validation loss 0.030153\n",
      "Iter 8953, training loss 0.000001, validation loss 0.030153\n",
      "Iter 8954, training loss 0.000001, validation loss 0.030153\n",
      "Iter 8955, training loss 0.000001, validation loss 0.030153\n",
      "Iter 8956, training loss 0.000001, validation loss 0.030153\n",
      "Iter 8957, training loss 0.000001, validation loss 0.030153\n",
      "Iter 8958, training loss 0.000001, validation loss 0.030152\n",
      "Iter 8959, training loss 0.000001, validation loss 0.030152\n",
      "Iter 8960, training loss 0.000001, validation loss 0.030152\n",
      "Iter 8961, training loss 0.000001, validation loss 0.030152\n",
      "Iter 8962, training loss 0.000001, validation loss 0.030152\n",
      "Iter 8963, training loss 0.000001, validation loss 0.030151\n",
      "Iter 8964, training loss 0.000001, validation loss 0.030151\n",
      "Iter 8965, training loss 0.000001, validation loss 0.030151\n",
      "Iter 8966, training loss 0.000001, validation loss 0.030151\n",
      "Iter 8967, training loss 0.000001, validation loss 0.030151\n",
      "Iter 8968, training loss 0.000001, validation loss 0.030151\n",
      "Iter 8969, training loss 0.000001, validation loss 0.030150\n",
      "Iter 8970, training loss 0.000001, validation loss 0.030150\n",
      "Iter 8971, training loss 0.000001, validation loss 0.030150\n",
      "Iter 8972, training loss 0.000001, validation loss 0.030150\n",
      "Iter 8973, training loss 0.000001, validation loss 0.030150\n",
      "Iter 8974, training loss 0.000001, validation loss 0.030149\n",
      "Iter 8975, training loss 0.000001, validation loss 0.030149\n",
      "Iter 8976, training loss 0.000001, validation loss 0.030149\n",
      "Iter 8977, training loss 0.000001, validation loss 0.030149\n",
      "Iter 8978, training loss 0.000001, validation loss 0.030149\n",
      "Iter 8979, training loss 0.000001, validation loss 0.030149\n",
      "Iter 8980, training loss 0.000001, validation loss 0.030149\n",
      "Iter 8981, training loss 0.000001, validation loss 0.030148\n",
      "Iter 8982, training loss 0.000001, validation loss 0.030148\n",
      "Iter 8983, training loss 0.000001, validation loss 0.030148\n",
      "Iter 8984, training loss 0.000001, validation loss 0.030148\n",
      "Iter 8985, training loss 0.000001, validation loss 0.030148\n",
      "Iter 8986, training loss 0.000001, validation loss 0.030148\n",
      "Iter 8987, training loss 0.000001, validation loss 0.030148\n",
      "Iter 8988, training loss 0.000001, validation loss 0.030147\n",
      "Iter 8989, training loss 0.000001, validation loss 0.030147\n",
      "Iter 8990, training loss 0.000001, validation loss 0.030147\n",
      "Iter 8991, training loss 0.000001, validation loss 0.030147\n",
      "Iter 8992, training loss 0.000001, validation loss 0.030147\n",
      "Iter 8993, training loss 0.000001, validation loss 0.030147\n",
      "Iter 8994, training loss 0.000001, validation loss 0.030147\n",
      "Iter 8995, training loss 0.000001, validation loss 0.030146\n",
      "Iter 8996, training loss 0.000001, validation loss 0.030146\n",
      "Iter 8997, training loss 0.000001, validation loss 0.030146\n",
      "Iter 8998, training loss 0.000001, validation loss 0.030146\n",
      "Iter 8999, training loss 0.000001, validation loss 0.030146\n",
      "Iter 9000, training loss 0.000001, validation loss 0.030146\n",
      "Iter 9001, training loss 0.000001, validation loss 0.030145\n",
      "Iter 9002, training loss 0.000001, validation loss 0.030145\n",
      "Iter 9003, training loss 0.000001, validation loss 0.030145\n",
      "Iter 9004, training loss 0.000001, validation loss 0.030145\n",
      "Iter 9005, training loss 0.000001, validation loss 0.030145\n",
      "Iter 9006, training loss 0.000001, validation loss 0.030145\n",
      "Iter 9007, training loss 0.000001, validation loss 0.030145\n",
      "Iter 9008, training loss 0.000001, validation loss 0.030144\n",
      "Iter 9009, training loss 0.000001, validation loss 0.030144\n",
      "Iter 9010, training loss 0.000001, validation loss 0.030144\n",
      "Iter 9011, training loss 0.000001, validation loss 0.030144\n",
      "Iter 9012, training loss 0.000001, validation loss 0.030144\n",
      "Iter 9013, training loss 0.000001, validation loss 0.030144\n",
      "Iter 9014, training loss 0.000001, validation loss 0.030144\n",
      "Iter 9015, training loss 0.000001, validation loss 0.030143\n",
      "Iter 9016, training loss 0.000001, validation loss 0.030143\n",
      "Iter 9017, training loss 0.000001, validation loss 0.030143\n",
      "Iter 9018, training loss 0.000001, validation loss 0.030143\n",
      "Iter 9019, training loss 0.000001, validation loss 0.030143\n",
      "Iter 9020, training loss 0.000001, validation loss 0.030143\n",
      "Iter 9021, training loss 0.000001, validation loss 0.030142\n",
      "Iter 9022, training loss 0.000001, validation loss 0.030142\n",
      "Iter 9023, training loss 0.000001, validation loss 0.030142\n",
      "Iter 9024, training loss 0.000001, validation loss 0.030142\n",
      "Iter 9025, training loss 0.000001, validation loss 0.030142\n",
      "Iter 9026, training loss 0.000001, validation loss 0.030142\n",
      "Iter 9027, training loss 0.000001, validation loss 0.030141\n",
      "Iter 9028, training loss 0.000001, validation loss 0.030141\n",
      "Iter 9029, training loss 0.000001, validation loss 0.030141\n",
      "Iter 9030, training loss 0.000001, validation loss 0.030141\n",
      "Iter 9031, training loss 0.000001, validation loss 0.030141\n",
      "Iter 9032, training loss 0.000001, validation loss 0.030141\n",
      "Iter 9033, training loss 0.000001, validation loss 0.030140\n",
      "Iter 9034, training loss 0.000001, validation loss 0.030140\n",
      "Iter 9035, training loss 0.000001, validation loss 0.030140\n",
      "Iter 9036, training loss 0.000001, validation loss 0.030140\n",
      "Iter 9037, training loss 0.000001, validation loss 0.030140\n",
      "Iter 9038, training loss 0.000001, validation loss 0.030140\n",
      "Iter 9039, training loss 0.000001, validation loss 0.030140\n",
      "Iter 9040, training loss 0.000001, validation loss 0.030139\n",
      "Iter 9041, training loss 0.000001, validation loss 0.030139\n",
      "Iter 9042, training loss 0.000001, validation loss 0.030139\n",
      "Iter 9043, training loss 0.000001, validation loss 0.030139\n",
      "Iter 9044, training loss 0.000001, validation loss 0.030139\n",
      "Iter 9045, training loss 0.000001, validation loss 0.030139\n",
      "Iter 9046, training loss 0.000001, validation loss 0.030139\n",
      "Iter 9047, training loss 0.000001, validation loss 0.030138\n",
      "Iter 9048, training loss 0.000001, validation loss 0.030138\n",
      "Iter 9049, training loss 0.000001, validation loss 0.030138\n",
      "Iter 9050, training loss 0.000001, validation loss 0.030138\n",
      "Iter 9051, training loss 0.000001, validation loss 0.030138\n",
      "Iter 9052, training loss 0.000001, validation loss 0.030138\n",
      "Iter 9053, training loss 0.000001, validation loss 0.030138\n",
      "Iter 9054, training loss 0.000001, validation loss 0.030137\n",
      "Iter 9055, training loss 0.000001, validation loss 0.030137\n",
      "Iter 9056, training loss 0.000001, validation loss 0.030137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9057, training loss 0.000001, validation loss 0.030137\n",
      "Iter 9058, training loss 0.000001, validation loss 0.030137\n",
      "Iter 9059, training loss 0.000001, validation loss 0.030137\n",
      "Iter 9060, training loss 0.000001, validation loss 0.030137\n",
      "Iter 9061, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9062, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9063, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9064, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9065, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9066, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9067, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9068, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9069, training loss 0.000001, validation loss 0.030135\n",
      "Iter 9070, training loss 0.000001, validation loss 0.030135\n",
      "Iter 9071, training loss 0.000001, validation loss 0.030135\n",
      "Iter 9072, training loss 0.000001, validation loss 0.030135\n",
      "Iter 9073, training loss 0.000001, validation loss 0.030135\n",
      "Iter 9074, training loss 0.000001, validation loss 0.030135\n",
      "Iter 9075, training loss 0.000001, validation loss 0.030135\n",
      "Iter 9076, training loss 0.000001, validation loss 0.030135\n",
      "Iter 9077, training loss 0.000001, validation loss 0.030134\n",
      "Iter 9078, training loss 0.000001, validation loss 0.030134\n",
      "Iter 9079, training loss 0.000001, validation loss 0.030134\n",
      "Iter 9080, training loss 0.000001, validation loss 0.030134\n",
      "Iter 9081, training loss 0.000001, validation loss 0.030134\n",
      "Iter 9082, training loss 0.000001, validation loss 0.030134\n",
      "Iter 9083, training loss 0.000001, validation loss 0.030134\n",
      "Iter 9084, training loss 0.000001, validation loss 0.030133\n",
      "Iter 9085, training loss 0.000001, validation loss 0.030133\n",
      "Iter 9086, training loss 0.000001, validation loss 0.030133\n",
      "Iter 9087, training loss 0.000001, validation loss 0.030133\n",
      "Iter 9088, training loss 0.000001, validation loss 0.030133\n",
      "Iter 9089, training loss 0.000000, validation loss 0.030133\n",
      "Iter 9090, training loss 0.000000, validation loss 0.030133\n",
      "Iter 9091, training loss 0.000000, validation loss 0.030133\n",
      "Iter 9092, training loss 0.000000, validation loss 0.030132\n",
      "Iter 9093, training loss 0.000000, validation loss 0.030133\n",
      "Iter 9094, training loss 0.000000, validation loss 0.030132\n",
      "Iter 9095, training loss 0.000000, validation loss 0.030132\n",
      "Iter 9096, training loss 0.000000, validation loss 0.030132\n",
      "Iter 9097, training loss 0.000000, validation loss 0.030132\n",
      "Iter 9098, training loss 0.000000, validation loss 0.030131\n",
      "Iter 9099, training loss 0.000000, validation loss 0.030132\n",
      "Iter 9100, training loss 0.000000, validation loss 0.030131\n",
      "Iter 9101, training loss 0.000000, validation loss 0.030132\n",
      "Iter 9102, training loss 0.000000, validation loss 0.030130\n",
      "Iter 9103, training loss 0.000000, validation loss 0.030133\n",
      "Iter 9104, training loss 0.000000, validation loss 0.030129\n",
      "Iter 9105, training loss 0.000000, validation loss 0.030133\n",
      "Iter 9106, training loss 0.000001, validation loss 0.030128\n",
      "Iter 9107, training loss 0.000001, validation loss 0.030134\n",
      "Iter 9108, training loss 0.000001, validation loss 0.030126\n",
      "Iter 9109, training loss 0.000001, validation loss 0.030136\n",
      "Iter 9110, training loss 0.000001, validation loss 0.030124\n",
      "Iter 9111, training loss 0.000001, validation loss 0.030139\n",
      "Iter 9112, training loss 0.000001, validation loss 0.030120\n",
      "Iter 9113, training loss 0.000001, validation loss 0.030144\n",
      "Iter 9114, training loss 0.000002, validation loss 0.030114\n",
      "Iter 9115, training loss 0.000003, validation loss 0.030153\n",
      "Iter 9116, training loss 0.000004, validation loss 0.030106\n",
      "Iter 9117, training loss 0.000006, validation loss 0.030170\n",
      "Iter 9118, training loss 0.000010, validation loss 0.030095\n",
      "Iter 9119, training loss 0.000016, validation loss 0.030202\n",
      "Iter 9120, training loss 0.000025, validation loss 0.030083\n",
      "Iter 9121, training loss 0.000042, validation loss 0.030269\n",
      "Iter 9122, training loss 0.000070, validation loss 0.030081\n",
      "Iter 9123, training loss 0.000117, validation loss 0.030414\n",
      "Iter 9124, training loss 0.000198, validation loss 0.030136\n",
      "Iter 9125, training loss 0.000335, validation loss 0.030764\n",
      "Iter 9126, training loss 0.000570, validation loss 0.030400\n",
      "Iter 9127, training loss 0.000974, validation loss 0.031662\n",
      "Iter 9128, training loss 0.001669, validation loss 0.031361\n",
      "Iter 9129, training loss 0.002861, validation loss 0.034078\n",
      "Iter 9130, training loss 0.004888, validation loss 0.034478\n",
      "Iter 9131, training loss 0.008264, validation loss 0.040614\n",
      "Iter 9132, training loss 0.013697, validation loss 0.043506\n",
      "Iter 9133, training loss 0.021817, validation loss 0.056445\n",
      "Iter 9134, training loss 0.032495, validation loss 0.063364\n",
      "Iter 9135, training loss 0.042899, validation loss 0.080512\n",
      "Iter 9136, training loss 0.046396, validation loss 0.077982\n",
      "Iter 9137, training loss 0.035305, validation loss 0.071164\n",
      "Iter 9138, training loss 0.013482, validation loss 0.041992\n",
      "Iter 9139, training loss 0.000417, validation loss 0.028958\n",
      "Iter 9140, training loss 0.007614, validation loss 0.037871\n",
      "Iter 9141, training loss 0.016800, validation loss 0.044587\n",
      "Iter 9142, training loss 0.009282, validation loss 0.039536\n",
      "Iter 9143, training loss 0.000413, validation loss 0.027895\n",
      "Iter 9144, training loss 0.006177, validation loss 0.033136\n",
      "Iter 9145, training loss 0.009341, validation loss 0.039215\n",
      "Iter 9146, training loss 0.001927, validation loss 0.028824\n",
      "Iter 9147, training loss 0.002007, validation loss 0.028853\n",
      "Iter 9148, training loss 0.006543, validation loss 0.035539\n",
      "Iter 9149, training loss 0.002375, validation loss 0.029135\n",
      "Iter 9150, training loss 0.000890, validation loss 0.027709\n",
      "Iter 9151, training loss 0.004476, validation loss 0.032843\n",
      "Iter 9152, training loss 0.001950, validation loss 0.028635\n",
      "Iter 9153, training loss 0.000609, validation loss 0.027372\n",
      "Iter 9154, training loss 0.003202, validation loss 0.031175\n",
      "Iter 9155, training loss 0.001334, validation loss 0.027961\n",
      "Iter 9156, training loss 0.000554, validation loss 0.027244\n",
      "Iter 9157, training loss 0.002364, validation loss 0.030087\n",
      "Iter 9158, training loss 0.000806, validation loss 0.027402\n",
      "Iter 9159, training loss 0.000571, validation loss 0.027175\n",
      "Iter 9160, training loss 0.001741, validation loss 0.029236\n",
      "Iter 9161, training loss 0.000435, validation loss 0.027008\n",
      "Iter 9162, training loss 0.000601, validation loss 0.027107\n",
      "Iter 9163, training loss 0.001244, validation loss 0.028507\n",
      "Iter 9164, training loss 0.000220, validation loss 0.026782\n",
      "Iter 9165, training loss 0.000607, validation loss 0.027044\n",
      "Iter 9166, training loss 0.000848, validation loss 0.027910\n",
      "Iter 9167, training loss 0.000124, validation loss 0.026701\n",
      "Iter 9168, training loss 0.000576, validation loss 0.026971\n",
      "Iter 9169, training loss 0.000546, validation loss 0.027440\n",
      "Iter 9170, training loss 0.000100, validation loss 0.026684\n",
      "Iter 9171, training loss 0.000508, validation loss 0.026863\n",
      "Iter 9172, training loss 0.000333, validation loss 0.027088\n",
      "Iter 9173, training loss 0.000111, validation loss 0.026696\n",
      "Iter 9174, training loss 0.000420, validation loss 0.026745\n",
      "Iter 9175, training loss 0.000195, validation loss 0.026840\n",
      "Iter 9176, training loss 0.000128, validation loss 0.026712\n",
      "Iter 9177, training loss 0.000328, validation loss 0.026638\n",
      "Iter 9178, training loss 0.000114, validation loss 0.026674\n",
      "Iter 9179, training loss 0.000137, validation loss 0.026711\n",
      "Iter 9180, training loss 0.000244, validation loss 0.026552\n",
      "Iter 9181, training loss 0.000072, validation loss 0.026562\n",
      "Iter 9182, training loss 0.000136, validation loss 0.026690\n",
      "Iter 9183, training loss 0.000176, validation loss 0.026487\n",
      "Iter 9184, training loss 0.000053, validation loss 0.026493\n",
      "Iter 9185, training loss 0.000126, validation loss 0.026661\n",
      "Iter 9186, training loss 0.000125, validation loss 0.026445\n",
      "Iter 9187, training loss 0.000046, validation loss 0.026454\n",
      "Iter 9188, training loss 0.000111, validation loss 0.026628\n",
      "Iter 9189, training loss 0.000089, validation loss 0.026416\n",
      "Iter 9190, training loss 0.000043, validation loss 0.026425\n",
      "Iter 9191, training loss 0.000095, validation loss 0.026588\n",
      "Iter 9192, training loss 0.000064, validation loss 0.026398\n",
      "Iter 9193, training loss 0.000042, validation loss 0.026406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9194, training loss 0.000079, validation loss 0.026552\n",
      "Iter 9195, training loss 0.000048, validation loss 0.026391\n",
      "Iter 9196, training loss 0.000040, validation loss 0.026393\n",
      "Iter 9197, training loss 0.000064, validation loss 0.026519\n",
      "Iter 9198, training loss 0.000038, validation loss 0.026387\n",
      "Iter 9199, training loss 0.000038, validation loss 0.026384\n",
      "Iter 9200, training loss 0.000053, validation loss 0.026491\n",
      "Iter 9201, training loss 0.000032, validation loss 0.026386\n",
      "Iter 9202, training loss 0.000035, validation loss 0.026377\n",
      "Iter 9203, training loss 0.000044, validation loss 0.026468\n",
      "Iter 9204, training loss 0.000028, validation loss 0.026386\n",
      "Iter 9205, training loss 0.000032, validation loss 0.026374\n",
      "Iter 9206, training loss 0.000037, validation loss 0.026449\n",
      "Iter 9207, training loss 0.000025, validation loss 0.026386\n",
      "Iter 9208, training loss 0.000030, validation loss 0.026370\n",
      "Iter 9209, training loss 0.000031, validation loss 0.026433\n",
      "Iter 9210, training loss 0.000023, validation loss 0.026384\n",
      "Iter 9211, training loss 0.000027, validation loss 0.026367\n",
      "Iter 9212, training loss 0.000027, validation loss 0.026421\n",
      "Iter 9213, training loss 0.000021, validation loss 0.026383\n",
      "Iter 9214, training loss 0.000024, validation loss 0.026366\n",
      "Iter 9215, training loss 0.000024, validation loss 0.026411\n",
      "Iter 9216, training loss 0.000020, validation loss 0.026382\n",
      "Iter 9217, training loss 0.000022, validation loss 0.026365\n",
      "Iter 9218, training loss 0.000022, validation loss 0.026403\n",
      "Iter 9219, training loss 0.000018, validation loss 0.026380\n",
      "Iter 9220, training loss 0.000020, validation loss 0.026364\n",
      "Iter 9221, training loss 0.000020, validation loss 0.026396\n",
      "Iter 9222, training loss 0.000017, validation loss 0.026378\n",
      "Iter 9223, training loss 0.000019, validation loss 0.026363\n",
      "Iter 9224, training loss 0.000018, validation loss 0.026390\n",
      "Iter 9225, training loss 0.000016, validation loss 0.026375\n",
      "Iter 9226, training loss 0.000017, validation loss 0.026362\n",
      "Iter 9227, training loss 0.000017, validation loss 0.026385\n",
      "Iter 9228, training loss 0.000015, validation loss 0.026372\n",
      "Iter 9229, training loss 0.000016, validation loss 0.026360\n",
      "Iter 9230, training loss 0.000015, validation loss 0.026380\n",
      "Iter 9231, training loss 0.000015, validation loss 0.026369\n",
      "Iter 9232, training loss 0.000015, validation loss 0.026359\n",
      "Iter 9233, training loss 0.000015, validation loss 0.026376\n",
      "Iter 9234, training loss 0.000014, validation loss 0.026367\n",
      "Iter 9235, training loss 0.000014, validation loss 0.026358\n",
      "Iter 9236, training loss 0.000014, validation loss 0.026373\n",
      "Iter 9237, training loss 0.000013, validation loss 0.026365\n",
      "Iter 9238, training loss 0.000013, validation loss 0.026358\n",
      "Iter 9239, training loss 0.000013, validation loss 0.026371\n",
      "Iter 9240, training loss 0.000012, validation loss 0.026364\n",
      "Iter 9241, training loss 0.000012, validation loss 0.026358\n",
      "Iter 9242, training loss 0.000012, validation loss 0.026369\n",
      "Iter 9243, training loss 0.000012, validation loss 0.026362\n",
      "Iter 9244, training loss 0.000012, validation loss 0.026357\n",
      "Iter 9245, training loss 0.000012, validation loss 0.026366\n",
      "Iter 9246, training loss 0.000011, validation loss 0.026360\n",
      "Iter 9247, training loss 0.000011, validation loss 0.026357\n",
      "Iter 9248, training loss 0.000011, validation loss 0.026364\n",
      "Iter 9249, training loss 0.000011, validation loss 0.026359\n",
      "Iter 9250, training loss 0.000011, validation loss 0.026356\n",
      "Iter 9251, training loss 0.000011, validation loss 0.026363\n",
      "Iter 9252, training loss 0.000010, validation loss 0.026357\n",
      "Iter 9253, training loss 0.000010, validation loss 0.026355\n",
      "Iter 9254, training loss 0.000010, validation loss 0.026360\n",
      "Iter 9255, training loss 0.000010, validation loss 0.026355\n",
      "Iter 9256, training loss 0.000010, validation loss 0.026353\n",
      "Iter 9257, training loss 0.000010, validation loss 0.026358\n",
      "Iter 9258, training loss 0.000009, validation loss 0.026353\n",
      "Iter 9259, training loss 0.000009, validation loss 0.026352\n",
      "Iter 9260, training loss 0.000009, validation loss 0.026355\n",
      "Iter 9261, training loss 0.000009, validation loss 0.026351\n",
      "Iter 9262, training loss 0.000009, validation loss 0.026351\n",
      "Iter 9263, training loss 0.000009, validation loss 0.026353\n",
      "Iter 9264, training loss 0.000009, validation loss 0.026349\n",
      "Iter 9265, training loss 0.000009, validation loss 0.026349\n",
      "Iter 9266, training loss 0.000009, validation loss 0.026351\n",
      "Iter 9267, training loss 0.000008, validation loss 0.026348\n",
      "Iter 9268, training loss 0.000008, validation loss 0.026348\n",
      "Iter 9269, training loss 0.000008, validation loss 0.026350\n",
      "Iter 9270, training loss 0.000008, validation loss 0.026347\n",
      "Iter 9271, training loss 0.000008, validation loss 0.026347\n",
      "Iter 9272, training loss 0.000008, validation loss 0.026348\n",
      "Iter 9273, training loss 0.000008, validation loss 0.026346\n",
      "Iter 9274, training loss 0.000008, validation loss 0.026346\n",
      "Iter 9275, training loss 0.000008, validation loss 0.026347\n",
      "Iter 9276, training loss 0.000007, validation loss 0.026345\n",
      "Iter 9277, training loss 0.000007, validation loss 0.026345\n",
      "Iter 9278, training loss 0.000007, validation loss 0.026346\n",
      "Iter 9279, training loss 0.000007, validation loss 0.026344\n",
      "Iter 9280, training loss 0.000007, validation loss 0.026345\n",
      "Iter 9281, training loss 0.000007, validation loss 0.026345\n",
      "Iter 9282, training loss 0.000007, validation loss 0.026343\n",
      "Iter 9283, training loss 0.000007, validation loss 0.026344\n",
      "Iter 9284, training loss 0.000007, validation loss 0.026344\n",
      "Iter 9285, training loss 0.000007, validation loss 0.026343\n",
      "Iter 9286, training loss 0.000007, validation loss 0.026343\n",
      "Iter 9287, training loss 0.000007, validation loss 0.026343\n",
      "Iter 9288, training loss 0.000006, validation loss 0.026342\n",
      "Iter 9289, training loss 0.000006, validation loss 0.026342\n",
      "Iter 9290, training loss 0.000006, validation loss 0.026342\n",
      "Iter 9291, training loss 0.000006, validation loss 0.026341\n",
      "Iter 9292, training loss 0.000006, validation loss 0.026342\n",
      "Iter 9293, training loss 0.000006, validation loss 0.026341\n",
      "Iter 9294, training loss 0.000006, validation loss 0.026340\n",
      "Iter 9295, training loss 0.000006, validation loss 0.026341\n",
      "Iter 9296, training loss 0.000006, validation loss 0.026340\n",
      "Iter 9297, training loss 0.000006, validation loss 0.026340\n",
      "Iter 9298, training loss 0.000006, validation loss 0.026340\n",
      "Iter 9299, training loss 0.000006, validation loss 0.026340\n",
      "Iter 9300, training loss 0.000006, validation loss 0.026339\n",
      "Iter 9301, training loss 0.000006, validation loss 0.026339\n",
      "Iter 9302, training loss 0.000006, validation loss 0.026339\n",
      "Iter 9303, training loss 0.000006, validation loss 0.026339\n",
      "Iter 9304, training loss 0.000005, validation loss 0.026339\n",
      "Iter 9305, training loss 0.000005, validation loss 0.026338\n",
      "Iter 9306, training loss 0.000005, validation loss 0.026338\n",
      "Iter 9307, training loss 0.000005, validation loss 0.026338\n",
      "Iter 9308, training loss 0.000005, validation loss 0.026338\n",
      "Iter 9309, training loss 0.000005, validation loss 0.026338\n",
      "Iter 9310, training loss 0.000005, validation loss 0.026337\n",
      "Iter 9311, training loss 0.000005, validation loss 0.026337\n",
      "Iter 9312, training loss 0.000005, validation loss 0.026337\n",
      "Iter 9313, training loss 0.000005, validation loss 0.026337\n",
      "Iter 9314, training loss 0.000005, validation loss 0.026336\n",
      "Iter 9315, training loss 0.000005, validation loss 0.026336\n",
      "Iter 9316, training loss 0.000005, validation loss 0.026336\n",
      "Iter 9317, training loss 0.000005, validation loss 0.026336\n",
      "Iter 9318, training loss 0.000005, validation loss 0.026336\n",
      "Iter 9319, training loss 0.000005, validation loss 0.026336\n",
      "Iter 9320, training loss 0.000005, validation loss 0.026335\n",
      "Iter 9321, training loss 0.000005, validation loss 0.026335\n",
      "Iter 9322, training loss 0.000005, validation loss 0.026335\n",
      "Iter 9323, training loss 0.000004, validation loss 0.026335\n",
      "Iter 9324, training loss 0.000004, validation loss 0.026335\n",
      "Iter 9325, training loss 0.000004, validation loss 0.026334\n",
      "Iter 9326, training loss 0.000004, validation loss 0.026334\n",
      "Iter 9327, training loss 0.000004, validation loss 0.026334\n",
      "Iter 9328, training loss 0.000004, validation loss 0.026334\n",
      "Iter 9329, training loss 0.000004, validation loss 0.026334\n",
      "Iter 9330, training loss 0.000004, validation loss 0.026333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9331, training loss 0.000004, validation loss 0.026333\n",
      "Iter 9332, training loss 0.000004, validation loss 0.026333\n",
      "Iter 9333, training loss 0.000004, validation loss 0.026333\n",
      "Iter 9334, training loss 0.000004, validation loss 0.026333\n",
      "Iter 9335, training loss 0.000004, validation loss 0.026333\n",
      "Iter 9336, training loss 0.000004, validation loss 0.026333\n",
      "Iter 9337, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9338, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9339, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9340, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9341, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9342, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9343, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9344, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9345, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9346, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9347, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9348, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9349, training loss 0.000004, validation loss 0.026332\n",
      "Iter 9350, training loss 0.000004, validation loss 0.026331\n",
      "Iter 9351, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9352, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9353, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9354, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9355, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9356, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9357, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9358, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9359, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9360, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9361, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9362, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9363, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9364, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9365, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9366, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9367, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9368, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9369, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9370, training loss 0.000003, validation loss 0.026331\n",
      "Iter 9371, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9372, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9373, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9374, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9375, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9376, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9377, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9378, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9379, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9380, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9381, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9382, training loss 0.000003, validation loss 0.026330\n",
      "Iter 9383, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9384, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9385, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9386, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9387, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9388, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9389, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9390, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9391, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9392, training loss 0.000003, validation loss 0.026329\n",
      "Iter 9393, training loss 0.000002, validation loss 0.026329\n",
      "Iter 9394, training loss 0.000002, validation loss 0.026328\n",
      "Iter 9395, training loss 0.000002, validation loss 0.026328\n",
      "Iter 9396, training loss 0.000002, validation loss 0.026328\n",
      "Iter 9397, training loss 0.000002, validation loss 0.026328\n",
      "Iter 9398, training loss 0.000002, validation loss 0.026328\n",
      "Iter 9399, training loss 0.000002, validation loss 0.026328\n",
      "Iter 9400, training loss 0.000002, validation loss 0.026328\n",
      "Iter 9401, training loss 0.000002, validation loss 0.026328\n",
      "Iter 9402, training loss 0.000002, validation loss 0.026327\n",
      "Iter 9403, training loss 0.000002, validation loss 0.026327\n",
      "Iter 9404, training loss 0.000002, validation loss 0.026327\n",
      "Iter 9405, training loss 0.000002, validation loss 0.026327\n",
      "Iter 9406, training loss 0.000002, validation loss 0.026327\n",
      "Iter 9407, training loss 0.000002, validation loss 0.026327\n",
      "Iter 9408, training loss 0.000002, validation loss 0.026326\n",
      "Iter 9409, training loss 0.000002, validation loss 0.026326\n",
      "Iter 9410, training loss 0.000002, validation loss 0.026326\n",
      "Iter 9411, training loss 0.000002, validation loss 0.026326\n",
      "Iter 9412, training loss 0.000002, validation loss 0.026326\n",
      "Iter 9413, training loss 0.000002, validation loss 0.026326\n",
      "Iter 9414, training loss 0.000002, validation loss 0.026326\n",
      "Iter 9415, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9416, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9417, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9418, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9419, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9420, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9421, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9422, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9423, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9424, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9425, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9426, training loss 0.000002, validation loss 0.026325\n",
      "Iter 9427, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9428, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9429, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9430, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9431, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9432, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9433, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9434, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9435, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9436, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9437, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9438, training loss 0.000002, validation loss 0.026324\n",
      "Iter 9439, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9440, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9441, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9442, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9443, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9444, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9445, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9446, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9447, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9448, training loss 0.000002, validation loss 0.026323\n",
      "Iter 9449, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9450, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9451, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9452, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9453, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9454, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9455, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9456, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9457, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9458, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9459, training loss 0.000002, validation loss 0.026322\n",
      "Iter 9460, training loss 0.000002, validation loss 0.026321\n",
      "Iter 9461, training loss 0.000002, validation loss 0.026321\n",
      "Iter 9462, training loss 0.000002, validation loss 0.026321\n",
      "Iter 9463, training loss 0.000002, validation loss 0.026321\n",
      "Iter 9464, training loss 0.000002, validation loss 0.026321\n",
      "Iter 9465, training loss 0.000002, validation loss 0.026321\n",
      "Iter 9466, training loss 0.000002, validation loss 0.026321\n",
      "Iter 9467, training loss 0.000002, validation loss 0.026321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9468, training loss 0.000002, validation loss 0.026321\n",
      "Iter 9469, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9470, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9471, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9472, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9473, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9474, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9475, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9476, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9477, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9478, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9479, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9480, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9481, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9482, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9483, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9484, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9485, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9486, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9487, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9488, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9489, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9490, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9491, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9492, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9493, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9494, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9495, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9496, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9497, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9498, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9499, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9500, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9501, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9502, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9503, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9504, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9505, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9506, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9507, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9508, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9509, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9510, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9511, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9512, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9513, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9514, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9515, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9516, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9517, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9518, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9519, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9520, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9521, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9522, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9523, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9524, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9525, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9526, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9527, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9528, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9529, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9530, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9531, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9532, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9533, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9534, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9535, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9536, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9537, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9538, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9539, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9540, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9541, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9542, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9543, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9544, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9545, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9546, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9547, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9548, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9549, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9550, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9551, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9552, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9553, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9554, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9555, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9556, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9557, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9558, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9559, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9560, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9561, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9562, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9563, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9564, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9565, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9566, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9567, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9568, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9569, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9570, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9571, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9572, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9573, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9574, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9575, training loss 0.000001, validation loss 0.026318\n",
      "Iter 9576, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9577, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9578, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9579, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9580, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9581, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9582, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9583, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9584, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9585, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9586, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9587, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9588, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9589, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9590, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9591, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9592, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9593, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9594, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9595, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9596, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9597, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9598, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9599, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9600, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9601, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9602, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9603, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9604, training loss 0.000001, validation loss 0.026319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9605, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9606, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9607, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9608, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9609, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9610, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9611, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9612, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9613, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9614, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9615, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9616, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9617, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9618, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9619, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9620, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9621, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9622, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9623, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9624, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9625, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9626, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9627, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9628, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9629, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9630, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9631, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9632, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9633, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9634, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9635, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9636, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9637, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9638, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9639, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9640, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9641, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9642, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9643, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9644, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9645, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9646, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9647, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9648, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9649, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9650, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9651, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9652, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9653, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9654, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9655, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9656, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9657, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9658, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9659, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9660, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9661, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9662, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9663, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9664, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9665, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9666, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9667, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9668, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9669, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9670, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9671, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9672, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9673, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9674, training loss 0.000001, validation loss 0.026320\n",
      "Iter 9675, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9676, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9677, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9678, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9679, training loss 0.000001, validation loss 0.026321\n",
      "Iter 9680, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9681, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9682, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9683, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9684, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9685, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9686, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9687, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9688, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9689, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9690, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9691, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9692, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9693, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9694, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9695, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9696, training loss 0.000000, validation loss 0.026321\n",
      "Iter 9697, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9698, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9699, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9700, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9701, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9702, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9703, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9704, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9705, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9706, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9707, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9708, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9709, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9710, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9711, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9712, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9713, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9714, training loss 0.000000, validation loss 0.026322\n",
      "Iter 9715, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9716, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9717, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9718, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9719, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9720, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9721, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9722, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9723, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9724, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9725, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9726, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9727, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9728, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9729, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9730, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9731, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9732, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9733, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9734, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9735, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9736, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9737, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9738, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9739, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9740, training loss 0.000000, validation loss 0.026323\n",
      "Iter 9741, training loss 0.000000, validation loss 0.026324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9742, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9743, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9744, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9745, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9746, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9747, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9748, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9749, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9750, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9751, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9752, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9753, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9754, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9755, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9756, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9757, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9758, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9759, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9760, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9761, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9762, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9763, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9764, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9765, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9766, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9767, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9768, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9769, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9770, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9771, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9772, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9773, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9774, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9775, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9776, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9777, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9778, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9779, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9780, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9781, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9782, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9783, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9784, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9785, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9786, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9787, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9788, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9789, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9790, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9791, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9792, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9793, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9794, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9795, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9796, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9797, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9798, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9799, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9800, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9801, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9802, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9803, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9804, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9805, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9806, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9807, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9808, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9809, training loss 0.000000, validation loss 0.026327\n",
      "Iter 9810, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9811, training loss 0.000000, validation loss 0.026327\n",
      "Iter 9812, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9813, training loss 0.000000, validation loss 0.026327\n",
      "Iter 9814, training loss 0.000000, validation loss 0.026326\n",
      "Iter 9815, training loss 0.000000, validation loss 0.026328\n",
      "Iter 9816, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9817, training loss 0.000000, validation loss 0.026328\n",
      "Iter 9818, training loss 0.000000, validation loss 0.026325\n",
      "Iter 9819, training loss 0.000000, validation loss 0.026329\n",
      "Iter 9820, training loss 0.000000, validation loss 0.026324\n",
      "Iter 9821, training loss 0.000000, validation loss 0.026331\n",
      "Iter 9822, training loss 0.000001, validation loss 0.026322\n",
      "Iter 9823, training loss 0.000001, validation loss 0.026334\n",
      "Iter 9824, training loss 0.000001, validation loss 0.026319\n",
      "Iter 9825, training loss 0.000002, validation loss 0.026340\n",
      "Iter 9826, training loss 0.000002, validation loss 0.026315\n",
      "Iter 9827, training loss 0.000004, validation loss 0.026350\n",
      "Iter 9828, training loss 0.000006, validation loss 0.026309\n",
      "Iter 9829, training loss 0.000011, validation loss 0.026371\n",
      "Iter 9830, training loss 0.000018, validation loss 0.026305\n",
      "Iter 9831, training loss 0.000032, validation loss 0.026418\n",
      "Iter 9832, training loss 0.000055, validation loss 0.026314\n",
      "Iter 9833, training loss 0.000095, validation loss 0.026531\n",
      "Iter 9834, training loss 0.000167, validation loss 0.026385\n",
      "Iter 9835, training loss 0.000294, validation loss 0.026828\n",
      "Iter 9836, training loss 0.000522, validation loss 0.026684\n",
      "Iter 9837, training loss 0.000927, validation loss 0.027672\n",
      "Iter 9838, training loss 0.001652, validation loss 0.027777\n",
      "Iter 9839, training loss 0.002938, validation loss 0.030172\n",
      "Iter 9840, training loss 0.005193, validation loss 0.031441\n",
      "Iter 9841, training loss 0.009013, validation loss 0.037420\n",
      "Iter 9842, training loss 0.015165, validation loss 0.042140\n",
      "Iter 9843, training loss 0.023911, validation loss 0.054772\n",
      "Iter 9844, training loss 0.033866, validation loss 0.062484\n",
      "Iter 9845, training loss 0.039183, validation loss 0.072218\n",
      "Iter 9846, training loss 0.032180, validation loss 0.059962\n",
      "Iter 9847, training loss 0.013138, validation loss 0.041060\n",
      "Iter 9848, training loss 0.000500, validation loss 0.025053\n",
      "Iter 9849, training loss 0.007035, validation loss 0.031438\n",
      "Iter 9850, training loss 0.014571, validation loss 0.042160\n",
      "Iter 9851, training loss 0.006337, validation loss 0.030372\n",
      "Iter 9852, training loss 0.000601, validation loss 0.024500\n",
      "Iter 9853, training loss 0.007452, validation loss 0.033351\n",
      "Iter 9854, training loss 0.006438, validation loss 0.030310\n",
      "Iter 9855, training loss 0.000392, validation loss 0.024299\n",
      "Iter 9856, training loss 0.004409, validation loss 0.029463\n",
      "Iter 9857, training loss 0.004630, validation loss 0.028335\n",
      "Iter 9858, training loss 0.000321, validation loss 0.024060\n",
      "Iter 9859, training loss 0.003360, validation loss 0.028005\n",
      "Iter 9860, training loss 0.002875, validation loss 0.026386\n",
      "Iter 9861, training loss 0.000307, validation loss 0.023810\n",
      "Iter 9862, training loss 0.002851, validation loss 0.027226\n",
      "Iter 9863, training loss 0.001452, validation loss 0.024786\n",
      "Iter 9864, training loss 0.000554, validation loss 0.023867\n",
      "Iter 9865, training loss 0.002261, validation loss 0.026411\n",
      "Iter 9866, training loss 0.000514, validation loss 0.023775\n",
      "Iter 9867, training loss 0.000927, validation loss 0.024135\n",
      "Iter 9868, training loss 0.001475, validation loss 0.025389\n",
      "Iter 9869, training loss 0.000163, validation loss 0.023459\n",
      "Iter 9870, training loss 0.001122, validation loss 0.024230\n",
      "Iter 9871, training loss 0.000704, validation loss 0.024299\n",
      "Iter 9872, training loss 0.000261, validation loss 0.023629\n",
      "Iter 9873, training loss 0.000978, validation loss 0.024012\n",
      "Iter 9874, training loss 0.000221, validation loss 0.023532\n",
      "Iter 9875, training loss 0.000481, validation loss 0.023912\n",
      "Iter 9876, training loss 0.000606, validation loss 0.023620\n",
      "Iter 9877, training loss 0.000103, validation loss 0.023235\n",
      "Iter 9878, training loss 0.000554, validation loss 0.023980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9879, training loss 0.000256, validation loss 0.023274\n",
      "Iter 9880, training loss 0.000194, validation loss 0.023215\n",
      "Iter 9881, training loss 0.000432, validation loss 0.023779\n",
      "Iter 9882, training loss 0.000087, validation loss 0.023148\n",
      "Iter 9883, training loss 0.000285, validation loss 0.023259\n",
      "Iter 9884, training loss 0.000237, validation loss 0.023482\n",
      "Iter 9885, training loss 0.000083, validation loss 0.023211\n",
      "Iter 9886, training loss 0.000275, validation loss 0.023238\n",
      "Iter 9887, training loss 0.000096, validation loss 0.023241\n",
      "Iter 9888, training loss 0.000136, validation loss 0.023314\n",
      "Iter 9889, training loss 0.000189, validation loss 0.023167\n",
      "Iter 9890, training loss 0.000051, validation loss 0.023121\n",
      "Iter 9891, training loss 0.000160, validation loss 0.023354\n",
      "Iter 9892, training loss 0.000098, validation loss 0.023109\n",
      "Iter 9893, training loss 0.000065, validation loss 0.023100\n",
      "Iter 9894, training loss 0.000135, validation loss 0.023324\n",
      "Iter 9895, training loss 0.000047, validation loss 0.023102\n",
      "Iter 9896, training loss 0.000086, validation loss 0.023105\n",
      "Iter 9897, training loss 0.000088, validation loss 0.023246\n",
      "Iter 9898, training loss 0.000038, validation loss 0.023129\n",
      "Iter 9899, training loss 0.000087, validation loss 0.023104\n",
      "Iter 9900, training loss 0.000049, validation loss 0.023172\n",
      "Iter 9901, training loss 0.000046, validation loss 0.023168\n",
      "Iter 9902, training loss 0.000070, validation loss 0.023102\n",
      "Iter 9903, training loss 0.000031, validation loss 0.023131\n",
      "Iter 9904, training loss 0.000053, validation loss 0.023197\n",
      "Iter 9905, training loss 0.000047, validation loss 0.023103\n",
      "Iter 9906, training loss 0.000029, validation loss 0.023112\n",
      "Iter 9907, training loss 0.000050, validation loss 0.023197\n",
      "Iter 9908, training loss 0.000031, validation loss 0.023106\n",
      "Iter 9909, training loss 0.000032, validation loss 0.023105\n",
      "Iter 9910, training loss 0.000040, validation loss 0.023180\n",
      "Iter 9911, training loss 0.000023, validation loss 0.023118\n",
      "Iter 9912, training loss 0.000033, validation loss 0.023103\n",
      "Iter 9913, training loss 0.000030, validation loss 0.023159\n",
      "Iter 9914, training loss 0.000022, validation loss 0.023132\n",
      "Iter 9915, training loss 0.000031, validation loss 0.023103\n",
      "Iter 9916, training loss 0.000022, validation loss 0.023139\n",
      "Iter 9917, training loss 0.000022, validation loss 0.023141\n",
      "Iter 9918, training loss 0.000026, validation loss 0.023103\n",
      "Iter 9919, training loss 0.000018, validation loss 0.023125\n",
      "Iter 9920, training loss 0.000022, validation loss 0.023146\n",
      "Iter 9921, training loss 0.000021, validation loss 0.023106\n",
      "Iter 9922, training loss 0.000017, validation loss 0.023115\n",
      "Iter 9923, training loss 0.000021, validation loss 0.023144\n",
      "Iter 9924, training loss 0.000017, validation loss 0.023109\n",
      "Iter 9925, training loss 0.000017, validation loss 0.023109\n",
      "Iter 9926, training loss 0.000019, validation loss 0.023138\n",
      "Iter 9927, training loss 0.000015, validation loss 0.023112\n",
      "Iter 9928, training loss 0.000016, validation loss 0.023106\n",
      "Iter 9929, training loss 0.000016, validation loss 0.023132\n",
      "Iter 9930, training loss 0.000014, validation loss 0.023119\n",
      "Iter 9931, training loss 0.000015, validation loss 0.023108\n",
      "Iter 9932, training loss 0.000014, validation loss 0.023128\n",
      "Iter 9933, training loss 0.000013, validation loss 0.023125\n",
      "Iter 9934, training loss 0.000014, validation loss 0.023110\n",
      "Iter 9935, training loss 0.000013, validation loss 0.023125\n",
      "Iter 9936, training loss 0.000013, validation loss 0.023128\n",
      "Iter 9937, training loss 0.000013, validation loss 0.023112\n",
      "Iter 9938, training loss 0.000012, validation loss 0.023121\n",
      "Iter 9939, training loss 0.000012, validation loss 0.023129\n",
      "Iter 9940, training loss 0.000012, validation loss 0.023114\n",
      "Iter 9941, training loss 0.000011, validation loss 0.023119\n",
      "Iter 9942, training loss 0.000012, validation loss 0.023128\n",
      "Iter 9943, training loss 0.000011, validation loss 0.023116\n",
      "Iter 9944, training loss 0.000011, validation loss 0.023117\n",
      "Iter 9945, training loss 0.000011, validation loss 0.023127\n",
      "Iter 9946, training loss 0.000010, validation loss 0.023118\n",
      "Iter 9947, training loss 0.000010, validation loss 0.023117\n",
      "Iter 9948, training loss 0.000010, validation loss 0.023126\n",
      "Iter 9949, training loss 0.000010, validation loss 0.023121\n",
      "Iter 9950, training loss 0.000010, validation loss 0.023117\n",
      "Iter 9951, training loss 0.000009, validation loss 0.023125\n",
      "Iter 9952, training loss 0.000009, validation loss 0.023122\n",
      "Iter 9953, training loss 0.000009, validation loss 0.023117\n",
      "Iter 9954, training loss 0.000009, validation loss 0.023124\n",
      "Iter 9955, training loss 0.000009, validation loss 0.023123\n",
      "Iter 9956, training loss 0.000009, validation loss 0.023118\n",
      "Iter 9957, training loss 0.000008, validation loss 0.023123\n",
      "Iter 9958, training loss 0.000008, validation loss 0.023124\n",
      "Iter 9959, training loss 0.000008, validation loss 0.023119\n",
      "Iter 9960, training loss 0.000008, validation loss 0.023122\n",
      "Iter 9961, training loss 0.000008, validation loss 0.023124\n",
      "Iter 9962, training loss 0.000008, validation loss 0.023119\n",
      "Iter 9963, training loss 0.000008, validation loss 0.023121\n",
      "Iter 9964, training loss 0.000007, validation loss 0.023124\n",
      "Iter 9965, training loss 0.000007, validation loss 0.023120\n",
      "Iter 9966, training loss 0.000007, validation loss 0.023121\n",
      "Iter 9967, training loss 0.000007, validation loss 0.023124\n",
      "Iter 9968, training loss 0.000007, validation loss 0.023121\n",
      "Iter 9969, training loss 0.000007, validation loss 0.023120\n",
      "Iter 9970, training loss 0.000007, validation loss 0.023123\n",
      "Iter 9971, training loss 0.000007, validation loss 0.023120\n",
      "Iter 9972, training loss 0.000007, validation loss 0.023119\n",
      "Iter 9973, training loss 0.000007, validation loss 0.023122\n",
      "Iter 9974, training loss 0.000006, validation loss 0.023120\n",
      "Iter 9975, training loss 0.000006, validation loss 0.023119\n",
      "Iter 9976, training loss 0.000006, validation loss 0.023121\n",
      "Iter 9977, training loss 0.000006, validation loss 0.023120\n",
      "Iter 9978, training loss 0.000006, validation loss 0.023119\n",
      "Iter 9979, training loss 0.000006, validation loss 0.023121\n",
      "Iter 9980, training loss 0.000006, validation loss 0.023120\n",
      "Iter 9981, training loss 0.000006, validation loss 0.023119\n",
      "Iter 9982, training loss 0.000006, validation loss 0.023120\n",
      "Iter 9983, training loss 0.000006, validation loss 0.023120\n",
      "Iter 9984, training loss 0.000006, validation loss 0.023118\n",
      "Iter 9985, training loss 0.000005, validation loss 0.023119\n",
      "Iter 9986, training loss 0.000005, validation loss 0.023119\n",
      "Iter 9987, training loss 0.000005, validation loss 0.023118\n",
      "Iter 9988, training loss 0.000005, validation loss 0.023118\n",
      "Iter 9989, training loss 0.000005, validation loss 0.023119\n",
      "Iter 9990, training loss 0.000005, validation loss 0.023118\n",
      "Iter 9991, training loss 0.000005, validation loss 0.023118\n",
      "Iter 9992, training loss 0.000005, validation loss 0.023118\n",
      "Iter 9993, training loss 0.000005, validation loss 0.023117\n",
      "Iter 9994, training loss 0.000005, validation loss 0.023118\n",
      "Iter 9995, training loss 0.000005, validation loss 0.023118\n",
      "Iter 9996, training loss 0.000005, validation loss 0.023117\n",
      "Iter 9997, training loss 0.000005, validation loss 0.023117\n",
      "Iter 9998, training loss 0.000005, validation loss 0.023118\n",
      "Iter 9999, training loss 0.000005, validation loss 0.023117\n",
      "Iter 10000, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10001, training loss 0.000004, validation loss 0.023118\n",
      "Iter 10002, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10003, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10004, training loss 0.000004, validation loss 0.023118\n",
      "Iter 10005, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10006, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10007, training loss 0.000004, validation loss 0.023118\n",
      "Iter 10008, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10009, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10010, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10011, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10012, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10013, training loss 0.000004, validation loss 0.023118\n",
      "Iter 10014, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10015, training loss 0.000004, validation loss 0.023117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10016, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10017, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10018, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10019, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10020, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10021, training loss 0.000004, validation loss 0.023117\n",
      "Iter 10022, training loss 0.000003, validation loss 0.023117\n",
      "Iter 10023, training loss 0.000003, validation loss 0.023117\n",
      "Iter 10024, training loss 0.000003, validation loss 0.023116\n",
      "Iter 10025, training loss 0.000003, validation loss 0.023116\n",
      "Iter 10026, training loss 0.000003, validation loss 0.023116\n",
      "Iter 10027, training loss 0.000003, validation loss 0.023116\n",
      "Iter 10028, training loss 0.000003, validation loss 0.023116\n",
      "Iter 10029, training loss 0.000003, validation loss 0.023116\n",
      "Iter 10030, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10031, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10032, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10033, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10034, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10035, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10036, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10037, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10038, training loss 0.000003, validation loss 0.023115\n",
      "Iter 10039, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10040, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10041, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10042, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10043, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10044, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10045, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10046, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10047, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10048, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10049, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10050, training loss 0.000003, validation loss 0.023114\n",
      "Iter 10051, training loss 0.000003, validation loss 0.023113\n",
      "Iter 10052, training loss 0.000003, validation loss 0.023113\n",
      "Iter 10053, training loss 0.000003, validation loss 0.023113\n",
      "Iter 10054, training loss 0.000003, validation loss 0.023113\n",
      "Iter 10055, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10056, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10057, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10058, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10059, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10060, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10061, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10062, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10063, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10064, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10065, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10066, training loss 0.000002, validation loss 0.023113\n",
      "Iter 10067, training loss 0.000002, validation loss 0.023112\n",
      "Iter 10068, training loss 0.000002, validation loss 0.023112\n",
      "Iter 10069, training loss 0.000002, validation loss 0.023112\n",
      "Iter 10070, training loss 0.000002, validation loss 0.023112\n",
      "Iter 10071, training loss 0.000002, validation loss 0.023112\n",
      "Iter 10072, training loss 0.000002, validation loss 0.023112\n",
      "Iter 10073, training loss 0.000002, validation loss 0.023112\n",
      "Iter 10074, training loss 0.000002, validation loss 0.023112\n",
      "Iter 10075, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10076, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10077, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10078, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10079, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10080, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10081, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10082, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10083, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10084, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10085, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10086, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10087, training loss 0.000002, validation loss 0.023111\n",
      "Iter 10088, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10089, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10090, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10091, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10092, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10093, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10094, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10095, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10096, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10097, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10098, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10099, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10100, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10101, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10102, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10103, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10104, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10105, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10106, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10107, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10108, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10109, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10110, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10111, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10112, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10113, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10114, training loss 0.000002, validation loss 0.023109\n",
      "Iter 10115, training loss 0.000001, validation loss 0.023109\n",
      "Iter 10116, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10117, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10118, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10119, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10120, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10121, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10122, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10123, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10124, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10125, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10126, training loss 0.000001, validation loss 0.023108\n",
      "Iter 10127, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10128, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10129, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10130, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10131, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10132, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10133, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10134, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10135, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10136, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10137, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10138, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10139, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10140, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10141, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10142, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10143, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10144, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10145, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10146, training loss 0.000001, validation loss 0.023107\n",
      "Iter 10147, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10148, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10149, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10150, training loss 0.000001, validation loss 0.023106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10151, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10152, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10153, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10154, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10155, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10156, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10157, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10158, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10159, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10160, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10161, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10162, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10163, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10164, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10165, training loss 0.000001, validation loss 0.023106\n",
      "Iter 10166, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10167, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10168, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10169, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10170, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10171, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10172, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10173, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10174, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10175, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10176, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10177, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10178, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10179, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10180, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10181, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10182, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10183, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10184, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10185, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10186, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10187, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10188, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10189, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10190, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10191, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10192, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10193, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10194, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10195, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10196, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10197, training loss 0.000001, validation loss 0.023105\n",
      "Iter 10198, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10199, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10200, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10201, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10202, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10203, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10204, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10205, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10206, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10207, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10208, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10209, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10210, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10211, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10212, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10213, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10214, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10215, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10216, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10217, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10218, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10219, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10220, training loss 0.000001, validation loss 0.023104\n",
      "Iter 10221, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10222, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10223, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10224, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10225, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10226, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10227, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10228, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10229, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10230, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10231, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10232, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10233, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10234, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10235, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10236, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10237, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10238, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10239, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10240, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10241, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10242, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10243, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10244, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10245, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10246, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10247, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10248, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10249, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10250, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10251, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10252, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10253, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10254, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10255, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10256, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10257, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10258, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10259, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10260, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10261, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10262, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10263, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10264, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10265, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10266, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10267, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10268, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10269, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10270, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10271, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10272, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10273, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10274, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10275, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10276, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10277, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10278, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10279, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10280, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10281, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10282, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10283, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10284, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10285, training loss 0.000001, validation loss 0.023103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10286, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10287, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10288, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10289, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10290, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10291, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10292, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10293, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10294, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10295, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10296, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10297, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10298, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10299, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10300, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10301, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10302, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10303, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10304, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10305, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10306, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10307, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10308, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10309, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10310, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10311, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10312, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10313, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10314, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10315, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10316, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10317, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10318, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10319, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10320, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10321, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10322, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10323, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10324, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10325, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10326, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10327, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10328, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10329, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10330, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10331, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10332, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10333, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10334, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10335, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10336, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10337, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10338, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10339, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10340, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10341, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10342, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10343, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10344, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10345, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10346, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10347, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10348, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10349, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10350, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10351, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10352, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10353, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10354, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10355, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10356, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10357, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10358, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10359, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10360, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10361, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10362, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10363, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10364, training loss 0.000000, validation loss 0.023103\n",
      "Iter 10365, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10366, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10367, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10368, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10369, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10370, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10371, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10372, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10373, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10374, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10375, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10376, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10377, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10378, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10379, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10380, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10381, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10382, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10383, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10384, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10385, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10386, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10387, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10388, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10389, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10390, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10391, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10392, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10393, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10394, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10395, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10396, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10397, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10398, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10399, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10400, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10401, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10402, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10403, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10404, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10405, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10406, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10407, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10408, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10409, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10410, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10411, training loss 0.000000, validation loss 0.023102\n",
      "Iter 10412, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10413, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10414, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10415, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10416, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10417, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10418, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10419, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10420, training loss 0.000000, validation loss 0.023101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10421, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10422, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10423, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10424, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10425, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10426, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10427, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10428, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10429, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10430, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10431, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10432, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10433, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10434, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10435, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10436, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10437, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10438, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10439, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10440, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10441, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10442, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10443, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10444, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10445, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10446, training loss 0.000000, validation loss 0.023101\n",
      "Iter 10447, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10448, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10449, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10450, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10451, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10452, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10453, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10454, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10455, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10456, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10457, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10458, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10459, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10460, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10461, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10462, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10463, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10464, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10465, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10466, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10467, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10468, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10469, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10470, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10471, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10472, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10473, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10474, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10475, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10476, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10477, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10478, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10479, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10480, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10481, training loss 0.000000, validation loss 0.023100\n",
      "Iter 10482, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10483, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10484, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10485, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10486, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10487, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10488, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10489, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10490, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10491, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10492, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10493, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10494, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10495, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10496, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10497, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10498, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10499, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10500, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10501, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10502, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10503, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10504, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10505, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10506, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10507, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10508, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10509, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10510, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10511, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10512, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10513, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10514, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10515, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10516, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10517, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10518, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10519, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10520, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10521, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10522, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10523, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10524, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10525, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10526, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10527, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10528, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10529, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10530, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10531, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10532, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10533, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10534, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10535, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10536, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10537, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10538, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10539, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10540, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10541, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10542, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10543, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10544, training loss 0.000000, validation loss 0.023098\n",
      "Iter 10545, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10546, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10547, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10548, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10549, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10550, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10551, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10552, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10553, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10554, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10555, training loss 0.000000, validation loss 0.023097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10556, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10557, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10558, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10559, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10560, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10561, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10562, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10563, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10564, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10565, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10566, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10567, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10568, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10569, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10570, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10571, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10572, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10573, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10574, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10575, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10576, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10577, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10578, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10579, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10580, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10581, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10582, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10583, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10584, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10585, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10586, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10587, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10588, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10589, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10590, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10591, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10592, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10593, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10594, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10595, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10596, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10597, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10598, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10599, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10600, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10601, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10602, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10603, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10604, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10605, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10606, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10607, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10608, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10609, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10610, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10611, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10612, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10613, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10614, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10615, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10616, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10617, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10618, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10619, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10620, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10621, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10622, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10623, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10624, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10625, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10626, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10627, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10628, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10629, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10630, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10631, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10632, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10633, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10634, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10635, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10636, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10637, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10638, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10639, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10640, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10641, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10642, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10643, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10644, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10645, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10646, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10647, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10648, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10649, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10650, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10651, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10652, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10653, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10654, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10655, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10656, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10657, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10658, training loss 0.000000, validation loss 0.023094\n",
      "Iter 10659, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10660, training loss 0.000000, validation loss 0.023093\n",
      "Iter 10661, training loss 0.000000, validation loss 0.023095\n",
      "Iter 10662, training loss 0.000000, validation loss 0.023093\n",
      "Iter 10663, training loss 0.000000, validation loss 0.023096\n",
      "Iter 10664, training loss 0.000000, validation loss 0.023092\n",
      "Iter 10665, training loss 0.000000, validation loss 0.023097\n",
      "Iter 10666, training loss 0.000000, validation loss 0.023091\n",
      "Iter 10667, training loss 0.000000, validation loss 0.023099\n",
      "Iter 10668, training loss 0.000001, validation loss 0.023089\n",
      "Iter 10669, training loss 0.000001, validation loss 0.023103\n",
      "Iter 10670, training loss 0.000001, validation loss 0.023085\n",
      "Iter 10671, training loss 0.000002, validation loss 0.023110\n",
      "Iter 10672, training loss 0.000004, validation loss 0.023080\n",
      "Iter 10673, training loss 0.000007, validation loss 0.023126\n",
      "Iter 10674, training loss 0.000012, validation loss 0.023075\n",
      "Iter 10675, training loss 0.000021, validation loss 0.023161\n",
      "Iter 10676, training loss 0.000038, validation loss 0.023080\n",
      "Iter 10677, training loss 0.000069, validation loss 0.023249\n",
      "Iter 10678, training loss 0.000125, validation loss 0.023132\n",
      "Iter 10679, training loss 0.000227, validation loss 0.023494\n",
      "Iter 10680, training loss 0.000416, validation loss 0.023379\n",
      "Iter 10681, training loss 0.000762, validation loss 0.024224\n",
      "Iter 10682, training loss 0.001398, validation loss 0.024343\n",
      "Iter 10683, training loss 0.002551, validation loss 0.026487\n",
      "Iter 10684, training loss 0.004614, validation loss 0.027737\n",
      "Iter 10685, training loss 0.008118, validation loss 0.033219\n",
      "Iter 10686, training loss 0.013672, validation loss 0.037641\n",
      "Iter 10687, training loss 0.020944, validation loss 0.048380\n",
      "Iter 10688, training loss 0.027437, validation loss 0.052768\n",
      "Iter 10689, training loss 0.026582, validation loss 0.054604\n",
      "Iter 10690, training loss 0.014873, validation loss 0.037750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10691, training loss 0.001852, validation loss 0.023968\n",
      "Iter 10692, training loss 0.002938, validation loss 0.025135\n",
      "Iter 10693, training loss 0.010524, validation loss 0.032105\n",
      "Iter 10694, training loss 0.006420, validation loss 0.029220\n",
      "Iter 10695, training loss 0.000341, validation loss 0.021165\n",
      "Iter 10696, training loss 0.004966, validation loss 0.025728\n",
      "Iter 10697, training loss 0.005288, validation loss 0.027581\n",
      "Iter 10698, training loss 0.000383, validation loss 0.020925\n",
      "Iter 10699, training loss 0.003218, validation loss 0.023732\n",
      "Iter 10700, training loss 0.003450, validation loss 0.024989\n",
      "Iter 10701, training loss 0.000257, validation loss 0.020630\n",
      "Iter 10702, training loss 0.002720, validation loss 0.023056\n",
      "Iter 10703, training loss 0.001843, validation loss 0.022657\n",
      "Iter 10704, training loss 0.000392, validation loss 0.020681\n",
      "Iter 10705, training loss 0.002299, validation loss 0.022456\n",
      "Iter 10706, training loss 0.000652, validation loss 0.020977\n",
      "Iter 10707, training loss 0.000830, validation loss 0.021203\n",
      "Iter 10708, training loss 0.001543, validation loss 0.021519\n",
      "Iter 10709, training loss 0.000158, validation loss 0.020187\n",
      "Iter 10710, training loss 0.001145, validation loss 0.021602\n",
      "Iter 10711, training loss 0.000668, validation loss 0.020518\n",
      "Iter 10712, training loss 0.000304, validation loss 0.020152\n",
      "Iter 10713, training loss 0.000976, validation loss 0.021317\n",
      "Iter 10714, training loss 0.000153, validation loss 0.019988\n",
      "Iter 10715, training loss 0.000595, validation loss 0.020356\n",
      "Iter 10716, training loss 0.000482, validation loss 0.020567\n",
      "Iter 10717, training loss 0.000152, validation loss 0.020055\n",
      "Iter 10718, training loss 0.000589, validation loss 0.020308\n",
      "Iter 10719, training loss 0.000123, validation loss 0.019979\n",
      "Iter 10720, training loss 0.000338, validation loss 0.020302\n",
      "Iter 10721, training loss 0.000318, validation loss 0.020008\n",
      "Iter 10722, training loss 0.000093, validation loss 0.019823\n",
      "Iter 10723, training loss 0.000359, validation loss 0.020322\n",
      "Iter 10724, training loss 0.000092, validation loss 0.019815\n",
      "Iter 10725, training loss 0.000201, validation loss 0.019895\n",
      "Iter 10726, training loss 0.000205, validation loss 0.020092\n",
      "Iter 10727, training loss 0.000062, validation loss 0.019851\n",
      "Iter 10728, training loss 0.000221, validation loss 0.019919\n",
      "Iter 10729, training loss 0.000067, validation loss 0.019865\n",
      "Iter 10730, training loss 0.000124, validation loss 0.019960\n",
      "Iter 10731, training loss 0.000134, validation loss 0.019848\n",
      "Iter 10732, training loss 0.000043, validation loss 0.019792\n",
      "Iter 10733, training loss 0.000138, validation loss 0.019990\n",
      "Iter 10734, training loss 0.000050, validation loss 0.019796\n",
      "Iter 10735, training loss 0.000077, validation loss 0.019813\n",
      "Iter 10736, training loss 0.000090, validation loss 0.019921\n",
      "Iter 10737, training loss 0.000032, validation loss 0.019812\n",
      "Iter 10738, training loss 0.000088, validation loss 0.019823\n",
      "Iter 10739, training loss 0.000039, validation loss 0.019836\n",
      "Iter 10740, training loss 0.000050, validation loss 0.019859\n",
      "Iter 10741, training loss 0.000062, validation loss 0.019808\n",
      "Iter 10742, training loss 0.000024, validation loss 0.019800\n",
      "Iter 10743, training loss 0.000057, validation loss 0.019883\n",
      "Iter 10744, training loss 0.000031, validation loss 0.019800\n",
      "Iter 10745, training loss 0.000033, validation loss 0.019803\n",
      "Iter 10746, training loss 0.000044, validation loss 0.019870\n",
      "Iter 10747, training loss 0.000019, validation loss 0.019814\n",
      "Iter 10748, training loss 0.000038, validation loss 0.019812\n",
      "Iter 10749, training loss 0.000025, validation loss 0.019844\n",
      "Iter 10750, training loss 0.000022, validation loss 0.019841\n",
      "Iter 10751, training loss 0.000032, validation loss 0.019818\n",
      "Iter 10752, training loss 0.000016, validation loss 0.019828\n",
      "Iter 10753, training loss 0.000026, validation loss 0.019860\n",
      "Iter 10754, training loss 0.000021, validation loss 0.019822\n",
      "Iter 10755, training loss 0.000016, validation loss 0.019826\n",
      "Iter 10756, training loss 0.000023, validation loss 0.019863\n",
      "Iter 10757, training loss 0.000014, validation loss 0.019832\n",
      "Iter 10758, training loss 0.000018, validation loss 0.019831\n",
      "Iter 10759, training loss 0.000017, validation loss 0.019858\n",
      "Iter 10760, training loss 0.000013, validation loss 0.019847\n",
      "Iter 10761, training loss 0.000018, validation loss 0.019836\n",
      "Iter 10762, training loss 0.000012, validation loss 0.019850\n",
      "Iter 10763, training loss 0.000014, validation loss 0.019857\n",
      "Iter 10764, training loss 0.000014, validation loss 0.019839\n",
      "Iter 10765, training loss 0.000011, validation loss 0.019846\n",
      "Iter 10766, training loss 0.000013, validation loss 0.019863\n",
      "Iter 10767, training loss 0.000011, validation loss 0.019846\n",
      "Iter 10768, training loss 0.000011, validation loss 0.019848\n",
      "Iter 10769, training loss 0.000012, validation loss 0.019866\n",
      "Iter 10770, training loss 0.000009, validation loss 0.019857\n",
      "Iter 10771, training loss 0.000011, validation loss 0.019854\n",
      "Iter 10772, training loss 0.000010, validation loss 0.019869\n",
      "Iter 10773, training loss 0.000009, validation loss 0.019868\n",
      "Iter 10774, training loss 0.000010, validation loss 0.019862\n",
      "Iter 10775, training loss 0.000008, validation loss 0.019872\n",
      "Iter 10776, training loss 0.000009, validation loss 0.019877\n",
      "Iter 10777, training loss 0.000009, validation loss 0.019870\n",
      "Iter 10778, training loss 0.000008, validation loss 0.019875\n",
      "Iter 10779, training loss 0.000008, validation loss 0.019885\n",
      "Iter 10780, training loss 0.000008, validation loss 0.019878\n",
      "Iter 10781, training loss 0.000007, validation loss 0.019880\n",
      "Iter 10782, training loss 0.000008, validation loss 0.019890\n",
      "Iter 10783, training loss 0.000007, validation loss 0.019886\n",
      "Iter 10784, training loss 0.000007, validation loss 0.019885\n",
      "Iter 10785, training loss 0.000007, validation loss 0.019893\n",
      "Iter 10786, training loss 0.000006, validation loss 0.019893\n",
      "Iter 10787, training loss 0.000007, validation loss 0.019890\n",
      "Iter 10788, training loss 0.000006, validation loss 0.019896\n",
      "Iter 10789, training loss 0.000006, validation loss 0.019899\n",
      "Iter 10790, training loss 0.000006, validation loss 0.019895\n",
      "Iter 10791, training loss 0.000006, validation loss 0.019899\n",
      "Iter 10792, training loss 0.000006, validation loss 0.019903\n",
      "Iter 10793, training loss 0.000006, validation loss 0.019899\n",
      "Iter 10794, training loss 0.000005, validation loss 0.019901\n",
      "Iter 10795, training loss 0.000005, validation loss 0.019905\n",
      "Iter 10796, training loss 0.000005, validation loss 0.019903\n",
      "Iter 10797, training loss 0.000005, validation loss 0.019903\n",
      "Iter 10798, training loss 0.000005, validation loss 0.019908\n",
      "Iter 10799, training loss 0.000005, validation loss 0.019907\n",
      "Iter 10800, training loss 0.000005, validation loss 0.019907\n",
      "Iter 10801, training loss 0.000005, validation loss 0.019910\n",
      "Iter 10802, training loss 0.000005, validation loss 0.019911\n",
      "Iter 10803, training loss 0.000005, validation loss 0.019910\n",
      "Iter 10804, training loss 0.000005, validation loss 0.019913\n",
      "Iter 10805, training loss 0.000004, validation loss 0.019914\n",
      "Iter 10806, training loss 0.000004, validation loss 0.019913\n",
      "Iter 10807, training loss 0.000004, validation loss 0.019915\n",
      "Iter 10808, training loss 0.000004, validation loss 0.019917\n",
      "Iter 10809, training loss 0.000004, validation loss 0.019916\n",
      "Iter 10810, training loss 0.000004, validation loss 0.019917\n",
      "Iter 10811, training loss 0.000004, validation loss 0.019920\n",
      "Iter 10812, training loss 0.000004, validation loss 0.019920\n",
      "Iter 10813, training loss 0.000004, validation loss 0.019920\n",
      "Iter 10814, training loss 0.000004, validation loss 0.019923\n",
      "Iter 10815, training loss 0.000004, validation loss 0.019923\n",
      "Iter 10816, training loss 0.000004, validation loss 0.019923\n",
      "Iter 10817, training loss 0.000004, validation loss 0.019925\n",
      "Iter 10818, training loss 0.000004, validation loss 0.019926\n",
      "Iter 10819, training loss 0.000004, validation loss 0.019926\n",
      "Iter 10820, training loss 0.000003, validation loss 0.019927\n",
      "Iter 10821, training loss 0.000003, validation loss 0.019928\n",
      "Iter 10822, training loss 0.000003, validation loss 0.019928\n",
      "Iter 10823, training loss 0.000003, validation loss 0.019930\n",
      "Iter 10824, training loss 0.000003, validation loss 0.019931\n",
      "Iter 10825, training loss 0.000003, validation loss 0.019931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10826, training loss 0.000003, validation loss 0.019932\n",
      "Iter 10827, training loss 0.000003, validation loss 0.019934\n",
      "Iter 10828, training loss 0.000003, validation loss 0.019934\n",
      "Iter 10829, training loss 0.000003, validation loss 0.019935\n",
      "Iter 10830, training loss 0.000003, validation loss 0.019937\n",
      "Iter 10831, training loss 0.000003, validation loss 0.019937\n",
      "Iter 10832, training loss 0.000003, validation loss 0.019938\n",
      "Iter 10833, training loss 0.000003, validation loss 0.019939\n",
      "Iter 10834, training loss 0.000003, validation loss 0.019940\n",
      "Iter 10835, training loss 0.000003, validation loss 0.019940\n",
      "Iter 10836, training loss 0.000003, validation loss 0.019942\n",
      "Iter 10837, training loss 0.000003, validation loss 0.019942\n",
      "Iter 10838, training loss 0.000003, validation loss 0.019943\n",
      "Iter 10839, training loss 0.000003, validation loss 0.019944\n",
      "Iter 10840, training loss 0.000003, validation loss 0.019945\n",
      "Iter 10841, training loss 0.000003, validation loss 0.019945\n",
      "Iter 10842, training loss 0.000003, validation loss 0.019946\n",
      "Iter 10843, training loss 0.000003, validation loss 0.019947\n",
      "Iter 10844, training loss 0.000002, validation loss 0.019948\n",
      "Iter 10845, training loss 0.000002, validation loss 0.019949\n",
      "Iter 10846, training loss 0.000002, validation loss 0.019950\n",
      "Iter 10847, training loss 0.000002, validation loss 0.019951\n",
      "Iter 10848, training loss 0.000002, validation loss 0.019951\n",
      "Iter 10849, training loss 0.000002, validation loss 0.019953\n",
      "Iter 10850, training loss 0.000002, validation loss 0.019953\n",
      "Iter 10851, training loss 0.000002, validation loss 0.019954\n",
      "Iter 10852, training loss 0.000002, validation loss 0.019955\n",
      "Iter 10853, training loss 0.000002, validation loss 0.019956\n",
      "Iter 10854, training loss 0.000002, validation loss 0.019956\n",
      "Iter 10855, training loss 0.000002, validation loss 0.019957\n",
      "Iter 10856, training loss 0.000002, validation loss 0.019958\n",
      "Iter 10857, training loss 0.000002, validation loss 0.019959\n",
      "Iter 10858, training loss 0.000002, validation loss 0.019960\n",
      "Iter 10859, training loss 0.000002, validation loss 0.019961\n",
      "Iter 10860, training loss 0.000002, validation loss 0.019961\n",
      "Iter 10861, training loss 0.000002, validation loss 0.019962\n",
      "Iter 10862, training loss 0.000002, validation loss 0.019963\n",
      "Iter 10863, training loss 0.000002, validation loss 0.019964\n",
      "Iter 10864, training loss 0.000002, validation loss 0.019965\n",
      "Iter 10865, training loss 0.000002, validation loss 0.019966\n",
      "Iter 10866, training loss 0.000002, validation loss 0.019966\n",
      "Iter 10867, training loss 0.000002, validation loss 0.019967\n",
      "Iter 10868, training loss 0.000002, validation loss 0.019968\n",
      "Iter 10869, training loss 0.000002, validation loss 0.019969\n",
      "Iter 10870, training loss 0.000002, validation loss 0.019969\n",
      "Iter 10871, training loss 0.000002, validation loss 0.019970\n",
      "Iter 10872, training loss 0.000002, validation loss 0.019971\n",
      "Iter 10873, training loss 0.000002, validation loss 0.019972\n",
      "Iter 10874, training loss 0.000002, validation loss 0.019972\n",
      "Iter 10875, training loss 0.000002, validation loss 0.019973\n",
      "Iter 10876, training loss 0.000002, validation loss 0.019974\n",
      "Iter 10877, training loss 0.000002, validation loss 0.019975\n",
      "Iter 10878, training loss 0.000002, validation loss 0.019975\n",
      "Iter 10879, training loss 0.000002, validation loss 0.019976\n",
      "Iter 10880, training loss 0.000002, validation loss 0.019976\n",
      "Iter 10881, training loss 0.000002, validation loss 0.019977\n",
      "Iter 10882, training loss 0.000002, validation loss 0.019978\n",
      "Iter 10883, training loss 0.000002, validation loss 0.019978\n",
      "Iter 10884, training loss 0.000002, validation loss 0.019979\n",
      "Iter 10885, training loss 0.000002, validation loss 0.019979\n",
      "Iter 10886, training loss 0.000002, validation loss 0.019980\n",
      "Iter 10887, training loss 0.000002, validation loss 0.019981\n",
      "Iter 10888, training loss 0.000001, validation loss 0.019981\n",
      "Iter 10889, training loss 0.000001, validation loss 0.019982\n",
      "Iter 10890, training loss 0.000001, validation loss 0.019982\n",
      "Iter 10891, training loss 0.000001, validation loss 0.019983\n",
      "Iter 10892, training loss 0.000001, validation loss 0.019983\n",
      "Iter 10893, training loss 0.000001, validation loss 0.019984\n",
      "Iter 10894, training loss 0.000001, validation loss 0.019984\n",
      "Iter 10895, training loss 0.000001, validation loss 0.019985\n",
      "Iter 10896, training loss 0.000001, validation loss 0.019985\n",
      "Iter 10897, training loss 0.000001, validation loss 0.019986\n",
      "Iter 10898, training loss 0.000001, validation loss 0.019987\n",
      "Iter 10899, training loss 0.000001, validation loss 0.019987\n",
      "Iter 10900, training loss 0.000001, validation loss 0.019988\n",
      "Iter 10901, training loss 0.000001, validation loss 0.019988\n",
      "Iter 10902, training loss 0.000001, validation loss 0.019989\n",
      "Iter 10903, training loss 0.000001, validation loss 0.019989\n",
      "Iter 10904, training loss 0.000001, validation loss 0.019990\n",
      "Iter 10905, training loss 0.000001, validation loss 0.019990\n",
      "Iter 10906, training loss 0.000001, validation loss 0.019990\n",
      "Iter 10907, training loss 0.000001, validation loss 0.019991\n",
      "Iter 10908, training loss 0.000001, validation loss 0.019991\n",
      "Iter 10909, training loss 0.000001, validation loss 0.019992\n",
      "Iter 10910, training loss 0.000001, validation loss 0.019992\n",
      "Iter 10911, training loss 0.000001, validation loss 0.019993\n",
      "Iter 10912, training loss 0.000001, validation loss 0.019993\n",
      "Iter 10913, training loss 0.000001, validation loss 0.019993\n",
      "Iter 10914, training loss 0.000001, validation loss 0.019994\n",
      "Iter 10915, training loss 0.000001, validation loss 0.019994\n",
      "Iter 10916, training loss 0.000001, validation loss 0.019994\n",
      "Iter 10917, training loss 0.000001, validation loss 0.019995\n",
      "Iter 10918, training loss 0.000001, validation loss 0.019995\n",
      "Iter 10919, training loss 0.000001, validation loss 0.019995\n",
      "Iter 10920, training loss 0.000001, validation loss 0.019996\n",
      "Iter 10921, training loss 0.000001, validation loss 0.019996\n",
      "Iter 10922, training loss 0.000001, validation loss 0.019996\n",
      "Iter 10923, training loss 0.000001, validation loss 0.019997\n",
      "Iter 10924, training loss 0.000001, validation loss 0.019997\n",
      "Iter 10925, training loss 0.000001, validation loss 0.019997\n",
      "Iter 10926, training loss 0.000001, validation loss 0.019998\n",
      "Iter 10927, training loss 0.000001, validation loss 0.019998\n",
      "Iter 10928, training loss 0.000001, validation loss 0.019998\n",
      "Iter 10929, training loss 0.000001, validation loss 0.019999\n",
      "Iter 10930, training loss 0.000001, validation loss 0.019999\n",
      "Iter 10931, training loss 0.000001, validation loss 0.019999\n",
      "Iter 10932, training loss 0.000001, validation loss 0.020000\n",
      "Iter 10933, training loss 0.000001, validation loss 0.020000\n",
      "Iter 10934, training loss 0.000001, validation loss 0.020000\n",
      "Iter 10935, training loss 0.000001, validation loss 0.020001\n",
      "Iter 10936, training loss 0.000001, validation loss 0.020001\n",
      "Iter 10937, training loss 0.000001, validation loss 0.020001\n",
      "Iter 10938, training loss 0.000001, validation loss 0.020002\n",
      "Iter 10939, training loss 0.000001, validation loss 0.020002\n",
      "Iter 10940, training loss 0.000001, validation loss 0.020002\n",
      "Iter 10941, training loss 0.000001, validation loss 0.020003\n",
      "Iter 10942, training loss 0.000001, validation loss 0.020003\n",
      "Iter 10943, training loss 0.000001, validation loss 0.020003\n",
      "Iter 10944, training loss 0.000001, validation loss 0.020003\n",
      "Iter 10945, training loss 0.000001, validation loss 0.020004\n",
      "Iter 10946, training loss 0.000001, validation loss 0.020004\n",
      "Iter 10947, training loss 0.000001, validation loss 0.020004\n",
      "Iter 10948, training loss 0.000001, validation loss 0.020005\n",
      "Iter 10949, training loss 0.000001, validation loss 0.020005\n",
      "Iter 10950, training loss 0.000001, validation loss 0.020005\n",
      "Iter 10951, training loss 0.000001, validation loss 0.020006\n",
      "Iter 10952, training loss 0.000001, validation loss 0.020006\n",
      "Iter 10953, training loss 0.000001, validation loss 0.020006\n",
      "Iter 10954, training loss 0.000001, validation loss 0.020006\n",
      "Iter 10955, training loss 0.000001, validation loss 0.020007\n",
      "Iter 10956, training loss 0.000001, validation loss 0.020007\n",
      "Iter 10957, training loss 0.000001, validation loss 0.020007\n",
      "Iter 10958, training loss 0.000001, validation loss 0.020008\n",
      "Iter 10959, training loss 0.000001, validation loss 0.020008\n",
      "Iter 10960, training loss 0.000001, validation loss 0.020008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10961, training loss 0.000001, validation loss 0.020008\n",
      "Iter 10962, training loss 0.000001, validation loss 0.020009\n",
      "Iter 10963, training loss 0.000001, validation loss 0.020009\n",
      "Iter 10964, training loss 0.000001, validation loss 0.020009\n",
      "Iter 10965, training loss 0.000001, validation loss 0.020009\n",
      "Iter 10966, training loss 0.000001, validation loss 0.020010\n",
      "Iter 10967, training loss 0.000001, validation loss 0.020010\n",
      "Iter 10968, training loss 0.000001, validation loss 0.020010\n",
      "Iter 10969, training loss 0.000001, validation loss 0.020011\n",
      "Iter 10970, training loss 0.000001, validation loss 0.020011\n",
      "Iter 10971, training loss 0.000001, validation loss 0.020011\n",
      "Iter 10972, training loss 0.000001, validation loss 0.020011\n",
      "Iter 10973, training loss 0.000001, validation loss 0.020011\n",
      "Iter 10974, training loss 0.000001, validation loss 0.020012\n",
      "Iter 10975, training loss 0.000001, validation loss 0.020012\n",
      "Iter 10976, training loss 0.000001, validation loss 0.020012\n",
      "Iter 10977, training loss 0.000001, validation loss 0.020013\n",
      "Iter 10978, training loss 0.000001, validation loss 0.020013\n",
      "Iter 10979, training loss 0.000001, validation loss 0.020013\n",
      "Iter 10980, training loss 0.000001, validation loss 0.020013\n",
      "Iter 10981, training loss 0.000001, validation loss 0.020013\n",
      "Iter 10982, training loss 0.000001, validation loss 0.020014\n",
      "Iter 10983, training loss 0.000001, validation loss 0.020014\n",
      "Iter 10984, training loss 0.000001, validation loss 0.020014\n",
      "Iter 10985, training loss 0.000001, validation loss 0.020014\n",
      "Iter 10986, training loss 0.000001, validation loss 0.020015\n",
      "Iter 10987, training loss 0.000001, validation loss 0.020015\n",
      "Iter 10988, training loss 0.000001, validation loss 0.020015\n",
      "Iter 10989, training loss 0.000001, validation loss 0.020015\n",
      "Iter 10990, training loss 0.000001, validation loss 0.020015\n",
      "Iter 10991, training loss 0.000001, validation loss 0.020016\n",
      "Iter 10992, training loss 0.000001, validation loss 0.020016\n",
      "Iter 10993, training loss 0.000001, validation loss 0.020016\n",
      "Iter 10994, training loss 0.000001, validation loss 0.020016\n",
      "Iter 10995, training loss 0.000001, validation loss 0.020017\n",
      "Iter 10996, training loss 0.000001, validation loss 0.020017\n",
      "Iter 10997, training loss 0.000001, validation loss 0.020017\n",
      "Iter 10998, training loss 0.000001, validation loss 0.020017\n",
      "Iter 10999, training loss 0.000001, validation loss 0.020017\n",
      "Iter 11000, training loss 0.000001, validation loss 0.020018\n",
      "Iter 11001, training loss 0.000001, validation loss 0.020018\n",
      "Iter 11002, training loss 0.000001, validation loss 0.020018\n",
      "Iter 11003, training loss 0.000001, validation loss 0.020018\n",
      "Iter 11004, training loss 0.000001, validation loss 0.020018\n",
      "Iter 11005, training loss 0.000001, validation loss 0.020019\n",
      "Iter 11006, training loss 0.000001, validation loss 0.020019\n",
      "Iter 11007, training loss 0.000001, validation loss 0.020019\n",
      "Iter 11008, training loss 0.000001, validation loss 0.020019\n",
      "Iter 11009, training loss 0.000001, validation loss 0.020020\n",
      "Iter 11010, training loss 0.000001, validation loss 0.020020\n",
      "Iter 11011, training loss 0.000001, validation loss 0.020020\n",
      "Iter 11012, training loss 0.000001, validation loss 0.020020\n",
      "Iter 11013, training loss 0.000001, validation loss 0.020020\n",
      "Iter 11014, training loss 0.000001, validation loss 0.020021\n",
      "Iter 11015, training loss 0.000001, validation loss 0.020021\n",
      "Iter 11016, training loss 0.000001, validation loss 0.020021\n",
      "Iter 11017, training loss 0.000001, validation loss 0.020021\n",
      "Iter 11018, training loss 0.000000, validation loss 0.020021\n",
      "Iter 11019, training loss 0.000000, validation loss 0.020021\n",
      "Iter 11020, training loss 0.000000, validation loss 0.020022\n",
      "Iter 11021, training loss 0.000000, validation loss 0.020022\n",
      "Iter 11022, training loss 0.000000, validation loss 0.020022\n",
      "Iter 11023, training loss 0.000000, validation loss 0.020022\n",
      "Iter 11024, training loss 0.000000, validation loss 0.020022\n",
      "Iter 11025, training loss 0.000000, validation loss 0.020023\n",
      "Iter 11026, training loss 0.000000, validation loss 0.020023\n",
      "Iter 11027, training loss 0.000000, validation loss 0.020023\n",
      "Iter 11028, training loss 0.000000, validation loss 0.020023\n",
      "Iter 11029, training loss 0.000000, validation loss 0.020023\n",
      "Iter 11030, training loss 0.000000, validation loss 0.020023\n",
      "Iter 11031, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11032, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11033, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11034, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11035, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11036, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11037, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11038, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11039, training loss 0.000000, validation loss 0.020024\n",
      "Iter 11040, training loss 0.000000, validation loss 0.020025\n",
      "Iter 11041, training loss 0.000000, validation loss 0.020025\n",
      "Iter 11042, training loss 0.000000, validation loss 0.020025\n",
      "Iter 11043, training loss 0.000000, validation loss 0.020025\n",
      "Iter 11044, training loss 0.000000, validation loss 0.020025\n",
      "Iter 11045, training loss 0.000000, validation loss 0.020025\n",
      "Iter 11046, training loss 0.000000, validation loss 0.020025\n",
      "Iter 11047, training loss 0.000000, validation loss 0.020025\n",
      "Iter 11048, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11049, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11050, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11051, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11052, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11053, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11054, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11055, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11056, training loss 0.000000, validation loss 0.020026\n",
      "Iter 11057, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11058, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11059, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11060, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11061, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11062, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11063, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11064, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11065, training loss 0.000000, validation loss 0.020027\n",
      "Iter 11066, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11067, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11068, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11069, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11070, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11071, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11072, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11073, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11074, training loss 0.000000, validation loss 0.020028\n",
      "Iter 11075, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11076, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11077, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11078, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11079, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11080, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11081, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11082, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11083, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11084, training loss 0.000000, validation loss 0.020029\n",
      "Iter 11085, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11086, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11087, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11088, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11089, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11090, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11091, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11092, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11093, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11094, training loss 0.000000, validation loss 0.020030\n",
      "Iter 11095, training loss 0.000000, validation loss 0.020031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11096, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11097, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11098, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11099, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11100, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11101, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11102, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11103, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11104, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11105, training loss 0.000000, validation loss 0.020031\n",
      "Iter 11106, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11107, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11108, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11109, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11110, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11111, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11112, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11113, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11114, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11115, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11116, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11117, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11118, training loss 0.000000, validation loss 0.020032\n",
      "Iter 11119, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11120, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11121, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11122, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11123, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11124, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11125, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11126, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11127, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11128, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11129, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11130, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11131, training loss 0.000000, validation loss 0.020033\n",
      "Iter 11132, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11133, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11134, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11135, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11136, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11137, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11138, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11139, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11140, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11141, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11142, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11143, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11144, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11145, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11146, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11147, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11148, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11149, training loss 0.000000, validation loss 0.020034\n",
      "Iter 11150, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11151, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11152, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11153, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11154, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11155, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11156, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11157, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11158, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11159, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11160, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11161, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11162, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11163, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11164, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11165, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11166, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11167, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11168, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11169, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11170, training loss 0.000000, validation loss 0.020035\n",
      "Iter 11171, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11172, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11173, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11174, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11175, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11176, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11177, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11178, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11179, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11180, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11181, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11182, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11183, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11184, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11185, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11186, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11187, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11188, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11189, training loss 0.000000, validation loss 0.020036\n",
      "Iter 11190, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11191, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11192, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11193, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11194, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11195, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11196, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11197, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11198, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11199, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11200, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11201, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11202, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11203, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11204, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11205, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11206, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11207, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11208, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11209, training loss 0.000000, validation loss 0.020037\n",
      "Iter 11210, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11211, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11212, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11213, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11214, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11215, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11216, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11217, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11218, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11219, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11220, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11221, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11222, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11223, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11224, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11225, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11226, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11227, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11228, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11229, training loss 0.000000, validation loss 0.020038\n",
      "Iter 11230, training loss 0.000000, validation loss 0.020039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11231, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11232, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11233, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11234, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11235, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11236, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11237, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11238, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11239, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11240, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11241, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11242, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11243, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11244, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11245, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11246, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11247, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11248, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11249, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11250, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11251, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11252, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11253, training loss 0.000000, validation loss 0.020039\n",
      "Iter 11254, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11255, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11256, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11257, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11258, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11259, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11260, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11261, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11262, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11263, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11264, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11265, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11266, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11267, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11268, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11269, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11270, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11271, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11272, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11273, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11274, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11275, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11276, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11277, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11278, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11279, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11280, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11281, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11282, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11283, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11284, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11285, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11286, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11287, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11288, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11289, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11290, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11291, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11292, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11293, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11294, training loss 0.000000, validation loss 0.020040\n",
      "Iter 11295, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11296, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11297, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11298, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11299, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11300, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11301, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11302, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11303, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11304, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11305, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11306, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11307, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11308, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11309, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11310, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11311, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11312, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11313, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11314, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11315, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11316, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11317, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11318, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11319, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11320, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11321, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11322, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11323, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11324, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11325, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11326, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11327, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11328, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11329, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11330, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11331, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11332, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11333, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11334, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11335, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11336, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11337, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11338, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11339, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11340, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11341, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11342, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11343, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11344, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11345, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11346, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11347, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11348, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11349, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11350, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11351, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11352, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11353, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11354, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11355, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11356, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11357, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11358, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11359, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11360, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11361, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11362, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11363, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11364, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11365, training loss 0.000000, validation loss 0.020041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11366, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11367, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11368, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11369, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11370, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11371, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11372, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11373, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11374, training loss 0.000000, validation loss 0.020041\n",
      "Iter 11375, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11376, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11377, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11378, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11379, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11380, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11381, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11382, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11383, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11384, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11385, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11386, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11387, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11388, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11389, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11390, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11391, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11392, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11393, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11394, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11395, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11396, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11397, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11398, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11399, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11400, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11401, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11402, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11403, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11404, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11405, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11406, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11407, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11408, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11409, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11410, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11411, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11412, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11413, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11414, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11415, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11416, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11417, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11418, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11419, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11420, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11421, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11422, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11423, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11424, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11425, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11426, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11427, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11428, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11429, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11430, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11431, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11432, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11433, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11434, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11435, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11436, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11437, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11438, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11439, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11440, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11441, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11442, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11443, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11444, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11445, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11446, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11447, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11448, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11449, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11450, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11451, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11452, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11453, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11454, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11455, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11456, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11457, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11458, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11459, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11460, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11461, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11462, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11463, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11464, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11465, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11466, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11467, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11468, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11469, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11470, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11471, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11472, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11473, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11474, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11475, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11476, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11477, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11478, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11479, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11480, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11481, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11482, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11483, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11484, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11485, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11486, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11487, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11488, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11489, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11490, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11491, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11492, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11493, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11494, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11495, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11496, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11497, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11498, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11499, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11500, training loss 0.000000, validation loss 0.020043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11501, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11502, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11503, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11504, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11505, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11506, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11507, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11508, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11509, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11510, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11511, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11512, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11513, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11514, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11515, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11516, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11517, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11518, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11519, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11520, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11521, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11522, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11523, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11524, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11525, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11526, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11527, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11528, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11529, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11530, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11531, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11532, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11533, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11534, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11535, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11536, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11537, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11538, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11539, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11540, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11541, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11542, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11543, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11544, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11545, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11546, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11547, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11548, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11549, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11550, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11551, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11552, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11553, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11554, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11555, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11556, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11557, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11558, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11559, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11560, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11561, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11562, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11563, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11564, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11565, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11566, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11567, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11568, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11569, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11570, training loss 0.000000, validation loss 0.020044\n",
      "Iter 11571, training loss 0.000000, validation loss 0.020045\n",
      "Iter 11572, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11573, training loss 0.000000, validation loss 0.020045\n",
      "Iter 11574, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11575, training loss 0.000000, validation loss 0.020045\n",
      "Iter 11576, training loss 0.000000, validation loss 0.020043\n",
      "Iter 11577, training loss 0.000000, validation loss 0.020046\n",
      "Iter 11578, training loss 0.000000, validation loss 0.020042\n",
      "Iter 11579, training loss 0.000000, validation loss 0.020048\n",
      "Iter 11580, training loss 0.000001, validation loss 0.020040\n",
      "Iter 11581, training loss 0.000001, validation loss 0.020051\n",
      "Iter 11582, training loss 0.000001, validation loss 0.020039\n",
      "Iter 11583, training loss 0.000002, validation loss 0.020056\n",
      "Iter 11584, training loss 0.000004, validation loss 0.020037\n",
      "Iter 11585, training loss 0.000007, validation loss 0.020068\n",
      "Iter 11586, training loss 0.000012, validation loss 0.020037\n",
      "Iter 11587, training loss 0.000021, validation loss 0.020097\n",
      "Iter 11588, training loss 0.000037, validation loss 0.020050\n",
      "Iter 11589, training loss 0.000065, validation loss 0.020169\n",
      "Iter 11590, training loss 0.000115, validation loss 0.020112\n",
      "Iter 11591, training loss 0.000203, validation loss 0.020368\n",
      "Iter 11592, training loss 0.000362, validation loss 0.020349\n",
      "Iter 11593, training loss 0.000643, validation loss 0.020948\n",
      "Iter 11594, training loss 0.001143, validation loss 0.021171\n",
      "Iter 11595, training loss 0.002011, validation loss 0.022667\n",
      "Iter 11596, training loss 0.003486, validation loss 0.023749\n",
      "Iter 11597, training loss 0.005809, validation loss 0.027295\n",
      "Iter 11598, training loss 0.009116, validation loss 0.030052\n",
      "Iter 11599, training loss 0.012632, validation loss 0.035443\n",
      "Iter 11600, training loss 0.014369, validation loss 0.035717\n",
      "Iter 11601, training loss 0.011272, validation loss 0.033381\n",
      "Iter 11602, training loss 0.004322, validation loss 0.023527\n",
      "Iter 11603, training loss 0.000204, validation loss 0.018914\n",
      "Iter 11604, training loss 0.002880, validation loss 0.022360\n",
      "Iter 11605, training loss 0.005471, validation loss 0.024370\n",
      "Iter 11606, training loss 0.002401, validation loss 0.021644\n",
      "Iter 11607, training loss 0.000221, validation loss 0.018638\n",
      "Iter 11608, training loss 0.002623, validation loss 0.021002\n",
      "Iter 11609, training loss 0.002704, validation loss 0.021871\n",
      "Iter 11610, training loss 0.000283, validation loss 0.018425\n",
      "Iter 11611, training loss 0.001199, validation loss 0.019324\n",
      "Iter 11612, training loss 0.002065, validation loss 0.020858\n",
      "Iter 11613, training loss 0.000360, validation loss 0.018352\n",
      "Iter 11614, training loss 0.000708, validation loss 0.018671\n",
      "Iter 11615, training loss 0.001468, validation loss 0.019983\n",
      "Iter 11616, training loss 0.000269, validation loss 0.018152\n",
      "Iter 11617, training loss 0.000541, validation loss 0.018393\n",
      "Iter 11618, training loss 0.001028, validation loss 0.019344\n",
      "Iter 11619, training loss 0.000154, validation loss 0.017969\n",
      "Iter 11620, training loss 0.000478, validation loss 0.018249\n",
      "Iter 11621, training loss 0.000694, validation loss 0.018833\n",
      "Iter 11622, training loss 0.000078, validation loss 0.017843\n",
      "Iter 11623, training loss 0.000438, validation loss 0.018137\n",
      "Iter 11624, training loss 0.000437, validation loss 0.018408\n",
      "Iter 11625, training loss 0.000055, validation loss 0.017785\n",
      "Iter 11626, training loss 0.000389, validation loss 0.018035\n",
      "Iter 11627, training loss 0.000249, validation loss 0.018094\n",
      "Iter 11628, training loss 0.000066, validation loss 0.017797\n",
      "Iter 11629, training loss 0.000321, validation loss 0.017948\n",
      "Iter 11630, training loss 0.000126, validation loss 0.017894\n",
      "Iter 11631, training loss 0.000089, validation loss 0.017831\n",
      "Iter 11632, training loss 0.000242, validation loss 0.017863\n",
      "Iter 11633, training loss 0.000057, validation loss 0.017768\n",
      "Iter 11634, training loss 0.000106, validation loss 0.017849\n",
      "Iter 11635, training loss 0.000167, validation loss 0.017785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11636, training loss 0.000028, validation loss 0.017703\n",
      "Iter 11637, training loss 0.000110, validation loss 0.017850\n",
      "Iter 11638, training loss 0.000104, validation loss 0.017730\n",
      "Iter 11639, training loss 0.000022, validation loss 0.017679\n",
      "Iter 11640, training loss 0.000100, validation loss 0.017832\n",
      "Iter 11641, training loss 0.000060, validation loss 0.017693\n",
      "Iter 11642, training loss 0.000025, validation loss 0.017673\n",
      "Iter 11643, training loss 0.000082, validation loss 0.017807\n",
      "Iter 11644, training loss 0.000032, validation loss 0.017678\n",
      "Iter 11645, training loss 0.000030, validation loss 0.017680\n",
      "Iter 11646, training loss 0.000062, validation loss 0.017787\n",
      "Iter 11647, training loss 0.000018, validation loss 0.017686\n",
      "Iter 11648, training loss 0.000032, validation loss 0.017696\n",
      "Iter 11649, training loss 0.000043, validation loss 0.017774\n",
      "Iter 11650, training loss 0.000012, validation loss 0.017702\n",
      "Iter 11651, training loss 0.000031, validation loss 0.017707\n",
      "Iter 11652, training loss 0.000029, validation loss 0.017760\n",
      "Iter 11653, training loss 0.000010, validation loss 0.017718\n",
      "Iter 11654, training loss 0.000028, validation loss 0.017715\n",
      "Iter 11655, training loss 0.000019, validation loss 0.017751\n",
      "Iter 11656, training loss 0.000010, validation loss 0.017732\n",
      "Iter 11657, training loss 0.000023, validation loss 0.017722\n",
      "Iter 11658, training loss 0.000012, validation loss 0.017747\n",
      "Iter 11659, training loss 0.000010, validation loss 0.017745\n",
      "Iter 11660, training loss 0.000018, validation loss 0.017729\n",
      "Iter 11661, training loss 0.000009, validation loss 0.017746\n",
      "Iter 11662, training loss 0.000010, validation loss 0.017753\n",
      "Iter 11663, training loss 0.000014, validation loss 0.017732\n",
      "Iter 11664, training loss 0.000007, validation loss 0.017744\n",
      "Iter 11665, training loss 0.000010, validation loss 0.017756\n",
      "Iter 11666, training loss 0.000011, validation loss 0.017734\n",
      "Iter 11667, training loss 0.000006, validation loss 0.017744\n",
      "Iter 11668, training loss 0.000009, validation loss 0.017759\n",
      "Iter 11669, training loss 0.000008, validation loss 0.017740\n",
      "Iter 11670, training loss 0.000005, validation loss 0.017747\n",
      "Iter 11671, training loss 0.000008, validation loss 0.017764\n",
      "Iter 11672, training loss 0.000007, validation loss 0.017747\n",
      "Iter 11673, training loss 0.000005, validation loss 0.017752\n",
      "Iter 11674, training loss 0.000007, validation loss 0.017768\n",
      "Iter 11675, training loss 0.000005, validation loss 0.017753\n",
      "Iter 11676, training loss 0.000004, validation loss 0.017757\n",
      "Iter 11677, training loss 0.000006, validation loss 0.017771\n",
      "Iter 11678, training loss 0.000004, validation loss 0.017760\n",
      "Iter 11679, training loss 0.000004, validation loss 0.017762\n",
      "Iter 11680, training loss 0.000005, validation loss 0.017775\n",
      "Iter 11681, training loss 0.000004, validation loss 0.017766\n",
      "Iter 11682, training loss 0.000004, validation loss 0.017768\n",
      "Iter 11683, training loss 0.000004, validation loss 0.017780\n",
      "Iter 11684, training loss 0.000003, validation loss 0.017773\n",
      "Iter 11685, training loss 0.000004, validation loss 0.017774\n",
      "Iter 11686, training loss 0.000004, validation loss 0.017785\n",
      "Iter 11687, training loss 0.000003, validation loss 0.017780\n",
      "Iter 11688, training loss 0.000003, validation loss 0.017780\n",
      "Iter 11689, training loss 0.000003, validation loss 0.017790\n",
      "Iter 11690, training loss 0.000003, validation loss 0.017786\n",
      "Iter 11691, training loss 0.000003, validation loss 0.017786\n",
      "Iter 11692, training loss 0.000003, validation loss 0.017795\n",
      "Iter 11693, training loss 0.000003, validation loss 0.017792\n",
      "Iter 11694, training loss 0.000003, validation loss 0.017792\n",
      "Iter 11695, training loss 0.000003, validation loss 0.017799\n",
      "Iter 11696, training loss 0.000002, validation loss 0.017798\n",
      "Iter 11697, training loss 0.000003, validation loss 0.017797\n",
      "Iter 11698, training loss 0.000002, validation loss 0.017804\n",
      "Iter 11699, training loss 0.000002, validation loss 0.017803\n",
      "Iter 11700, training loss 0.000002, validation loss 0.017803\n",
      "Iter 11701, training loss 0.000002, validation loss 0.017809\n",
      "Iter 11702, training loss 0.000002, validation loss 0.017808\n",
      "Iter 11703, training loss 0.000002, validation loss 0.017808\n",
      "Iter 11704, training loss 0.000002, validation loss 0.017813\n",
      "Iter 11705, training loss 0.000002, validation loss 0.017813\n",
      "Iter 11706, training loss 0.000002, validation loss 0.017813\n",
      "Iter 11707, training loss 0.000002, validation loss 0.017817\n",
      "Iter 11708, training loss 0.000002, validation loss 0.017818\n",
      "Iter 11709, training loss 0.000002, validation loss 0.017817\n",
      "Iter 11710, training loss 0.000002, validation loss 0.017821\n",
      "Iter 11711, training loss 0.000002, validation loss 0.017822\n",
      "Iter 11712, training loss 0.000002, validation loss 0.017822\n",
      "Iter 11713, training loss 0.000002, validation loss 0.017825\n",
      "Iter 11714, training loss 0.000002, validation loss 0.017826\n",
      "Iter 11715, training loss 0.000002, validation loss 0.017826\n",
      "Iter 11716, training loss 0.000002, validation loss 0.017829\n",
      "Iter 11717, training loss 0.000002, validation loss 0.017829\n",
      "Iter 11718, training loss 0.000002, validation loss 0.017829\n",
      "Iter 11719, training loss 0.000002, validation loss 0.017832\n",
      "Iter 11720, training loss 0.000001, validation loss 0.017833\n",
      "Iter 11721, training loss 0.000001, validation loss 0.017833\n",
      "Iter 11722, training loss 0.000001, validation loss 0.017835\n",
      "Iter 11723, training loss 0.000001, validation loss 0.017836\n",
      "Iter 11724, training loss 0.000001, validation loss 0.017837\n",
      "Iter 11725, training loss 0.000001, validation loss 0.017839\n",
      "Iter 11726, training loss 0.000001, validation loss 0.017839\n",
      "Iter 11727, training loss 0.000001, validation loss 0.017840\n",
      "Iter 11728, training loss 0.000001, validation loss 0.017842\n",
      "Iter 11729, training loss 0.000001, validation loss 0.017842\n",
      "Iter 11730, training loss 0.000001, validation loss 0.017843\n",
      "Iter 11731, training loss 0.000001, validation loss 0.017845\n",
      "Iter 11732, training loss 0.000001, validation loss 0.017845\n",
      "Iter 11733, training loss 0.000001, validation loss 0.017846\n",
      "Iter 11734, training loss 0.000001, validation loss 0.017847\n",
      "Iter 11735, training loss 0.000001, validation loss 0.017848\n",
      "Iter 11736, training loss 0.000001, validation loss 0.017848\n",
      "Iter 11737, training loss 0.000001, validation loss 0.017849\n",
      "Iter 11738, training loss 0.000001, validation loss 0.017850\n",
      "Iter 11739, training loss 0.000001, validation loss 0.017850\n",
      "Iter 11740, training loss 0.000001, validation loss 0.017852\n",
      "Iter 11741, training loss 0.000001, validation loss 0.017852\n",
      "Iter 11742, training loss 0.000001, validation loss 0.017853\n",
      "Iter 11743, training loss 0.000001, validation loss 0.017854\n",
      "Iter 11744, training loss 0.000001, validation loss 0.017854\n",
      "Iter 11745, training loss 0.000001, validation loss 0.017854\n",
      "Iter 11746, training loss 0.000001, validation loss 0.017855\n",
      "Iter 11747, training loss 0.000001, validation loss 0.017856\n",
      "Iter 11748, training loss 0.000001, validation loss 0.017856\n",
      "Iter 11749, training loss 0.000001, validation loss 0.017857\n",
      "Iter 11750, training loss 0.000001, validation loss 0.017857\n",
      "Iter 11751, training loss 0.000001, validation loss 0.017858\n",
      "Iter 11752, training loss 0.000001, validation loss 0.017859\n",
      "Iter 11753, training loss 0.000001, validation loss 0.017859\n",
      "Iter 11754, training loss 0.000001, validation loss 0.017859\n",
      "Iter 11755, training loss 0.000001, validation loss 0.017860\n",
      "Iter 11756, training loss 0.000001, validation loss 0.017860\n",
      "Iter 11757, training loss 0.000001, validation loss 0.017861\n",
      "Iter 11758, training loss 0.000001, validation loss 0.017861\n",
      "Iter 11759, training loss 0.000001, validation loss 0.017862\n",
      "Iter 11760, training loss 0.000001, validation loss 0.017862\n",
      "Iter 11761, training loss 0.000001, validation loss 0.017863\n",
      "Iter 11762, training loss 0.000001, validation loss 0.017863\n",
      "Iter 11763, training loss 0.000001, validation loss 0.017863\n",
      "Iter 11764, training loss 0.000001, validation loss 0.017864\n",
      "Iter 11765, training loss 0.000001, validation loss 0.017864\n",
      "Iter 11766, training loss 0.000001, validation loss 0.017865\n",
      "Iter 11767, training loss 0.000001, validation loss 0.017865\n",
      "Iter 11768, training loss 0.000001, validation loss 0.017865\n",
      "Iter 11769, training loss 0.000001, validation loss 0.017866\n",
      "Iter 11770, training loss 0.000001, validation loss 0.017866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11771, training loss 0.000001, validation loss 0.017866\n",
      "Iter 11772, training loss 0.000001, validation loss 0.017867\n",
      "Iter 11773, training loss 0.000001, validation loss 0.017867\n",
      "Iter 11774, training loss 0.000001, validation loss 0.017868\n",
      "Iter 11775, training loss 0.000001, validation loss 0.017868\n",
      "Iter 11776, training loss 0.000001, validation loss 0.017868\n",
      "Iter 11777, training loss 0.000001, validation loss 0.017869\n",
      "Iter 11778, training loss 0.000001, validation loss 0.017869\n",
      "Iter 11779, training loss 0.000001, validation loss 0.017869\n",
      "Iter 11780, training loss 0.000001, validation loss 0.017870\n",
      "Iter 11781, training loss 0.000001, validation loss 0.017870\n",
      "Iter 11782, training loss 0.000001, validation loss 0.017870\n",
      "Iter 11783, training loss 0.000001, validation loss 0.017871\n",
      "Iter 11784, training loss 0.000001, validation loss 0.017871\n",
      "Iter 11785, training loss 0.000001, validation loss 0.017871\n",
      "Iter 11786, training loss 0.000001, validation loss 0.017871\n",
      "Iter 11787, training loss 0.000001, validation loss 0.017872\n",
      "Iter 11788, training loss 0.000001, validation loss 0.017872\n",
      "Iter 11789, training loss 0.000001, validation loss 0.017872\n",
      "Iter 11790, training loss 0.000001, validation loss 0.017873\n",
      "Iter 11791, training loss 0.000001, validation loss 0.017873\n",
      "Iter 11792, training loss 0.000001, validation loss 0.017873\n",
      "Iter 11793, training loss 0.000001, validation loss 0.017874\n",
      "Iter 11794, training loss 0.000001, validation loss 0.017874\n",
      "Iter 11795, training loss 0.000001, validation loss 0.017874\n",
      "Iter 11796, training loss 0.000001, validation loss 0.017875\n",
      "Iter 11797, training loss 0.000001, validation loss 0.017875\n",
      "Iter 11798, training loss 0.000001, validation loss 0.017875\n",
      "Iter 11799, training loss 0.000001, validation loss 0.017875\n",
      "Iter 11800, training loss 0.000000, validation loss 0.017876\n",
      "Iter 11801, training loss 0.000000, validation loss 0.017876\n",
      "Iter 11802, training loss 0.000000, validation loss 0.017876\n",
      "Iter 11803, training loss 0.000000, validation loss 0.017877\n",
      "Iter 11804, training loss 0.000000, validation loss 0.017877\n",
      "Iter 11805, training loss 0.000000, validation loss 0.017877\n",
      "Iter 11806, training loss 0.000000, validation loss 0.017877\n",
      "Iter 11807, training loss 0.000000, validation loss 0.017878\n",
      "Iter 11808, training loss 0.000000, validation loss 0.017878\n",
      "Iter 11809, training loss 0.000000, validation loss 0.017878\n",
      "Iter 11810, training loss 0.000000, validation loss 0.017878\n",
      "Iter 11811, training loss 0.000000, validation loss 0.017879\n",
      "Iter 11812, training loss 0.000000, validation loss 0.017879\n",
      "Iter 11813, training loss 0.000000, validation loss 0.017879\n",
      "Iter 11814, training loss 0.000000, validation loss 0.017879\n",
      "Iter 11815, training loss 0.000000, validation loss 0.017880\n",
      "Iter 11816, training loss 0.000000, validation loss 0.017880\n",
      "Iter 11817, training loss 0.000000, validation loss 0.017880\n",
      "Iter 11818, training loss 0.000000, validation loss 0.017880\n",
      "Iter 11819, training loss 0.000000, validation loss 0.017880\n",
      "Iter 11820, training loss 0.000000, validation loss 0.017881\n",
      "Iter 11821, training loss 0.000000, validation loss 0.017881\n",
      "Iter 11822, training loss 0.000000, validation loss 0.017881\n",
      "Iter 11823, training loss 0.000000, validation loss 0.017881\n",
      "Iter 11824, training loss 0.000000, validation loss 0.017882\n",
      "Iter 11825, training loss 0.000000, validation loss 0.017882\n",
      "Iter 11826, training loss 0.000000, validation loss 0.017882\n",
      "Iter 11827, training loss 0.000000, validation loss 0.017882\n",
      "Iter 11828, training loss 0.000000, validation loss 0.017883\n",
      "Iter 11829, training loss 0.000000, validation loss 0.017883\n",
      "Iter 11830, training loss 0.000000, validation loss 0.017883\n",
      "Iter 11831, training loss 0.000000, validation loss 0.017883\n",
      "Iter 11832, training loss 0.000000, validation loss 0.017883\n",
      "Iter 11833, training loss 0.000000, validation loss 0.017884\n",
      "Iter 11834, training loss 0.000000, validation loss 0.017884\n",
      "Iter 11835, training loss 0.000000, validation loss 0.017884\n",
      "Iter 11836, training loss 0.000000, validation loss 0.017884\n",
      "Iter 11837, training loss 0.000000, validation loss 0.017885\n",
      "Iter 11838, training loss 0.000000, validation loss 0.017885\n",
      "Iter 11839, training loss 0.000000, validation loss 0.017885\n",
      "Iter 11840, training loss 0.000000, validation loss 0.017885\n",
      "Iter 11841, training loss 0.000000, validation loss 0.017885\n",
      "Iter 11842, training loss 0.000000, validation loss 0.017886\n",
      "Iter 11843, training loss 0.000000, validation loss 0.017886\n",
      "Iter 11844, training loss 0.000000, validation loss 0.017886\n",
      "Iter 11845, training loss 0.000000, validation loss 0.017886\n",
      "Iter 11846, training loss 0.000000, validation loss 0.017886\n",
      "Iter 11847, training loss 0.000000, validation loss 0.017887\n",
      "Iter 11848, training loss 0.000000, validation loss 0.017887\n",
      "Iter 11849, training loss 0.000000, validation loss 0.017887\n",
      "Iter 11850, training loss 0.000000, validation loss 0.017887\n",
      "Iter 11851, training loss 0.000000, validation loss 0.017887\n",
      "Iter 11852, training loss 0.000000, validation loss 0.017888\n",
      "Iter 11853, training loss 0.000000, validation loss 0.017888\n",
      "Iter 11854, training loss 0.000000, validation loss 0.017888\n",
      "Iter 11855, training loss 0.000000, validation loss 0.017888\n",
      "Iter 11856, training loss 0.000000, validation loss 0.017888\n",
      "Iter 11857, training loss 0.000000, validation loss 0.017888\n",
      "Iter 11858, training loss 0.000000, validation loss 0.017889\n",
      "Iter 11859, training loss 0.000000, validation loss 0.017889\n",
      "Iter 11860, training loss 0.000000, validation loss 0.017889\n",
      "Iter 11861, training loss 0.000000, validation loss 0.017889\n",
      "Iter 11862, training loss 0.000000, validation loss 0.017889\n",
      "Iter 11863, training loss 0.000000, validation loss 0.017890\n",
      "Iter 11864, training loss 0.000000, validation loss 0.017890\n",
      "Iter 11865, training loss 0.000000, validation loss 0.017890\n",
      "Iter 11866, training loss 0.000000, validation loss 0.017890\n",
      "Iter 11867, training loss 0.000000, validation loss 0.017890\n",
      "Iter 11868, training loss 0.000000, validation loss 0.017890\n",
      "Iter 11869, training loss 0.000000, validation loss 0.017890\n",
      "Iter 11870, training loss 0.000000, validation loss 0.017891\n",
      "Iter 11871, training loss 0.000000, validation loss 0.017891\n",
      "Iter 11872, training loss 0.000000, validation loss 0.017891\n",
      "Iter 11873, training loss 0.000000, validation loss 0.017891\n",
      "Iter 11874, training loss 0.000000, validation loss 0.017891\n",
      "Iter 11875, training loss 0.000000, validation loss 0.017891\n",
      "Iter 11876, training loss 0.000000, validation loss 0.017892\n",
      "Iter 11877, training loss 0.000000, validation loss 0.017892\n",
      "Iter 11878, training loss 0.000000, validation loss 0.017892\n",
      "Iter 11879, training loss 0.000000, validation loss 0.017892\n",
      "Iter 11880, training loss 0.000000, validation loss 0.017892\n",
      "Iter 11881, training loss 0.000000, validation loss 0.017892\n",
      "Iter 11882, training loss 0.000000, validation loss 0.017892\n",
      "Iter 11883, training loss 0.000000, validation loss 0.017892\n",
      "Iter 11884, training loss 0.000000, validation loss 0.017893\n",
      "Iter 11885, training loss 0.000000, validation loss 0.017893\n",
      "Iter 11886, training loss 0.000000, validation loss 0.017893\n",
      "Iter 11887, training loss 0.000000, validation loss 0.017893\n",
      "Iter 11888, training loss 0.000000, validation loss 0.017893\n",
      "Iter 11889, training loss 0.000000, validation loss 0.017893\n",
      "Iter 11890, training loss 0.000000, validation loss 0.017893\n",
      "Iter 11891, training loss 0.000000, validation loss 0.017893\n",
      "Iter 11892, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11893, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11894, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11895, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11896, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11897, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11898, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11899, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11900, training loss 0.000000, validation loss 0.017894\n",
      "Iter 11901, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11902, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11903, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11904, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11905, training loss 0.000000, validation loss 0.017895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11906, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11907, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11908, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11909, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11910, training loss 0.000000, validation loss 0.017895\n",
      "Iter 11911, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11912, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11913, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11914, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11915, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11916, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11917, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11918, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11919, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11920, training loss 0.000000, validation loss 0.017896\n",
      "Iter 11921, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11922, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11923, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11924, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11925, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11926, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11927, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11928, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11929, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11930, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11931, training loss 0.000000, validation loss 0.017897\n",
      "Iter 11932, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11933, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11934, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11935, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11936, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11937, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11938, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11939, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11940, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11941, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11942, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11943, training loss 0.000000, validation loss 0.017898\n",
      "Iter 11944, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11945, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11946, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11947, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11948, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11949, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11950, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11951, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11952, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11953, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11954, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11955, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11956, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11957, training loss 0.000000, validation loss 0.017899\n",
      "Iter 11958, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11959, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11960, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11961, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11962, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11963, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11964, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11965, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11966, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11967, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11968, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11969, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11970, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11971, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11972, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11973, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11974, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11975, training loss 0.000000, validation loss 0.017900\n",
      "Iter 11976, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11977, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11978, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11979, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11980, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11981, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11982, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11983, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11984, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11985, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11986, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11987, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11988, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11989, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11990, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11991, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11992, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11993, training loss 0.000000, validation loss 0.017901\n",
      "Iter 11994, training loss 0.000000, validation loss 0.017902\n",
      "Iter 11995, training loss 0.000000, validation loss 0.017902\n",
      "Iter 11996, training loss 0.000000, validation loss 0.017902\n",
      "Iter 11997, training loss 0.000000, validation loss 0.017902\n",
      "Iter 11998, training loss 0.000000, validation loss 0.017902\n",
      "Iter 11999, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12000, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12001, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12002, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12003, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12004, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12005, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12006, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12007, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12008, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12009, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12010, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12011, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12012, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12013, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12014, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12015, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12016, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12017, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12018, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12019, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12020, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12021, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12022, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12023, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12024, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12025, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12026, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12027, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12028, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12029, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12030, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12031, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12032, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12033, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12034, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12035, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12036, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12037, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12038, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12039, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12040, training loss 0.000000, validation loss 0.017904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12041, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12042, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12043, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12044, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12045, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12046, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12047, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12048, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12049, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12050, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12051, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12052, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12053, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12054, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12055, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12056, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12057, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12058, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12059, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12060, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12061, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12062, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12063, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12064, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12065, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12066, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12067, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12068, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12069, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12070, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12071, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12072, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12073, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12074, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12075, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12076, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12077, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12078, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12079, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12080, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12081, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12082, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12083, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12084, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12085, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12086, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12087, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12088, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12089, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12090, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12091, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12092, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12093, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12094, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12095, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12096, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12097, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12098, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12099, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12100, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12101, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12102, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12103, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12104, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12105, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12106, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12107, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12108, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12109, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12110, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12111, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12112, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12113, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12114, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12115, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12116, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12117, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12118, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12119, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12120, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12121, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12122, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12123, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12124, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12125, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12126, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12127, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12128, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12129, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12130, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12131, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12132, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12133, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12134, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12135, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12136, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12137, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12138, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12139, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12140, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12141, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12142, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12143, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12144, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12145, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12146, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12147, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12148, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12149, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12150, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12151, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12152, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12153, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12154, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12155, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12156, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12157, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12158, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12159, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12160, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12161, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12162, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12163, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12164, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12165, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12166, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12167, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12168, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12169, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12170, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12171, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12172, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12173, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12174, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12175, training loss 0.000000, validation loss 0.017906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12176, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12177, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12178, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12179, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12180, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12181, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12182, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12183, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12184, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12185, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12186, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12187, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12188, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12189, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12190, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12191, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12192, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12193, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12194, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12195, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12196, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12197, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12198, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12199, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12200, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12201, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12202, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12203, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12204, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12205, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12206, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12207, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12208, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12209, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12210, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12211, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12212, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12213, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12214, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12215, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12216, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12217, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12218, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12219, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12220, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12221, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12222, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12223, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12224, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12225, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12226, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12227, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12228, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12229, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12230, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12231, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12232, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12233, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12234, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12235, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12236, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12237, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12238, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12239, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12240, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12241, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12242, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12243, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12244, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12245, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12246, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12247, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12248, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12249, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12250, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12251, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12252, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12253, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12254, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12255, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12256, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12257, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12258, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12259, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12260, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12261, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12262, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12263, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12264, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12265, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12266, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12267, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12268, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12269, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12270, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12271, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12272, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12273, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12274, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12275, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12276, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12277, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12278, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12279, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12280, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12281, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12282, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12283, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12284, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12285, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12286, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12287, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12288, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12289, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12290, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12291, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12292, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12293, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12294, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12295, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12296, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12297, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12298, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12299, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12300, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12301, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12302, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12303, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12304, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12305, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12306, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12307, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12308, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12309, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12310, training loss 0.000000, validation loss 0.017905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12311, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12312, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12313, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12314, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12315, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12316, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12317, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12318, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12319, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12320, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12321, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12322, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12323, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12324, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12325, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12326, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12327, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12328, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12329, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12330, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12331, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12332, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12333, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12334, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12335, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12336, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12337, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12338, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12339, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12340, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12341, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12342, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12343, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12344, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12345, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12346, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12347, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12348, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12349, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12350, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12351, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12352, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12353, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12354, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12355, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12356, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12357, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12358, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12359, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12360, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12361, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12362, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12363, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12364, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12365, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12366, training loss 0.000000, validation loss 0.017905\n",
      "Iter 12367, training loss 0.000000, validation loss 0.017904\n",
      "Iter 12368, training loss 0.000000, validation loss 0.017906\n",
      "Iter 12369, training loss 0.000000, validation loss 0.017903\n",
      "Iter 12370, training loss 0.000000, validation loss 0.017907\n",
      "Iter 12371, training loss 0.000000, validation loss 0.017902\n",
      "Iter 12372, training loss 0.000001, validation loss 0.017910\n",
      "Iter 12373, training loss 0.000001, validation loss 0.017899\n",
      "Iter 12374, training loss 0.000003, validation loss 0.017918\n",
      "Iter 12375, training loss 0.000006, validation loss 0.017898\n",
      "Iter 12376, training loss 0.000013, validation loss 0.017941\n",
      "Iter 12377, training loss 0.000029, validation loss 0.017909\n",
      "Iter 12378, training loss 0.000062, validation loss 0.018022\n",
      "Iter 12379, training loss 0.000134, validation loss 0.018000\n",
      "Iter 12380, training loss 0.000290, validation loss 0.018346\n",
      "Iter 12381, training loss 0.000627, validation loss 0.018516\n",
      "Iter 12382, training loss 0.001351, validation loss 0.019729\n",
      "Iter 12383, training loss 0.002882, validation loss 0.021058\n",
      "Iter 12384, training loss 0.005923, validation loss 0.025445\n",
      "Iter 12385, training loss 0.011426, validation loss 0.030979\n",
      "Iter 12386, training loss 0.018748, validation loss 0.041134\n",
      "Iter 12387, training loss 0.022931, validation loss 0.044196\n",
      "Iter 12388, training loss 0.014558, validation loss 0.034993\n",
      "Iter 12389, training loss 0.001621, validation loss 0.017814\n",
      "Iter 12390, training loss 0.004458, validation loss 0.020752\n",
      "Iter 12391, training loss 0.009071, validation loss 0.027362\n",
      "Iter 12392, training loss 0.001345, validation loss 0.016883\n",
      "Iter 12393, training loss 0.003805, validation loss 0.019472\n",
      "Iter 12394, training loss 0.004627, validation loss 0.021440\n",
      "Iter 12395, training loss 0.000549, validation loss 0.015872\n",
      "Iter 12396, training loss 0.004256, validation loss 0.019677\n",
      "Iter 12397, training loss 0.000734, validation loss 0.015878\n",
      "Iter 12398, training loss 0.002551, validation loss 0.018201\n",
      "Iter 12399, training loss 0.001417, validation loss 0.016125\n",
      "Iter 12400, training loss 0.001304, validation loss 0.015910\n",
      "Iter 12401, training loss 0.001656, validation loss 0.016785\n",
      "Iter 12402, training loss 0.000658, validation loss 0.015376\n",
      "Iter 12403, training loss 0.001572, validation loss 0.015997\n",
      "Iter 12404, training loss 0.000369, validation loss 0.014659\n",
      "Iter 12405, training loss 0.001359, validation loss 0.016212\n",
      "Iter 12406, training loss 0.000238, validation loss 0.014555\n",
      "Iter 12407, training loss 0.001123, validation loss 0.015307\n",
      "Iter 12408, training loss 0.000178, validation loss 0.014287\n",
      "Iter 12409, training loss 0.000914, validation loss 0.015391\n",
      "Iter 12410, training loss 0.000144, validation loss 0.014209\n",
      "Iter 12411, training loss 0.000740, validation loss 0.014721\n",
      "Iter 12412, training loss 0.000123, validation loss 0.014063\n",
      "Iter 12413, training loss 0.000599, validation loss 0.014777\n",
      "Iter 12414, training loss 0.000106, validation loss 0.014019\n",
      "Iter 12415, training loss 0.000484, validation loss 0.014334\n",
      "Iter 12416, training loss 0.000094, validation loss 0.013914\n",
      "Iter 12417, training loss 0.000390, validation loss 0.014369\n",
      "Iter 12418, training loss 0.000085, validation loss 0.013889\n",
      "Iter 12419, training loss 0.000313, validation loss 0.014045\n",
      "Iter 12420, training loss 0.000079, validation loss 0.013804\n",
      "Iter 12421, training loss 0.000250, validation loss 0.014109\n",
      "Iter 12422, training loss 0.000074, validation loss 0.013822\n",
      "Iter 12423, training loss 0.000199, validation loss 0.013876\n",
      "Iter 12424, training loss 0.000070, validation loss 0.013748\n",
      "Iter 12425, training loss 0.000157, validation loss 0.013929\n",
      "Iter 12426, training loss 0.000066, validation loss 0.013785\n",
      "Iter 12427, training loss 0.000123, validation loss 0.013788\n",
      "Iter 12428, training loss 0.000062, validation loss 0.013725\n",
      "Iter 12429, training loss 0.000097, validation loss 0.013818\n",
      "Iter 12430, training loss 0.000059, validation loss 0.013751\n",
      "Iter 12431, training loss 0.000075, validation loss 0.013715\n",
      "Iter 12432, training loss 0.000055, validation loss 0.013693\n",
      "Iter 12433, training loss 0.000059, validation loss 0.013747\n",
      "Iter 12434, training loss 0.000050, validation loss 0.013733\n",
      "Iter 12435, training loss 0.000046, validation loss 0.013684\n",
      "Iter 12436, training loss 0.000046, validation loss 0.013682\n",
      "Iter 12437, training loss 0.000036, validation loss 0.013707\n",
      "Iter 12438, training loss 0.000041, validation loss 0.013716\n",
      "Iter 12439, training loss 0.000029, validation loss 0.013666\n",
      "Iter 12440, training loss 0.000037, validation loss 0.013672\n",
      "Iter 12441, training loss 0.000023, validation loss 0.013686\n",
      "Iter 12442, training loss 0.000033, validation loss 0.013707\n",
      "Iter 12443, training loss 0.000019, validation loss 0.013663\n",
      "Iter 12444, training loss 0.000029, validation loss 0.013669\n",
      "Iter 12445, training loss 0.000016, validation loss 0.013678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12446, training loss 0.000026, validation loss 0.013700\n",
      "Iter 12447, training loss 0.000014, validation loss 0.013663\n",
      "Iter 12448, training loss 0.000023, validation loss 0.013665\n",
      "Iter 12449, training loss 0.000012, validation loss 0.013670\n",
      "Iter 12450, training loss 0.000020, validation loss 0.013690\n",
      "Iter 12451, training loss 0.000010, validation loss 0.013661\n",
      "Iter 12452, training loss 0.000017, validation loss 0.013660\n",
      "Iter 12453, training loss 0.000009, validation loss 0.013664\n",
      "Iter 12454, training loss 0.000015, validation loss 0.013681\n",
      "Iter 12455, training loss 0.000008, validation loss 0.013660\n",
      "Iter 12456, training loss 0.000013, validation loss 0.013657\n",
      "Iter 12457, training loss 0.000008, validation loss 0.013661\n",
      "Iter 12458, training loss 0.000012, validation loss 0.013675\n",
      "Iter 12459, training loss 0.000007, validation loss 0.013660\n",
      "Iter 12460, training loss 0.000010, validation loss 0.013656\n",
      "Iter 12461, training loss 0.000007, validation loss 0.013660\n",
      "Iter 12462, training loss 0.000009, validation loss 0.013672\n",
      "Iter 12463, training loss 0.000006, validation loss 0.013664\n",
      "Iter 12464, training loss 0.000008, validation loss 0.013660\n",
      "Iter 12465, training loss 0.000006, validation loss 0.013664\n",
      "Iter 12466, training loss 0.000007, validation loss 0.013674\n",
      "Iter 12467, training loss 0.000005, validation loss 0.013668\n",
      "Iter 12468, training loss 0.000007, validation loss 0.013664\n",
      "Iter 12469, training loss 0.000005, validation loss 0.013668\n",
      "Iter 12470, training loss 0.000006, validation loss 0.013676\n",
      "Iter 12471, training loss 0.000005, validation loss 0.013672\n",
      "Iter 12472, training loss 0.000005, validation loss 0.013669\n",
      "Iter 12473, training loss 0.000005, validation loss 0.013672\n",
      "Iter 12474, training loss 0.000005, validation loss 0.013679\n",
      "Iter 12475, training loss 0.000004, validation loss 0.013678\n",
      "Iter 12476, training loss 0.000005, validation loss 0.013675\n",
      "Iter 12477, training loss 0.000004, validation loss 0.013678\n",
      "Iter 12478, training loss 0.000004, validation loss 0.013683\n",
      "Iter 12479, training loss 0.000004, validation loss 0.013682\n",
      "Iter 12480, training loss 0.000004, validation loss 0.013679\n",
      "Iter 12481, training loss 0.000004, validation loss 0.013680\n",
      "Iter 12482, training loss 0.000004, validation loss 0.013685\n",
      "Iter 12483, training loss 0.000003, validation loss 0.013684\n",
      "Iter 12484, training loss 0.000003, validation loss 0.013682\n",
      "Iter 12485, training loss 0.000003, validation loss 0.013683\n",
      "Iter 12486, training loss 0.000003, validation loss 0.013687\n",
      "Iter 12487, training loss 0.000003, validation loss 0.013686\n",
      "Iter 12488, training loss 0.000003, validation loss 0.013684\n",
      "Iter 12489, training loss 0.000003, validation loss 0.013686\n",
      "Iter 12490, training loss 0.000003, validation loss 0.013688\n",
      "Iter 12491, training loss 0.000003, validation loss 0.013688\n",
      "Iter 12492, training loss 0.000003, validation loss 0.013687\n",
      "Iter 12493, training loss 0.000003, validation loss 0.013688\n",
      "Iter 12494, training loss 0.000003, validation loss 0.013691\n",
      "Iter 12495, training loss 0.000003, validation loss 0.013691\n",
      "Iter 12496, training loss 0.000003, validation loss 0.013690\n",
      "Iter 12497, training loss 0.000002, validation loss 0.013691\n",
      "Iter 12498, training loss 0.000002, validation loss 0.013693\n",
      "Iter 12499, training loss 0.000002, validation loss 0.013693\n",
      "Iter 12500, training loss 0.000002, validation loss 0.013693\n",
      "Iter 12501, training loss 0.000002, validation loss 0.013694\n",
      "Iter 12502, training loss 0.000002, validation loss 0.013695\n",
      "Iter 12503, training loss 0.000002, validation loss 0.013696\n",
      "Iter 12504, training loss 0.000002, validation loss 0.013695\n",
      "Iter 12505, training loss 0.000002, validation loss 0.013696\n",
      "Iter 12506, training loss 0.000002, validation loss 0.013698\n",
      "Iter 12507, training loss 0.000002, validation loss 0.013698\n",
      "Iter 12508, training loss 0.000002, validation loss 0.013698\n",
      "Iter 12509, training loss 0.000002, validation loss 0.013699\n",
      "Iter 12510, training loss 0.000002, validation loss 0.013700\n",
      "Iter 12511, training loss 0.000002, validation loss 0.013700\n",
      "Iter 12512, training loss 0.000002, validation loss 0.013700\n",
      "Iter 12513, training loss 0.000002, validation loss 0.013700\n",
      "Iter 12514, training loss 0.000002, validation loss 0.013702\n",
      "Iter 12515, training loss 0.000002, validation loss 0.013702\n",
      "Iter 12516, training loss 0.000002, validation loss 0.013702\n",
      "Iter 12517, training loss 0.000002, validation loss 0.013702\n",
      "Iter 12518, training loss 0.000002, validation loss 0.013703\n",
      "Iter 12519, training loss 0.000002, validation loss 0.013703\n",
      "Iter 12520, training loss 0.000002, validation loss 0.013703\n",
      "Iter 12521, training loss 0.000002, validation loss 0.013704\n",
      "Iter 12522, training loss 0.000002, validation loss 0.013704\n",
      "Iter 12523, training loss 0.000001, validation loss 0.013705\n",
      "Iter 12524, training loss 0.000001, validation loss 0.013705\n",
      "Iter 12525, training loss 0.000001, validation loss 0.013705\n",
      "Iter 12526, training loss 0.000001, validation loss 0.013706\n",
      "Iter 12527, training loss 0.000001, validation loss 0.013706\n",
      "Iter 12528, training loss 0.000001, validation loss 0.013706\n",
      "Iter 12529, training loss 0.000001, validation loss 0.013707\n",
      "Iter 12530, training loss 0.000001, validation loss 0.013707\n",
      "Iter 12531, training loss 0.000001, validation loss 0.013707\n",
      "Iter 12532, training loss 0.000001, validation loss 0.013707\n",
      "Iter 12533, training loss 0.000001, validation loss 0.013707\n",
      "Iter 12534, training loss 0.000001, validation loss 0.013708\n",
      "Iter 12535, training loss 0.000001, validation loss 0.013708\n",
      "Iter 12536, training loss 0.000001, validation loss 0.013708\n",
      "Iter 12537, training loss 0.000001, validation loss 0.013709\n",
      "Iter 12538, training loss 0.000001, validation loss 0.013709\n",
      "Iter 12539, training loss 0.000001, validation loss 0.013709\n",
      "Iter 12540, training loss 0.000001, validation loss 0.013709\n",
      "Iter 12541, training loss 0.000001, validation loss 0.013710\n",
      "Iter 12542, training loss 0.000001, validation loss 0.013710\n",
      "Iter 12543, training loss 0.000001, validation loss 0.013711\n",
      "Iter 12544, training loss 0.000001, validation loss 0.013711\n",
      "Iter 12545, training loss 0.000001, validation loss 0.013711\n",
      "Iter 12546, training loss 0.000001, validation loss 0.013712\n",
      "Iter 12547, training loss 0.000001, validation loss 0.013712\n",
      "Iter 12548, training loss 0.000001, validation loss 0.013712\n",
      "Iter 12549, training loss 0.000001, validation loss 0.013712\n",
      "Iter 12550, training loss 0.000001, validation loss 0.013712\n",
      "Iter 12551, training loss 0.000001, validation loss 0.013712\n",
      "Iter 12552, training loss 0.000001, validation loss 0.013713\n",
      "Iter 12553, training loss 0.000001, validation loss 0.013713\n",
      "Iter 12554, training loss 0.000001, validation loss 0.013713\n",
      "Iter 12555, training loss 0.000001, validation loss 0.013713\n",
      "Iter 12556, training loss 0.000001, validation loss 0.013714\n",
      "Iter 12557, training loss 0.000001, validation loss 0.013714\n",
      "Iter 12558, training loss 0.000001, validation loss 0.013714\n",
      "Iter 12559, training loss 0.000001, validation loss 0.013714\n",
      "Iter 12560, training loss 0.000001, validation loss 0.013714\n",
      "Iter 12561, training loss 0.000001, validation loss 0.013715\n",
      "Iter 12562, training loss 0.000001, validation loss 0.013715\n",
      "Iter 12563, training loss 0.000001, validation loss 0.013715\n",
      "Iter 12564, training loss 0.000001, validation loss 0.013715\n",
      "Iter 12565, training loss 0.000001, validation loss 0.013715\n",
      "Iter 12566, training loss 0.000001, validation loss 0.013716\n",
      "Iter 12567, training loss 0.000001, validation loss 0.013716\n",
      "Iter 12568, training loss 0.000001, validation loss 0.013716\n",
      "Iter 12569, training loss 0.000001, validation loss 0.013716\n",
      "Iter 12570, training loss 0.000001, validation loss 0.013716\n",
      "Iter 12571, training loss 0.000001, validation loss 0.013716\n",
      "Iter 12572, training loss 0.000001, validation loss 0.013717\n",
      "Iter 12573, training loss 0.000001, validation loss 0.013717\n",
      "Iter 12574, training loss 0.000001, validation loss 0.013717\n",
      "Iter 12575, training loss 0.000001, validation loss 0.013717\n",
      "Iter 12576, training loss 0.000001, validation loss 0.013717\n",
      "Iter 12577, training loss 0.000001, validation loss 0.013717\n",
      "Iter 12578, training loss 0.000001, validation loss 0.013717\n",
      "Iter 12579, training loss 0.000001, validation loss 0.013718\n",
      "Iter 12580, training loss 0.000001, validation loss 0.013718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12581, training loss 0.000001, validation loss 0.013718\n",
      "Iter 12582, training loss 0.000001, validation loss 0.013718\n",
      "Iter 12583, training loss 0.000001, validation loss 0.013718\n",
      "Iter 12584, training loss 0.000001, validation loss 0.013718\n",
      "Iter 12585, training loss 0.000001, validation loss 0.013719\n",
      "Iter 12586, training loss 0.000001, validation loss 0.013719\n",
      "Iter 12587, training loss 0.000001, validation loss 0.013719\n",
      "Iter 12588, training loss 0.000001, validation loss 0.013719\n",
      "Iter 12589, training loss 0.000001, validation loss 0.013719\n",
      "Iter 12590, training loss 0.000001, validation loss 0.013720\n",
      "Iter 12591, training loss 0.000001, validation loss 0.013720\n",
      "Iter 12592, training loss 0.000001, validation loss 0.013720\n",
      "Iter 12593, training loss 0.000001, validation loss 0.013720\n",
      "Iter 12594, training loss 0.000001, validation loss 0.013720\n",
      "Iter 12595, training loss 0.000001, validation loss 0.013720\n",
      "Iter 12596, training loss 0.000001, validation loss 0.013721\n",
      "Iter 12597, training loss 0.000001, validation loss 0.013721\n",
      "Iter 12598, training loss 0.000001, validation loss 0.013721\n",
      "Iter 12599, training loss 0.000001, validation loss 0.013721\n",
      "Iter 12600, training loss 0.000001, validation loss 0.013721\n",
      "Iter 12601, training loss 0.000000, validation loss 0.013721\n",
      "Iter 12602, training loss 0.000000, validation loss 0.013721\n",
      "Iter 12603, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12604, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12605, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12606, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12607, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12608, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12609, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12610, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12611, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12612, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12613, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12614, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12615, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12616, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12617, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12618, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12619, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12620, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12621, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12622, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12623, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12624, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12625, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12626, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12627, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12628, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12629, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12630, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12631, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12632, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12633, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12634, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12635, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12636, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12637, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12638, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12639, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12640, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12641, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12642, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12643, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12644, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12645, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12646, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12647, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12648, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12649, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12650, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12651, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12652, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12653, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12654, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12655, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12656, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12657, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12658, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12659, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12660, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12661, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12662, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12663, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12664, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12665, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12666, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12667, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12668, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12669, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12670, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12671, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12672, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12673, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12674, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12675, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12676, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12677, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12678, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12679, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12680, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12681, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12682, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12683, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12684, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12685, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12686, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12687, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12688, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12689, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12690, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12691, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12692, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12693, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12694, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12695, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12696, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12697, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12698, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12699, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12700, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12701, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12702, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12703, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12704, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12705, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12706, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12707, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12708, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12709, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12710, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12711, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12712, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12713, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12714, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12715, training loss 0.000000, validation loss 0.013728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12716, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12717, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12718, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12719, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12720, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12721, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12722, training loss 0.000000, validation loss 0.013728\n",
      "Iter 12723, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12724, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12725, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12726, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12727, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12728, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12729, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12730, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12731, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12732, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12733, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12734, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12735, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12736, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12737, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12738, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12739, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12740, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12741, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12742, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12743, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12744, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12745, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12746, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12747, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12748, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12749, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12750, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12751, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12752, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12753, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12754, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12755, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12756, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12757, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12758, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12759, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12760, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12761, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12762, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12763, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12764, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12765, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12766, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12767, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12768, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12769, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12770, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12771, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12772, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12773, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12774, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12775, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12776, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12777, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12778, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12779, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12780, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12781, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12782, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12783, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12784, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12785, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12786, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12787, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12788, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12789, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12790, training loss 0.000000, validation loss 0.013727\n",
      "Iter 12791, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12792, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12793, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12794, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12795, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12796, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12797, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12798, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12799, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12800, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12801, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12802, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12803, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12804, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12805, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12806, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12807, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12808, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12809, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12810, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12811, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12812, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12813, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12814, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12815, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12816, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12817, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12818, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12819, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12820, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12821, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12822, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12823, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12824, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12825, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12826, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12827, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12828, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12829, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12830, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12831, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12832, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12833, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12834, training loss 0.000000, validation loss 0.013726\n",
      "Iter 12835, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12836, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12837, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12838, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12839, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12840, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12841, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12842, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12843, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12844, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12845, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12846, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12847, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12848, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12849, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12850, training loss 0.000000, validation loss 0.013725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12851, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12852, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12853, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12854, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12855, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12856, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12857, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12858, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12859, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12860, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12861, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12862, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12863, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12864, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12865, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12866, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12867, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12868, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12869, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12870, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12871, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12872, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12873, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12874, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12875, training loss 0.000000, validation loss 0.013725\n",
      "Iter 12876, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12877, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12878, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12879, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12880, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12881, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12882, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12883, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12884, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12885, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12886, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12887, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12888, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12889, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12890, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12891, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12892, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12893, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12894, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12895, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12896, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12897, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12898, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12899, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12900, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12901, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12902, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12903, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12904, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12905, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12906, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12907, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12908, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12909, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12910, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12911, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12912, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12913, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12914, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12915, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12916, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12917, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12918, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12919, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12920, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12921, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12922, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12923, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12924, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12925, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12926, training loss 0.000000, validation loss 0.013724\n",
      "Iter 12927, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12928, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12929, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12930, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12931, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12932, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12933, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12934, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12935, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12936, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12937, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12938, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12939, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12940, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12941, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12942, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12943, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12944, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12945, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12946, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12947, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12948, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12949, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12950, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12951, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12952, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12953, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12954, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12955, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12956, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12957, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12958, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12959, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12960, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12961, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12962, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12963, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12964, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12965, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12966, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12967, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12968, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12969, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12970, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12971, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12972, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12973, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12974, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12975, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12976, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12977, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12978, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12979, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12980, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12981, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12982, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12983, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12984, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12985, training loss 0.000000, validation loss 0.013723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12986, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12987, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12988, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12989, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12990, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12991, training loss 0.000000, validation loss 0.013723\n",
      "Iter 12992, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12993, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12994, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12995, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12996, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12997, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12998, training loss 0.000000, validation loss 0.013722\n",
      "Iter 12999, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13000, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13001, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13002, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13003, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13004, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13005, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13006, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13007, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13008, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13009, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13010, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13011, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13012, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13013, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13014, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13015, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13016, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13017, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13018, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13019, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13020, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13021, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13022, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13023, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13024, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13025, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13026, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13027, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13028, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13029, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13030, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13031, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13032, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13033, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13034, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13035, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13036, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13037, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13038, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13039, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13040, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13041, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13042, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13043, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13044, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13045, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13046, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13047, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13048, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13049, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13050, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13051, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13052, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13053, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13054, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13055, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13056, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13057, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13058, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13059, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13060, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13061, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13062, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13063, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13064, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13065, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13066, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13067, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13068, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13069, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13070, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13071, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13072, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13073, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13074, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13075, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13076, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13077, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13078, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13079, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13080, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13081, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13082, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13083, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13084, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13085, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13086, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13087, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13088, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13089, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13090, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13091, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13092, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13093, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13094, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13095, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13096, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13097, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13098, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13099, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13100, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13101, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13102, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13103, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13104, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13105, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13106, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13107, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13108, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13109, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13110, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13111, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13112, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13113, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13114, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13115, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13116, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13117, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13118, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13119, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13120, training loss 0.000000, validation loss 0.013721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13121, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13122, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13123, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13124, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13125, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13126, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13127, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13128, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13129, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13130, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13131, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13132, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13133, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13134, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13135, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13136, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13137, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13138, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13139, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13140, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13141, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13142, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13143, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13144, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13145, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13146, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13147, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13148, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13149, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13150, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13151, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13152, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13153, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13154, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13155, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13156, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13157, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13158, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13159, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13160, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13161, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13162, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13163, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13164, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13165, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13166, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13167, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13168, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13169, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13170, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13171, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13172, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13173, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13174, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13175, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13176, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13177, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13178, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13179, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13180, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13181, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13182, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13183, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13184, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13185, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13186, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13187, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13188, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13189, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13190, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13191, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13192, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13193, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13194, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13195, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13196, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13197, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13198, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13199, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13200, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13201, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13202, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13203, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13204, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13205, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13206, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13207, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13208, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13209, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13210, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13211, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13212, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13213, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13214, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13215, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13216, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13217, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13218, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13219, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13220, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13221, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13222, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13223, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13224, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13225, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13226, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13227, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13228, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13229, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13230, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13231, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13232, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13233, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13234, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13235, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13236, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13237, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13238, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13239, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13240, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13241, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13242, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13243, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13244, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13245, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13246, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13247, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13248, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13249, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13250, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13251, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13252, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13253, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13254, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13255, training loss 0.000000, validation loss 0.013721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13256, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13257, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13258, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13259, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13260, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13261, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13262, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13263, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13264, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13265, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13266, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13267, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13268, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13269, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13270, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13271, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13272, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13273, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13274, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13275, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13276, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13277, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13278, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13279, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13280, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13281, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13282, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13283, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13284, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13285, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13286, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13287, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13288, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13289, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13290, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13291, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13292, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13293, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13294, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13295, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13296, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13297, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13298, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13299, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13300, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13301, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13302, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13303, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13304, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13305, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13306, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13307, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13308, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13309, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13310, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13311, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13312, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13313, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13314, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13315, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13316, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13317, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13318, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13319, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13320, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13321, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13322, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13323, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13324, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13325, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13326, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13327, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13328, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13329, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13330, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13331, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13332, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13333, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13334, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13335, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13336, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13337, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13338, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13339, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13340, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13341, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13342, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13343, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13344, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13345, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13346, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13347, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13348, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13349, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13350, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13351, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13352, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13353, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13354, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13355, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13356, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13357, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13358, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13359, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13360, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13361, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13362, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13363, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13364, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13365, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13366, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13367, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13368, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13369, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13370, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13371, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13372, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13373, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13374, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13375, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13376, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13377, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13378, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13379, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13380, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13381, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13382, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13383, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13384, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13385, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13386, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13387, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13388, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13389, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13390, training loss 0.000000, validation loss 0.013720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13391, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13392, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13393, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13394, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13395, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13396, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13397, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13398, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13399, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13400, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13401, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13402, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13403, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13404, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13405, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13406, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13407, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13408, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13409, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13410, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13411, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13412, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13413, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13414, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13415, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13416, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13417, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13418, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13419, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13420, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13421, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13422, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13423, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13424, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13425, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13426, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13427, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13428, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13429, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13430, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13431, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13432, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13433, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13434, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13435, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13436, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13437, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13438, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13439, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13440, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13441, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13442, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13443, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13444, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13445, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13446, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13447, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13448, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13449, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13450, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13451, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13452, training loss 0.000000, validation loss 0.013720\n",
      "Iter 13453, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13454, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13455, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13456, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13457, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13458, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13459, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13460, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13461, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13462, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13463, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13464, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13465, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13466, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13467, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13468, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13469, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13470, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13471, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13472, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13473, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13474, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13475, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13476, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13477, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13478, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13479, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13480, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13481, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13482, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13483, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13484, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13485, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13486, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13487, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13488, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13489, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13490, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13491, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13492, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13493, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13494, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13495, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13496, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13497, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13498, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13499, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13500, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13501, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13502, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13503, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13504, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13505, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13506, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13507, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13508, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13509, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13510, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13511, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13512, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13513, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13514, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13515, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13516, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13517, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13518, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13519, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13520, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13521, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13522, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13523, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13524, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13525, training loss 0.000000, validation loss 0.013721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13526, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13527, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13528, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13529, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13530, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13531, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13532, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13533, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13534, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13535, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13536, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13537, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13538, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13539, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13540, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13541, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13542, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13543, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13544, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13545, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13546, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13547, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13548, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13549, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13550, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13551, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13552, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13553, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13554, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13555, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13556, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13557, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13558, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13559, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13560, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13561, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13562, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13563, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13564, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13565, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13566, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13567, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13568, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13569, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13570, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13571, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13572, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13573, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13574, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13575, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13576, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13577, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13578, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13579, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13580, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13581, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13582, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13583, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13584, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13585, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13586, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13587, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13588, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13589, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13590, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13591, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13592, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13593, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13594, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13595, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13596, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13597, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13598, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13599, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13600, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13601, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13602, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13603, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13604, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13605, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13606, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13607, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13608, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13609, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13610, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13611, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13612, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13613, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13614, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13615, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13616, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13617, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13618, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13619, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13620, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13621, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13622, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13623, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13624, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13625, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13626, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13627, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13628, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13629, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13630, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13631, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13632, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13633, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13634, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13635, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13636, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13637, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13638, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13639, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13640, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13641, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13642, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13643, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13644, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13645, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13646, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13647, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13648, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13649, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13650, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13651, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13652, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13653, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13654, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13655, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13656, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13657, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13658, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13659, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13660, training loss 0.000000, validation loss 0.013721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13661, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13662, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13663, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13664, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13665, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13666, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13667, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13668, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13669, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13670, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13671, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13672, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13673, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13674, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13675, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13676, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13677, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13678, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13679, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13680, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13681, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13682, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13683, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13684, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13685, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13686, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13687, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13688, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13689, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13690, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13691, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13692, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13693, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13694, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13695, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13696, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13697, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13698, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13699, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13700, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13701, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13702, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13703, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13704, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13705, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13706, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13707, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13708, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13709, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13710, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13711, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13712, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13713, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13714, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13715, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13716, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13717, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13718, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13719, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13720, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13721, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13722, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13723, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13724, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13725, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13726, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13727, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13728, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13729, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13730, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13731, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13732, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13733, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13734, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13735, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13736, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13737, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13738, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13739, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13740, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13741, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13742, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13743, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13744, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13745, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13746, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13747, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13748, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13749, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13750, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13751, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13752, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13753, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13754, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13755, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13756, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13757, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13758, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13759, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13760, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13761, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13762, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13763, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13764, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13765, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13766, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13767, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13768, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13769, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13770, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13771, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13772, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13773, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13774, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13775, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13776, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13777, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13778, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13779, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13780, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13781, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13782, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13783, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13784, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13785, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13786, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13787, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13788, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13789, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13790, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13791, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13792, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13793, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13794, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13795, training loss 0.000000, validation loss 0.013721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13796, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13797, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13798, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13799, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13800, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13801, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13802, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13803, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13804, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13805, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13806, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13807, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13808, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13809, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13810, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13811, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13812, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13813, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13814, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13815, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13816, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13817, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13818, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13819, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13820, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13821, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13822, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13823, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13824, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13825, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13826, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13827, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13828, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13829, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13830, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13831, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13832, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13833, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13834, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13835, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13836, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13837, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13838, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13839, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13840, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13841, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13842, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13843, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13844, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13845, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13846, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13847, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13848, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13849, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13850, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13851, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13852, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13853, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13854, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13855, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13856, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13857, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13858, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13859, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13860, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13861, training loss 0.000000, validation loss 0.013721\n",
      "Iter 13862, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13863, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13864, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13865, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13866, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13867, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13868, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13869, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13870, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13871, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13872, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13873, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13874, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13875, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13876, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13877, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13878, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13879, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13880, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13881, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13882, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13883, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13884, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13885, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13886, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13887, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13888, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13889, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13890, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13891, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13892, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13893, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13894, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13895, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13896, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13897, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13898, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13899, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13900, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13901, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13902, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13903, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13904, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13905, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13906, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13907, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13908, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13909, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13910, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13911, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13912, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13913, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13914, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13915, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13916, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13917, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13918, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13919, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13920, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13921, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13922, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13923, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13924, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13925, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13926, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13927, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13928, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13929, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13930, training loss 0.000000, validation loss 0.013722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13931, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13932, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13933, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13934, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13935, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13936, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13937, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13938, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13939, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13940, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13941, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13942, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13943, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13944, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13945, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13946, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13947, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13948, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13949, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13950, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13951, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13952, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13953, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13954, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13955, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13956, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13957, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13958, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13959, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13960, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13961, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13962, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13963, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13964, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13965, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13966, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13967, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13968, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13969, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13970, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13971, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13972, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13973, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13974, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13975, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13976, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13977, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13978, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13979, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13980, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13981, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13982, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13983, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13984, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13985, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13986, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13987, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13988, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13989, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13990, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13991, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13992, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13993, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13994, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13995, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13996, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13997, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13998, training loss 0.000000, validation loss 0.013722\n",
      "Iter 13999, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14000, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14001, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14002, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14003, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14004, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14005, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14006, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14007, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14008, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14009, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14010, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14011, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14012, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14013, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14014, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14015, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14016, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14017, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14018, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14019, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14020, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14021, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14022, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14023, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14024, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14025, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14026, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14027, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14028, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14029, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14030, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14031, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14032, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14033, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14034, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14035, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14036, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14037, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14038, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14039, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14040, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14041, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14042, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14043, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14044, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14045, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14046, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14047, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14048, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14049, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14050, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14051, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14052, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14053, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14054, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14055, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14056, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14057, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14058, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14059, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14060, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14061, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14062, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14063, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14064, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14065, training loss 0.000000, validation loss 0.013722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14066, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14067, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14068, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14069, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14070, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14071, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14072, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14073, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14074, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14075, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14076, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14077, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14078, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14079, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14080, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14081, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14082, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14083, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14084, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14085, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14086, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14087, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14088, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14089, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14090, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14091, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14092, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14093, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14094, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14095, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14096, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14097, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14098, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14099, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14100, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14101, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14102, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14103, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14104, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14105, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14106, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14107, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14108, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14109, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14110, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14111, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14112, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14113, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14114, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14115, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14116, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14117, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14118, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14119, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14120, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14121, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14122, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14123, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14124, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14125, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14126, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14127, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14128, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14129, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14130, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14131, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14132, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14133, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14134, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14135, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14136, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14137, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14138, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14139, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14140, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14141, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14142, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14143, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14144, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14145, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14146, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14147, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14148, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14149, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14150, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14151, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14152, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14153, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14154, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14155, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14156, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14157, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14158, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14159, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14160, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14161, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14162, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14163, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14164, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14165, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14166, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14167, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14168, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14169, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14170, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14171, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14172, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14173, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14174, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14175, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14176, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14177, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14178, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14179, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14180, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14181, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14182, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14183, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14184, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14185, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14186, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14187, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14188, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14189, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14190, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14191, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14192, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14193, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14194, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14195, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14196, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14197, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14198, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14199, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14200, training loss 0.000000, validation loss 0.013722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14201, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14202, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14203, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14204, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14205, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14206, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14207, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14208, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14209, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14210, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14211, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14212, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14213, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14214, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14215, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14216, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14217, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14218, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14219, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14220, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14221, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14222, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14223, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14224, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14225, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14226, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14227, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14228, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14229, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14230, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14231, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14232, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14233, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14234, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14235, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14236, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14237, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14238, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14239, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14240, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14241, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14242, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14243, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14244, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14245, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14246, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14247, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14248, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14249, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14250, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14251, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14252, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14253, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14254, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14255, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14256, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14257, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14258, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14259, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14260, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14261, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14262, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14263, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14264, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14265, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14266, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14267, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14268, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14269, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14270, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14271, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14272, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14273, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14274, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14275, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14276, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14277, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14278, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14279, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14280, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14281, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14282, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14283, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14284, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14285, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14286, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14287, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14288, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14289, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14290, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14291, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14292, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14293, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14294, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14295, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14296, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14297, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14298, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14299, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14300, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14301, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14302, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14303, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14304, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14305, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14306, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14307, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14308, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14309, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14310, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14311, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14312, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14313, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14314, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14315, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14316, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14317, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14318, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14319, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14320, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14321, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14322, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14323, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14324, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14325, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14326, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14327, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14328, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14329, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14330, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14331, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14332, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14333, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14334, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14335, training loss 0.000000, validation loss 0.013722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14336, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14337, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14338, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14339, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14340, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14341, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14342, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14343, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14344, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14345, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14346, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14347, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14348, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14349, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14350, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14351, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14352, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14353, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14354, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14355, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14356, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14357, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14358, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14359, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14360, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14361, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14362, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14363, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14364, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14365, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14366, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14367, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14368, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14369, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14370, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14371, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14372, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14373, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14374, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14375, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14376, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14377, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14378, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14379, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14380, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14381, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14382, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14383, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14384, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14385, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14386, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14387, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14388, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14389, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14390, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14391, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14392, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14393, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14394, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14395, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14396, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14397, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14398, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14399, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14400, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14401, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14402, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14403, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14404, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14405, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14406, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14407, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14408, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14409, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14410, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14411, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14412, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14413, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14414, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14415, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14416, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14417, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14418, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14419, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14420, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14421, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14422, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14423, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14424, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14425, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14426, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14427, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14428, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14429, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14430, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14431, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14432, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14433, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14434, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14435, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14436, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14437, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14438, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14439, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14440, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14441, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14442, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14443, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14444, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14445, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14446, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14447, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14448, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14449, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14450, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14451, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14452, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14453, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14454, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14455, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14456, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14457, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14458, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14459, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14460, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14461, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14462, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14463, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14464, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14465, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14466, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14467, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14468, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14469, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14470, training loss 0.000000, validation loss 0.013722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14471, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14472, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14473, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14474, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14475, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14476, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14477, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14478, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14479, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14480, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14481, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14482, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14483, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14484, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14485, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14486, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14487, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14488, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14489, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14490, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14491, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14492, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14493, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14494, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14495, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14496, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14497, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14498, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14499, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14500, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14501, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14502, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14503, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14504, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14505, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14506, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14507, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14508, training loss 0.000000, validation loss 0.013722\n",
      "Iter 14509, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14510, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14511, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14512, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14513, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14514, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14515, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14516, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14517, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14518, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14519, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14520, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14521, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14522, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14523, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14524, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14525, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14526, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14527, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14528, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14529, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14530, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14531, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14532, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14533, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14534, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14535, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14536, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14537, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14538, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14539, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14540, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14541, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14542, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14543, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14544, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14545, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14546, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14547, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14548, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14549, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14550, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14551, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14552, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14553, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14554, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14555, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14556, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14557, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14558, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14559, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14560, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14561, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14562, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14563, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14564, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14565, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14566, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14567, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14568, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14569, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14570, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14571, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14572, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14573, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14574, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14575, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14576, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14577, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14578, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14579, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14580, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14581, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14582, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14583, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14584, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14585, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14586, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14587, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14588, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14589, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14590, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14591, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14592, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14593, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14594, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14595, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14596, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14597, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14598, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14599, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14600, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14601, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14602, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14603, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14604, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14605, training loss 0.000000, validation loss 0.013723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14606, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14607, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14608, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14609, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14610, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14611, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14612, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14613, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14614, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14615, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14616, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14617, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14618, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14619, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14620, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14621, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14622, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14623, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14624, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14625, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14626, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14627, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14628, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14629, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14630, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14631, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14632, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14633, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14634, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14635, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14636, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14637, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14638, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14639, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14640, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14641, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14642, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14643, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14644, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14645, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14646, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14647, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14648, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14649, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14650, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14651, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14652, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14653, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14654, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14655, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14656, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14657, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14658, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14659, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14660, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14661, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14662, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14663, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14664, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14665, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14666, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14667, training loss 0.000000, validation loss 0.013723\n",
      "Iter 14668, training loss 0.000000, validation loss 0.013721\n",
      "Iter 14669, training loss 0.000001, validation loss 0.013729\n",
      "Iter 14670, training loss 0.000010, validation loss 0.013719\n",
      "Iter 14671, training loss 0.000123, validation loss 0.013932\n",
      "Iter 14672, training loss 0.001491, validation loss 0.015369\n",
      "Iter 14673, training loss 0.017606, validation loss 0.036278\n",
      "Iter 14674, training loss 0.158735, validation loss 0.214329\n",
      "Iter 14675, training loss 0.283854, validation loss 0.398019\n",
      "Iter 14676, training loss 0.007072, validation loss 0.015531\n",
      "Iter 14677, training loss 0.089238, validation loss 0.082551\n",
      "Iter 14678, training loss 0.010744, validation loss 0.017099\n",
      "Iter 14679, training loss 0.044934, validation loss 0.050439\n",
      "Iter 14680, training loss 0.012275, validation loss 0.013873\n",
      "Iter 14681, training loss 0.014777, validation loss 0.016658\n",
      "Iter 14682, training loss 0.007024, validation loss 0.010214\n",
      "Iter 14683, training loss 0.008756, validation loss 0.011950\n",
      "Iter 14684, training loss 0.002564, validation loss 0.004089\n",
      "Iter 14685, training loss 0.003452, validation loss 0.004972\n",
      "Iter 14686, training loss 0.002942, validation loss 0.004937\n",
      "Iter 14687, training loss 0.001445, validation loss 0.003149\n",
      "Iter 14688, training loss 0.001916, validation loss 0.003178\n",
      "Iter 14689, training loss 0.001678, validation loss 0.002891\n",
      "Iter 14690, training loss 0.001100, validation loss 0.002474\n",
      "Iter 14691, training loss 0.001261, validation loss 0.002644\n",
      "Iter 14692, training loss 0.000927, validation loss 0.002083\n",
      "Iter 14693, training loss 0.000782, validation loss 0.001644\n",
      "Iter 14694, training loss 0.000960, validation loss 0.001670\n",
      "Iter 14695, training loss 0.000616, validation loss 0.001287\n",
      "Iter 14696, training loss 0.000532, validation loss 0.001163\n",
      "Iter 14697, training loss 0.000625, validation loss 0.001187\n",
      "Iter 14698, training loss 0.000458, validation loss 0.000947\n",
      "Iter 14699, training loss 0.000434, validation loss 0.000857\n",
      "Iter 14700, training loss 0.000523, validation loss 0.000900\n",
      "Iter 14701, training loss 0.000414, validation loss 0.000767\n",
      "Iter 14702, training loss 0.000317, validation loss 0.000660\n",
      "Iter 14703, training loss 0.000378, validation loss 0.000719\n",
      "Iter 14704, training loss 0.000365, validation loss 0.000702\n",
      "Iter 14705, training loss 0.000270, validation loss 0.000597\n",
      "Iter 14706, training loss 0.000277, validation loss 0.000590\n",
      "Iter 14707, training loss 0.000302, validation loss 0.000606\n",
      "Iter 14708, training loss 0.000240, validation loss 0.000538\n",
      "Iter 14709, training loss 0.000206, validation loss 0.000497\n",
      "Iter 14710, training loss 0.000237, validation loss 0.000520\n",
      "Iter 14711, training loss 0.000221, validation loss 0.000502\n",
      "Iter 14712, training loss 0.000178, validation loss 0.000471\n",
      "Iter 14713, training loss 0.000189, validation loss 0.000497\n",
      "Iter 14714, training loss 0.000198, validation loss 0.000515\n",
      "Iter 14715, training loss 0.000167, validation loss 0.000479\n",
      "Iter 14716, training loss 0.000156, validation loss 0.000456\n",
      "Iter 14717, training loss 0.000170, validation loss 0.000463\n",
      "Iter 14718, training loss 0.000158, validation loss 0.000453\n",
      "Iter 14719, training loss 0.000138, validation loss 0.000440\n",
      "Iter 14720, training loss 0.000142, validation loss 0.000452\n",
      "Iter 14721, training loss 0.000144, validation loss 0.000456\n",
      "Iter 14722, training loss 0.000128, validation loss 0.000437\n",
      "Iter 14723, training loss 0.000121, validation loss 0.000424\n",
      "Iter 14724, training loss 0.000125, validation loss 0.000426\n",
      "Iter 14725, training loss 0.000121, validation loss 0.000424\n",
      "Iter 14726, training loss 0.000111, validation loss 0.000421\n",
      "Iter 14727, training loss 0.000111, validation loss 0.000430\n",
      "Iter 14728, training loss 0.000114, validation loss 0.000437\n",
      "Iter 14729, training loss 0.000107, validation loss 0.000429\n",
      "Iter 14730, training loss 0.000101, validation loss 0.000421\n",
      "Iter 14731, training loss 0.000103, validation loss 0.000420\n",
      "Iter 14732, training loss 0.000102, validation loss 0.000419\n",
      "Iter 14733, training loss 0.000096, validation loss 0.000418\n",
      "Iter 14734, training loss 0.000094, validation loss 0.000422\n",
      "Iter 14735, training loss 0.000095, validation loss 0.000427\n",
      "Iter 14736, training loss 0.000091, validation loss 0.000425\n",
      "Iter 14737, training loss 0.000088, validation loss 0.000419\n",
      "Iter 14738, training loss 0.000087, validation loss 0.000416\n",
      "Iter 14739, training loss 0.000087, validation loss 0.000416\n",
      "Iter 14740, training loss 0.000084, validation loss 0.000416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14741, training loss 0.000082, validation loss 0.000418\n",
      "Iter 14742, training loss 0.000082, validation loss 0.000421\n",
      "Iter 14743, training loss 0.000081, validation loss 0.000420\n",
      "Iter 14744, training loss 0.000078, validation loss 0.000416\n",
      "Iter 14745, training loss 0.000077, validation loss 0.000413\n",
      "Iter 14746, training loss 0.000077, validation loss 0.000412\n",
      "Iter 14747, training loss 0.000075, validation loss 0.000412\n",
      "Iter 14748, training loss 0.000074, validation loss 0.000413\n",
      "Iter 14749, training loss 0.000073, validation loss 0.000416\n",
      "Iter 14750, training loss 0.000072, validation loss 0.000416\n",
      "Iter 14751, training loss 0.000071, validation loss 0.000415\n",
      "Iter 14752, training loss 0.000070, validation loss 0.000413\n",
      "Iter 14753, training loss 0.000070, validation loss 0.000413\n",
      "Iter 14754, training loss 0.000069, validation loss 0.000413\n",
      "Iter 14755, training loss 0.000068, validation loss 0.000414\n",
      "Iter 14756, training loss 0.000067, validation loss 0.000416\n",
      "Iter 14757, training loss 0.000066, validation loss 0.000416\n",
      "Iter 14758, training loss 0.000065, validation loss 0.000415\n",
      "Iter 14759, training loss 0.000065, validation loss 0.000414\n",
      "Iter 14760, training loss 0.000064, validation loss 0.000414\n",
      "Iter 14761, training loss 0.000063, validation loss 0.000414\n",
      "Iter 14762, training loss 0.000062, validation loss 0.000414\n",
      "Iter 14763, training loss 0.000062, validation loss 0.000415\n",
      "Iter 14764, training loss 0.000061, validation loss 0.000416\n",
      "Iter 14765, training loss 0.000061, validation loss 0.000415\n",
      "Iter 14766, training loss 0.000060, validation loss 0.000414\n",
      "Iter 14767, training loss 0.000059, validation loss 0.000413\n",
      "Iter 14768, training loss 0.000059, validation loss 0.000413\n",
      "Iter 14769, training loss 0.000058, validation loss 0.000413\n",
      "Iter 14770, training loss 0.000058, validation loss 0.000413\n",
      "Iter 14771, training loss 0.000057, validation loss 0.000414\n",
      "Iter 14772, training loss 0.000057, validation loss 0.000413\n",
      "Iter 14773, training loss 0.000056, validation loss 0.000413\n",
      "Iter 14774, training loss 0.000056, validation loss 0.000412\n",
      "Iter 14775, training loss 0.000055, validation loss 0.000412\n",
      "Iter 14776, training loss 0.000055, validation loss 0.000413\n",
      "Iter 14777, training loss 0.000054, validation loss 0.000413\n",
      "Iter 14778, training loss 0.000054, validation loss 0.000414\n",
      "Iter 14779, training loss 0.000053, validation loss 0.000414\n",
      "Iter 14780, training loss 0.000053, validation loss 0.000414\n",
      "Iter 14781, training loss 0.000052, validation loss 0.000413\n",
      "Iter 14782, training loss 0.000052, validation loss 0.000413\n",
      "Iter 14783, training loss 0.000051, validation loss 0.000413\n",
      "Iter 14784, training loss 0.000051, validation loss 0.000414\n",
      "Iter 14785, training loss 0.000051, validation loss 0.000414\n",
      "Iter 14786, training loss 0.000050, validation loss 0.000414\n",
      "Iter 14787, training loss 0.000050, validation loss 0.000413\n",
      "Iter 14788, training loss 0.000049, validation loss 0.000413\n",
      "Iter 14789, training loss 0.000049, validation loss 0.000413\n",
      "Iter 14790, training loss 0.000049, validation loss 0.000413\n",
      "Iter 14791, training loss 0.000048, validation loss 0.000413\n",
      "Iter 14792, training loss 0.000048, validation loss 0.000413\n",
      "Iter 14793, training loss 0.000048, validation loss 0.000413\n",
      "Iter 14794, training loss 0.000047, validation loss 0.000413\n",
      "Iter 14795, training loss 0.000047, validation loss 0.000413\n",
      "Iter 14796, training loss 0.000047, validation loss 0.000412\n",
      "Iter 14797, training loss 0.000046, validation loss 0.000413\n",
      "Iter 14798, training loss 0.000046, validation loss 0.000413\n",
      "Iter 14799, training loss 0.000046, validation loss 0.000413\n",
      "Iter 14800, training loss 0.000045, validation loss 0.000413\n",
      "Iter 14801, training loss 0.000045, validation loss 0.000412\n",
      "Iter 14802, training loss 0.000045, validation loss 0.000412\n",
      "Iter 14803, training loss 0.000044, validation loss 0.000412\n",
      "Iter 14804, training loss 0.000044, validation loss 0.000412\n",
      "Iter 14805, training loss 0.000044, validation loss 0.000412\n",
      "Iter 14806, training loss 0.000044, validation loss 0.000412\n",
      "Iter 14807, training loss 0.000043, validation loss 0.000412\n",
      "Iter 14808, training loss 0.000043, validation loss 0.000412\n",
      "Iter 14809, training loss 0.000043, validation loss 0.000412\n",
      "Iter 14810, training loss 0.000042, validation loss 0.000412\n",
      "Iter 14811, training loss 0.000042, validation loss 0.000412\n",
      "Iter 14812, training loss 0.000042, validation loss 0.000412\n",
      "Iter 14813, training loss 0.000042, validation loss 0.000412\n",
      "Iter 14814, training loss 0.000041, validation loss 0.000412\n",
      "Iter 14815, training loss 0.000041, validation loss 0.000412\n",
      "Iter 14816, training loss 0.000041, validation loss 0.000412\n",
      "Iter 14817, training loss 0.000041, validation loss 0.000412\n",
      "Iter 14818, training loss 0.000040, validation loss 0.000412\n",
      "Iter 14819, training loss 0.000040, validation loss 0.000412\n",
      "Iter 14820, training loss 0.000040, validation loss 0.000412\n",
      "Iter 14821, training loss 0.000040, validation loss 0.000412\n",
      "Iter 14822, training loss 0.000039, validation loss 0.000412\n",
      "Iter 14823, training loss 0.000039, validation loss 0.000412\n",
      "Iter 14824, training loss 0.000039, validation loss 0.000412\n",
      "Iter 14825, training loss 0.000039, validation loss 0.000412\n",
      "Iter 14826, training loss 0.000039, validation loss 0.000412\n",
      "Iter 14827, training loss 0.000038, validation loss 0.000412\n",
      "Iter 14828, training loss 0.000038, validation loss 0.000412\n",
      "Iter 14829, training loss 0.000038, validation loss 0.000412\n",
      "Iter 14830, training loss 0.000038, validation loss 0.000412\n",
      "Iter 14831, training loss 0.000037, validation loss 0.000412\n",
      "Iter 14832, training loss 0.000037, validation loss 0.000412\n",
      "Iter 14833, training loss 0.000037, validation loss 0.000412\n",
      "Iter 14834, training loss 0.000037, validation loss 0.000412\n",
      "Iter 14835, training loss 0.000037, validation loss 0.000412\n",
      "Iter 14836, training loss 0.000037, validation loss 0.000412\n",
      "Iter 14837, training loss 0.000036, validation loss 0.000412\n",
      "Iter 14838, training loss 0.000036, validation loss 0.000412\n",
      "Iter 14839, training loss 0.000036, validation loss 0.000412\n",
      "Iter 14840, training loss 0.000036, validation loss 0.000412\n",
      "Iter 14841, training loss 0.000036, validation loss 0.000412\n",
      "Iter 14842, training loss 0.000035, validation loss 0.000411\n",
      "Iter 14843, training loss 0.000035, validation loss 0.000411\n",
      "Iter 14844, training loss 0.000035, validation loss 0.000411\n",
      "Iter 14845, training loss 0.000035, validation loss 0.000411\n",
      "Iter 14846, training loss 0.000035, validation loss 0.000411\n",
      "Iter 14847, training loss 0.000035, validation loss 0.000411\n",
      "Iter 14848, training loss 0.000034, validation loss 0.000411\n",
      "Iter 14849, training loss 0.000034, validation loss 0.000411\n",
      "Iter 14850, training loss 0.000034, validation loss 0.000411\n",
      "Iter 14851, training loss 0.000034, validation loss 0.000411\n",
      "Iter 14852, training loss 0.000034, validation loss 0.000411\n",
      "Iter 14853, training loss 0.000034, validation loss 0.000411\n",
      "Iter 14854, training loss 0.000033, validation loss 0.000411\n",
      "Iter 14855, training loss 0.000033, validation loss 0.000411\n",
      "Iter 14856, training loss 0.000033, validation loss 0.000411\n",
      "Iter 14857, training loss 0.000033, validation loss 0.000411\n",
      "Iter 14858, training loss 0.000033, validation loss 0.000411\n",
      "Iter 14859, training loss 0.000033, validation loss 0.000411\n",
      "Iter 14860, training loss 0.000033, validation loss 0.000411\n",
      "Iter 14861, training loss 0.000032, validation loss 0.000411\n",
      "Iter 14862, training loss 0.000032, validation loss 0.000411\n",
      "Iter 14863, training loss 0.000032, validation loss 0.000411\n",
      "Iter 14864, training loss 0.000032, validation loss 0.000411\n",
      "Iter 14865, training loss 0.000032, validation loss 0.000411\n",
      "Iter 14866, training loss 0.000032, validation loss 0.000411\n",
      "Iter 14867, training loss 0.000032, validation loss 0.000411\n",
      "Iter 14868, training loss 0.000031, validation loss 0.000411\n",
      "Iter 14869, training loss 0.000031, validation loss 0.000411\n",
      "Iter 14870, training loss 0.000031, validation loss 0.000410\n",
      "Iter 14871, training loss 0.000031, validation loss 0.000410\n",
      "Iter 14872, training loss 0.000031, validation loss 0.000410\n",
      "Iter 14873, training loss 0.000031, validation loss 0.000410\n",
      "Iter 14874, training loss 0.000031, validation loss 0.000410\n",
      "Iter 14875, training loss 0.000031, validation loss 0.000410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14876, training loss 0.000030, validation loss 0.000410\n",
      "Iter 14877, training loss 0.000030, validation loss 0.000410\n",
      "Iter 14878, training loss 0.000030, validation loss 0.000410\n",
      "Iter 14879, training loss 0.000030, validation loss 0.000410\n",
      "Iter 14880, training loss 0.000030, validation loss 0.000410\n",
      "Iter 14881, training loss 0.000030, validation loss 0.000410\n",
      "Iter 14882, training loss 0.000030, validation loss 0.000410\n",
      "Iter 14883, training loss 0.000030, validation loss 0.000410\n",
      "Iter 14884, training loss 0.000029, validation loss 0.000410\n",
      "Iter 14885, training loss 0.000029, validation loss 0.000410\n",
      "Iter 14886, training loss 0.000029, validation loss 0.000410\n",
      "Iter 14887, training loss 0.000029, validation loss 0.000410\n",
      "Iter 14888, training loss 0.000029, validation loss 0.000410\n",
      "Iter 14889, training loss 0.000029, validation loss 0.000410\n",
      "Iter 14890, training loss 0.000029, validation loss 0.000410\n",
      "Iter 14891, training loss 0.000029, validation loss 0.000410\n",
      "Iter 14892, training loss 0.000029, validation loss 0.000409\n",
      "Iter 14893, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14894, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14895, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14896, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14897, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14898, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14899, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14900, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14901, training loss 0.000028, validation loss 0.000409\n",
      "Iter 14902, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14903, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14904, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14905, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14906, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14907, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14908, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14909, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14910, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14911, training loss 0.000027, validation loss 0.000409\n",
      "Iter 14912, training loss 0.000026, validation loss 0.000409\n",
      "Iter 14913, training loss 0.000026, validation loss 0.000409\n",
      "Iter 14914, training loss 0.000026, validation loss 0.000409\n",
      "Iter 14915, training loss 0.000026, validation loss 0.000409\n",
      "Iter 14916, training loss 0.000026, validation loss 0.000408\n",
      "Iter 14917, training loss 0.000026, validation loss 0.000408\n",
      "Iter 14918, training loss 0.000026, validation loss 0.000408\n",
      "Iter 14919, training loss 0.000026, validation loss 0.000408\n",
      "Iter 14920, training loss 0.000026, validation loss 0.000408\n",
      "Iter 14921, training loss 0.000026, validation loss 0.000408\n",
      "Iter 14922, training loss 0.000026, validation loss 0.000408\n",
      "Iter 14923, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14924, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14925, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14926, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14927, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14928, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14929, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14930, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14931, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14932, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14933, training loss 0.000025, validation loss 0.000408\n",
      "Iter 14934, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14935, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14936, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14937, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14938, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14939, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14940, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14941, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14942, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14943, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14944, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14945, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14946, training loss 0.000024, validation loss 0.000408\n",
      "Iter 14947, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14948, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14949, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14950, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14951, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14952, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14953, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14954, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14955, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14956, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14957, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14958, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14959, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14960, training loss 0.000023, validation loss 0.000408\n",
      "Iter 14961, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14962, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14963, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14964, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14965, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14966, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14967, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14968, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14969, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14970, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14971, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14972, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14973, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14974, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14975, training loss 0.000022, validation loss 0.000407\n",
      "Iter 14976, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14977, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14978, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14979, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14980, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14981, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14982, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14983, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14984, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14985, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14986, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14987, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14988, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14989, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14990, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14991, training loss 0.000021, validation loss 0.000407\n",
      "Iter 14992, training loss 0.000020, validation loss 0.000407\n",
      "Iter 14993, training loss 0.000020, validation loss 0.000407\n",
      "Iter 14994, training loss 0.000020, validation loss 0.000407\n",
      "Iter 14995, training loss 0.000020, validation loss 0.000407\n",
      "Iter 14996, training loss 0.000020, validation loss 0.000407\n",
      "Iter 14997, training loss 0.000020, validation loss 0.000406\n",
      "Iter 14998, training loss 0.000020, validation loss 0.000406\n",
      "Iter 14999, training loss 0.000020, validation loss 0.000407\n",
      "Iter 15000, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15001, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15002, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15003, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15004, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15005, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15006, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15007, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15008, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15009, training loss 0.000020, validation loss 0.000406\n",
      "Iter 15010, training loss 0.000019, validation loss 0.000406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15011, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15012, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15013, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15014, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15015, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15016, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15017, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15018, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15019, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15020, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15021, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15022, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15023, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15024, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15025, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15026, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15027, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15028, training loss 0.000019, validation loss 0.000406\n",
      "Iter 15029, training loss 0.000018, validation loss 0.000406\n",
      "Iter 15030, training loss 0.000018, validation loss 0.000406\n",
      "Iter 15031, training loss 0.000018, validation loss 0.000406\n",
      "Iter 15032, training loss 0.000018, validation loss 0.000406\n",
      "Iter 15033, training loss 0.000018, validation loss 0.000406\n",
      "Iter 15034, training loss 0.000018, validation loss 0.000406\n",
      "Iter 15035, training loss 0.000018, validation loss 0.000406\n",
      "Iter 15036, training loss 0.000018, validation loss 0.000406\n",
      "Iter 15037, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15038, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15039, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15040, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15041, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15042, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15043, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15044, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15045, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15046, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15047, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15048, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15049, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15050, training loss 0.000018, validation loss 0.000405\n",
      "Iter 15051, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15052, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15053, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15054, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15055, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15056, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15057, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15058, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15059, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15060, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15061, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15062, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15063, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15064, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15065, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15066, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15067, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15068, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15069, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15070, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15071, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15072, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15073, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15074, training loss 0.000017, validation loss 0.000405\n",
      "Iter 15075, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15076, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15077, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15078, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15079, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15080, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15081, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15082, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15083, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15084, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15085, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15086, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15087, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15088, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15089, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15090, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15091, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15092, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15093, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15094, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15095, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15096, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15097, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15098, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15099, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15100, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15101, training loss 0.000016, validation loss 0.000405\n",
      "Iter 15102, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15103, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15104, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15105, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15106, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15107, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15108, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15109, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15110, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15111, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15112, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15113, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15114, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15115, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15116, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15117, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15118, training loss 0.000015, validation loss 0.000405\n",
      "Iter 15119, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15120, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15121, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15122, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15123, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15124, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15125, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15126, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15127, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15128, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15129, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15130, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15131, training loss 0.000015, validation loss 0.000406\n",
      "Iter 15132, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15133, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15134, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15135, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15136, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15137, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15138, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15139, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15140, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15141, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15142, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15143, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15144, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15145, training loss 0.000014, validation loss 0.000406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15146, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15147, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15148, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15149, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15150, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15151, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15152, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15153, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15154, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15155, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15156, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15157, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15158, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15159, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15160, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15161, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15162, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15163, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15164, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15165, training loss 0.000014, validation loss 0.000406\n",
      "Iter 15166, training loss 0.000014, validation loss 0.000407\n",
      "Iter 15167, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15168, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15169, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15170, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15171, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15172, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15173, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15174, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15175, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15176, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15177, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15178, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15179, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15180, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15181, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15182, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15183, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15184, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15185, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15186, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15187, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15188, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15189, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15190, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15191, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15192, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15193, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15194, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15195, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15196, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15197, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15198, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15199, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15200, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15201, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15202, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15203, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15204, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15205, training loss 0.000013, validation loss 0.000407\n",
      "Iter 15206, training loss 0.000012, validation loss 0.000407\n",
      "Iter 15207, training loss 0.000012, validation loss 0.000407\n",
      "Iter 15208, training loss 0.000012, validation loss 0.000407\n",
      "Iter 15209, training loss 0.000012, validation loss 0.000407\n",
      "Iter 15210, training loss 0.000012, validation loss 0.000407\n",
      "Iter 15211, training loss 0.000012, validation loss 0.000407\n",
      "Iter 15212, training loss 0.000012, validation loss 0.000407\n",
      "Iter 15213, training loss 0.000012, validation loss 0.000407\n",
      "Iter 15214, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15215, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15216, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15217, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15218, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15219, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15220, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15221, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15222, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15223, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15224, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15225, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15226, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15227, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15228, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15229, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15230, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15231, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15232, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15233, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15234, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15235, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15236, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15237, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15238, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15239, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15240, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15241, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15242, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15243, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15244, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15245, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15246, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15247, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15248, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15249, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15250, training loss 0.000012, validation loss 0.000408\n",
      "Iter 15251, training loss 0.000011, validation loss 0.000408\n",
      "Iter 15252, training loss 0.000011, validation loss 0.000408\n",
      "Iter 15253, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15254, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15255, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15256, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15257, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15258, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15259, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15260, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15261, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15262, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15263, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15264, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15265, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15266, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15267, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15268, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15269, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15270, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15271, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15272, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15273, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15274, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15275, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15276, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15277, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15278, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15279, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15280, training loss 0.000011, validation loss 0.000409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15281, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15282, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15283, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15284, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15285, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15286, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15287, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15288, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15289, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15290, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15291, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15292, training loss 0.000011, validation loss 0.000409\n",
      "Iter 15293, training loss 0.000011, validation loss 0.000410\n",
      "Iter 15294, training loss 0.000011, validation loss 0.000410\n",
      "Iter 15295, training loss 0.000011, validation loss 0.000410\n",
      "Iter 15296, training loss 0.000011, validation loss 0.000410\n",
      "Iter 15297, training loss 0.000011, validation loss 0.000410\n",
      "Iter 15298, training loss 0.000011, validation loss 0.000410\n",
      "Iter 15299, training loss 0.000011, validation loss 0.000410\n",
      "Iter 15300, training loss 0.000011, validation loss 0.000410\n",
      "Iter 15301, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15302, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15303, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15304, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15305, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15306, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15307, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15308, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15309, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15310, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15311, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15312, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15313, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15314, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15315, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15316, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15317, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15318, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15319, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15320, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15321, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15322, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15323, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15324, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15325, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15326, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15327, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15328, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15329, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15330, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15331, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15332, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15333, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15334, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15335, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15336, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15337, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15338, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15339, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15340, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15341, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15342, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15343, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15344, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15345, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15346, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15347, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15348, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15349, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15350, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15351, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15352, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15353, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15354, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15355, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15356, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15357, training loss 0.000010, validation loss 0.000410\n",
      "Iter 15358, training loss 0.000010, validation loss 0.000411\n",
      "Iter 15359, training loss 0.000010, validation loss 0.000411\n",
      "Iter 15360, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15361, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15362, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15363, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15364, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15365, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15366, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15367, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15368, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15369, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15370, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15371, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15372, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15373, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15374, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15375, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15376, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15377, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15378, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15379, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15380, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15381, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15382, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15383, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15384, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15385, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15386, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15387, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15388, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15389, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15390, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15391, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15392, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15393, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15394, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15395, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15396, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15397, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15398, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15399, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15400, training loss 0.000009, validation loss 0.000411\n",
      "Iter 15401, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15402, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15403, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15404, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15405, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15406, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15407, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15408, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15409, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15410, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15411, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15412, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15413, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15414, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15415, training loss 0.000009, validation loss 0.000412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15416, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15417, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15418, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15419, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15420, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15421, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15422, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15423, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15424, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15425, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15426, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15427, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15428, training loss 0.000009, validation loss 0.000412\n",
      "Iter 15429, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15430, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15431, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15432, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15433, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15434, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15435, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15436, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15437, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15438, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15439, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15440, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15441, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15442, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15443, training loss 0.000008, validation loss 0.000412\n",
      "Iter 15444, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15445, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15446, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15447, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15448, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15449, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15450, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15451, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15452, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15453, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15454, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15455, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15456, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15457, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15458, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15459, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15460, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15461, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15462, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15463, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15464, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15465, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15466, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15467, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15468, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15469, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15470, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15471, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15472, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15473, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15474, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15475, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15476, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15477, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15478, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15479, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15480, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15481, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15482, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15483, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15484, training loss 0.000008, validation loss 0.000413\n",
      "Iter 15485, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15486, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15487, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15488, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15489, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15490, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15491, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15492, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15493, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15494, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15495, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15496, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15497, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15498, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15499, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15500, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15501, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15502, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15503, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15504, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15505, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15506, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15507, training loss 0.000008, validation loss 0.000414\n",
      "Iter 15508, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15509, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15510, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15511, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15512, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15513, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15514, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15515, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15516, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15517, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15518, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15519, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15520, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15521, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15522, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15523, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15524, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15525, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15526, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15527, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15528, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15529, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15530, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15531, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15532, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15533, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15534, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15535, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15536, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15537, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15538, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15539, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15540, training loss 0.000007, validation loss 0.000414\n",
      "Iter 15541, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15542, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15543, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15544, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15545, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15546, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15547, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15548, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15549, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15550, training loss 0.000007, validation loss 0.000415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15551, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15552, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15553, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15554, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15555, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15556, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15557, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15558, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15559, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15560, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15561, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15562, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15563, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15564, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15565, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15566, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15567, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15568, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15569, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15570, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15571, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15572, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15573, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15574, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15575, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15576, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15577, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15578, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15579, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15580, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15581, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15582, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15583, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15584, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15585, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15586, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15587, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15588, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15589, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15590, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15591, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15592, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15593, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15594, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15595, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15596, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15597, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15598, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15599, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15600, training loss 0.000007, validation loss 0.000415\n",
      "Iter 15601, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15602, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15603, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15604, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15605, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15606, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15607, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15608, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15609, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15610, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15611, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15612, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15613, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15614, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15615, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15616, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15617, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15618, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15619, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15620, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15621, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15622, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15623, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15624, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15625, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15626, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15627, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15628, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15629, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15630, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15631, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15632, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15633, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15634, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15635, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15636, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15637, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15638, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15639, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15640, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15641, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15642, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15643, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15644, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15645, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15646, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15647, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15648, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15649, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15650, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15651, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15652, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15653, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15654, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15655, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15656, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15657, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15658, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15659, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15660, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15661, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15662, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15663, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15664, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15665, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15666, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15667, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15668, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15669, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15670, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15671, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15672, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15673, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15674, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15675, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15676, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15677, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15678, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15679, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15680, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15681, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15682, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15683, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15684, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15685, training loss 0.000006, validation loss 0.000415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15686, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15687, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15688, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15689, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15690, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15691, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15692, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15693, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15694, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15695, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15696, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15697, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15698, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15699, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15700, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15701, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15702, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15703, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15704, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15705, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15706, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15707, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15708, training loss 0.000006, validation loss 0.000415\n",
      "Iter 15709, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15710, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15711, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15712, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15713, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15714, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15715, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15716, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15717, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15718, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15719, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15720, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15721, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15722, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15723, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15724, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15725, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15726, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15727, training loss 0.000006, validation loss 0.000416\n",
      "Iter 15728, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15729, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15730, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15731, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15732, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15733, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15734, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15735, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15736, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15737, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15738, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15739, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15740, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15741, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15742, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15743, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15744, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15745, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15746, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15747, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15748, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15749, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15750, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15751, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15752, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15753, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15754, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15755, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15756, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15757, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15758, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15759, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15760, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15761, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15762, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15763, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15764, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15765, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15766, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15767, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15768, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15769, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15770, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15771, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15772, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15773, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15774, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15775, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15776, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15777, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15778, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15779, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15780, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15781, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15782, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15783, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15784, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15785, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15786, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15787, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15788, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15789, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15790, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15791, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15792, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15793, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15794, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15795, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15796, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15797, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15798, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15799, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15800, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15801, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15802, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15803, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15804, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15805, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15806, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15807, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15808, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15809, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15810, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15811, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15812, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15813, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15814, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15815, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15816, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15817, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15818, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15819, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15820, training loss 0.000005, validation loss 0.000416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15821, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15822, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15823, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15824, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15825, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15826, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15827, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15828, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15829, training loss 0.000005, validation loss 0.000416\n",
      "Iter 15830, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15831, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15832, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15833, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15834, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15835, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15836, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15837, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15838, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15839, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15840, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15841, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15842, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15843, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15844, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15845, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15846, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15847, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15848, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15849, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15850, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15851, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15852, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15853, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15854, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15855, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15856, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15857, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15858, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15859, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15860, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15861, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15862, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15863, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15864, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15865, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15866, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15867, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15868, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15869, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15870, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15871, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15872, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15873, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15874, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15875, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15876, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15877, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15878, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15879, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15880, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15881, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15882, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15883, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15884, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15885, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15886, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15887, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15888, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15889, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15890, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15891, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15892, training loss 0.000005, validation loss 0.000417\n",
      "Iter 15893, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15894, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15895, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15896, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15897, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15898, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15899, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15900, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15901, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15902, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15903, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15904, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15905, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15906, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15907, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15908, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15909, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15910, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15911, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15912, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15913, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15914, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15915, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15916, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15917, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15918, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15919, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15920, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15921, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15922, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15923, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15924, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15925, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15926, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15927, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15928, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15929, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15930, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15931, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15932, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15933, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15934, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15935, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15936, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15937, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15938, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15939, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15940, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15941, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15942, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15943, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15944, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15945, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15946, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15947, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15948, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15949, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15950, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15951, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15952, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15953, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15954, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15955, training loss 0.000004, validation loss 0.000417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15956, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15957, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15958, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15959, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15960, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15961, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15962, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15963, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15964, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15965, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15966, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15967, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15968, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15969, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15970, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15971, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15972, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15973, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15974, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15975, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15976, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15977, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15978, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15979, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15980, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15981, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15982, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15983, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15984, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15985, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15986, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15987, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15988, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15989, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15990, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15991, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15992, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15993, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15994, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15995, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15996, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15997, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15998, training loss 0.000004, validation loss 0.000417\n",
      "Iter 15999, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16000, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16001, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16002, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16003, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16004, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16005, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16006, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16007, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16008, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16009, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16010, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16011, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16012, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16013, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16014, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16015, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16016, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16017, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16018, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16019, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16020, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16021, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16022, training loss 0.000004, validation loss 0.000417\n",
      "Iter 16023, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16024, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16025, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16026, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16027, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16028, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16029, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16030, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16031, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16032, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16033, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16034, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16035, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16036, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16037, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16038, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16039, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16040, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16041, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16042, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16043, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16044, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16045, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16046, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16047, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16048, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16049, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16050, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16051, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16052, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16053, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16054, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16055, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16056, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16057, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16058, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16059, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16060, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16061, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16062, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16063, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16064, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16065, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16066, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16067, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16068, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16069, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16070, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16071, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16072, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16073, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16074, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16075, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16076, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16077, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16078, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16079, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16080, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16081, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16082, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16083, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16084, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16085, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16086, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16087, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16088, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16089, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16090, training loss 0.000004, validation loss 0.000418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16091, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16092, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16093, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16094, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16095, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16096, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16097, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16098, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16099, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16100, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16101, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16102, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16103, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16104, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16105, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16106, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16107, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16108, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16109, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16110, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16111, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16112, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16113, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16114, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16115, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16116, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16117, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16118, training loss 0.000004, validation loss 0.000418\n",
      "Iter 16119, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16120, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16121, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16122, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16123, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16124, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16125, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16126, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16127, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16128, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16129, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16130, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16131, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16132, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16133, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16134, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16135, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16136, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16137, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16138, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16139, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16140, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16141, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16142, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16143, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16144, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16145, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16146, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16147, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16148, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16149, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16150, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16151, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16152, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16153, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16154, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16155, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16156, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16157, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16158, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16159, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16160, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16161, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16162, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16163, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16164, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16165, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16166, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16167, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16168, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16169, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16170, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16171, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16172, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16173, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16174, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16175, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16176, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16177, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16178, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16179, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16180, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16181, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16182, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16183, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16184, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16185, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16186, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16187, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16188, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16189, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16190, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16191, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16192, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16193, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16194, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16195, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16196, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16197, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16198, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16199, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16200, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16201, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16202, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16203, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16204, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16205, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16206, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16207, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16208, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16209, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16210, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16211, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16212, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16213, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16214, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16215, training loss 0.000003, validation loss 0.000418\n",
      "Iter 16216, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16217, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16218, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16219, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16220, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16221, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16222, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16223, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16224, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16225, training loss 0.000003, validation loss 0.000419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16226, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16227, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16228, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16229, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16230, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16231, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16232, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16233, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16234, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16235, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16236, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16237, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16238, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16239, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16240, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16241, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16242, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16243, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16244, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16245, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16246, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16247, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16248, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16249, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16250, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16251, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16252, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16253, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16254, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16255, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16256, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16257, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16258, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16259, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16260, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16261, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16262, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16263, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16264, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16265, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16266, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16267, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16268, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16269, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16270, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16271, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16272, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16273, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16274, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16275, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16276, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16277, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16278, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16279, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16280, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16281, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16282, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16283, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16284, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16285, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16286, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16287, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16288, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16289, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16290, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16291, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16292, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16293, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16294, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16295, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16296, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16297, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16298, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16299, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16300, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16301, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16302, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16303, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16304, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16305, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16306, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16307, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16308, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16309, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16310, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16311, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16312, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16313, training loss 0.000003, validation loss 0.000419\n",
      "Iter 16314, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16315, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16316, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16317, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16318, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16319, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16320, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16321, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16322, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16323, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16324, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16325, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16326, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16327, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16328, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16329, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16330, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16331, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16332, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16333, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16334, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16335, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16336, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16337, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16338, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16339, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16340, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16341, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16342, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16343, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16344, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16345, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16346, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16347, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16348, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16349, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16350, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16351, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16352, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16353, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16354, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16355, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16356, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16357, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16358, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16359, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16360, training loss 0.000003, validation loss 0.000420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16361, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16362, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16363, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16364, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16365, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16366, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16367, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16368, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16369, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16370, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16371, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16372, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16373, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16374, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16375, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16376, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16377, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16378, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16379, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16380, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16381, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16382, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16383, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16384, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16385, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16386, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16387, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16388, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16389, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16390, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16391, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16392, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16393, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16394, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16395, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16396, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16397, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16398, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16399, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16400, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16401, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16402, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16403, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16404, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16405, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16406, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16407, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16408, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16409, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16410, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16411, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16412, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16413, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16414, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16415, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16416, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16417, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16418, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16419, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16420, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16421, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16422, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16423, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16424, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16425, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16426, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16427, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16428, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16429, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16430, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16431, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16432, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16433, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16434, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16435, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16436, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16437, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16438, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16439, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16440, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16441, training loss 0.000003, validation loss 0.000420\n",
      "Iter 16442, training loss 0.000002, validation loss 0.000420\n",
      "Iter 16443, training loss 0.000002, validation loss 0.000420\n",
      "Iter 16444, training loss 0.000002, validation loss 0.000420\n",
      "Iter 16445, training loss 0.000002, validation loss 0.000420\n",
      "Iter 16446, training loss 0.000002, validation loss 0.000420\n",
      "Iter 16447, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16448, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16449, training loss 0.000002, validation loss 0.000420\n",
      "Iter 16450, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16451, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16452, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16453, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16454, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16455, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16456, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16457, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16458, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16459, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16460, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16461, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16462, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16463, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16464, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16465, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16466, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16467, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16468, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16469, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16470, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16471, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16472, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16473, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16474, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16475, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16476, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16477, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16478, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16479, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16480, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16481, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16482, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16483, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16484, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16485, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16486, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16487, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16488, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16489, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16490, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16491, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16492, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16493, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16494, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16495, training loss 0.000002, validation loss 0.000421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16496, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16497, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16498, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16499, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16500, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16501, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16502, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16503, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16504, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16505, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16506, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16507, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16508, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16509, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16510, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16511, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16512, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16513, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16514, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16515, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16516, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16517, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16518, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16519, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16520, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16521, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16522, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16523, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16524, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16525, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16526, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16527, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16528, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16529, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16530, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16531, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16532, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16533, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16534, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16535, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16536, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16537, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16538, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16539, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16540, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16541, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16542, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16543, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16544, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16545, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16546, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16547, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16548, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16549, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16550, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16551, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16552, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16553, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16554, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16555, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16556, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16557, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16558, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16559, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16560, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16561, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16562, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16563, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16564, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16565, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16566, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16567, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16568, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16569, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16570, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16571, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16572, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16573, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16574, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16575, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16576, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16577, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16578, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16579, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16580, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16581, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16582, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16583, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16584, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16585, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16586, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16587, training loss 0.000002, validation loss 0.000421\n",
      "Iter 16588, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16589, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16590, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16591, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16592, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16593, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16594, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16595, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16596, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16597, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16598, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16599, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16600, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16601, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16602, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16603, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16604, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16605, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16606, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16607, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16608, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16609, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16610, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16611, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16612, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16613, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16614, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16615, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16616, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16617, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16618, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16619, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16620, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16621, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16622, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16623, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16624, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16625, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16626, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16627, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16628, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16629, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16630, training loss 0.000002, validation loss 0.000422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16631, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16632, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16633, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16634, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16635, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16636, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16637, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16638, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16639, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16640, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16641, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16642, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16643, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16644, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16645, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16646, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16647, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16648, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16649, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16650, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16651, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16652, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16653, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16654, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16655, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16656, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16657, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16658, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16659, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16660, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16661, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16662, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16663, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16664, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16665, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16666, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16667, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16668, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16669, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16670, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16671, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16672, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16673, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16674, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16675, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16676, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16677, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16678, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16679, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16680, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16681, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16682, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16683, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16684, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16685, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16686, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16687, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16688, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16689, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16690, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16691, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16692, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16693, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16694, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16695, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16696, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16697, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16698, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16699, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16700, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16701, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16702, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16703, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16704, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16705, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16706, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16707, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16708, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16709, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16710, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16711, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16712, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16713, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16714, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16715, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16716, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16717, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16718, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16719, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16720, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16721, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16722, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16723, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16724, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16725, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16726, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16727, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16728, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16729, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16730, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16731, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16732, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16733, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16734, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16735, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16736, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16737, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16738, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16739, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16740, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16741, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16742, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16743, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16744, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16745, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16746, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16747, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16748, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16749, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16750, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16751, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16752, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16753, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16754, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16755, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16756, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16757, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16758, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16759, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16760, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16761, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16762, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16763, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16764, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16765, training loss 0.000002, validation loss 0.000422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16766, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16767, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16768, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16769, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16770, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16771, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16772, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16773, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16774, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16775, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16776, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16777, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16778, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16779, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16780, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16781, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16782, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16783, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16784, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16785, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16786, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16787, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16788, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16789, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16790, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16791, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16792, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16793, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16794, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16795, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16796, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16797, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16798, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16799, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16800, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16801, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16802, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16803, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16804, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16805, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16806, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16807, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16808, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16809, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16810, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16811, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16812, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16813, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16814, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16815, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16816, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16817, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16818, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16819, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16820, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16821, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16822, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16823, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16824, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16825, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16826, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16827, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16828, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16829, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16830, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16831, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16832, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16833, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16834, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16835, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16836, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16837, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16838, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16839, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16840, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16841, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16842, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16843, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16844, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16845, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16846, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16847, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16848, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16849, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16850, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16851, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16852, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16853, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16854, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16855, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16856, training loss 0.000002, validation loss 0.000422\n",
      "Iter 16857, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16858, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16859, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16860, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16861, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16862, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16863, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16864, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16865, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16866, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16867, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16868, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16869, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16870, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16871, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16872, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16873, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16874, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16875, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16876, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16877, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16878, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16879, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16880, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16881, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16882, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16883, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16884, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16885, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16886, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16887, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16888, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16889, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16890, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16891, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16892, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16893, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16894, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16895, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16896, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16897, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16898, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16899, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16900, training loss 0.000002, validation loss 0.000423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16901, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16902, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16903, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16904, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16905, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16906, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16907, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16908, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16909, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16910, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16911, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16912, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16913, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16914, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16915, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16916, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16917, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16918, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16919, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16920, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16921, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16922, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16923, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16924, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16925, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16926, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16927, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16928, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16929, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16930, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16931, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16932, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16933, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16934, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16935, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16936, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16937, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16938, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16939, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16940, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16941, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16942, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16943, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16944, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16945, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16946, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16947, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16948, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16949, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16950, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16951, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16952, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16953, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16954, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16955, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16956, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16957, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16958, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16959, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16960, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16961, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16962, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16963, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16964, training loss 0.000002, validation loss 0.000423\n",
      "Iter 16965, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16966, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16967, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16968, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16969, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16970, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16971, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16972, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16973, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16974, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16975, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16976, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16977, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16978, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16979, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16980, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16981, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16982, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16983, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16984, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16985, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16986, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16987, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16988, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16989, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16990, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16991, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16992, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16993, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16994, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16995, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16996, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16997, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16998, training loss 0.000001, validation loss 0.000423\n",
      "Iter 16999, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17000, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17001, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17002, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17003, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17004, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17005, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17006, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17007, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17008, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17009, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17010, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17011, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17012, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17013, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17014, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17015, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17016, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17017, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17018, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17019, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17020, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17021, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17022, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17023, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17024, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17025, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17026, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17027, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17028, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17029, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17030, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17031, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17032, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17033, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17034, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17035, training loss 0.000001, validation loss 0.000423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17036, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17037, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17038, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17039, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17040, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17041, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17042, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17043, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17044, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17045, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17046, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17047, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17048, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17049, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17050, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17051, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17052, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17053, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17054, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17055, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17056, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17057, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17058, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17059, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17060, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17061, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17062, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17063, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17064, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17065, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17066, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17067, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17068, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17069, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17070, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17071, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17072, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17073, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17074, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17075, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17076, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17077, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17078, training loss 0.000001, validation loss 0.000423\n",
      "Iter 17079, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17080, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17081, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17082, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17083, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17084, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17085, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17086, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17087, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17088, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17089, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17090, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17091, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17092, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17093, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17094, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17095, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17096, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17097, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17098, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17099, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17100, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17101, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17102, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17103, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17104, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17105, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17106, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17107, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17108, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17109, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17110, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17111, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17112, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17113, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17114, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17115, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17116, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17117, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17118, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17119, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17120, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17121, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17122, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17123, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17124, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17125, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17126, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17127, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17128, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17129, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17130, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17131, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17132, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17133, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17134, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17135, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17136, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17137, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17138, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17139, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17140, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17141, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17142, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17143, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17144, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17145, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17146, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17147, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17148, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17149, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17150, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17151, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17152, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17153, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17154, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17155, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17156, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17157, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17158, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17159, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17160, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17161, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17162, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17163, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17164, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17165, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17166, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17167, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17168, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17169, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17170, training loss 0.000001, validation loss 0.000424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17171, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17172, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17173, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17174, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17175, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17176, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17177, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17178, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17179, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17180, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17181, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17182, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17183, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17184, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17185, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17186, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17187, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17188, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17189, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17190, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17191, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17192, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17193, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17194, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17195, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17196, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17197, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17198, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17199, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17200, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17201, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17202, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17203, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17204, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17205, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17206, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17207, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17208, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17209, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17210, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17211, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17212, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17213, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17214, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17215, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17216, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17217, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17218, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17219, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17220, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17221, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17222, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17223, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17224, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17225, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17226, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17227, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17228, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17229, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17230, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17231, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17232, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17233, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17234, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17235, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17236, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17237, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17238, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17239, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17240, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17241, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17242, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17243, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17244, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17245, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17246, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17247, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17248, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17249, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17250, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17251, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17252, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17253, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17254, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17255, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17256, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17257, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17258, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17259, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17260, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17261, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17262, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17263, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17264, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17265, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17266, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17267, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17268, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17269, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17270, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17271, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17272, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17273, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17274, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17275, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17276, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17277, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17278, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17279, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17280, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17281, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17282, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17283, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17284, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17285, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17286, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17287, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17288, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17289, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17290, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17291, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17292, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17293, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17294, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17295, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17296, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17297, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17298, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17299, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17300, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17301, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17302, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17303, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17304, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17305, training loss 0.000001, validation loss 0.000424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17306, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17307, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17308, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17309, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17310, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17311, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17312, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17313, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17314, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17315, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17316, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17317, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17318, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17319, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17320, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17321, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17322, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17323, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17324, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17325, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17326, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17327, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17328, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17329, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17330, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17331, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17332, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17333, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17334, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17335, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17336, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17337, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17338, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17339, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17340, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17341, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17342, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17343, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17344, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17345, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17346, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17347, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17348, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17349, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17350, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17351, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17352, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17353, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17354, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17355, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17356, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17357, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17358, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17359, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17360, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17361, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17362, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17363, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17364, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17365, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17366, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17367, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17368, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17369, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17370, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17371, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17372, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17373, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17374, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17375, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17376, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17377, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17378, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17379, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17380, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17381, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17382, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17383, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17384, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17385, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17386, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17387, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17388, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17389, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17390, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17391, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17392, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17393, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17394, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17395, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17396, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17397, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17398, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17399, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17400, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17401, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17402, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17403, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17404, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17405, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17406, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17407, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17408, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17409, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17410, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17411, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17412, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17413, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17414, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17415, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17416, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17417, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17418, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17419, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17420, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17421, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17422, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17423, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17424, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17425, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17426, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17427, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17428, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17429, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17430, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17431, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17432, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17433, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17434, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17435, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17436, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17437, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17438, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17439, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17440, training loss 0.000001, validation loss 0.000425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17441, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17442, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17443, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17444, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17445, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17446, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17447, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17448, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17449, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17450, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17451, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17452, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17453, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17454, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17455, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17456, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17457, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17458, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17459, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17460, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17461, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17462, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17463, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17464, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17465, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17466, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17467, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17468, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17469, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17470, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17471, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17472, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17473, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17474, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17475, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17476, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17477, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17478, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17479, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17480, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17481, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17482, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17483, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17484, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17485, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17486, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17487, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17488, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17489, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17490, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17491, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17492, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17493, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17494, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17495, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17496, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17497, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17498, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17499, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17500, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17501, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17502, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17503, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17504, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17505, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17506, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17507, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17508, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17509, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17510, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17511, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17512, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17513, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17514, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17515, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17516, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17517, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17518, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17519, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17520, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17521, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17522, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17523, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17524, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17525, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17526, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17527, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17528, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17529, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17530, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17531, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17532, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17533, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17534, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17535, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17536, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17537, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17538, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17539, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17540, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17541, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17542, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17543, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17544, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17545, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17546, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17547, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17548, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17549, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17550, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17551, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17552, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17553, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17554, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17555, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17556, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17557, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17558, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17559, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17560, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17561, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17562, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17563, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17564, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17565, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17566, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17567, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17568, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17569, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17570, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17571, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17572, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17573, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17574, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17575, training loss 0.000001, validation loss 0.000425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17576, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17577, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17578, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17579, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17580, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17581, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17582, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17583, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17584, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17585, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17586, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17587, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17588, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17589, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17590, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17591, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17592, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17593, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17594, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17595, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17596, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17597, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17598, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17599, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17600, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17601, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17602, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17603, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17604, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17605, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17606, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17607, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17608, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17609, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17610, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17611, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17612, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17613, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17614, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17615, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17616, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17617, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17618, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17619, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17620, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17621, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17622, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17623, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17624, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17625, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17626, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17627, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17628, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17629, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17630, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17631, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17632, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17633, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17634, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17635, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17636, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17637, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17638, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17639, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17640, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17641, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17642, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17643, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17644, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17645, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17646, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17647, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17648, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17649, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17650, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17651, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17652, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17653, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17654, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17655, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17656, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17657, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17658, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17659, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17660, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17661, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17662, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17663, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17664, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17665, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17666, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17667, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17668, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17669, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17670, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17671, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17672, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17673, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17674, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17675, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17676, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17677, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17678, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17679, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17680, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17681, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17682, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17683, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17684, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17685, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17686, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17687, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17688, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17689, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17690, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17691, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17692, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17693, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17694, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17695, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17696, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17697, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17698, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17699, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17700, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17701, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17702, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17703, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17704, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17705, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17706, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17707, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17708, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17709, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17710, training loss 0.000001, validation loss 0.000426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17711, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17712, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17713, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17714, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17715, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17716, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17717, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17718, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17719, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17720, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17721, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17722, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17723, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17724, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17725, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17726, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17727, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17728, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17729, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17730, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17731, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17732, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17733, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17734, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17735, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17736, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17737, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17738, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17739, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17740, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17741, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17742, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17743, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17744, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17745, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17746, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17747, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17748, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17749, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17750, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17751, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17752, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17753, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17754, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17755, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17756, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17757, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17758, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17759, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17760, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17761, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17762, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17763, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17764, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17765, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17766, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17767, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17768, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17769, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17770, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17771, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17772, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17773, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17774, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17775, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17776, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17777, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17778, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17779, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17780, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17781, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17782, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17783, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17784, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17785, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17786, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17787, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17788, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17789, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17790, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17791, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17792, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17793, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17794, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17795, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17796, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17797, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17798, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17799, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17800, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17801, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17802, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17803, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17804, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17805, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17806, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17807, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17808, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17809, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17810, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17811, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17812, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17813, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17814, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17815, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17816, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17817, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17818, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17819, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17820, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17821, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17822, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17823, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17824, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17825, training loss 0.000001, validation loss 0.000428\n",
      "Iter 17826, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17827, training loss 0.000001, validation loss 0.000428\n",
      "Iter 17828, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17829, training loss 0.000002, validation loss 0.000428\n",
      "Iter 17830, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17831, training loss 0.000001, validation loss 0.000428\n",
      "Iter 17832, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17833, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17834, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17835, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17836, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17837, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17838, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17839, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17840, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17841, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17842, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17843, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17844, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17845, training loss 0.000001, validation loss 0.000425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17846, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17847, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17848, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17849, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17850, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17851, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17852, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17853, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17854, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17855, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17856, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17857, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17858, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17859, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17860, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17861, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17862, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17863, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17864, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17865, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17866, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17867, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17868, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17869, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17870, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17871, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17872, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17873, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17874, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17875, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17876, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17877, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17878, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17879, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17880, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17881, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17882, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17883, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17884, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17885, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17886, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17887, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17888, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17889, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17890, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17891, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17892, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17893, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17894, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17895, training loss 0.000001, validation loss 0.000428\n",
      "Iter 17896, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17897, training loss 0.000002, validation loss 0.000428\n",
      "Iter 17898, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17899, training loss 0.000002, validation loss 0.000429\n",
      "Iter 17900, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17901, training loss 0.000002, validation loss 0.000429\n",
      "Iter 17902, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17903, training loss 0.000002, validation loss 0.000429\n",
      "Iter 17904, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17905, training loss 0.000001, validation loss 0.000428\n",
      "Iter 17906, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17907, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17908, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17909, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17910, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17911, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17912, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17913, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17914, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17915, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17916, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17917, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17918, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17919, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17920, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17921, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17922, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17923, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17924, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17925, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17926, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17927, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17928, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17929, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17930, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17931, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17932, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17933, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17934, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17935, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17936, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17937, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17938, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17939, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17940, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17941, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17942, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17943, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17944, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17945, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17946, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17947, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17948, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17949, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17950, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17951, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17952, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17953, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17954, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17955, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17956, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17957, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17958, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17959, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17960, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17961, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17962, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17963, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17964, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17965, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17966, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17967, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17968, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17969, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17970, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17971, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17972, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17973, training loss 0.000001, validation loss 0.000428\n",
      "Iter 17974, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17975, training loss 0.000002, validation loss 0.000430\n",
      "Iter 17976, training loss 0.000003, validation loss 0.000425\n",
      "Iter 17977, training loss 0.000004, validation loss 0.000432\n",
      "Iter 17978, training loss 0.000005, validation loss 0.000426\n",
      "Iter 17979, training loss 0.000005, validation loss 0.000434\n",
      "Iter 17980, training loss 0.000005, validation loss 0.000426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17981, training loss 0.000005, validation loss 0.000434\n",
      "Iter 17982, training loss 0.000004, validation loss 0.000426\n",
      "Iter 17983, training loss 0.000003, validation loss 0.000431\n",
      "Iter 17984, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17985, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17986, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17987, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17988, training loss 0.000002, validation loss 0.000428\n",
      "Iter 17989, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17990, training loss 0.000002, validation loss 0.000430\n",
      "Iter 17991, training loss 0.000002, validation loss 0.000425\n",
      "Iter 17992, training loss 0.000002, validation loss 0.000429\n",
      "Iter 17993, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17994, training loss 0.000001, validation loss 0.000426\n",
      "Iter 17995, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17996, training loss 0.000001, validation loss 0.000425\n",
      "Iter 17997, training loss 0.000001, validation loss 0.000427\n",
      "Iter 17998, training loss 0.000001, validation loss 0.000424\n",
      "Iter 17999, training loss 0.000001, validation loss 0.000428\n",
      "Iter 18000, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18001, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18002, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18003, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18004, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18005, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18006, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18007, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18008, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18009, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18010, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18011, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18012, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18013, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18014, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18015, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18016, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18017, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18018, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18019, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18020, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18021, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18022, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18023, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18024, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18025, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18026, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18027, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18028, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18029, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18030, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18031, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18032, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18033, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18034, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18035, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18036, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18037, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18038, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18039, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18040, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18041, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18042, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18043, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18044, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18045, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18046, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18047, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18048, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18049, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18050, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18051, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18052, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18053, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18054, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18055, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18056, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18057, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18058, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18059, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18060, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18061, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18062, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18063, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18064, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18065, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18066, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18067, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18068, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18069, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18070, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18071, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18072, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18073, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18074, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18075, training loss 0.000001, validation loss 0.000428\n",
      "Iter 18076, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18077, training loss 0.000002, validation loss 0.000429\n",
      "Iter 18078, training loss 0.000002, validation loss 0.000425\n",
      "Iter 18079, training loss 0.000002, validation loss 0.000430\n",
      "Iter 18080, training loss 0.000003, validation loss 0.000425\n",
      "Iter 18081, training loss 0.000003, validation loss 0.000431\n",
      "Iter 18082, training loss 0.000004, validation loss 0.000425\n",
      "Iter 18083, training loss 0.000004, validation loss 0.000433\n",
      "Iter 18084, training loss 0.000004, validation loss 0.000426\n",
      "Iter 18085, training loss 0.000005, validation loss 0.000433\n",
      "Iter 18086, training loss 0.000004, validation loss 0.000426\n",
      "Iter 18087, training loss 0.000004, validation loss 0.000432\n",
      "Iter 18088, training loss 0.000003, validation loss 0.000425\n",
      "Iter 18089, training loss 0.000002, validation loss 0.000429\n",
      "Iter 18090, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18091, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18092, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18093, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18094, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18095, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18096, training loss 0.000001, validation loss 0.000428\n",
      "Iter 18097, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18098, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18099, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18100, training loss 0.000001, validation loss 0.000428\n",
      "Iter 18101, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18102, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18103, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18104, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18105, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18106, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18107, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18108, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18109, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18110, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18111, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18112, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18113, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18114, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18115, training loss 0.000001, validation loss 0.000426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18116, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18117, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18118, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18119, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18120, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18121, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18122, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18123, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18124, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18125, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18126, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18127, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18128, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18129, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18130, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18131, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18132, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18133, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18134, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18135, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18136, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18137, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18138, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18139, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18140, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18141, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18142, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18143, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18144, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18145, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18146, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18147, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18148, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18149, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18150, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18151, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18152, training loss 0.000001, validation loss 0.000428\n",
      "Iter 18153, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18154, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18155, training loss 0.000002, validation loss 0.000425\n",
      "Iter 18156, training loss 0.000002, validation loss 0.000429\n",
      "Iter 18157, training loss 0.000003, validation loss 0.000425\n",
      "Iter 18158, training loss 0.000003, validation loss 0.000431\n",
      "Iter 18159, training loss 0.000004, validation loss 0.000425\n",
      "Iter 18160, training loss 0.000004, validation loss 0.000432\n",
      "Iter 18161, training loss 0.000005, validation loss 0.000426\n",
      "Iter 18162, training loss 0.000005, validation loss 0.000434\n",
      "Iter 18163, training loss 0.000005, validation loss 0.000426\n",
      "Iter 18164, training loss 0.000005, validation loss 0.000434\n",
      "Iter 18165, training loss 0.000005, validation loss 0.000426\n",
      "Iter 18166, training loss 0.000004, validation loss 0.000432\n",
      "Iter 18167, training loss 0.000003, validation loss 0.000425\n",
      "Iter 18168, training loss 0.000002, validation loss 0.000429\n",
      "Iter 18169, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18170, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18171, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18172, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18173, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18174, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18175, training loss 0.000001, validation loss 0.000428\n",
      "Iter 18176, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18177, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18178, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18179, training loss 0.000001, validation loss 0.000428\n",
      "Iter 18180, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18181, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18182, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18183, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18184, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18185, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18186, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18187, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18188, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18189, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18190, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18191, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18192, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18193, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18194, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18195, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18196, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18197, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18198, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18199, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18200, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18201, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18202, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18203, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18204, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18205, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18206, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18207, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18208, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18209, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18210, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18211, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18212, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18213, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18214, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18215, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18216, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18217, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18218, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18219, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18220, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18221, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18222, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18223, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18224, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18225, training loss 0.000001, validation loss 0.000428\n",
      "Iter 18226, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18227, training loss 0.000002, validation loss 0.000429\n",
      "Iter 18228, training loss 0.000002, validation loss 0.000425\n",
      "Iter 18229, training loss 0.000002, validation loss 0.000430\n",
      "Iter 18230, training loss 0.000003, validation loss 0.000425\n",
      "Iter 18231, training loss 0.000003, validation loss 0.000431\n",
      "Iter 18232, training loss 0.000004, validation loss 0.000425\n",
      "Iter 18233, training loss 0.000004, validation loss 0.000433\n",
      "Iter 18234, training loss 0.000005, validation loss 0.000426\n",
      "Iter 18235, training loss 0.000005, validation loss 0.000434\n",
      "Iter 18236, training loss 0.000005, validation loss 0.000426\n",
      "Iter 18237, training loss 0.000005, validation loss 0.000434\n",
      "Iter 18238, training loss 0.000005, validation loss 0.000426\n",
      "Iter 18239, training loss 0.000004, validation loss 0.000433\n",
      "Iter 18240, training loss 0.000004, validation loss 0.000425\n",
      "Iter 18241, training loss 0.000003, validation loss 0.000430\n",
      "Iter 18242, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18243, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18244, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18245, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18246, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18247, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18248, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18249, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18250, training loss 0.000002, validation loss 0.000428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18251, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18252, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18253, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18254, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18255, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18256, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18257, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18258, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18259, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18260, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18261, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18262, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18263, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18264, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18265, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18266, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18267, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18268, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18269, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18270, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18271, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18272, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18273, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18274, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18275, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18276, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18277, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18278, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18279, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18280, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18281, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18282, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18283, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18284, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18285, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18286, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18287, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18288, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18289, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18290, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18291, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18292, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18293, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18294, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18295, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18296, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18297, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18298, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18299, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18300, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18301, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18302, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18303, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18304, training loss 0.000003, validation loss 0.000430\n",
      "Iter 18305, training loss 0.000004, validation loss 0.000425\n",
      "Iter 18306, training loss 0.000005, validation loss 0.000433\n",
      "Iter 18307, training loss 0.000006, validation loss 0.000426\n",
      "Iter 18308, training loss 0.000008, validation loss 0.000438\n",
      "Iter 18309, training loss 0.000010, validation loss 0.000429\n",
      "Iter 18310, training loss 0.000012, validation loss 0.000442\n",
      "Iter 18311, training loss 0.000013, validation loss 0.000431\n",
      "Iter 18312, training loss 0.000013, validation loss 0.000445\n",
      "Iter 18313, training loss 0.000012, validation loss 0.000430\n",
      "Iter 18314, training loss 0.000010, validation loss 0.000440\n",
      "Iter 18315, training loss 0.000006, validation loss 0.000426\n",
      "Iter 18316, training loss 0.000003, validation loss 0.000430\n",
      "Iter 18317, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18318, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18319, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18320, training loss 0.000003, validation loss 0.000424\n",
      "Iter 18321, training loss 0.000004, validation loss 0.000432\n",
      "Iter 18322, training loss 0.000005, validation loss 0.000425\n",
      "Iter 18323, training loss 0.000005, validation loss 0.000432\n",
      "Iter 18324, training loss 0.000003, validation loss 0.000424\n",
      "Iter 18325, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18326, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18327, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18328, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18329, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18330, training loss 0.000002, validation loss 0.000429\n",
      "Iter 18331, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18332, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18333, training loss 0.000002, validation loss 0.000423\n",
      "Iter 18334, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18335, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18336, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18337, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18338, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18339, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18340, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18341, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18342, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18343, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18344, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18345, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18346, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18347, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18348, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18349, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18350, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18351, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18352, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18353, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18354, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18355, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18356, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18357, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18358, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18359, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18360, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18361, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18362, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18363, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18364, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18365, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18366, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18367, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18368, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18369, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18370, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18371, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18372, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18373, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18374, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18375, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18376, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18377, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18378, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18379, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18380, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18381, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18382, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18383, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18384, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18385, training loss 0.000000, validation loss 0.000425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18386, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18387, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18388, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18389, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18390, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18391, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18392, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18393, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18394, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18395, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18396, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18397, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18398, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18399, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18400, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18401, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18402, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18403, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18404, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18405, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18406, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18407, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18408, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18409, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18410, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18411, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18412, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18413, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18414, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18415, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18416, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18417, training loss 0.000003, validation loss 0.000430\n",
      "Iter 18418, training loss 0.000004, validation loss 0.000425\n",
      "Iter 18419, training loss 0.000005, validation loss 0.000434\n",
      "Iter 18420, training loss 0.000007, validation loss 0.000427\n",
      "Iter 18421, training loss 0.000009, validation loss 0.000439\n",
      "Iter 18422, training loss 0.000012, validation loss 0.000430\n",
      "Iter 18423, training loss 0.000015, validation loss 0.000446\n",
      "Iter 18424, training loss 0.000017, validation loss 0.000434\n",
      "Iter 18425, training loss 0.000018, validation loss 0.000450\n",
      "Iter 18426, training loss 0.000017, validation loss 0.000434\n",
      "Iter 18427, training loss 0.000014, validation loss 0.000444\n",
      "Iter 18428, training loss 0.000009, validation loss 0.000428\n",
      "Iter 18429, training loss 0.000004, validation loss 0.000431\n",
      "Iter 18430, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18431, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18432, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18433, training loss 0.000004, validation loss 0.000424\n",
      "Iter 18434, training loss 0.000006, validation loss 0.000434\n",
      "Iter 18435, training loss 0.000007, validation loss 0.000426\n",
      "Iter 18436, training loss 0.000006, validation loss 0.000434\n",
      "Iter 18437, training loss 0.000004, validation loss 0.000424\n",
      "Iter 18438, training loss 0.000002, validation loss 0.000427\n",
      "Iter 18439, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18440, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18441, training loss 0.000002, validation loss 0.000427\n",
      "Iter 18442, training loss 0.000003, validation loss 0.000423\n",
      "Iter 18443, training loss 0.000003, validation loss 0.000430\n",
      "Iter 18444, training loss 0.000003, validation loss 0.000423\n",
      "Iter 18445, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18446, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18447, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18448, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18449, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18450, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18451, training loss 0.000002, validation loss 0.000423\n",
      "Iter 18452, training loss 0.000002, validation loss 0.000427\n",
      "Iter 18453, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18454, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18455, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18456, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18457, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18458, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18459, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18460, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18461, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18462, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18463, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18464, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18465, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18466, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18467, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18468, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18469, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18470, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18471, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18472, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18473, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18474, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18475, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18476, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18477, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18478, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18479, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18480, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18481, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18482, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18483, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18484, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18485, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18486, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18487, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18488, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18489, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18490, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18491, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18492, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18493, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18494, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18495, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18496, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18497, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18498, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18499, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18500, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18501, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18502, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18503, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18504, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18505, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18506, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18507, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18508, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18509, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18510, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18511, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18512, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18513, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18514, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18515, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18516, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18517, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18518, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18519, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18520, training loss 0.000000, validation loss 0.000424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18521, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18522, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18523, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18524, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18525, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18526, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18527, training loss 0.000000, validation loss 0.000425\n",
      "Iter 18528, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18529, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18530, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18531, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18532, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18533, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18534, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18535, training loss 0.000001, validation loss 0.000427\n",
      "Iter 18536, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18537, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18538, training loss 0.000003, validation loss 0.000424\n",
      "Iter 18539, training loss 0.000004, validation loss 0.000431\n",
      "Iter 18540, training loss 0.000005, validation loss 0.000425\n",
      "Iter 18541, training loss 0.000006, validation loss 0.000435\n",
      "Iter 18542, training loss 0.000009, validation loss 0.000427\n",
      "Iter 18543, training loss 0.000012, validation loss 0.000441\n",
      "Iter 18544, training loss 0.000015, validation loss 0.000432\n",
      "Iter 18545, training loss 0.000018, validation loss 0.000450\n",
      "Iter 18546, training loss 0.000021, validation loss 0.000436\n",
      "Iter 18547, training loss 0.000022, validation loss 0.000454\n",
      "Iter 18548, training loss 0.000020, validation loss 0.000436\n",
      "Iter 18549, training loss 0.000016, validation loss 0.000446\n",
      "Iter 18550, training loss 0.000009, validation loss 0.000427\n",
      "Iter 18551, training loss 0.000004, validation loss 0.000430\n",
      "Iter 18552, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18553, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18554, training loss 0.000003, validation loss 0.000429\n",
      "Iter 18555, training loss 0.000006, validation loss 0.000425\n",
      "Iter 18556, training loss 0.000008, validation loss 0.000436\n",
      "Iter 18557, training loss 0.000008, validation loss 0.000426\n",
      "Iter 18558, training loss 0.000007, validation loss 0.000434\n",
      "Iter 18559, training loss 0.000004, validation loss 0.000424\n",
      "Iter 18560, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18561, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18562, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18563, training loss 0.000002, validation loss 0.000428\n",
      "Iter 18564, training loss 0.000004, validation loss 0.000423\n",
      "Iter 18565, training loss 0.000004, validation loss 0.000430\n",
      "Iter 18566, training loss 0.000004, validation loss 0.000423\n",
      "Iter 18567, training loss 0.000002, validation loss 0.000427\n",
      "Iter 18568, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18569, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18570, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18571, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18572, training loss 0.000002, validation loss 0.000426\n",
      "Iter 18573, training loss 0.000002, validation loss 0.000422\n",
      "Iter 18574, training loss 0.000002, validation loss 0.000427\n",
      "Iter 18575, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18576, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18577, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18578, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18579, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18580, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18581, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18582, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18583, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18584, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18585, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18586, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18587, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18588, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18589, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18590, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18591, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18592, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18593, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18594, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18595, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18596, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18597, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18598, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18599, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18600, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18601, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18602, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18603, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18604, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18605, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18606, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18607, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18608, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18609, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18610, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18611, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18612, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18613, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18614, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18615, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18616, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18617, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18618, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18619, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18620, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18621, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18622, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18623, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18624, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18625, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18626, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18627, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18628, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18629, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18630, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18631, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18632, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18633, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18634, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18635, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18636, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18637, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18638, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18639, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18640, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18641, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18642, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18643, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18644, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18645, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18646, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18647, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18648, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18649, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18650, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18651, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18652, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18653, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18654, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18655, training loss 0.000001, validation loss 0.000425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18656, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18657, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18658, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18659, training loss 0.000001, validation loss 0.000426\n",
      "Iter 18660, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18661, training loss 0.000002, validation loss 0.000427\n",
      "Iter 18662, training loss 0.000002, validation loss 0.000423\n",
      "Iter 18663, training loss 0.000003, validation loss 0.000429\n",
      "Iter 18664, training loss 0.000004, validation loss 0.000424\n",
      "Iter 18665, training loss 0.000006, validation loss 0.000433\n",
      "Iter 18666, training loss 0.000008, validation loss 0.000426\n",
      "Iter 18667, training loss 0.000010, validation loss 0.000439\n",
      "Iter 18668, training loss 0.000014, validation loss 0.000430\n",
      "Iter 18669, training loss 0.000018, validation loss 0.000448\n",
      "Iter 18670, training loss 0.000021, validation loss 0.000437\n",
      "Iter 18671, training loss 0.000025, validation loss 0.000456\n",
      "Iter 18672, training loss 0.000025, validation loss 0.000440\n",
      "Iter 18673, training loss 0.000023, validation loss 0.000454\n",
      "Iter 18674, training loss 0.000018, validation loss 0.000433\n",
      "Iter 18675, training loss 0.000010, validation loss 0.000438\n",
      "Iter 18676, training loss 0.000004, validation loss 0.000423\n",
      "Iter 18677, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18678, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18679, training loss 0.000004, validation loss 0.000423\n",
      "Iter 18680, training loss 0.000007, validation loss 0.000434\n",
      "Iter 18681, training loss 0.000010, validation loss 0.000426\n",
      "Iter 18682, training loss 0.000010, validation loss 0.000437\n",
      "Iter 18683, training loss 0.000007, validation loss 0.000425\n",
      "Iter 18684, training loss 0.000004, validation loss 0.000429\n",
      "Iter 18685, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18686, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18687, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18688, training loss 0.000003, validation loss 0.000422\n",
      "Iter 18689, training loss 0.000005, validation loss 0.000430\n",
      "Iter 18690, training loss 0.000005, validation loss 0.000423\n",
      "Iter 18691, training loss 0.000004, validation loss 0.000429\n",
      "Iter 18692, training loss 0.000002, validation loss 0.000421\n",
      "Iter 18693, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18694, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18695, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18696, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18697, training loss 0.000002, validation loss 0.000421\n",
      "Iter 18698, training loss 0.000002, validation loss 0.000426\n",
      "Iter 18699, training loss 0.000002, validation loss 0.000421\n",
      "Iter 18700, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18701, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18702, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18703, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18704, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18705, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18706, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18707, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18708, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18709, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18710, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18711, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18712, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18713, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18714, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18715, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18716, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18717, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18718, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18719, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18720, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18721, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18722, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18723, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18724, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18725, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18726, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18727, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18728, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18729, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18730, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18731, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18732, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18733, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18734, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18735, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18736, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18737, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18738, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18739, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18740, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18741, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18742, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18743, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18744, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18745, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18746, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18747, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18748, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18749, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18750, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18751, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18752, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18753, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18754, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18755, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18756, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18757, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18758, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18759, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18760, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18761, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18762, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18763, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18764, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18765, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18766, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18767, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18768, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18769, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18770, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18771, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18772, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18773, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18774, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18775, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18776, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18777, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18778, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18779, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18780, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18781, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18782, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18783, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18784, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18785, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18786, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18787, training loss 0.000000, validation loss 0.000423\n",
      "Iter 18788, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18789, training loss 0.000000, validation loss 0.000424\n",
      "Iter 18790, training loss 0.000001, validation loss 0.000422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18791, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18792, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18793, training loss 0.000001, validation loss 0.000425\n",
      "Iter 18794, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18795, training loss 0.000002, validation loss 0.000427\n",
      "Iter 18796, training loss 0.000003, validation loss 0.000422\n",
      "Iter 18797, training loss 0.000004, validation loss 0.000430\n",
      "Iter 18798, training loss 0.000006, validation loss 0.000424\n",
      "Iter 18799, training loss 0.000009, validation loss 0.000437\n",
      "Iter 18800, training loss 0.000013, validation loss 0.000429\n",
      "Iter 18801, training loss 0.000019, validation loss 0.000449\n",
      "Iter 18802, training loss 0.000026, validation loss 0.000440\n",
      "Iter 18803, training loss 0.000034, validation loss 0.000467\n",
      "Iter 18804, training loss 0.000041, validation loss 0.000452\n",
      "Iter 18805, training loss 0.000043, validation loss 0.000477\n",
      "Iter 18806, training loss 0.000038, validation loss 0.000449\n",
      "Iter 18807, training loss 0.000025, validation loss 0.000455\n",
      "Iter 18808, training loss 0.000011, validation loss 0.000427\n",
      "Iter 18809, training loss 0.000002, validation loss 0.000425\n",
      "Iter 18810, training loss 0.000001, validation loss 0.000424\n",
      "Iter 18811, training loss 0.000008, validation loss 0.000424\n",
      "Iter 18812, training loss 0.000016, validation loss 0.000443\n",
      "Iter 18813, training loss 0.000020, validation loss 0.000433\n",
      "Iter 18814, training loss 0.000018, validation loss 0.000446\n",
      "Iter 18815, training loss 0.000010, validation loss 0.000426\n",
      "Iter 18816, training loss 0.000003, validation loss 0.000427\n",
      "Iter 18817, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18818, training loss 0.000003, validation loss 0.000421\n",
      "Iter 18819, training loss 0.000008, validation loss 0.000433\n",
      "Iter 18820, training loss 0.000010, validation loss 0.000425\n",
      "Iter 18821, training loss 0.000008, validation loss 0.000433\n",
      "Iter 18822, training loss 0.000004, validation loss 0.000421\n",
      "Iter 18823, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18824, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18825, training loss 0.000002, validation loss 0.000419\n",
      "Iter 18826, training loss 0.000004, validation loss 0.000427\n",
      "Iter 18827, training loss 0.000005, validation loss 0.000421\n",
      "Iter 18828, training loss 0.000003, validation loss 0.000426\n",
      "Iter 18829, training loss 0.000001, validation loss 0.000420\n",
      "Iter 18830, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18831, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18832, training loss 0.000002, validation loss 0.000419\n",
      "Iter 18833, training loss 0.000003, validation loss 0.000425\n",
      "Iter 18834, training loss 0.000002, validation loss 0.000419\n",
      "Iter 18835, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18836, training loss 0.000001, validation loss 0.000419\n",
      "Iter 18837, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18838, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18839, training loss 0.000001, validation loss 0.000419\n",
      "Iter 18840, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18841, training loss 0.000001, validation loss 0.000419\n",
      "Iter 18842, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18843, training loss 0.000000, validation loss 0.000419\n",
      "Iter 18844, training loss 0.000000, validation loss 0.000419\n",
      "Iter 18845, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18846, training loss 0.000001, validation loss 0.000419\n",
      "Iter 18847, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18848, training loss 0.000001, validation loss 0.000419\n",
      "Iter 18849, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18850, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18851, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18852, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18853, training loss 0.000001, validation loss 0.000419\n",
      "Iter 18854, training loss 0.000001, validation loss 0.000421\n",
      "Iter 18855, training loss 0.000001, validation loss 0.000419\n",
      "Iter 18856, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18857, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18858, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18859, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18860, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18861, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18862, training loss 0.000000, validation loss 0.000419\n",
      "Iter 18863, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18864, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18865, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18866, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18867, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18868, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18869, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18870, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18871, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18872, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18873, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18874, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18875, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18876, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18877, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18878, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18879, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18880, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18881, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18882, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18883, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18884, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18885, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18886, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18887, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18888, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18889, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18890, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18891, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18892, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18893, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18894, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18895, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18896, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18897, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18898, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18899, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18900, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18901, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18902, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18903, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18904, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18905, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18906, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18907, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18908, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18909, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18910, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18911, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18912, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18913, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18914, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18915, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18916, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18917, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18918, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18919, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18920, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18921, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18922, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18923, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18924, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18925, training loss 0.000000, validation loss 0.000421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18926, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18927, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18928, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18929, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18930, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18931, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18932, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18933, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18934, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18935, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18936, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18937, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18938, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18939, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18940, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18941, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18942, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18943, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18944, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18945, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18946, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18947, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18948, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18949, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18950, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18951, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18952, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18953, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18954, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18955, training loss 0.000000, validation loss 0.000421\n",
      "Iter 18956, training loss 0.000000, validation loss 0.000420\n",
      "Iter 18957, training loss 0.000000, validation loss 0.000422\n",
      "Iter 18958, training loss 0.000001, validation loss 0.000420\n",
      "Iter 18959, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18960, training loss 0.000001, validation loss 0.000420\n",
      "Iter 18961, training loss 0.000001, validation loss 0.000423\n",
      "Iter 18962, training loss 0.000002, validation loss 0.000420\n",
      "Iter 18963, training loss 0.000002, validation loss 0.000425\n",
      "Iter 18964, training loss 0.000003, validation loss 0.000421\n",
      "Iter 18965, training loss 0.000005, validation loss 0.000429\n",
      "Iter 18966, training loss 0.000007, validation loss 0.000423\n",
      "Iter 18967, training loss 0.000010, validation loss 0.000436\n",
      "Iter 18968, training loss 0.000014, validation loss 0.000428\n",
      "Iter 18969, training loss 0.000020, validation loss 0.000449\n",
      "Iter 18970, training loss 0.000028, validation loss 0.000439\n",
      "Iter 18971, training loss 0.000036, validation loss 0.000467\n",
      "Iter 18972, training loss 0.000043, validation loss 0.000452\n",
      "Iter 18973, training loss 0.000045, validation loss 0.000476\n",
      "Iter 18974, training loss 0.000039, validation loss 0.000448\n",
      "Iter 18975, training loss 0.000026, validation loss 0.000455\n",
      "Iter 18976, training loss 0.000012, validation loss 0.000425\n",
      "Iter 18977, training loss 0.000002, validation loss 0.000424\n",
      "Iter 18978, training loss 0.000001, validation loss 0.000422\n",
      "Iter 18979, training loss 0.000007, validation loss 0.000422\n",
      "Iter 18980, training loss 0.000014, validation loss 0.000440\n",
      "Iter 18981, training loss 0.000020, validation loss 0.000431\n",
      "Iter 18982, training loss 0.000019, validation loss 0.000446\n",
      "Iter 18983, training loss 0.000013, validation loss 0.000426\n",
      "Iter 18984, training loss 0.000005, validation loss 0.000428\n",
      "Iter 18985, training loss 0.000001, validation loss 0.000419\n",
      "Iter 18986, training loss 0.000002, validation loss 0.000419\n",
      "Iter 18987, training loss 0.000006, validation loss 0.000430\n",
      "Iter 18988, training loss 0.000010, validation loss 0.000423\n",
      "Iter 18989, training loss 0.000010, validation loss 0.000433\n",
      "Iter 18990, training loss 0.000006, validation loss 0.000420\n",
      "Iter 18991, training loss 0.000003, validation loss 0.000423\n",
      "Iter 18992, training loss 0.000000, validation loss 0.000418\n",
      "Iter 18993, training loss 0.000001, validation loss 0.000417\n",
      "Iter 18994, training loss 0.000003, validation loss 0.000424\n",
      "Iter 18995, training loss 0.000005, validation loss 0.000419\n",
      "Iter 18996, training loss 0.000004, validation loss 0.000426\n",
      "Iter 18997, training loss 0.000003, validation loss 0.000418\n",
      "Iter 18998, training loss 0.000001, validation loss 0.000420\n",
      "Iter 18999, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19000, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19001, training loss 0.000002, validation loss 0.000422\n",
      "Iter 19002, training loss 0.000002, validation loss 0.000418\n",
      "Iter 19003, training loss 0.000002, validation loss 0.000422\n",
      "Iter 19004, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19005, training loss 0.000001, validation loss 0.000419\n",
      "Iter 19006, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19007, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19008, training loss 0.000001, validation loss 0.000420\n",
      "Iter 19009, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19010, training loss 0.000001, validation loss 0.000420\n",
      "Iter 19011, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19012, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19013, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19014, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19015, training loss 0.000001, validation loss 0.000420\n",
      "Iter 19016, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19017, training loss 0.000001, validation loss 0.000420\n",
      "Iter 19018, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19019, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19020, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19021, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19022, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19023, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19024, training loss 0.000001, validation loss 0.000419\n",
      "Iter 19025, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19026, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19027, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19028, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19029, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19030, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19031, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19032, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19033, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19034, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19035, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19036, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19037, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19038, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19039, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19040, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19041, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19042, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19043, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19044, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19045, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19046, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19047, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19048, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19049, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19050, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19051, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19052, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19053, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19054, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19055, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19056, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19057, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19058, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19059, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19060, training loss 0.000000, validation loss 0.000419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19061, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19062, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19063, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19064, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19065, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19066, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19067, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19068, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19069, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19070, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19071, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19072, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19073, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19074, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19075, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19076, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19077, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19078, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19079, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19080, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19081, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19082, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19083, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19084, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19085, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19086, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19087, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19088, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19089, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19090, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19091, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19092, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19093, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19094, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19095, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19096, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19097, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19098, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19099, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19100, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19101, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19102, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19103, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19104, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19105, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19106, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19107, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19108, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19109, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19110, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19111, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19112, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19113, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19114, training loss 0.000000, validation loss 0.000420\n",
      "Iter 19115, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19116, training loss 0.000000, validation loss 0.000420\n",
      "Iter 19117, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19118, training loss 0.000000, validation loss 0.000420\n",
      "Iter 19119, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19120, training loss 0.000001, validation loss 0.000420\n",
      "Iter 19121, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19122, training loss 0.000001, validation loss 0.000421\n",
      "Iter 19123, training loss 0.000001, validation loss 0.000419\n",
      "Iter 19124, training loss 0.000002, validation loss 0.000423\n",
      "Iter 19125, training loss 0.000002, validation loss 0.000419\n",
      "Iter 19126, training loss 0.000003, validation loss 0.000426\n",
      "Iter 19127, training loss 0.000005, validation loss 0.000420\n",
      "Iter 19128, training loss 0.000007, validation loss 0.000431\n",
      "Iter 19129, training loss 0.000011, validation loss 0.000424\n",
      "Iter 19130, training loss 0.000016, validation loss 0.000442\n",
      "Iter 19131, training loss 0.000023, validation loss 0.000434\n",
      "Iter 19132, training loss 0.000031, validation loss 0.000460\n",
      "Iter 19133, training loss 0.000041, validation loss 0.000449\n",
      "Iter 19134, training loss 0.000049, validation loss 0.000480\n",
      "Iter 19135, training loss 0.000051, validation loss 0.000458\n",
      "Iter 19136, training loss 0.000045, validation loss 0.000475\n",
      "Iter 19137, training loss 0.000031, validation loss 0.000439\n",
      "Iter 19138, training loss 0.000014, validation loss 0.000438\n",
      "Iter 19139, training loss 0.000002, validation loss 0.000417\n",
      "Iter 19140, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19141, training loss 0.000008, validation loss 0.000430\n",
      "Iter 19142, training loss 0.000017, validation loss 0.000428\n",
      "Iter 19143, training loss 0.000023, validation loss 0.000449\n",
      "Iter 19144, training loss 0.000021, validation loss 0.000431\n",
      "Iter 19145, training loss 0.000013, validation loss 0.000437\n",
      "Iter 19146, training loss 0.000005, validation loss 0.000419\n",
      "Iter 19147, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19148, training loss 0.000003, validation loss 0.000424\n",
      "Iter 19149, training loss 0.000008, validation loss 0.000421\n",
      "Iter 19150, training loss 0.000012, validation loss 0.000435\n",
      "Iter 19151, training loss 0.000011, validation loss 0.000422\n",
      "Iter 19152, training loss 0.000006, validation loss 0.000427\n",
      "Iter 19153, training loss 0.000002, validation loss 0.000416\n",
      "Iter 19154, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19155, training loss 0.000002, validation loss 0.000421\n",
      "Iter 19156, training loss 0.000005, validation loss 0.000418\n",
      "Iter 19157, training loss 0.000006, validation loss 0.000427\n",
      "Iter 19158, training loss 0.000004, validation loss 0.000418\n",
      "Iter 19159, training loss 0.000002, validation loss 0.000421\n",
      "Iter 19160, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19161, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19162, training loss 0.000002, validation loss 0.000420\n",
      "Iter 19163, training loss 0.000003, validation loss 0.000416\n",
      "Iter 19164, training loss 0.000003, validation loss 0.000422\n",
      "Iter 19165, training loss 0.000002, validation loss 0.000416\n",
      "Iter 19166, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19167, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19168, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19169, training loss 0.000001, validation loss 0.000420\n",
      "Iter 19170, training loss 0.000002, validation loss 0.000416\n",
      "Iter 19171, training loss 0.000001, validation loss 0.000420\n",
      "Iter 19172, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19173, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19174, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19175, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19176, training loss 0.000001, validation loss 0.000419\n",
      "Iter 19177, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19178, training loss 0.000001, validation loss 0.000419\n",
      "Iter 19179, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19180, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19181, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19182, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19183, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19184, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19185, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19186, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19187, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19188, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19189, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19190, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19191, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19192, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19193, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19194, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19195, training loss 0.000000, validation loss 0.000417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19196, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19197, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19198, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19199, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19200, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19201, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19202, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19203, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19204, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19205, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19206, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19207, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19208, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19209, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19210, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19211, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19212, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19213, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19214, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19215, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19216, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19217, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19218, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19219, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19220, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19221, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19222, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19223, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19224, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19225, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19226, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19227, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19228, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19229, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19230, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19231, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19232, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19233, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19234, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19235, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19236, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19237, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19238, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19239, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19240, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19241, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19242, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19243, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19244, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19245, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19246, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19247, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19248, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19249, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19250, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19251, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19252, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19253, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19254, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19255, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19256, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19257, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19258, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19259, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19260, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19261, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19262, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19263, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19264, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19265, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19266, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19267, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19268, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19269, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19270, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19271, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19272, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19273, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19274, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19275, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19276, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19277, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19278, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19279, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19280, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19281, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19282, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19283, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19284, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19285, training loss 0.000000, validation loss 0.000418\n",
      "Iter 19286, training loss 0.000000, validation loss 0.000419\n",
      "Iter 19287, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19288, training loss 0.000001, validation loss 0.000419\n",
      "Iter 19289, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19290, training loss 0.000001, validation loss 0.000420\n",
      "Iter 19291, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19292, training loss 0.000002, validation loss 0.000421\n",
      "Iter 19293, training loss 0.000002, validation loss 0.000418\n",
      "Iter 19294, training loss 0.000003, validation loss 0.000424\n",
      "Iter 19295, training loss 0.000005, validation loss 0.000419\n",
      "Iter 19296, training loss 0.000007, validation loss 0.000430\n",
      "Iter 19297, training loss 0.000011, validation loss 0.000423\n",
      "Iter 19298, training loss 0.000015, validation loss 0.000440\n",
      "Iter 19299, training loss 0.000022, validation loss 0.000432\n",
      "Iter 19300, training loss 0.000031, validation loss 0.000459\n",
      "Iter 19301, training loss 0.000042, validation loss 0.000449\n",
      "Iter 19302, training loss 0.000051, validation loss 0.000482\n",
      "Iter 19303, training loss 0.000056, validation loss 0.000461\n",
      "Iter 19304, training loss 0.000052, validation loss 0.000482\n",
      "Iter 19305, training loss 0.000039, validation loss 0.000445\n",
      "Iter 19306, training loss 0.000020, validation loss 0.000444\n",
      "Iter 19307, training loss 0.000005, validation loss 0.000418\n",
      "Iter 19308, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19309, training loss 0.000006, validation loss 0.000427\n",
      "Iter 19310, training loss 0.000016, validation loss 0.000426\n",
      "Iter 19311, training loss 0.000024, validation loss 0.000449\n",
      "Iter 19312, training loss 0.000024, validation loss 0.000433\n",
      "Iter 19313, training loss 0.000017, validation loss 0.000440\n",
      "Iter 19314, training loss 0.000006, validation loss 0.000419\n",
      "Iter 19315, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19316, training loss 0.000002, validation loss 0.000421\n",
      "Iter 19317, training loss 0.000008, validation loss 0.000420\n",
      "Iter 19318, training loss 0.000013, validation loss 0.000435\n",
      "Iter 19319, training loss 0.000012, validation loss 0.000422\n",
      "Iter 19320, training loss 0.000008, validation loss 0.000428\n",
      "Iter 19321, training loss 0.000003, validation loss 0.000415\n",
      "Iter 19322, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19323, training loss 0.000002, validation loss 0.000419\n",
      "Iter 19324, training loss 0.000005, validation loss 0.000417\n",
      "Iter 19325, training loss 0.000006, validation loss 0.000427\n",
      "Iter 19326, training loss 0.000005, validation loss 0.000417\n",
      "Iter 19327, training loss 0.000003, validation loss 0.000421\n",
      "Iter 19328, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19329, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19330, training loss 0.000002, validation loss 0.000419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19331, training loss 0.000003, validation loss 0.000415\n",
      "Iter 19332, training loss 0.000003, validation loss 0.000421\n",
      "Iter 19333, training loss 0.000002, validation loss 0.000414\n",
      "Iter 19334, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19335, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19336, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19337, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19338, training loss 0.000002, validation loss 0.000414\n",
      "Iter 19339, training loss 0.000002, validation loss 0.000418\n",
      "Iter 19340, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19341, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19342, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19343, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19344, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19345, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19346, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19347, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19348, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19349, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19350, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19351, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19352, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19353, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19354, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19355, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19356, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19357, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19358, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19359, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19360, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19361, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19362, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19363, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19364, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19365, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19366, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19367, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19368, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19369, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19370, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19371, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19372, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19373, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19374, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19375, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19376, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19377, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19378, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19379, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19380, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19381, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19382, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19383, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19384, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19385, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19386, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19387, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19388, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19389, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19390, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19391, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19392, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19393, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19394, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19395, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19396, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19397, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19398, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19399, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19400, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19401, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19402, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19403, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19404, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19405, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19406, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19407, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19408, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19409, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19410, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19411, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19412, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19413, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19414, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19415, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19416, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19417, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19418, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19419, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19420, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19421, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19422, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19423, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19424, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19425, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19426, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19427, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19428, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19429, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19430, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19431, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19432, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19433, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19434, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19435, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19436, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19437, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19438, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19439, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19440, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19441, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19442, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19443, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19444, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19445, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19446, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19447, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19448, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19449, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19450, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19451, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19452, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19453, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19454, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19455, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19456, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19457, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19458, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19459, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19460, training loss 0.000000, validation loss 0.000417\n",
      "Iter 19461, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19462, training loss 0.000001, validation loss 0.000418\n",
      "Iter 19463, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19464, training loss 0.000001, validation loss 0.000419\n",
      "Iter 19465, training loss 0.000001, validation loss 0.000416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19466, training loss 0.000002, validation loss 0.000420\n",
      "Iter 19467, training loss 0.000003, validation loss 0.000416\n",
      "Iter 19468, training loss 0.000004, validation loss 0.000424\n",
      "Iter 19469, training loss 0.000006, validation loss 0.000418\n",
      "Iter 19470, training loss 0.000010, validation loss 0.000431\n",
      "Iter 19471, training loss 0.000014, validation loss 0.000424\n",
      "Iter 19472, training loss 0.000021, validation loss 0.000446\n",
      "Iter 19473, training loss 0.000031, validation loss 0.000438\n",
      "Iter 19474, training loss 0.000043, validation loss 0.000471\n",
      "Iter 19475, training loss 0.000056, validation loss 0.000460\n",
      "Iter 19476, training loss 0.000065, validation loss 0.000495\n",
      "Iter 19477, training loss 0.000066, validation loss 0.000469\n",
      "Iter 19478, training loss 0.000054, validation loss 0.000482\n",
      "Iter 19479, training loss 0.000032, validation loss 0.000437\n",
      "Iter 19480, training loss 0.000010, validation loss 0.000430\n",
      "Iter 19481, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19482, training loss 0.000005, validation loss 0.000415\n",
      "Iter 19483, training loss 0.000017, validation loss 0.000438\n",
      "Iter 19484, training loss 0.000027, validation loss 0.000433\n",
      "Iter 19485, training loss 0.000029, validation loss 0.000453\n",
      "Iter 19486, training loss 0.000021, validation loss 0.000428\n",
      "Iter 19487, training loss 0.000008, validation loss 0.000426\n",
      "Iter 19488, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19489, training loss 0.000003, validation loss 0.000414\n",
      "Iter 19490, training loss 0.000011, validation loss 0.000430\n",
      "Iter 19491, training loss 0.000016, validation loss 0.000423\n",
      "Iter 19492, training loss 0.000014, validation loss 0.000435\n",
      "Iter 19493, training loss 0.000008, validation loss 0.000416\n",
      "Iter 19494, training loss 0.000002, validation loss 0.000417\n",
      "Iter 19495, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19496, training loss 0.000004, validation loss 0.000413\n",
      "Iter 19497, training loss 0.000007, validation loss 0.000425\n",
      "Iter 19498, training loss 0.000008, validation loss 0.000417\n",
      "Iter 19499, training loss 0.000005, validation loss 0.000422\n",
      "Iter 19500, training loss 0.000002, validation loss 0.000413\n",
      "Iter 19501, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19502, training loss 0.000002, validation loss 0.000416\n",
      "Iter 19503, training loss 0.000004, validation loss 0.000413\n",
      "Iter 19504, training loss 0.000004, validation loss 0.000420\n",
      "Iter 19505, training loss 0.000003, validation loss 0.000413\n",
      "Iter 19506, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19507, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19508, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19509, training loss 0.000002, validation loss 0.000417\n",
      "Iter 19510, training loss 0.000002, validation loss 0.000413\n",
      "Iter 19511, training loss 0.000002, validation loss 0.000417\n",
      "Iter 19512, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19513, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19514, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19515, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19516, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19517, training loss 0.000001, validation loss 0.000413\n",
      "Iter 19518, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19519, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19520, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19521, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19522, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19523, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19524, training loss 0.000001, validation loss 0.000413\n",
      "Iter 19525, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19526, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19527, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19528, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19529, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19530, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19531, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19532, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19533, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19534, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19535, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19536, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19537, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19538, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19539, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19540, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19541, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19542, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19543, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19544, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19545, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19546, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19547, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19548, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19549, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19550, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19551, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19552, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19553, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19554, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19555, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19556, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19557, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19558, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19559, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19560, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19561, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19562, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19563, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19564, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19565, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19566, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19567, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19568, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19569, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19570, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19571, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19572, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19573, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19574, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19575, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19576, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19577, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19578, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19579, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19580, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19581, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19582, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19583, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19584, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19585, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19586, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19587, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19588, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19589, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19590, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19591, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19592, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19593, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19594, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19595, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19596, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19597, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19598, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19599, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19600, training loss 0.000000, validation loss 0.000414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19601, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19602, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19603, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19604, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19605, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19606, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19607, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19608, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19609, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19610, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19611, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19612, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19613, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19614, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19615, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19616, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19617, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19618, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19619, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19620, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19621, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19622, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19623, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19624, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19625, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19626, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19627, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19628, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19629, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19630, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19631, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19632, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19633, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19634, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19635, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19636, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19637, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19638, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19639, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19640, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19641, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19642, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19643, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19644, training loss 0.000000, validation loss 0.000415\n",
      "Iter 19645, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19646, training loss 0.000000, validation loss 0.000416\n",
      "Iter 19647, training loss 0.000000, validation loss 0.000414\n",
      "Iter 19648, training loss 0.000001, validation loss 0.000416\n",
      "Iter 19649, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19650, training loss 0.000001, validation loss 0.000417\n",
      "Iter 19651, training loss 0.000002, validation loss 0.000414\n",
      "Iter 19652, training loss 0.000002, validation loss 0.000419\n",
      "Iter 19653, training loss 0.000003, validation loss 0.000415\n",
      "Iter 19654, training loss 0.000005, validation loss 0.000424\n",
      "Iter 19655, training loss 0.000008, validation loss 0.000418\n",
      "Iter 19656, training loss 0.000012, validation loss 0.000432\n",
      "Iter 19657, training loss 0.000018, validation loss 0.000425\n",
      "Iter 19658, training loss 0.000026, validation loss 0.000450\n",
      "Iter 19659, training loss 0.000037, validation loss 0.000442\n",
      "Iter 19660, training loss 0.000051, validation loss 0.000478\n",
      "Iter 19661, training loss 0.000064, validation loss 0.000466\n",
      "Iter 19662, training loss 0.000072, validation loss 0.000502\n",
      "Iter 19663, training loss 0.000068, validation loss 0.000469\n",
      "Iter 19664, training loss 0.000050, validation loss 0.000476\n",
      "Iter 19665, training loss 0.000025, validation loss 0.000430\n",
      "Iter 19666, training loss 0.000006, validation loss 0.000422\n",
      "Iter 19667, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19668, training loss 0.000009, validation loss 0.000417\n",
      "Iter 19669, training loss 0.000023, validation loss 0.000444\n",
      "Iter 19670, training loss 0.000031, validation loss 0.000435\n",
      "Iter 19671, training loss 0.000028, validation loss 0.000450\n",
      "Iter 19672, training loss 0.000016, validation loss 0.000422\n",
      "Iter 19673, training loss 0.000004, validation loss 0.000418\n",
      "Iter 19674, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19675, training loss 0.000007, validation loss 0.000415\n",
      "Iter 19676, training loss 0.000015, validation loss 0.000434\n",
      "Iter 19677, training loss 0.000017, validation loss 0.000422\n",
      "Iter 19678, training loss 0.000012, validation loss 0.000430\n",
      "Iter 19679, training loss 0.000004, validation loss 0.000412\n",
      "Iter 19680, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19681, training loss 0.000002, validation loss 0.000416\n",
      "Iter 19682, training loss 0.000007, validation loss 0.000414\n",
      "Iter 19683, training loss 0.000009, validation loss 0.000426\n",
      "Iter 19684, training loss 0.000007, validation loss 0.000415\n",
      "Iter 19685, training loss 0.000003, validation loss 0.000418\n",
      "Iter 19686, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19687, training loss 0.000001, validation loss 0.000411\n",
      "Iter 19688, training loss 0.000003, validation loss 0.000417\n",
      "Iter 19689, training loss 0.000005, validation loss 0.000411\n",
      "Iter 19690, training loss 0.000004, validation loss 0.000417\n",
      "Iter 19691, training loss 0.000002, validation loss 0.000410\n",
      "Iter 19692, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19693, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19694, training loss 0.000002, validation loss 0.000411\n",
      "Iter 19695, training loss 0.000003, validation loss 0.000416\n",
      "Iter 19696, training loss 0.000002, validation loss 0.000411\n",
      "Iter 19697, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19698, training loss 0.000000, validation loss 0.000410\n",
      "Iter 19699, training loss 0.000000, validation loss 0.000410\n",
      "Iter 19700, training loss 0.000001, validation loss 0.000413\n",
      "Iter 19701, training loss 0.000001, validation loss 0.000411\n",
      "Iter 19702, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19703, training loss 0.000001, validation loss 0.000411\n",
      "Iter 19704, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19705, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19706, training loss 0.000000, validation loss 0.000410\n",
      "Iter 19707, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19708, training loss 0.000001, validation loss 0.000410\n",
      "Iter 19709, training loss 0.000001, validation loss 0.000413\n",
      "Iter 19710, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19711, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19712, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19713, training loss 0.000000, validation loss 0.000410\n",
      "Iter 19714, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19715, training loss 0.000001, validation loss 0.000410\n",
      "Iter 19716, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19717, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19718, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19719, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19720, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19721, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19722, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19723, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19724, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19725, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19726, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19727, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19728, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19729, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19730, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19731, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19732, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19733, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19734, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19735, training loss 0.000000, validation loss 0.000412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19736, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19737, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19738, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19739, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19740, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19741, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19742, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19743, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19744, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19745, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19746, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19747, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19748, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19749, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19750, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19751, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19752, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19753, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19754, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19755, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19756, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19757, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19758, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19759, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19760, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19761, training loss 0.000000, validation loss 0.000411\n",
      "Iter 19762, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19763, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19764, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19765, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19766, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19767, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19768, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19769, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19770, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19771, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19772, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19773, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19774, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19775, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19776, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19777, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19778, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19779, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19780, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19781, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19782, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19783, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19784, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19785, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19786, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19787, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19788, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19789, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19790, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19791, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19792, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19793, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19794, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19795, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19796, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19797, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19798, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19799, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19800, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19801, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19802, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19803, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19804, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19805, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19806, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19807, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19808, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19809, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19810, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19811, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19812, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19813, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19814, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19815, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19816, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19817, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19818, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19819, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19820, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19821, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19822, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19823, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19824, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19825, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19826, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19827, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19828, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19829, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19830, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19831, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19832, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19833, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19834, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19835, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19836, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19837, training loss 0.000000, validation loss 0.000413\n",
      "Iter 19838, training loss 0.000000, validation loss 0.000412\n",
      "Iter 19839, training loss 0.000001, validation loss 0.000414\n",
      "Iter 19840, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19841, training loss 0.000001, validation loss 0.000415\n",
      "Iter 19842, training loss 0.000002, validation loss 0.000412\n",
      "Iter 19843, training loss 0.000003, validation loss 0.000418\n",
      "Iter 19844, training loss 0.000004, validation loss 0.000413\n",
      "Iter 19845, training loss 0.000006, validation loss 0.000423\n",
      "Iter 19846, training loss 0.000010, validation loss 0.000417\n",
      "Iter 19847, training loss 0.000015, validation loss 0.000434\n",
      "Iter 19848, training loss 0.000023, validation loss 0.000427\n",
      "Iter 19849, training loss 0.000033, validation loss 0.000456\n",
      "Iter 19850, training loss 0.000047, validation loss 0.000449\n",
      "Iter 19851, training loss 0.000063, validation loss 0.000490\n",
      "Iter 19852, training loss 0.000076, validation loss 0.000475\n",
      "Iter 19853, training loss 0.000079, validation loss 0.000507\n",
      "Iter 19854, training loss 0.000067, validation loss 0.000466\n",
      "Iter 19855, training loss 0.000042, validation loss 0.000464\n",
      "Iter 19856, training loss 0.000015, validation loss 0.000419\n",
      "Iter 19857, training loss 0.000001, validation loss 0.000412\n",
      "Iter 19858, training loss 0.000004, validation loss 0.000418\n",
      "Iter 19859, training loss 0.000018, validation loss 0.000422\n",
      "Iter 19860, training loss 0.000031, validation loss 0.000451\n",
      "Iter 19861, training loss 0.000033, validation loss 0.000435\n",
      "Iter 19862, training loss 0.000022, validation loss 0.000440\n",
      "Iter 19863, training loss 0.000008, validation loss 0.000413\n",
      "Iter 19864, training loss 0.000001, validation loss 0.000410\n",
      "Iter 19865, training loss 0.000005, validation loss 0.000418\n",
      "Iter 19866, training loss 0.000014, validation loss 0.000417\n",
      "Iter 19867, training loss 0.000019, validation loss 0.000436\n",
      "Iter 19868, training loss 0.000015, validation loss 0.000417\n",
      "Iter 19869, training loss 0.000007, validation loss 0.000419\n",
      "Iter 19870, training loss 0.000001, validation loss 0.000407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19871, training loss 0.000002, validation loss 0.000408\n",
      "Iter 19872, training loss 0.000007, validation loss 0.000420\n",
      "Iter 19873, training loss 0.000010, validation loss 0.000414\n",
      "Iter 19874, training loss 0.000008, validation loss 0.000422\n",
      "Iter 19875, training loss 0.000004, validation loss 0.000410\n",
      "Iter 19876, training loss 0.000001, validation loss 0.000410\n",
      "Iter 19877, training loss 0.000001, validation loss 0.000411\n",
      "Iter 19878, training loss 0.000003, validation loss 0.000408\n",
      "Iter 19879, training loss 0.000005, validation loss 0.000417\n",
      "Iter 19880, training loss 0.000004, validation loss 0.000408\n",
      "Iter 19881, training loss 0.000002, validation loss 0.000412\n",
      "Iter 19882, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19883, training loss 0.000001, validation loss 0.000408\n",
      "Iter 19884, training loss 0.000002, validation loss 0.000413\n",
      "Iter 19885, training loss 0.000003, validation loss 0.000408\n",
      "Iter 19886, training loss 0.000002, validation loss 0.000413\n",
      "Iter 19887, training loss 0.000001, validation loss 0.000407\n",
      "Iter 19888, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19889, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19890, training loss 0.000001, validation loss 0.000408\n",
      "Iter 19891, training loss 0.000002, validation loss 0.000412\n",
      "Iter 19892, training loss 0.000001, validation loss 0.000408\n",
      "Iter 19893, training loss 0.000001, validation loss 0.000410\n",
      "Iter 19894, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19895, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19896, training loss 0.000001, validation loss 0.000409\n",
      "Iter 19897, training loss 0.000001, validation loss 0.000407\n",
      "Iter 19898, training loss 0.000001, validation loss 0.000411\n",
      "Iter 19899, training loss 0.000001, validation loss 0.000408\n",
      "Iter 19900, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19901, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19902, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19903, training loss 0.000001, validation loss 0.000410\n",
      "Iter 19904, training loss 0.000001, validation loss 0.000407\n",
      "Iter 19905, training loss 0.000001, validation loss 0.000409\n",
      "Iter 19906, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19907, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19908, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19909, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19910, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19911, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19912, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19913, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19914, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19915, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19916, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19917, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19918, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19919, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19920, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19921, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19922, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19923, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19924, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19925, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19926, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19927, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19928, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19929, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19930, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19931, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19932, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19933, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19934, training loss 0.000000, validation loss 0.000408\n",
      "Iter 19935, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19936, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19937, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19938, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19939, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19940, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19941, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19942, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19943, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19944, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19945, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19946, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19947, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19948, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19949, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19950, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19951, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19952, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19953, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19954, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19955, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19956, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19957, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19958, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19959, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19960, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19961, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19962, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19963, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19964, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19965, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19966, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19967, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19968, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19969, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19970, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19971, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19972, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19973, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19974, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19975, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19976, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19977, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19978, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19979, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19980, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19981, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19982, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19983, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19984, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19985, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19986, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19987, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19988, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19989, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19990, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19991, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19992, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19993, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19994, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19995, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19996, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19997, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19998, training loss 0.000000, validation loss 0.000409\n",
      "Iter 19999, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20000, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20001, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20002, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20003, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20004, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20005, training loss 0.000000, validation loss 0.000409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20006, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20007, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20008, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20009, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20010, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20011, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20012, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20013, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20014, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20015, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20016, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20017, training loss 0.000000, validation loss 0.000410\n",
      "Iter 20018, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20019, training loss 0.000000, validation loss 0.000410\n",
      "Iter 20020, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20021, training loss 0.000000, validation loss 0.000410\n",
      "Iter 20022, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20023, training loss 0.000000, validation loss 0.000410\n",
      "Iter 20024, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20025, training loss 0.000000, validation loss 0.000410\n",
      "Iter 20026, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20027, training loss 0.000000, validation loss 0.000410\n",
      "Iter 20028, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20029, training loss 0.000000, validation loss 0.000410\n",
      "Iter 20030, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20031, training loss 0.000000, validation loss 0.000410\n",
      "Iter 20032, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20033, training loss 0.000000, validation loss 0.000411\n",
      "Iter 20034, training loss 0.000000, validation loss 0.000409\n",
      "Iter 20035, training loss 0.000001, validation loss 0.000411\n",
      "Iter 20036, training loss 0.000001, validation loss 0.000409\n",
      "Iter 20037, training loss 0.000001, validation loss 0.000412\n",
      "Iter 20038, training loss 0.000002, validation loss 0.000409\n",
      "Iter 20039, training loss 0.000002, validation loss 0.000415\n",
      "Iter 20040, training loss 0.000004, validation loss 0.000410\n",
      "Iter 20041, training loss 0.000006, validation loss 0.000419\n",
      "Iter 20042, training loss 0.000009, validation loss 0.000413\n",
      "Iter 20043, training loss 0.000013, validation loss 0.000429\n",
      "Iter 20044, training loss 0.000020, validation loss 0.000422\n",
      "Iter 20045, training loss 0.000030, validation loss 0.000449\n",
      "Iter 20046, training loss 0.000043, validation loss 0.000442\n",
      "Iter 20047, training loss 0.000058, validation loss 0.000483\n",
      "Iter 20048, training loss 0.000074, validation loss 0.000471\n",
      "Iter 20049, training loss 0.000082, validation loss 0.000509\n",
      "Iter 20050, training loss 0.000077, validation loss 0.000473\n",
      "Iter 20051, training loss 0.000056, validation loss 0.000479\n",
      "Iter 20052, training loss 0.000027, validation loss 0.000427\n",
      "Iter 20053, training loss 0.000005, validation loss 0.000417\n",
      "Iter 20054, training loss 0.000001, validation loss 0.000410\n",
      "Iter 20055, training loss 0.000012, validation loss 0.000414\n",
      "Iter 20056, training loss 0.000027, validation loss 0.000444\n",
      "Iter 20057, training loss 0.000034, validation loss 0.000433\n",
      "Iter 20058, training loss 0.000028, validation loss 0.000444\n",
      "Iter 20059, training loss 0.000013, validation loss 0.000415\n",
      "Iter 20060, training loss 0.000002, validation loss 0.000411\n",
      "Iter 20061, training loss 0.000002, validation loss 0.000412\n",
      "Iter 20062, training loss 0.000011, validation loss 0.000413\n",
      "Iter 20063, training loss 0.000018, validation loss 0.000433\n",
      "Iter 20064, training loss 0.000018, validation loss 0.000417\n",
      "Iter 20065, training loss 0.000010, validation loss 0.000421\n",
      "Iter 20066, training loss 0.000002, validation loss 0.000405\n",
      "Iter 20067, training loss 0.000001, validation loss 0.000406\n",
      "Iter 20068, training loss 0.000005, validation loss 0.000415\n",
      "Iter 20069, training loss 0.000010, validation loss 0.000412\n",
      "Iter 20070, training loss 0.000010, validation loss 0.000423\n",
      "Iter 20071, training loss 0.000006, validation loss 0.000409\n",
      "Iter 20072, training loss 0.000002, validation loss 0.000410\n",
      "Iter 20073, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20074, training loss 0.000003, validation loss 0.000405\n",
      "Iter 20075, training loss 0.000005, validation loss 0.000415\n",
      "Iter 20076, training loss 0.000005, validation loss 0.000407\n",
      "Iter 20077, training loss 0.000003, validation loss 0.000412\n",
      "Iter 20078, training loss 0.000001, validation loss 0.000405\n",
      "Iter 20079, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20080, training loss 0.000001, validation loss 0.000410\n",
      "Iter 20081, training loss 0.000003, validation loss 0.000406\n",
      "Iter 20082, training loss 0.000003, validation loss 0.000412\n",
      "Iter 20083, training loss 0.000002, validation loss 0.000405\n",
      "Iter 20084, training loss 0.000001, validation loss 0.000407\n",
      "Iter 20085, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20086, training loss 0.000001, validation loss 0.000405\n",
      "Iter 20087, training loss 0.000002, validation loss 0.000410\n",
      "Iter 20088, training loss 0.000002, validation loss 0.000406\n",
      "Iter 20089, training loss 0.000001, validation loss 0.000409\n",
      "Iter 20090, training loss 0.000000, validation loss 0.000405\n",
      "Iter 20091, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20092, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20093, training loss 0.000001, validation loss 0.000406\n",
      "Iter 20094, training loss 0.000001, validation loss 0.000409\n",
      "Iter 20095, training loss 0.000001, validation loss 0.000406\n",
      "Iter 20096, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20097, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20098, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20099, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20100, training loss 0.000001, validation loss 0.000405\n",
      "Iter 20101, training loss 0.000001, validation loss 0.000408\n",
      "Iter 20102, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20103, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20104, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20105, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20106, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20107, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20108, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20109, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20110, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20111, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20112, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20113, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20114, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20115, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20116, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20117, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20118, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20119, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20120, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20121, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20122, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20123, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20124, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20125, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20126, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20127, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20128, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20129, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20130, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20131, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20132, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20133, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20134, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20135, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20136, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20137, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20138, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20139, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20140, training loss 0.000000, validation loss 0.000407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20141, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20142, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20143, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20144, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20145, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20146, training loss 0.000000, validation loss 0.000406\n",
      "Iter 20147, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20148, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20149, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20150, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20151, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20152, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20153, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20154, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20155, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20156, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20157, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20158, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20159, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20160, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20161, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20162, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20163, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20164, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20165, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20166, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20167, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20168, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20169, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20170, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20171, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20172, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20173, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20174, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20175, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20176, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20177, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20178, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20179, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20180, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20181, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20182, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20183, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20184, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20185, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20186, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20187, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20188, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20189, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20190, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20191, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20192, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20193, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20194, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20195, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20196, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20197, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20198, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20199, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20200, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20201, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20202, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20203, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20204, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20205, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20206, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20207, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20208, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20209, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20210, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20211, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20212, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20213, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20214, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20215, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20216, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20217, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20218, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20219, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20220, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20221, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20222, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20223, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20224, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20225, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20226, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20227, training loss 0.000000, validation loss 0.000408\n",
      "Iter 20228, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20229, training loss 0.000000, validation loss 0.000408\n",
      "Iter 20230, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20231, training loss 0.000000, validation loss 0.000408\n",
      "Iter 20232, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20233, training loss 0.000000, validation loss 0.000408\n",
      "Iter 20234, training loss 0.000000, validation loss 0.000407\n",
      "Iter 20235, training loss 0.000001, validation loss 0.000409\n",
      "Iter 20236, training loss 0.000001, validation loss 0.000407\n",
      "Iter 20237, training loss 0.000001, validation loss 0.000410\n",
      "Iter 20238, training loss 0.000002, validation loss 0.000407\n",
      "Iter 20239, training loss 0.000003, validation loss 0.000413\n",
      "Iter 20240, training loss 0.000004, validation loss 0.000408\n",
      "Iter 20241, training loss 0.000007, validation loss 0.000419\n",
      "Iter 20242, training loss 0.000011, validation loss 0.000413\n",
      "Iter 20243, training loss 0.000017, validation loss 0.000431\n",
      "Iter 20244, training loss 0.000025, validation loss 0.000425\n",
      "Iter 20245, training loss 0.000038, validation loss 0.000457\n",
      "Iter 20246, training loss 0.000055, validation loss 0.000452\n",
      "Iter 20247, training loss 0.000073, validation loss 0.000497\n",
      "Iter 20248, training loss 0.000089, validation loss 0.000483\n",
      "Iter 20249, training loss 0.000092, validation loss 0.000517\n",
      "Iter 20250, training loss 0.000078, validation loss 0.000471\n",
      "Iter 20251, training loss 0.000047, validation loss 0.000465\n",
      "Iter 20252, training loss 0.000016, validation loss 0.000414\n",
      "Iter 20253, training loss 0.000001, validation loss 0.000406\n",
      "Iter 20254, training loss 0.000007, validation loss 0.000416\n",
      "Iter 20255, training loss 0.000024, validation loss 0.000421\n",
      "Iter 20256, training loss 0.000037, validation loss 0.000453\n",
      "Iter 20257, training loss 0.000035, validation loss 0.000431\n",
      "Iter 20258, training loss 0.000019, validation loss 0.000430\n",
      "Iter 20259, training loss 0.000004, validation loss 0.000404\n",
      "Iter 20260, training loss 0.000001, validation loss 0.000403\n",
      "Iter 20261, training loss 0.000010, validation loss 0.000420\n",
      "Iter 20262, training loss 0.000020, validation loss 0.000416\n",
      "Iter 20263, training loss 0.000020, validation loss 0.000432\n",
      "Iter 20264, training loss 0.000012, validation loss 0.000408\n",
      "Iter 20265, training loss 0.000003, validation loss 0.000407\n",
      "Iter 20266, training loss 0.000001, validation loss 0.000404\n",
      "Iter 20267, training loss 0.000006, validation loss 0.000405\n",
      "Iter 20268, training loss 0.000011, validation loss 0.000421\n",
      "Iter 20269, training loss 0.000011, validation loss 0.000410\n",
      "Iter 20270, training loss 0.000006, validation loss 0.000413\n",
      "Iter 20271, training loss 0.000001, validation loss 0.000402\n",
      "Iter 20272, training loss 0.000001, validation loss 0.000401\n",
      "Iter 20273, training loss 0.000004, validation loss 0.000410\n",
      "Iter 20274, training loss 0.000006, validation loss 0.000404\n",
      "Iter 20275, training loss 0.000005, validation loss 0.000412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20276, training loss 0.000002, validation loss 0.000402\n",
      "Iter 20277, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20278, training loss 0.000001, validation loss 0.000405\n",
      "Iter 20279, training loss 0.000003, validation loss 0.000403\n",
      "Iter 20280, training loss 0.000004, validation loss 0.000410\n",
      "Iter 20281, training loss 0.000003, validation loss 0.000403\n",
      "Iter 20282, training loss 0.000001, validation loss 0.000405\n",
      "Iter 20283, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20284, training loss 0.000001, validation loss 0.000402\n",
      "Iter 20285, training loss 0.000002, validation loss 0.000407\n",
      "Iter 20286, training loss 0.000002, validation loss 0.000403\n",
      "Iter 20287, training loss 0.000001, validation loss 0.000406\n",
      "Iter 20288, training loss 0.000000, validation loss 0.000402\n",
      "Iter 20289, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20290, training loss 0.000001, validation loss 0.000404\n",
      "Iter 20291, training loss 0.000001, validation loss 0.000402\n",
      "Iter 20292, training loss 0.000001, validation loss 0.000406\n",
      "Iter 20293, training loss 0.000001, validation loss 0.000403\n",
      "Iter 20294, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20295, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20296, training loss 0.000000, validation loss 0.000402\n",
      "Iter 20297, training loss 0.000001, validation loss 0.000405\n",
      "Iter 20298, training loss 0.000001, validation loss 0.000402\n",
      "Iter 20299, training loss 0.000001, validation loss 0.000404\n",
      "Iter 20300, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20301, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20302, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20303, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20304, training loss 0.000000, validation loss 0.000405\n",
      "Iter 20305, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20306, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20307, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20308, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20309, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20310, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20311, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20312, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20313, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20314, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20315, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20316, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20317, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20318, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20319, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20320, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20321, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20322, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20323, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20324, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20325, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20326, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20327, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20328, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20329, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20330, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20331, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20332, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20333, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20334, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20335, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20336, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20337, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20338, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20339, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20340, training loss 0.000000, validation loss 0.000403\n",
      "Iter 20341, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20342, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20343, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20344, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20345, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20346, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20347, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20348, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20349, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20350, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20351, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20352, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20353, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20354, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20355, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20356, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20357, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20358, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20359, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20360, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20361, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20362, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20363, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20364, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20365, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20366, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20367, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20368, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20369, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20370, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20371, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20372, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20373, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20374, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20375, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20376, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20377, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20378, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20379, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20380, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20381, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20382, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20383, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20384, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20385, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20386, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20387, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20388, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20389, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20390, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20391, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20392, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20393, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20394, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20395, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20396, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20397, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20398, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20399, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20400, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20401, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20402, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20403, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20404, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20405, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20406, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20407, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20408, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20409, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20410, training loss 0.000000, validation loss 0.000404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20411, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20412, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20413, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20414, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20415, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20416, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20417, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20418, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20419, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20420, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20421, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20422, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20423, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20424, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20425, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20426, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20427, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20428, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20429, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20430, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20431, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20432, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20433, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20434, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20435, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20436, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20437, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20438, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20439, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20440, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20441, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20442, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20443, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20444, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20445, training loss 0.000000, validation loss 0.000405\n",
      "Iter 20446, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20447, training loss 0.000000, validation loss 0.000405\n",
      "Iter 20448, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20449, training loss 0.000000, validation loss 0.000405\n",
      "Iter 20450, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20451, training loss 0.000000, validation loss 0.000405\n",
      "Iter 20452, training loss 0.000000, validation loss 0.000404\n",
      "Iter 20453, training loss 0.000001, validation loss 0.000406\n",
      "Iter 20454, training loss 0.000001, validation loss 0.000404\n",
      "Iter 20455, training loss 0.000002, validation loss 0.000408\n",
      "Iter 20456, training loss 0.000003, validation loss 0.000404\n",
      "Iter 20457, training loss 0.000004, validation loss 0.000412\n",
      "Iter 20458, training loss 0.000007, validation loss 0.000407\n",
      "Iter 20459, training loss 0.000011, validation loss 0.000422\n",
      "Iter 20460, training loss 0.000018, validation loss 0.000416\n",
      "Iter 20461, training loss 0.000029, validation loss 0.000444\n",
      "Iter 20462, training loss 0.000046, validation loss 0.000441\n",
      "Iter 20463, training loss 0.000068, validation loss 0.000489\n",
      "Iter 20464, training loss 0.000095, validation loss 0.000487\n",
      "Iter 20465, training loss 0.000116, validation loss 0.000541\n",
      "Iter 20466, training loss 0.000119, validation loss 0.000510\n",
      "Iter 20467, training loss 0.000094, validation loss 0.000516\n",
      "Iter 20468, training loss 0.000048, validation loss 0.000439\n",
      "Iter 20469, training loss 0.000010, validation loss 0.000414\n",
      "Iter 20470, training loss 0.000002, validation loss 0.000403\n",
      "Iter 20471, training loss 0.000022, validation loss 0.000415\n",
      "Iter 20472, training loss 0.000046, validation loss 0.000460\n",
      "Iter 20473, training loss 0.000051, validation loss 0.000443\n",
      "Iter 20474, training loss 0.000032, validation loss 0.000440\n",
      "Iter 20475, training loss 0.000007, validation loss 0.000403\n",
      "Iter 20476, training loss 0.000002, validation loss 0.000400\n",
      "Iter 20477, training loss 0.000016, validation loss 0.000424\n",
      "Iter 20478, training loss 0.000029, validation loss 0.000422\n",
      "Iter 20479, training loss 0.000027, validation loss 0.000437\n",
      "Iter 20480, training loss 0.000012, validation loss 0.000405\n",
      "Iter 20481, training loss 0.000001, validation loss 0.000399\n",
      "Iter 20482, training loss 0.000005, validation loss 0.000406\n",
      "Iter 20483, training loss 0.000015, validation loss 0.000407\n",
      "Iter 20484, training loss 0.000017, validation loss 0.000424\n",
      "Iter 20485, training loss 0.000010, validation loss 0.000404\n",
      "Iter 20486, training loss 0.000002, validation loss 0.000402\n",
      "Iter 20487, training loss 0.000002, validation loss 0.000401\n",
      "Iter 20488, training loss 0.000007, validation loss 0.000400\n",
      "Iter 20489, training loss 0.000010, validation loss 0.000413\n",
      "Iter 20490, training loss 0.000006, validation loss 0.000399\n",
      "Iter 20491, training loss 0.000001, validation loss 0.000401\n",
      "Iter 20492, training loss 0.000001, validation loss 0.000399\n",
      "Iter 20493, training loss 0.000004, validation loss 0.000398\n",
      "Iter 20494, training loss 0.000006, validation loss 0.000408\n",
      "Iter 20495, training loss 0.000004, validation loss 0.000398\n",
      "Iter 20496, training loss 0.000001, validation loss 0.000400\n",
      "Iter 20497, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20498, training loss 0.000002, validation loss 0.000397\n",
      "Iter 20499, training loss 0.000004, validation loss 0.000405\n",
      "Iter 20500, training loss 0.000003, validation loss 0.000398\n",
      "Iter 20501, training loss 0.000001, validation loss 0.000400\n",
      "Iter 20502, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20503, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20504, training loss 0.000002, validation loss 0.000402\n",
      "Iter 20505, training loss 0.000002, validation loss 0.000397\n",
      "Iter 20506, training loss 0.000001, validation loss 0.000400\n",
      "Iter 20507, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20508, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20509, training loss 0.000001, validation loss 0.000401\n",
      "Iter 20510, training loss 0.000001, validation loss 0.000398\n",
      "Iter 20511, training loss 0.000001, validation loss 0.000400\n",
      "Iter 20512, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20513, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20514, training loss 0.000001, validation loss 0.000400\n",
      "Iter 20515, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20516, training loss 0.000001, validation loss 0.000400\n",
      "Iter 20517, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20518, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20519, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20520, training loss 0.000001, validation loss 0.000398\n",
      "Iter 20521, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20522, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20523, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20524, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20525, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20526, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20527, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20528, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20529, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20530, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20531, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20532, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20533, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20534, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20535, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20536, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20537, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20538, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20539, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20540, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20541, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20542, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20543, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20544, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20545, training loss 0.000000, validation loss 0.000399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20546, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20547, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20548, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20549, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20550, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20551, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20552, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20553, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20554, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20555, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20556, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20557, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20558, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20559, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20560, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20561, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20562, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20563, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20564, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20565, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20566, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20567, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20568, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20569, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20570, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20571, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20572, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20573, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20574, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20575, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20576, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20577, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20578, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20579, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20580, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20581, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20582, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20583, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20584, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20585, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20586, training loss 0.000000, validation loss 0.000399\n",
      "Iter 20587, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20588, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20589, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20590, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20591, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20592, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20593, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20594, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20595, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20596, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20597, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20598, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20599, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20600, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20601, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20602, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20603, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20604, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20605, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20606, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20607, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20608, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20609, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20610, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20611, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20612, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20613, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20614, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20615, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20616, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20617, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20618, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20619, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20620, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20621, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20622, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20623, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20624, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20625, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20626, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20627, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20628, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20629, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20630, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20631, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20632, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20633, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20634, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20635, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20636, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20637, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20638, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20639, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20640, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20641, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20642, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20643, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20644, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20645, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20646, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20647, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20648, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20649, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20650, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20651, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20652, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20653, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20654, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20655, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20656, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20657, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20658, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20659, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20660, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20661, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20662, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20663, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20664, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20665, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20666, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20667, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20668, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20669, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20670, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20671, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20672, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20673, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20674, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20675, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20676, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20677, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20678, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20679, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20680, training loss 0.000000, validation loss 0.000400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20681, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20682, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20683, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20684, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20685, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20686, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20687, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20688, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20689, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20690, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20691, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20692, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20693, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20694, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20695, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20696, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20697, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20698, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20699, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20700, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20701, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20702, training loss 0.000000, validation loss 0.000401\n",
      "Iter 20703, training loss 0.000000, validation loss 0.000400\n",
      "Iter 20704, training loss 0.000000, validation loss 0.000402\n",
      "Iter 20705, training loss 0.000001, validation loss 0.000400\n",
      "Iter 20706, training loss 0.000001, validation loss 0.000403\n",
      "Iter 20707, training loss 0.000002, validation loss 0.000400\n",
      "Iter 20708, training loss 0.000003, validation loss 0.000406\n",
      "Iter 20709, training loss 0.000004, validation loss 0.000402\n",
      "Iter 20710, training loss 0.000007, validation loss 0.000412\n",
      "Iter 20711, training loss 0.000011, validation loss 0.000407\n",
      "Iter 20712, training loss 0.000018, validation loss 0.000426\n",
      "Iter 20713, training loss 0.000028, validation loss 0.000421\n",
      "Iter 20714, training loss 0.000043, validation loss 0.000457\n",
      "Iter 20715, training loss 0.000064, validation loss 0.000454\n",
      "Iter 20716, training loss 0.000088, validation loss 0.000508\n",
      "Iter 20717, training loss 0.000109, validation loss 0.000498\n",
      "Iter 20718, training loss 0.000115, validation loss 0.000536\n",
      "Iter 20719, training loss 0.000094, validation loss 0.000482\n",
      "Iter 20720, training loss 0.000053, validation loss 0.000465\n",
      "Iter 20721, training loss 0.000014, validation loss 0.000405\n",
      "Iter 20722, training loss 0.000001, validation loss 0.000396\n",
      "Iter 20723, training loss 0.000015, validation loss 0.000419\n",
      "Iter 20724, training loss 0.000038, validation loss 0.000427\n",
      "Iter 20725, training loss 0.000048, validation loss 0.000458\n",
      "Iter 20726, training loss 0.000034, validation loss 0.000423\n",
      "Iter 20727, training loss 0.000011, validation loss 0.000411\n",
      "Iter 20728, training loss 0.000001, validation loss 0.000395\n",
      "Iter 20729, training loss 0.000010, validation loss 0.000400\n",
      "Iter 20730, training loss 0.000024, validation loss 0.000430\n",
      "Iter 20731, training loss 0.000027, validation loss 0.000414\n",
      "Iter 20732, training loss 0.000016, validation loss 0.000417\n",
      "Iter 20733, training loss 0.000003, validation loss 0.000394\n",
      "Iter 20734, training loss 0.000002, validation loss 0.000394\n",
      "Iter 20735, training loss 0.000010, validation loss 0.000412\n",
      "Iter 20736, training loss 0.000016, validation loss 0.000407\n",
      "Iter 20737, training loss 0.000013, validation loss 0.000415\n",
      "Iter 20738, training loss 0.000005, validation loss 0.000395\n",
      "Iter 20739, training loss 0.000001, validation loss 0.000393\n",
      "Iter 20740, training loss 0.000004, validation loss 0.000400\n",
      "Iter 20741, training loss 0.000009, validation loss 0.000397\n",
      "Iter 20742, training loss 0.000009, validation loss 0.000408\n",
      "Iter 20743, training loss 0.000004, validation loss 0.000395\n",
      "Iter 20744, training loss 0.000001, validation loss 0.000396\n",
      "Iter 20745, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20746, training loss 0.000004, validation loss 0.000395\n",
      "Iter 20747, training loss 0.000005, validation loss 0.000404\n",
      "Iter 20748, training loss 0.000003, validation loss 0.000395\n",
      "Iter 20749, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20750, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20751, training loss 0.000002, validation loss 0.000394\n",
      "Iter 20752, training loss 0.000003, validation loss 0.000400\n",
      "Iter 20753, training loss 0.000002, validation loss 0.000394\n",
      "Iter 20754, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20755, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20756, training loss 0.000001, validation loss 0.000394\n",
      "Iter 20757, training loss 0.000002, validation loss 0.000398\n",
      "Iter 20758, training loss 0.000002, validation loss 0.000394\n",
      "Iter 20759, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20760, training loss 0.000000, validation loss 0.000394\n",
      "Iter 20761, training loss 0.000000, validation loss 0.000394\n",
      "Iter 20762, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20763, training loss 0.000001, validation loss 0.000394\n",
      "Iter 20764, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20765, training loss 0.000000, validation loss 0.000394\n",
      "Iter 20766, training loss 0.000000, validation loss 0.000394\n",
      "Iter 20767, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20768, training loss 0.000001, validation loss 0.000394\n",
      "Iter 20769, training loss 0.000001, validation loss 0.000397\n",
      "Iter 20770, training loss 0.000000, validation loss 0.000394\n",
      "Iter 20771, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20772, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20773, training loss 0.000000, validation loss 0.000394\n",
      "Iter 20774, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20775, training loss 0.000000, validation loss 0.000394\n",
      "Iter 20776, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20777, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20778, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20779, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20780, training loss 0.000000, validation loss 0.000394\n",
      "Iter 20781, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20782, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20783, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20784, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20785, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20786, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20787, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20788, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20789, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20790, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20791, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20792, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20793, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20794, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20795, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20796, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20797, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20798, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20799, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20800, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20801, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20802, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20803, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20804, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20805, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20806, training loss 0.000000, validation loss 0.000395\n",
      "Iter 20807, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20808, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20809, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20810, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20811, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20812, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20813, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20814, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20815, training loss 0.000000, validation loss 0.000396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20816, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20817, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20818, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20819, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20820, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20821, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20822, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20823, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20824, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20825, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20826, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20827, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20828, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20829, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20830, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20831, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20832, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20833, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20834, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20835, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20836, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20837, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20838, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20839, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20840, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20841, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20842, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20843, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20844, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20845, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20846, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20847, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20848, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20849, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20850, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20851, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20852, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20853, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20854, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20855, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20856, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20857, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20858, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20859, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20860, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20861, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20862, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20863, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20864, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20865, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20866, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20867, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20868, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20869, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20870, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20871, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20872, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20873, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20874, training loss 0.000000, validation loss 0.000396\n",
      "Iter 20875, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20876, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20877, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20878, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20879, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20880, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20881, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20882, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20883, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20884, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20885, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20886, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20887, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20888, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20889, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20890, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20891, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20892, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20893, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20894, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20895, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20896, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20897, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20898, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20899, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20900, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20901, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20902, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20903, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20904, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20905, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20906, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20907, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20908, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20909, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20910, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20911, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20912, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20913, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20914, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20915, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20916, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20917, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20918, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20919, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20920, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20921, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20922, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20923, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20924, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20925, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20926, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20927, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20928, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20929, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20930, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20931, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20932, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20933, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20934, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20935, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20936, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20937, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20938, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20939, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20940, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20941, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20942, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20943, training loss 0.000000, validation loss 0.000398\n",
      "Iter 20944, training loss 0.000000, validation loss 0.000397\n",
      "Iter 20945, training loss 0.000001, validation loss 0.000399\n",
      "Iter 20946, training loss 0.000001, validation loss 0.000396\n",
      "Iter 20947, training loss 0.000001, validation loss 0.000400\n",
      "Iter 20948, training loss 0.000002, validation loss 0.000397\n",
      "Iter 20949, training loss 0.000003, validation loss 0.000404\n",
      "Iter 20950, training loss 0.000005, validation loss 0.000399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20951, training loss 0.000008, validation loss 0.000410\n",
      "Iter 20952, training loss 0.000013, validation loss 0.000404\n",
      "Iter 20953, training loss 0.000020, validation loss 0.000425\n",
      "Iter 20954, training loss 0.000030, validation loss 0.000420\n",
      "Iter 20955, training loss 0.000045, validation loss 0.000455\n",
      "Iter 20956, training loss 0.000063, validation loss 0.000451\n",
      "Iter 20957, training loss 0.000084, validation loss 0.000500\n",
      "Iter 20958, training loss 0.000098, validation loss 0.000485\n",
      "Iter 20959, training loss 0.000099, validation loss 0.000516\n",
      "Iter 20960, training loss 0.000078, validation loss 0.000462\n",
      "Iter 20961, training loss 0.000042, validation loss 0.000450\n",
      "Iter 20962, training loss 0.000011, validation loss 0.000400\n",
      "Iter 20963, training loss 0.000001, validation loss 0.000394\n",
      "Iter 20964, training loss 0.000012, validation loss 0.000412\n",
      "Iter 20965, training loss 0.000031, validation loss 0.000418\n",
      "Iter 20966, training loss 0.000040, validation loss 0.000446\n",
      "Iter 20967, training loss 0.000031, validation loss 0.000418\n",
      "Iter 20968, training loss 0.000012, validation loss 0.000411\n",
      "Iter 20969, training loss 0.000001, validation loss 0.000392\n",
      "Iter 20970, training loss 0.000005, validation loss 0.000394\n",
      "Iter 20971, training loss 0.000016, validation loss 0.000417\n",
      "Iter 20972, training loss 0.000022, validation loss 0.000408\n",
      "Iter 20973, training loss 0.000017, validation loss 0.000416\n",
      "Iter 20974, training loss 0.000006, validation loss 0.000394\n",
      "Iter 20975, training loss 0.000000, validation loss 0.000393\n",
      "Iter 20976, training loss 0.000004, validation loss 0.000401\n",
      "Iter 20977, training loss 0.000011, validation loss 0.000400\n",
      "Iter 20978, training loss 0.000013, validation loss 0.000412\n",
      "Iter 20979, training loss 0.000008, validation loss 0.000395\n",
      "Iter 20980, training loss 0.000002, validation loss 0.000394\n",
      "Iter 20981, training loss 0.000000, validation loss 0.000392\n",
      "Iter 20982, training loss 0.000004, validation loss 0.000392\n",
      "Iter 20983, training loss 0.000007, validation loss 0.000405\n",
      "Iter 20984, training loss 0.000007, validation loss 0.000395\n",
      "Iter 20985, training loss 0.000004, validation loss 0.000399\n",
      "Iter 20986, training loss 0.000001, validation loss 0.000391\n",
      "Iter 20987, training loss 0.000001, validation loss 0.000391\n",
      "Iter 20988, training loss 0.000003, validation loss 0.000398\n",
      "Iter 20989, training loss 0.000005, validation loss 0.000394\n",
      "Iter 20990, training loss 0.000004, validation loss 0.000399\n",
      "Iter 20991, training loss 0.000001, validation loss 0.000391\n",
      "Iter 20992, training loss 0.000000, validation loss 0.000392\n",
      "Iter 20993, training loss 0.000001, validation loss 0.000394\n",
      "Iter 20994, training loss 0.000002, validation loss 0.000392\n",
      "Iter 20995, training loss 0.000003, validation loss 0.000398\n",
      "Iter 20996, training loss 0.000002, validation loss 0.000392\n",
      "Iter 20997, training loss 0.000001, validation loss 0.000394\n",
      "Iter 20998, training loss 0.000000, validation loss 0.000392\n",
      "Iter 20999, training loss 0.000001, validation loss 0.000391\n",
      "Iter 21000, training loss 0.000001, validation loss 0.000395\n",
      "Iter 21001, training loss 0.000001, validation loss 0.000392\n",
      "Iter 21002, training loss 0.000001, validation loss 0.000394\n",
      "Iter 21003, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21004, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21005, training loss 0.000001, validation loss 0.000393\n",
      "Iter 21006, training loss 0.000001, validation loss 0.000391\n",
      "Iter 21007, training loss 0.000001, validation loss 0.000394\n",
      "Iter 21008, training loss 0.000001, validation loss 0.000392\n",
      "Iter 21009, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21010, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21011, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21012, training loss 0.000001, validation loss 0.000394\n",
      "Iter 21013, training loss 0.000001, validation loss 0.000391\n",
      "Iter 21014, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21015, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21016, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21017, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21018, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21019, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21020, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21021, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21022, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21023, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21024, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21025, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21026, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21027, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21028, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21029, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21030, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21031, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21032, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21033, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21034, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21035, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21036, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21037, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21038, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21039, training loss 0.000000, validation loss 0.000392\n",
      "Iter 21040, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21041, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21042, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21043, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21044, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21045, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21046, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21047, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21048, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21049, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21050, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21051, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21052, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21053, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21054, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21055, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21056, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21057, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21058, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21059, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21060, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21061, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21062, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21063, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21064, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21065, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21066, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21067, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21068, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21069, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21070, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21071, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21072, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21073, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21074, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21075, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21076, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21077, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21078, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21079, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21080, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21081, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21082, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21083, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21084, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21085, training loss 0.000000, validation loss 0.000393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21086, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21087, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21088, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21089, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21090, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21091, training loss 0.000000, validation loss 0.000393\n",
      "Iter 21092, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21093, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21094, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21095, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21096, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21097, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21098, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21099, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21100, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21101, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21102, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21103, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21104, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21105, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21106, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21107, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21108, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21109, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21110, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21111, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21112, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21113, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21114, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21115, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21116, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21117, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21118, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21119, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21120, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21121, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21122, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21123, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21124, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21125, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21126, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21127, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21128, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21129, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21130, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21131, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21132, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21133, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21134, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21135, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21136, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21137, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21138, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21139, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21140, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21141, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21142, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21143, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21144, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21145, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21146, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21147, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21148, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21149, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21150, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21151, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21152, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21153, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21154, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21155, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21156, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21157, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21158, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21159, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21160, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21161, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21162, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21163, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21164, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21165, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21166, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21167, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21168, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21169, training loss 0.000000, validation loss 0.000395\n",
      "Iter 21170, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21171, training loss 0.000000, validation loss 0.000395\n",
      "Iter 21172, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21173, training loss 0.000000, validation loss 0.000395\n",
      "Iter 21174, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21175, training loss 0.000000, validation loss 0.000395\n",
      "Iter 21176, training loss 0.000000, validation loss 0.000394\n",
      "Iter 21177, training loss 0.000001, validation loss 0.000396\n",
      "Iter 21178, training loss 0.000001, validation loss 0.000394\n",
      "Iter 21179, training loss 0.000002, validation loss 0.000398\n",
      "Iter 21180, training loss 0.000003, validation loss 0.000395\n",
      "Iter 21181, training loss 0.000005, validation loss 0.000403\n",
      "Iter 21182, training loss 0.000008, validation loss 0.000398\n",
      "Iter 21183, training loss 0.000013, validation loss 0.000414\n",
      "Iter 21184, training loss 0.000022, validation loss 0.000410\n",
      "Iter 21185, training loss 0.000036, validation loss 0.000442\n",
      "Iter 21186, training loss 0.000057, validation loss 0.000442\n",
      "Iter 21187, training loss 0.000085, validation loss 0.000499\n",
      "Iter 21188, training loss 0.000117, validation loss 0.000502\n",
      "Iter 21189, training loss 0.000141, validation loss 0.000560\n",
      "Iter 21190, training loss 0.000138, validation loss 0.000520\n",
      "Iter 21191, training loss 0.000098, validation loss 0.000514\n",
      "Iter 21192, training loss 0.000042, validation loss 0.000426\n",
      "Iter 21193, training loss 0.000005, validation loss 0.000397\n",
      "Iter 21194, training loss 0.000008, validation loss 0.000398\n",
      "Iter 21195, training loss 0.000037, validation loss 0.000413\n",
      "Iter 21196, training loss 0.000059, validation loss 0.000465\n",
      "Iter 21197, training loss 0.000053, validation loss 0.000436\n",
      "Iter 21198, training loss 0.000024, validation loss 0.000416\n",
      "Iter 21199, training loss 0.000003, validation loss 0.000384\n",
      "Iter 21200, training loss 0.000009, validation loss 0.000386\n",
      "Iter 21201, training loss 0.000026, validation loss 0.000424\n",
      "Iter 21202, training loss 0.000034, validation loss 0.000414\n",
      "Iter 21203, training loss 0.000022, validation loss 0.000416\n",
      "Iter 21204, training loss 0.000005, validation loss 0.000385\n",
      "Iter 21205, training loss 0.000003, validation loss 0.000382\n",
      "Iter 21206, training loss 0.000015, validation loss 0.000405\n",
      "Iter 21207, training loss 0.000021, validation loss 0.000402\n",
      "Iter 21208, training loss 0.000015, validation loss 0.000410\n",
      "Iter 21209, training loss 0.000004, validation loss 0.000387\n",
      "Iter 21210, training loss 0.000002, validation loss 0.000384\n",
      "Iter 21211, training loss 0.000008, validation loss 0.000397\n",
      "Iter 21212, training loss 0.000013, validation loss 0.000391\n",
      "Iter 21213, training loss 0.000009, validation loss 0.000399\n",
      "Iter 21214, training loss 0.000002, validation loss 0.000385\n",
      "Iter 21215, training loss 0.000001, validation loss 0.000384\n",
      "Iter 21216, training loss 0.000005, validation loss 0.000394\n",
      "Iter 21217, training loss 0.000007, validation loss 0.000388\n",
      "Iter 21218, training loss 0.000005, validation loss 0.000393\n",
      "Iter 21219, training loss 0.000001, validation loss 0.000384\n",
      "Iter 21220, training loss 0.000001, validation loss 0.000384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21221, training loss 0.000003, validation loss 0.000391\n",
      "Iter 21222, training loss 0.000004, validation loss 0.000386\n",
      "Iter 21223, training loss 0.000002, validation loss 0.000390\n",
      "Iter 21224, training loss 0.000001, validation loss 0.000384\n",
      "Iter 21225, training loss 0.000001, validation loss 0.000383\n",
      "Iter 21226, training loss 0.000002, validation loss 0.000389\n",
      "Iter 21227, training loss 0.000003, validation loss 0.000385\n",
      "Iter 21228, training loss 0.000001, validation loss 0.000389\n",
      "Iter 21229, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21230, training loss 0.000001, validation loss 0.000385\n",
      "Iter 21231, training loss 0.000002, validation loss 0.000389\n",
      "Iter 21232, training loss 0.000002, validation loss 0.000384\n",
      "Iter 21233, training loss 0.000001, validation loss 0.000387\n",
      "Iter 21234, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21235, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21236, training loss 0.000001, validation loss 0.000388\n",
      "Iter 21237, training loss 0.000001, validation loss 0.000385\n",
      "Iter 21238, training loss 0.000001, validation loss 0.000387\n",
      "Iter 21239, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21240, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21241, training loss 0.000001, validation loss 0.000387\n",
      "Iter 21242, training loss 0.000001, validation loss 0.000385\n",
      "Iter 21243, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21244, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21245, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21246, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21247, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21248, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21249, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21250, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21251, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21252, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21253, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21254, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21255, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21256, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21257, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21258, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21259, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21260, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21261, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21262, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21263, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21264, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21265, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21266, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21267, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21268, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21269, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21270, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21271, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21272, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21273, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21274, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21275, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21276, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21277, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21278, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21279, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21280, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21281, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21282, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21283, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21284, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21285, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21286, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21287, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21288, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21289, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21290, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21291, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21292, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21293, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21294, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21295, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21296, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21297, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21298, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21299, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21300, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21301, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21302, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21303, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21304, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21305, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21306, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21307, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21308, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21309, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21310, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21311, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21312, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21313, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21314, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21315, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21316, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21317, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21318, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21319, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21320, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21321, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21322, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21323, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21324, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21325, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21326, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21327, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21328, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21329, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21330, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21331, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21332, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21333, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21334, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21335, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21336, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21337, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21338, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21339, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21340, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21341, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21342, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21343, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21344, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21345, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21346, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21347, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21348, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21349, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21350, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21351, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21352, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21353, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21354, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21355, training loss 0.000000, validation loss 0.000387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21356, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21357, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21358, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21359, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21360, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21361, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21362, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21363, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21364, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21365, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21366, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21367, training loss 0.000000, validation loss 0.000387\n",
      "Iter 21368, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21369, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21370, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21371, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21372, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21373, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21374, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21375, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21376, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21377, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21378, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21379, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21380, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21381, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21382, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21383, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21384, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21385, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21386, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21387, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21388, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21389, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21390, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21391, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21392, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21393, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21394, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21395, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21396, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21397, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21398, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21399, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21400, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21401, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21402, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21403, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21404, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21405, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21406, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21407, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21408, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21409, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21410, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21411, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21412, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21413, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21414, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21415, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21416, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21417, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21418, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21419, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21420, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21421, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21422, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21423, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21424, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21425, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21426, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21427, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21428, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21429, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21430, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21431, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21432, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21433, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21434, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21435, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21436, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21437, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21438, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21439, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21440, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21441, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21442, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21443, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21444, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21445, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21446, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21447, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21448, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21449, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21450, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21451, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21452, training loss 0.000000, validation loss 0.000389\n",
      "Iter 21453, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21454, training loss 0.000000, validation loss 0.000389\n",
      "Iter 21455, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21456, training loss 0.000000, validation loss 0.000389\n",
      "Iter 21457, training loss 0.000000, validation loss 0.000388\n",
      "Iter 21458, training loss 0.000001, validation loss 0.000390\n",
      "Iter 21459, training loss 0.000001, validation loss 0.000388\n",
      "Iter 21460, training loss 0.000001, validation loss 0.000392\n",
      "Iter 21461, training loss 0.000002, validation loss 0.000388\n",
      "Iter 21462, training loss 0.000004, validation loss 0.000396\n",
      "Iter 21463, training loss 0.000006, validation loss 0.000390\n",
      "Iter 21464, training loss 0.000010, validation loss 0.000404\n",
      "Iter 21465, training loss 0.000016, validation loss 0.000398\n",
      "Iter 21466, training loss 0.000026, validation loss 0.000425\n",
      "Iter 21467, training loss 0.000041, validation loss 0.000421\n",
      "Iter 21468, training loss 0.000062, validation loss 0.000468\n",
      "Iter 21469, training loss 0.000088, validation loss 0.000467\n",
      "Iter 21470, training loss 0.000113, validation loss 0.000526\n",
      "Iter 21471, training loss 0.000124, validation loss 0.000502\n",
      "Iter 21472, training loss 0.000107, validation loss 0.000519\n",
      "Iter 21473, training loss 0.000065, validation loss 0.000441\n",
      "Iter 21474, training loss 0.000020, validation loss 0.000411\n",
      "Iter 21475, training loss 0.000001, validation loss 0.000385\n",
      "Iter 21476, training loss 0.000015, validation loss 0.000394\n",
      "Iter 21477, training loss 0.000041, validation loss 0.000440\n",
      "Iter 21478, training loss 0.000053, validation loss 0.000429\n",
      "Iter 21479, training loss 0.000037, validation loss 0.000431\n",
      "Iter 21480, training loss 0.000010, validation loss 0.000391\n",
      "Iter 21481, training loss 0.000001, validation loss 0.000386\n",
      "Iter 21482, training loss 0.000013, validation loss 0.000404\n",
      "Iter 21483, training loss 0.000028, validation loss 0.000403\n",
      "Iter 21484, training loss 0.000029, validation loss 0.000421\n",
      "Iter 21485, training loss 0.000014, validation loss 0.000389\n",
      "Iter 21486, training loss 0.000001, validation loss 0.000386\n",
      "Iter 21487, training loss 0.000004, validation loss 0.000393\n",
      "Iter 21488, training loss 0.000015, validation loss 0.000395\n",
      "Iter 21489, training loss 0.000018, validation loss 0.000411\n",
      "Iter 21490, training loss 0.000011, validation loss 0.000387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21491, training loss 0.000002, validation loss 0.000384\n",
      "Iter 21492, training loss 0.000002, validation loss 0.000385\n",
      "Iter 21493, training loss 0.000008, validation loss 0.000388\n",
      "Iter 21494, training loss 0.000012, validation loss 0.000402\n",
      "Iter 21495, training loss 0.000008, validation loss 0.000386\n",
      "Iter 21496, training loss 0.000002, validation loss 0.000386\n",
      "Iter 21497, training loss 0.000001, validation loss 0.000384\n",
      "Iter 21498, training loss 0.000004, validation loss 0.000385\n",
      "Iter 21499, training loss 0.000007, validation loss 0.000395\n",
      "Iter 21500, training loss 0.000006, validation loss 0.000384\n",
      "Iter 21501, training loss 0.000002, validation loss 0.000386\n",
      "Iter 21502, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21503, training loss 0.000002, validation loss 0.000383\n",
      "Iter 21504, training loss 0.000004, validation loss 0.000390\n",
      "Iter 21505, training loss 0.000003, validation loss 0.000382\n",
      "Iter 21506, training loss 0.000001, validation loss 0.000385\n",
      "Iter 21507, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21508, training loss 0.000001, validation loss 0.000383\n",
      "Iter 21509, training loss 0.000002, validation loss 0.000388\n",
      "Iter 21510, training loss 0.000002, validation loss 0.000382\n",
      "Iter 21511, training loss 0.000001, validation loss 0.000384\n",
      "Iter 21512, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21513, training loss 0.000001, validation loss 0.000382\n",
      "Iter 21514, training loss 0.000001, validation loss 0.000387\n",
      "Iter 21515, training loss 0.000001, validation loss 0.000383\n",
      "Iter 21516, training loss 0.000001, validation loss 0.000384\n",
      "Iter 21517, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21518, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21519, training loss 0.000001, validation loss 0.000385\n",
      "Iter 21520, training loss 0.000001, validation loss 0.000383\n",
      "Iter 21521, training loss 0.000001, validation loss 0.000385\n",
      "Iter 21522, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21523, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21524, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21525, training loss 0.000001, validation loss 0.000382\n",
      "Iter 21526, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21527, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21528, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21529, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21530, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21531, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21532, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21533, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21534, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21535, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21536, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21537, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21538, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21539, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21540, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21541, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21542, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21543, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21544, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21545, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21546, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21547, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21548, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21549, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21550, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21551, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21552, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21553, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21554, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21555, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21556, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21557, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21558, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21559, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21560, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21561, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21562, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21563, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21564, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21565, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21566, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21567, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21568, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21569, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21570, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21571, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21572, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21573, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21574, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21575, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21576, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21577, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21578, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21579, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21580, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21581, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21582, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21583, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21584, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21585, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21586, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21587, training loss 0.000000, validation loss 0.000383\n",
      "Iter 21588, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21589, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21590, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21591, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21592, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21593, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21594, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21595, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21596, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21597, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21598, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21599, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21600, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21601, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21602, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21603, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21604, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21605, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21606, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21607, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21608, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21609, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21610, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21611, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21612, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21613, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21614, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21615, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21616, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21617, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21618, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21619, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21620, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21621, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21622, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21623, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21624, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21625, training loss 0.000000, validation loss 0.000384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21626, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21627, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21628, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21629, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21630, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21631, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21632, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21633, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21634, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21635, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21636, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21637, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21638, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21639, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21640, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21641, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21642, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21643, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21644, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21645, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21646, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21647, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21648, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21649, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21650, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21651, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21652, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21653, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21654, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21655, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21656, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21657, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21658, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21659, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21660, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21661, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21662, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21663, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21664, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21665, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21666, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21667, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21668, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21669, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21670, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21671, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21672, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21673, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21674, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21675, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21676, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21677, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21678, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21679, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21680, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21681, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21682, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21683, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21684, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21685, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21686, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21687, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21688, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21689, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21690, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21691, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21692, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21693, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21694, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21695, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21696, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21697, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21698, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21699, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21700, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21701, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21702, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21703, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21704, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21705, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21706, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21707, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21708, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21709, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21710, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21711, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21712, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21713, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21714, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21715, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21716, training loss 0.000000, validation loss 0.000385\n",
      "Iter 21717, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21718, training loss 0.000000, validation loss 0.000386\n",
      "Iter 21719, training loss 0.000000, validation loss 0.000384\n",
      "Iter 21720, training loss 0.000001, validation loss 0.000387\n",
      "Iter 21721, training loss 0.000001, validation loss 0.000384\n",
      "Iter 21722, training loss 0.000002, validation loss 0.000388\n",
      "Iter 21723, training loss 0.000003, validation loss 0.000384\n",
      "Iter 21724, training loss 0.000004, validation loss 0.000392\n",
      "Iter 21725, training loss 0.000006, validation loss 0.000387\n",
      "Iter 21726, training loss 0.000010, validation loss 0.000401\n",
      "Iter 21727, training loss 0.000016, validation loss 0.000395\n",
      "Iter 21728, training loss 0.000025, validation loss 0.000420\n",
      "Iter 21729, training loss 0.000038, validation loss 0.000415\n",
      "Iter 21730, training loss 0.000056, validation loss 0.000457\n",
      "Iter 21731, training loss 0.000077, validation loss 0.000453\n",
      "Iter 21732, training loss 0.000097, validation loss 0.000505\n",
      "Iter 21733, training loss 0.000106, validation loss 0.000481\n",
      "Iter 21734, training loss 0.000093, validation loss 0.000500\n",
      "Iter 21735, training loss 0.000060, validation loss 0.000435\n",
      "Iter 21736, training loss 0.000023, validation loss 0.000414\n",
      "Iter 21737, training loss 0.000002, validation loss 0.000381\n",
      "Iter 21738, training loss 0.000006, validation loss 0.000383\n",
      "Iter 21739, training loss 0.000025, validation loss 0.000416\n",
      "Iter 21740, training loss 0.000040, validation loss 0.000413\n",
      "Iter 21741, training loss 0.000036, validation loss 0.000429\n",
      "Iter 21742, training loss 0.000018, validation loss 0.000396\n",
      "Iter 21743, training loss 0.000003, validation loss 0.000387\n",
      "Iter 21744, training loss 0.000002, validation loss 0.000384\n",
      "Iter 21745, training loss 0.000012, validation loss 0.000385\n",
      "Iter 21746, training loss 0.000021, validation loss 0.000409\n",
      "Iter 21747, training loss 0.000018, validation loss 0.000392\n",
      "Iter 21748, training loss 0.000008, validation loss 0.000394\n",
      "Iter 21749, training loss 0.000001, validation loss 0.000381\n",
      "Iter 21750, training loss 0.000003, validation loss 0.000381\n",
      "Iter 21751, training loss 0.000009, validation loss 0.000394\n",
      "Iter 21752, training loss 0.000011, validation loss 0.000385\n",
      "Iter 21753, training loss 0.000008, validation loss 0.000391\n",
      "Iter 21754, training loss 0.000002, validation loss 0.000378\n",
      "Iter 21755, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21756, training loss 0.000003, validation loss 0.000386\n",
      "Iter 21757, training loss 0.000006, validation loss 0.000382\n",
      "Iter 21758, training loss 0.000006, validation loss 0.000390\n",
      "Iter 21759, training loss 0.000003, validation loss 0.000379\n",
      "Iter 21760, training loss 0.000001, validation loss 0.000380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21761, training loss 0.000001, validation loss 0.000381\n",
      "Iter 21762, training loss 0.000003, validation loss 0.000379\n",
      "Iter 21763, training loss 0.000004, validation loss 0.000387\n",
      "Iter 21764, training loss 0.000003, validation loss 0.000379\n",
      "Iter 21765, training loss 0.000001, validation loss 0.000381\n",
      "Iter 21766, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21767, training loss 0.000001, validation loss 0.000379\n",
      "Iter 21768, training loss 0.000002, validation loss 0.000384\n",
      "Iter 21769, training loss 0.000002, validation loss 0.000379\n",
      "Iter 21770, training loss 0.000001, validation loss 0.000381\n",
      "Iter 21771, training loss 0.000000, validation loss 0.000378\n",
      "Iter 21772, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21773, training loss 0.000001, validation loss 0.000382\n",
      "Iter 21774, training loss 0.000001, validation loss 0.000379\n",
      "Iter 21775, training loss 0.000001, validation loss 0.000382\n",
      "Iter 21776, training loss 0.000001, validation loss 0.000379\n",
      "Iter 21777, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21778, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21779, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21780, training loss 0.000001, validation loss 0.000381\n",
      "Iter 21781, training loss 0.000001, validation loss 0.000379\n",
      "Iter 21782, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21783, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21784, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21785, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21786, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21787, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21788, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21789, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21790, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21791, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21792, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21793, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21794, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21795, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21796, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21797, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21798, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21799, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21800, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21801, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21802, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21803, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21804, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21805, training loss 0.000000, validation loss 0.000379\n",
      "Iter 21806, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21807, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21808, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21809, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21810, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21811, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21812, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21813, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21814, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21815, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21816, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21817, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21818, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21819, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21820, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21821, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21822, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21823, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21824, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21825, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21826, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21827, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21828, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21829, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21830, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21831, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21832, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21833, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21834, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21835, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21836, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21837, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21838, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21839, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21840, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21841, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21842, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21843, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21844, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21845, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21846, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21847, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21848, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21849, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21850, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21851, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21852, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21853, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21854, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21855, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21856, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21857, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21858, training loss 0.000000, validation loss 0.000380\n",
      "Iter 21859, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21860, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21861, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21862, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21863, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21864, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21865, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21866, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21867, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21868, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21869, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21870, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21871, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21872, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21873, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21874, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21875, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21876, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21877, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21878, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21879, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21880, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21881, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21882, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21883, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21884, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21885, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21886, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21887, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21888, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21889, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21890, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21891, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21892, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21893, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21894, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21895, training loss 0.000000, validation loss 0.000381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 21896, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21897, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21898, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21899, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21900, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21901, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21902, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21903, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21904, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21905, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21906, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21907, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21908, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21909, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21910, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21911, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21912, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21913, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21914, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21915, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21916, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21917, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21918, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21919, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21920, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21921, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21922, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21923, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21924, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21925, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21926, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21927, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21928, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21929, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21930, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21931, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21932, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21933, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21934, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21935, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21936, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21937, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21938, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21939, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21940, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21941, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21942, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21943, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21944, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21945, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21946, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21947, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21948, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21949, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21950, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21951, training loss 0.000000, validation loss 0.000382\n",
      "Iter 21952, training loss 0.000000, validation loss 0.000381\n",
      "Iter 21953, training loss 0.000001, validation loss 0.000383\n",
      "Iter 21954, training loss 0.000001, validation loss 0.000380\n",
      "Iter 21955, training loss 0.000002, validation loss 0.000385\n",
      "Iter 21956, training loss 0.000002, validation loss 0.000381\n",
      "Iter 21957, training loss 0.000004, validation loss 0.000389\n",
      "Iter 21958, training loss 0.000007, validation loss 0.000384\n",
      "Iter 21959, training loss 0.000011, validation loss 0.000399\n",
      "Iter 21960, training loss 0.000018, validation loss 0.000393\n",
      "Iter 21961, training loss 0.000028, validation loss 0.000421\n",
      "Iter 21962, training loss 0.000045, validation loss 0.000418\n",
      "Iter 21963, training loss 0.000067, validation loss 0.000468\n",
      "Iter 21964, training loss 0.000094, validation loss 0.000467\n",
      "Iter 21965, training loss 0.000119, validation loss 0.000529\n",
      "Iter 21966, training loss 0.000126, validation loss 0.000497\n",
      "Iter 21967, training loss 0.000103, validation loss 0.000506\n",
      "Iter 21968, training loss 0.000057, validation loss 0.000423\n",
      "Iter 21969, training loss 0.000015, validation loss 0.000397\n",
      "Iter 21970, training loss 0.000002, validation loss 0.000378\n",
      "Iter 21971, training loss 0.000017, validation loss 0.000390\n",
      "Iter 21972, training loss 0.000039, validation loss 0.000434\n",
      "Iter 21973, training loss 0.000046, validation loss 0.000415\n",
      "Iter 21974, training loss 0.000031, validation loss 0.000414\n",
      "Iter 21975, training loss 0.000009, validation loss 0.000379\n",
      "Iter 21976, training loss 0.000002, validation loss 0.000378\n",
      "Iter 21977, training loss 0.000011, validation loss 0.000396\n",
      "Iter 21978, training loss 0.000023, validation loss 0.000391\n",
      "Iter 21979, training loss 0.000023, validation loss 0.000402\n",
      "Iter 21980, training loss 0.000011, validation loss 0.000379\n",
      "Iter 21981, training loss 0.000002, validation loss 0.000381\n",
      "Iter 21982, training loss 0.000004, validation loss 0.000385\n",
      "Iter 21983, training loss 0.000011, validation loss 0.000380\n",
      "Iter 21984, training loss 0.000013, validation loss 0.000391\n",
      "Iter 21985, training loss 0.000009, validation loss 0.000375\n",
      "Iter 21986, training loss 0.000003, validation loss 0.000380\n",
      "Iter 21987, training loss 0.000002, validation loss 0.000380\n",
      "Iter 21988, training loss 0.000005, validation loss 0.000377\n",
      "Iter 21989, training loss 0.000007, validation loss 0.000385\n",
      "Iter 21990, training loss 0.000006, validation loss 0.000374\n",
      "Iter 21991, training loss 0.000003, validation loss 0.000378\n",
      "Iter 21992, training loss 0.000001, validation loss 0.000377\n",
      "Iter 21993, training loss 0.000002, validation loss 0.000376\n",
      "Iter 21994, training loss 0.000003, validation loss 0.000382\n",
      "Iter 21995, training loss 0.000004, validation loss 0.000375\n",
      "Iter 21996, training loss 0.000002, validation loss 0.000379\n",
      "Iter 21997, training loss 0.000001, validation loss 0.000375\n",
      "Iter 21998, training loss 0.000001, validation loss 0.000375\n",
      "Iter 21999, training loss 0.000001, validation loss 0.000378\n",
      "Iter 22000, training loss 0.000002, validation loss 0.000375\n",
      "Iter 22001, training loss 0.000002, validation loss 0.000380\n",
      "Iter 22002, training loss 0.000001, validation loss 0.000375\n",
      "Iter 22003, training loss 0.000001, validation loss 0.000375\n",
      "Iter 22004, training loss 0.000001, validation loss 0.000376\n",
      "Iter 22005, training loss 0.000001, validation loss 0.000374\n",
      "Iter 22006, training loss 0.000002, validation loss 0.000379\n",
      "Iter 22007, training loss 0.000001, validation loss 0.000376\n",
      "Iter 22008, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22009, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22010, training loss 0.000001, validation loss 0.000375\n",
      "Iter 22011, training loss 0.000001, validation loss 0.000377\n",
      "Iter 22012, training loss 0.000001, validation loss 0.000375\n",
      "Iter 22013, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22014, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22015, training loss 0.000000, validation loss 0.000375\n",
      "Iter 22016, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22017, training loss 0.000001, validation loss 0.000374\n",
      "Iter 22018, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22019, training loss 0.000000, validation loss 0.000375\n",
      "Iter 22020, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22021, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22022, training loss 0.000000, validation loss 0.000375\n",
      "Iter 22023, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22024, training loss 0.000000, validation loss 0.000375\n",
      "Iter 22025, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22026, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22027, training loss 0.000000, validation loss 0.000375\n",
      "Iter 22028, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22029, training loss 0.000000, validation loss 0.000375\n",
      "Iter 22030, training loss 0.000000, validation loss 0.000376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22031, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22032, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22033, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22034, training loss 0.000000, validation loss 0.000375\n",
      "Iter 22035, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22036, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22037, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22038, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22039, training loss 0.000000, validation loss 0.000375\n",
      "Iter 22040, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22041, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22042, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22043, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22044, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22045, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22046, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22047, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22048, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22049, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22050, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22051, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22052, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22053, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22054, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22055, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22056, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22057, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22058, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22059, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22060, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22061, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22062, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22063, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22064, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22065, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22066, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22067, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22068, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22069, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22070, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22071, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22072, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22073, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22074, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22075, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22076, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22077, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22078, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22079, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22080, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22081, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22082, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22083, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22084, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22085, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22086, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22087, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22088, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22089, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22090, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22091, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22092, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22093, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22094, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22095, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22096, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22097, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22098, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22099, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22100, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22101, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22102, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22103, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22104, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22105, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22106, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22107, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22108, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22109, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22110, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22111, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22112, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22113, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22114, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22115, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22116, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22117, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22118, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22119, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22120, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22121, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22122, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22123, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22124, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22125, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22126, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22127, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22128, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22129, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22130, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22131, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22132, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22133, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22134, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22135, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22136, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22137, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22138, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22139, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22140, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22141, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22142, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22143, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22144, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22145, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22146, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22147, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22148, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22149, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22150, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22151, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22152, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22153, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22154, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22155, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22156, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22157, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22158, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22159, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22160, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22161, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22162, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22163, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22164, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22165, training loss 0.000000, validation loss 0.000377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22166, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22167, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22168, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22169, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22170, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22171, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22172, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22173, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22174, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22175, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22176, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22177, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22178, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22179, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22180, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22181, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22182, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22183, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22184, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22185, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22186, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22187, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22188, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22189, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22190, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22191, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22192, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22193, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22194, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22195, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22196, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22197, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22198, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22199, training loss 0.000000, validation loss 0.000377\n",
      "Iter 22200, training loss 0.000000, validation loss 0.000378\n",
      "Iter 22201, training loss 0.000000, validation loss 0.000376\n",
      "Iter 22202, training loss 0.000000, validation loss 0.000378\n",
      "Iter 22203, training loss 0.000001, validation loss 0.000376\n",
      "Iter 22204, training loss 0.000001, validation loss 0.000379\n",
      "Iter 22205, training loss 0.000001, validation loss 0.000376\n",
      "Iter 22206, training loss 0.000002, validation loss 0.000382\n",
      "Iter 22207, training loss 0.000003, validation loss 0.000377\n",
      "Iter 22208, training loss 0.000005, validation loss 0.000387\n",
      "Iter 22209, training loss 0.000009, validation loss 0.000381\n",
      "Iter 22210, training loss 0.000014, validation loss 0.000399\n",
      "Iter 22211, training loss 0.000023, validation loss 0.000393\n",
      "Iter 22212, training loss 0.000036, validation loss 0.000427\n",
      "Iter 22213, training loss 0.000055, validation loss 0.000423\n",
      "Iter 22214, training loss 0.000079, validation loss 0.000478\n",
      "Iter 22215, training loss 0.000103, validation loss 0.000470\n",
      "Iter 22216, training loss 0.000116, validation loss 0.000520\n",
      "Iter 22217, training loss 0.000107, validation loss 0.000471\n",
      "Iter 22218, training loss 0.000076, validation loss 0.000466\n",
      "Iter 22219, training loss 0.000036, validation loss 0.000399\n",
      "Iter 22220, training loss 0.000007, validation loss 0.000386\n",
      "Iter 22221, training loss 0.000005, validation loss 0.000382\n",
      "Iter 22222, training loss 0.000022, validation loss 0.000382\n",
      "Iter 22223, training loss 0.000036, validation loss 0.000413\n",
      "Iter 22224, training loss 0.000035, validation loss 0.000394\n",
      "Iter 22225, training loss 0.000020, validation loss 0.000402\n",
      "Iter 22226, training loss 0.000006, validation loss 0.000378\n",
      "Iter 22227, training loss 0.000004, validation loss 0.000372\n",
      "Iter 22228, training loss 0.000007, validation loss 0.000376\n",
      "Iter 22229, training loss 0.000011, validation loss 0.000373\n",
      "Iter 22230, training loss 0.000012, validation loss 0.000389\n",
      "Iter 22231, training loss 0.000007, validation loss 0.000376\n",
      "Iter 22232, training loss 0.000005, validation loss 0.000378\n",
      "Iter 22233, training loss 0.000002, validation loss 0.000369\n",
      "Iter 22234, training loss 0.000002, validation loss 0.000370\n",
      "Iter 22235, training loss 0.000003, validation loss 0.000375\n",
      "Iter 22236, training loss 0.000004, validation loss 0.000372\n",
      "Iter 22237, training loss 0.000004, validation loss 0.000377\n",
      "Iter 22238, training loss 0.000003, validation loss 0.000369\n",
      "Iter 22239, training loss 0.000002, validation loss 0.000372\n",
      "Iter 22240, training loss 0.000001, validation loss 0.000371\n",
      "Iter 22241, training loss 0.000001, validation loss 0.000371\n",
      "Iter 22242, training loss 0.000003, validation loss 0.000375\n",
      "Iter 22243, training loss 0.000002, validation loss 0.000370\n",
      "Iter 22244, training loss 0.000002, validation loss 0.000373\n",
      "Iter 22245, training loss 0.000001, validation loss 0.000370\n",
      "Iter 22246, training loss 0.000001, validation loss 0.000371\n",
      "Iter 22247, training loss 0.000001, validation loss 0.000373\n",
      "Iter 22248, training loss 0.000001, validation loss 0.000371\n",
      "Iter 22249, training loss 0.000001, validation loss 0.000374\n",
      "Iter 22250, training loss 0.000001, validation loss 0.000370\n",
      "Iter 22251, training loss 0.000001, validation loss 0.000372\n",
      "Iter 22252, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22253, training loss 0.000001, validation loss 0.000371\n",
      "Iter 22254, training loss 0.000001, validation loss 0.000373\n",
      "Iter 22255, training loss 0.000001, validation loss 0.000370\n",
      "Iter 22256, training loss 0.000001, validation loss 0.000372\n",
      "Iter 22257, training loss 0.000000, validation loss 0.000370\n",
      "Iter 22258, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22259, training loss 0.000000, validation loss 0.000373\n",
      "Iter 22260, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22261, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22262, training loss 0.000000, validation loss 0.000370\n",
      "Iter 22263, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22264, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22265, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22266, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22267, training loss 0.000000, validation loss 0.000370\n",
      "Iter 22268, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22269, training loss 0.000000, validation loss 0.000370\n",
      "Iter 22270, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22271, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22272, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22273, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22274, training loss 0.000000, validation loss 0.000370\n",
      "Iter 22275, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22276, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22277, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22278, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22279, training loss 0.000000, validation loss 0.000370\n",
      "Iter 22280, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22281, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22282, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22283, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22284, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22285, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22286, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22287, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22288, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22289, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22290, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22291, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22292, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22293, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22294, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22295, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22296, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22297, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22298, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22299, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22300, training loss 0.000000, validation loss 0.000371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22301, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22302, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22303, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22304, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22305, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22306, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22307, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22308, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22309, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22310, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22311, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22312, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22313, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22314, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22315, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22316, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22317, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22318, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22319, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22320, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22321, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22322, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22323, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22324, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22325, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22326, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22327, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22328, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22329, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22330, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22331, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22332, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22333, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22334, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22335, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22336, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22337, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22338, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22339, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22340, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22341, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22342, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22343, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22344, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22345, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22346, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22347, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22348, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22349, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22350, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22351, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22352, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22353, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22354, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22355, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22356, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22357, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22358, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22359, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22360, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22361, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22362, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22363, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22364, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22365, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22366, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22367, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22368, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22369, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22370, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22371, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22372, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22373, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22374, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22375, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22376, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22377, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22378, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22379, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22380, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22381, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22382, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22383, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22384, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22385, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22386, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22387, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22388, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22389, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22390, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22391, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22392, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22393, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22394, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22395, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22396, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22397, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22398, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22399, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22400, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22401, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22402, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22403, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22404, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22405, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22406, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22407, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22408, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22409, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22410, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22411, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22412, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22413, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22414, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22415, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22416, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22417, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22418, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22419, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22420, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22421, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22422, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22423, training loss 0.000000, validation loss 0.000373\n",
      "Iter 22424, training loss 0.000000, validation loss 0.000372\n",
      "Iter 22425, training loss 0.000000, validation loss 0.000373\n",
      "Iter 22426, training loss 0.000000, validation loss 0.000371\n",
      "Iter 22427, training loss 0.000001, validation loss 0.000374\n",
      "Iter 22428, training loss 0.000001, validation loss 0.000371\n",
      "Iter 22429, training loss 0.000002, validation loss 0.000376\n",
      "Iter 22430, training loss 0.000003, validation loss 0.000372\n",
      "Iter 22431, training loss 0.000005, validation loss 0.000381\n",
      "Iter 22432, training loss 0.000008, validation loss 0.000375\n",
      "Iter 22433, training loss 0.000013, validation loss 0.000392\n",
      "Iter 22434, training loss 0.000021, validation loss 0.000386\n",
      "Iter 22435, training loss 0.000033, validation loss 0.000418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22436, training loss 0.000051, validation loss 0.000415\n",
      "Iter 22437, training loss 0.000074, validation loss 0.000469\n",
      "Iter 22438, training loss 0.000100, validation loss 0.000464\n",
      "Iter 22439, training loss 0.000117, validation loss 0.000516\n",
      "Iter 22440, training loss 0.000112, validation loss 0.000473\n",
      "Iter 22441, training loss 0.000079, validation loss 0.000474\n",
      "Iter 22442, training loss 0.000033, validation loss 0.000394\n",
      "Iter 22443, training loss 0.000004, validation loss 0.000374\n",
      "Iter 22444, training loss 0.000006, validation loss 0.000379\n",
      "Iter 22445, training loss 0.000028, validation loss 0.000390\n",
      "Iter 22446, training loss 0.000046, validation loss 0.000429\n",
      "Iter 22447, training loss 0.000040, validation loss 0.000402\n",
      "Iter 22448, training loss 0.000019, validation loss 0.000393\n",
      "Iter 22449, training loss 0.000002, validation loss 0.000369\n",
      "Iter 22450, training loss 0.000005, validation loss 0.000370\n",
      "Iter 22451, training loss 0.000016, validation loss 0.000394\n",
      "Iter 22452, training loss 0.000023, validation loss 0.000383\n",
      "Iter 22453, training loss 0.000018, validation loss 0.000388\n",
      "Iter 22454, training loss 0.000006, validation loss 0.000369\n",
      "Iter 22455, training loss 0.000002, validation loss 0.000372\n",
      "Iter 22456, training loss 0.000005, validation loss 0.000378\n",
      "Iter 22457, training loss 0.000011, validation loss 0.000373\n",
      "Iter 22458, training loss 0.000011, validation loss 0.000380\n",
      "Iter 22459, training loss 0.000006, validation loss 0.000367\n",
      "Iter 22460, training loss 0.000002, validation loss 0.000373\n",
      "Iter 22461, training loss 0.000002, validation loss 0.000375\n",
      "Iter 22462, training loss 0.000005, validation loss 0.000372\n",
      "Iter 22463, training loss 0.000006, validation loss 0.000375\n",
      "Iter 22464, training loss 0.000005, validation loss 0.000365\n",
      "Iter 22465, training loss 0.000002, validation loss 0.000371\n",
      "Iter 22466, training loss 0.000001, validation loss 0.000372\n",
      "Iter 22467, training loss 0.000002, validation loss 0.000370\n",
      "Iter 22468, training loss 0.000003, validation loss 0.000372\n",
      "Iter 22469, training loss 0.000003, validation loss 0.000365\n",
      "Iter 22470, training loss 0.000002, validation loss 0.000369\n",
      "Iter 22471, training loss 0.000001, validation loss 0.000368\n",
      "Iter 22472, training loss 0.000001, validation loss 0.000368\n",
      "Iter 22473, training loss 0.000002, validation loss 0.000370\n",
      "Iter 22474, training loss 0.000002, validation loss 0.000366\n",
      "Iter 22475, training loss 0.000002, validation loss 0.000370\n",
      "Iter 22476, training loss 0.000001, validation loss 0.000366\n",
      "Iter 22477, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22478, training loss 0.000001, validation loss 0.000368\n",
      "Iter 22479, training loss 0.000001, validation loss 0.000367\n",
      "Iter 22480, training loss 0.000001, validation loss 0.000369\n",
      "Iter 22481, training loss 0.000001, validation loss 0.000366\n",
      "Iter 22482, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22483, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22484, training loss 0.000001, validation loss 0.000366\n",
      "Iter 22485, training loss 0.000001, validation loss 0.000369\n",
      "Iter 22486, training loss 0.000001, validation loss 0.000366\n",
      "Iter 22487, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22488, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22489, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22490, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22491, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22492, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22493, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22494, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22495, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22496, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22497, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22498, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22499, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22500, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22501, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22502, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22503, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22504, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22505, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22506, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22507, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22508, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22509, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22510, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22511, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22512, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22513, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22514, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22515, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22516, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22517, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22518, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22519, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22520, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22521, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22522, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22523, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22524, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22525, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22526, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22527, training loss 0.000000, validation loss 0.000366\n",
      "Iter 22528, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22529, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22530, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22531, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22532, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22533, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22534, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22535, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22536, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22537, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22538, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22539, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22540, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22541, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22542, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22543, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22544, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22545, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22546, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22547, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22548, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22549, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22550, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22551, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22552, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22553, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22554, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22555, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22556, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22557, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22558, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22559, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22560, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22561, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22562, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22563, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22564, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22565, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22566, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22567, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22568, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22569, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22570, training loss 0.000000, validation loss 0.000367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22571, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22572, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22573, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22574, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22575, training loss 0.000000, validation loss 0.000367\n",
      "Iter 22576, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22577, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22578, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22579, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22580, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22581, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22582, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22583, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22584, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22585, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22586, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22587, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22588, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22589, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22590, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22591, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22592, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22593, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22594, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22595, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22596, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22597, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22598, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22599, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22600, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22601, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22602, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22603, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22604, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22605, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22606, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22607, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22608, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22609, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22610, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22611, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22612, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22613, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22614, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22615, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22616, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22617, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22618, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22619, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22620, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22621, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22622, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22623, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22624, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22625, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22626, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22627, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22628, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22629, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22630, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22631, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22632, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22633, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22634, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22635, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22636, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22637, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22638, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22639, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22640, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22641, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22642, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22643, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22644, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22645, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22646, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22647, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22648, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22649, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22650, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22651, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22652, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22653, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22654, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22655, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22656, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22657, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22658, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22659, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22660, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22661, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22662, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22663, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22664, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22665, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22666, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22667, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22668, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22669, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22670, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22671, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22672, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22673, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22674, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22675, training loss 0.000000, validation loss 0.000369\n",
      "Iter 22676, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22677, training loss 0.000000, validation loss 0.000369\n",
      "Iter 22678, training loss 0.000000, validation loss 0.000368\n",
      "Iter 22679, training loss 0.000000, validation loss 0.000370\n",
      "Iter 22680, training loss 0.000001, validation loss 0.000367\n",
      "Iter 22681, training loss 0.000001, validation loss 0.000371\n",
      "Iter 22682, training loss 0.000002, validation loss 0.000368\n",
      "Iter 22683, training loss 0.000003, validation loss 0.000374\n",
      "Iter 22684, training loss 0.000004, validation loss 0.000369\n",
      "Iter 22685, training loss 0.000007, validation loss 0.000381\n",
      "Iter 22686, training loss 0.000012, validation loss 0.000375\n",
      "Iter 22687, training loss 0.000019, validation loss 0.000397\n",
      "Iter 22688, training loss 0.000031, validation loss 0.000393\n",
      "Iter 22689, training loss 0.000049, validation loss 0.000434\n",
      "Iter 22690, training loss 0.000072, validation loss 0.000433\n",
      "Iter 22691, training loss 0.000098, validation loss 0.000493\n",
      "Iter 22692, training loss 0.000117, validation loss 0.000477\n",
      "Iter 22693, training loss 0.000115, validation loss 0.000509\n",
      "Iter 22694, training loss 0.000085, validation loss 0.000440\n",
      "Iter 22695, training loss 0.000044, validation loss 0.000422\n",
      "Iter 22696, training loss 0.000014, validation loss 0.000369\n",
      "Iter 22697, training loss 0.000004, validation loss 0.000365\n",
      "Iter 22698, training loss 0.000015, validation loss 0.000390\n",
      "Iter 22699, training loss 0.000035, validation loss 0.000389\n",
      "Iter 22700, training loss 0.000042, validation loss 0.000414\n",
      "Iter 22701, training loss 0.000030, validation loss 0.000383\n",
      "Iter 22702, training loss 0.000011, validation loss 0.000378\n",
      "Iter 22703, training loss 0.000003, validation loss 0.000365\n",
      "Iter 22704, training loss 0.000008, validation loss 0.000364\n",
      "Iter 22705, training loss 0.000014, validation loss 0.000378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22706, training loss 0.000015, validation loss 0.000370\n",
      "Iter 22707, training loss 0.000010, validation loss 0.000378\n",
      "Iter 22708, training loss 0.000005, validation loss 0.000363\n",
      "Iter 22709, training loss 0.000004, validation loss 0.000361\n",
      "Iter 22710, training loss 0.000003, validation loss 0.000361\n",
      "Iter 22711, training loss 0.000005, validation loss 0.000364\n",
      "Iter 22712, training loss 0.000006, validation loss 0.000373\n",
      "Iter 22713, training loss 0.000004, validation loss 0.000364\n",
      "Iter 22714, training loss 0.000004, validation loss 0.000364\n",
      "Iter 22715, training loss 0.000002, validation loss 0.000357\n",
      "Iter 22716, training loss 0.000001, validation loss 0.000358\n",
      "Iter 22717, training loss 0.000003, validation loss 0.000365\n",
      "Iter 22718, training loss 0.000003, validation loss 0.000364\n",
      "Iter 22719, training loss 0.000003, validation loss 0.000368\n",
      "Iter 22720, training loss 0.000002, validation loss 0.000361\n",
      "Iter 22721, training loss 0.000001, validation loss 0.000359\n",
      "Iter 22722, training loss 0.000001, validation loss 0.000357\n",
      "Iter 22723, training loss 0.000001, validation loss 0.000358\n",
      "Iter 22724, training loss 0.000001, validation loss 0.000364\n",
      "Iter 22725, training loss 0.000001, validation loss 0.000363\n",
      "Iter 22726, training loss 0.000001, validation loss 0.000364\n",
      "Iter 22727, training loss 0.000001, validation loss 0.000359\n",
      "Iter 22728, training loss 0.000001, validation loss 0.000357\n",
      "Iter 22729, training loss 0.000001, validation loss 0.000358\n",
      "Iter 22730, training loss 0.000001, validation loss 0.000359\n",
      "Iter 22731, training loss 0.000001, validation loss 0.000363\n",
      "Iter 22732, training loss 0.000001, validation loss 0.000362\n",
      "Iter 22733, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22734, training loss 0.000000, validation loss 0.000359\n",
      "Iter 22735, training loss 0.000001, validation loss 0.000358\n",
      "Iter 22736, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22737, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22738, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22739, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22740, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22741, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22742, training loss 0.000000, validation loss 0.000359\n",
      "Iter 22743, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22744, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22745, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22746, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22747, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22748, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22749, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22750, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22751, training loss 0.000000, validation loss 0.000359\n",
      "Iter 22752, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22753, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22754, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22755, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22756, training loss 0.000000, validation loss 0.000359\n",
      "Iter 22757, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22758, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22759, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22760, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22761, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22762, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22763, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22764, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22765, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22766, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22767, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22768, training loss 0.000000, validation loss 0.000360\n",
      "Iter 22769, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22770, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22771, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22772, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22773, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22774, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22775, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22776, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22777, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22778, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22779, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22780, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22781, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22782, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22783, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22784, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22785, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22786, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22787, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22788, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22789, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22790, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22791, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22792, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22793, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22794, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22795, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22796, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22797, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22798, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22799, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22800, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22801, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22802, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22803, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22804, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22805, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22806, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22807, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22808, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22809, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22810, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22811, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22812, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22813, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22814, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22815, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22816, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22817, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22818, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22819, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22820, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22821, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22822, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22823, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22824, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22825, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22826, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22827, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22828, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22829, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22830, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22831, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22832, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22833, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22834, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22835, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22836, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22837, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22838, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22839, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22840, training loss 0.000000, validation loss 0.000361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22841, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22842, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22843, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22844, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22845, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22846, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22847, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22848, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22849, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22850, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22851, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22852, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22853, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22854, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22855, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22856, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22857, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22858, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22859, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22860, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22861, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22862, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22863, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22864, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22865, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22866, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22867, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22868, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22869, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22870, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22871, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22872, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22873, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22874, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22875, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22876, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22877, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22878, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22879, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22880, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22881, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22882, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22883, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22884, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22885, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22886, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22887, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22888, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22889, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22890, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22891, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22892, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22893, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22894, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22895, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22896, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22897, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22898, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22899, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22900, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22901, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22902, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22903, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22904, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22905, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22906, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22907, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22908, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22909, training loss 0.000000, validation loss 0.000362\n",
      "Iter 22910, training loss 0.000000, validation loss 0.000363\n",
      "Iter 22911, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22912, training loss 0.000000, validation loss 0.000363\n",
      "Iter 22913, training loss 0.000000, validation loss 0.000361\n",
      "Iter 22914, training loss 0.000001, validation loss 0.000364\n",
      "Iter 22915, training loss 0.000001, validation loss 0.000361\n",
      "Iter 22916, training loss 0.000002, validation loss 0.000366\n",
      "Iter 22917, training loss 0.000003, validation loss 0.000362\n",
      "Iter 22918, training loss 0.000005, validation loss 0.000371\n",
      "Iter 22919, training loss 0.000008, validation loss 0.000365\n",
      "Iter 22920, training loss 0.000012, validation loss 0.000382\n",
      "Iter 22921, training loss 0.000020, validation loss 0.000376\n",
      "Iter 22922, training loss 0.000032, validation loss 0.000408\n",
      "Iter 22923, training loss 0.000050, validation loss 0.000404\n",
      "Iter 22924, training loss 0.000072, validation loss 0.000458\n",
      "Iter 22925, training loss 0.000096, validation loss 0.000449\n",
      "Iter 22926, training loss 0.000111, validation loss 0.000501\n",
      "Iter 22927, training loss 0.000104, validation loss 0.000457\n",
      "Iter 22928, training loss 0.000072, validation loss 0.000459\n",
      "Iter 22929, training loss 0.000031, validation loss 0.000383\n",
      "Iter 22930, training loss 0.000005, validation loss 0.000361\n",
      "Iter 22931, training loss 0.000005, validation loss 0.000362\n",
      "Iter 22932, training loss 0.000024, validation loss 0.000370\n",
      "Iter 22933, training loss 0.000041, validation loss 0.000415\n",
      "Iter 22934, training loss 0.000040, validation loss 0.000392\n",
      "Iter 22935, training loss 0.000023, validation loss 0.000386\n",
      "Iter 22936, training loss 0.000005, validation loss 0.000361\n",
      "Iter 22937, training loss 0.000003, validation loss 0.000359\n",
      "Iter 22938, training loss 0.000014, validation loss 0.000378\n",
      "Iter 22939, training loss 0.000023, validation loss 0.000369\n",
      "Iter 22940, training loss 0.000020, validation loss 0.000381\n",
      "Iter 22941, training loss 0.000008, validation loss 0.000361\n",
      "Iter 22942, training loss 0.000001, validation loss 0.000362\n",
      "Iter 22943, training loss 0.000005, validation loss 0.000369\n",
      "Iter 22944, training loss 0.000012, validation loss 0.000363\n",
      "Iter 22945, training loss 0.000013, validation loss 0.000371\n",
      "Iter 22946, training loss 0.000008, validation loss 0.000354\n",
      "Iter 22947, training loss 0.000002, validation loss 0.000358\n",
      "Iter 22948, training loss 0.000002, validation loss 0.000362\n",
      "Iter 22949, training loss 0.000005, validation loss 0.000361\n",
      "Iter 22950, training loss 0.000008, validation loss 0.000369\n",
      "Iter 22951, training loss 0.000006, validation loss 0.000356\n",
      "Iter 22952, training loss 0.000003, validation loss 0.000358\n",
      "Iter 22953, training loss 0.000001, validation loss 0.000355\n",
      "Iter 22954, training loss 0.000002, validation loss 0.000356\n",
      "Iter 22955, training loss 0.000004, validation loss 0.000365\n",
      "Iter 22956, training loss 0.000004, validation loss 0.000358\n",
      "Iter 22957, training loss 0.000002, validation loss 0.000360\n",
      "Iter 22958, training loss 0.000001, validation loss 0.000355\n",
      "Iter 22959, training loss 0.000001, validation loss 0.000356\n",
      "Iter 22960, training loss 0.000002, validation loss 0.000362\n",
      "Iter 22961, training loss 0.000002, validation loss 0.000357\n",
      "Iter 22962, training loss 0.000002, validation loss 0.000359\n",
      "Iter 22963, training loss 0.000001, validation loss 0.000353\n",
      "Iter 22964, training loss 0.000000, validation loss 0.000355\n",
      "Iter 22965, training loss 0.000001, validation loss 0.000359\n",
      "Iter 22966, training loss 0.000001, validation loss 0.000358\n",
      "Iter 22967, training loss 0.000001, validation loss 0.000361\n",
      "Iter 22968, training loss 0.000001, validation loss 0.000355\n",
      "Iter 22969, training loss 0.000000, validation loss 0.000356\n",
      "Iter 22970, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22971, training loss 0.000001, validation loss 0.000356\n",
      "Iter 22972, training loss 0.000001, validation loss 0.000360\n",
      "Iter 22973, training loss 0.000001, validation loss 0.000357\n",
      "Iter 22974, training loss 0.000000, validation loss 0.000358\n",
      "Iter 22975, training loss 0.000000, validation loss 0.000357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22976, training loss 0.000000, validation loss 0.000356\n",
      "Iter 22977, training loss 0.000001, validation loss 0.000358\n",
      "Iter 22978, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22979, training loss 0.000000, validation loss 0.000358\n",
      "Iter 22980, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22981, training loss 0.000000, validation loss 0.000356\n",
      "Iter 22982, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22983, training loss 0.000000, validation loss 0.000356\n",
      "Iter 22984, training loss 0.000000, validation loss 0.000358\n",
      "Iter 22985, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22986, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22987, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22988, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22989, training loss 0.000000, validation loss 0.000358\n",
      "Iter 22990, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22991, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22992, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22993, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22994, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22995, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22996, training loss 0.000000, validation loss 0.000358\n",
      "Iter 22997, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22998, training loss 0.000000, validation loss 0.000357\n",
      "Iter 22999, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23000, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23001, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23002, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23003, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23004, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23005, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23006, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23007, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23008, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23009, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23010, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23011, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23012, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23013, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23014, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23015, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23016, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23017, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23018, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23019, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23020, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23021, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23022, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23023, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23024, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23025, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23026, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23027, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23028, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23029, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23030, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23031, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23032, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23033, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23034, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23035, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23036, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23037, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23038, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23039, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23040, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23041, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23042, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23043, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23044, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23045, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23046, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23047, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23048, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23049, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23050, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23051, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23052, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23053, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23054, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23055, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23056, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23057, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23058, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23059, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23060, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23061, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23062, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23063, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23064, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23065, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23066, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23067, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23068, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23069, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23070, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23071, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23072, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23073, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23074, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23075, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23076, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23077, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23078, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23079, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23080, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23081, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23082, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23083, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23084, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23085, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23086, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23087, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23088, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23089, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23090, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23091, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23092, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23093, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23094, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23095, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23096, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23097, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23098, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23099, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23100, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23101, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23102, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23103, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23104, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23105, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23106, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23107, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23108, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23109, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23110, training loss 0.000000, validation loss 0.000358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23111, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23112, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23113, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23114, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23115, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23116, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23117, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23118, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23119, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23120, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23121, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23122, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23123, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23124, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23125, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23126, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23127, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23128, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23129, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23130, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23131, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23132, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23133, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23134, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23135, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23136, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23137, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23138, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23139, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23140, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23141, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23142, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23143, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23144, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23145, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23146, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23147, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23148, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23149, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23150, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23151, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23152, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23153, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23154, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23155, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23156, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23157, training loss 0.000000, validation loss 0.000359\n",
      "Iter 23158, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23159, training loss 0.000000, validation loss 0.000359\n",
      "Iter 23160, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23161, training loss 0.000000, validation loss 0.000359\n",
      "Iter 23162, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23163, training loss 0.000000, validation loss 0.000359\n",
      "Iter 23164, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23165, training loss 0.000000, validation loss 0.000359\n",
      "Iter 23166, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23167, training loss 0.000000, validation loss 0.000359\n",
      "Iter 23168, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23169, training loss 0.000000, validation loss 0.000359\n",
      "Iter 23170, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23171, training loss 0.000000, validation loss 0.000360\n",
      "Iter 23172, training loss 0.000000, validation loss 0.000358\n",
      "Iter 23173, training loss 0.000001, validation loss 0.000361\n",
      "Iter 23174, training loss 0.000001, validation loss 0.000358\n",
      "Iter 23175, training loss 0.000002, validation loss 0.000363\n",
      "Iter 23176, training loss 0.000003, validation loss 0.000359\n",
      "Iter 23177, training loss 0.000005, validation loss 0.000368\n",
      "Iter 23178, training loss 0.000008, validation loss 0.000362\n",
      "Iter 23179, training loss 0.000013, validation loss 0.000379\n",
      "Iter 23180, training loss 0.000020, validation loss 0.000373\n",
      "Iter 23181, training loss 0.000032, validation loss 0.000405\n",
      "Iter 23182, training loss 0.000049, validation loss 0.000401\n",
      "Iter 23183, training loss 0.000071, validation loss 0.000453\n",
      "Iter 23184, training loss 0.000094, validation loss 0.000447\n",
      "Iter 23185, training loss 0.000108, validation loss 0.000497\n",
      "Iter 23186, training loss 0.000102, validation loss 0.000449\n",
      "Iter 23187, training loss 0.000073, validation loss 0.000449\n",
      "Iter 23188, training loss 0.000037, validation loss 0.000388\n",
      "Iter 23189, training loss 0.000008, validation loss 0.000372\n",
      "Iter 23190, training loss 0.000004, validation loss 0.000360\n",
      "Iter 23191, training loss 0.000015, validation loss 0.000359\n",
      "Iter 23192, training loss 0.000029, validation loss 0.000388\n",
      "Iter 23193, training loss 0.000031, validation loss 0.000374\n",
      "Iter 23194, training loss 0.000022, validation loss 0.000385\n",
      "Iter 23195, training loss 0.000011, validation loss 0.000362\n",
      "Iter 23196, training loss 0.000002, validation loss 0.000355\n",
      "Iter 23197, training loss 0.000004, validation loss 0.000355\n",
      "Iter 23198, training loss 0.000009, validation loss 0.000353\n",
      "Iter 23199, training loss 0.000012, validation loss 0.000369\n",
      "Iter 23200, training loss 0.000011, validation loss 0.000356\n",
      "Iter 23201, training loss 0.000005, validation loss 0.000360\n",
      "Iter 23202, training loss 0.000002, validation loss 0.000354\n",
      "Iter 23203, training loss 0.000002, validation loss 0.000356\n",
      "Iter 23204, training loss 0.000005, validation loss 0.000362\n",
      "Iter 23205, training loss 0.000006, validation loss 0.000354\n",
      "Iter 23206, training loss 0.000005, validation loss 0.000358\n",
      "Iter 23207, training loss 0.000002, validation loss 0.000349\n",
      "Iter 23208, training loss 0.000001, validation loss 0.000354\n",
      "Iter 23209, training loss 0.000002, validation loss 0.000359\n",
      "Iter 23210, training loss 0.000003, validation loss 0.000355\n",
      "Iter 23211, training loss 0.000004, validation loss 0.000360\n",
      "Iter 23212, training loss 0.000003, validation loss 0.000352\n",
      "Iter 23213, training loss 0.000001, validation loss 0.000355\n",
      "Iter 23214, training loss 0.000001, validation loss 0.000354\n",
      "Iter 23215, training loss 0.000001, validation loss 0.000353\n",
      "Iter 23216, training loss 0.000002, validation loss 0.000358\n",
      "Iter 23217, training loss 0.000002, validation loss 0.000354\n",
      "Iter 23218, training loss 0.000001, validation loss 0.000358\n",
      "Iter 23219, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23220, training loss 0.000000, validation loss 0.000353\n",
      "Iter 23221, training loss 0.000001, validation loss 0.000355\n",
      "Iter 23222, training loss 0.000001, validation loss 0.000351\n",
      "Iter 23223, training loss 0.000001, validation loss 0.000356\n",
      "Iter 23224, training loss 0.000001, validation loss 0.000354\n",
      "Iter 23225, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23226, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23227, training loss 0.000001, validation loss 0.000353\n",
      "Iter 23228, training loss 0.000001, validation loss 0.000356\n",
      "Iter 23229, training loss 0.000001, validation loss 0.000353\n",
      "Iter 23230, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23231, training loss 0.000000, validation loss 0.000354\n",
      "Iter 23232, training loss 0.000000, validation loss 0.000354\n",
      "Iter 23233, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23234, training loss 0.000000, validation loss 0.000354\n",
      "Iter 23235, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23236, training loss 0.000000, validation loss 0.000354\n",
      "Iter 23237, training loss 0.000000, validation loss 0.000354\n",
      "Iter 23238, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23239, training loss 0.000000, validation loss 0.000354\n",
      "Iter 23240, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23241, training loss 0.000000, validation loss 0.000354\n",
      "Iter 23242, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23243, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23244, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23245, training loss 0.000000, validation loss 0.000355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23246, training loss 0.000000, validation loss 0.000354\n",
      "Iter 23247, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23248, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23249, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23250, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23251, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23252, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23253, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23254, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23255, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23256, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23257, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23258, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23259, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23260, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23261, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23262, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23263, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23264, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23265, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23266, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23267, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23268, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23269, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23270, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23271, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23272, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23273, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23274, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23275, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23276, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23277, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23278, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23279, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23280, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23281, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23282, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23283, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23284, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23285, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23286, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23287, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23288, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23289, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23290, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23291, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23292, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23293, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23294, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23295, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23296, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23297, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23298, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23299, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23300, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23301, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23302, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23303, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23304, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23305, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23306, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23307, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23308, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23309, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23310, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23311, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23312, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23313, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23314, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23315, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23316, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23317, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23318, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23319, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23320, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23321, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23322, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23323, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23324, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23325, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23326, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23327, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23328, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23329, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23330, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23331, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23332, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23333, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23334, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23335, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23336, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23337, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23338, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23339, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23340, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23341, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23342, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23343, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23344, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23345, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23346, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23347, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23348, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23349, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23350, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23351, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23352, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23353, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23354, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23355, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23356, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23357, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23358, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23359, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23360, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23361, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23362, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23363, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23364, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23365, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23366, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23367, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23368, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23369, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23370, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23371, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23372, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23373, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23374, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23375, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23376, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23377, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23378, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23379, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23380, training loss 0.000000, validation loss 0.000356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23381, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23382, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23383, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23384, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23385, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23386, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23387, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23388, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23389, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23390, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23391, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23392, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23393, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23394, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23395, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23396, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23397, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23398, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23399, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23400, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23401, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23402, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23403, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23404, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23405, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23406, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23407, training loss 0.000000, validation loss 0.000356\n",
      "Iter 23408, training loss 0.000000, validation loss 0.000357\n",
      "Iter 23409, training loss 0.000000, validation loss 0.000355\n",
      "Iter 23410, training loss 0.000001, validation loss 0.000358\n",
      "Iter 23411, training loss 0.000001, validation loss 0.000355\n",
      "Iter 23412, training loss 0.000001, validation loss 0.000360\n",
      "Iter 23413, training loss 0.000002, validation loss 0.000356\n",
      "Iter 23414, training loss 0.000004, validation loss 0.000365\n",
      "Iter 23415, training loss 0.000007, validation loss 0.000358\n",
      "Iter 23416, training loss 0.000011, validation loss 0.000375\n",
      "Iter 23417, training loss 0.000018, validation loss 0.000368\n",
      "Iter 23418, training loss 0.000030, validation loss 0.000400\n",
      "Iter 23419, training loss 0.000047, validation loss 0.000395\n",
      "Iter 23420, training loss 0.000070, validation loss 0.000451\n",
      "Iter 23421, training loss 0.000096, validation loss 0.000444\n",
      "Iter 23422, training loss 0.000118, validation loss 0.000505\n",
      "Iter 23423, training loss 0.000120, validation loss 0.000461\n",
      "Iter 23424, training loss 0.000094, validation loss 0.000480\n",
      "Iter 23425, training loss 0.000061, validation loss 0.000398\n",
      "Iter 23426, training loss 0.000037, validation loss 0.000368\n",
      "Iter 23427, training loss 0.000026, validation loss 0.000345\n",
      "Iter 23428, training loss 0.000012, validation loss 0.000362\n",
      "Iter 23429, training loss 0.000017, validation loss 0.000370\n",
      "Iter 23430, training loss 0.000012, validation loss 0.000359\n",
      "Iter 23431, training loss 0.000009, validation loss 0.000342\n",
      "Iter 23432, training loss 0.000013, validation loss 0.000340\n",
      "Iter 23433, training loss 0.000014, validation loss 0.000354\n",
      "Iter 23434, training loss 0.000012, validation loss 0.000353\n",
      "Iter 23435, training loss 0.000012, validation loss 0.000362\n",
      "Iter 23436, training loss 0.000013, validation loss 0.000347\n",
      "Iter 23437, training loss 0.000010, validation loss 0.000352\n",
      "Iter 23438, training loss 0.000007, validation loss 0.000341\n",
      "Iter 23439, training loss 0.000006, validation loss 0.000347\n",
      "Iter 23440, training loss 0.000005, validation loss 0.000350\n",
      "Iter 23441, training loss 0.000006, validation loss 0.000348\n",
      "Iter 23442, training loss 0.000006, validation loss 0.000350\n",
      "Iter 23443, training loss 0.000005, validation loss 0.000339\n",
      "Iter 23444, training loss 0.000005, validation loss 0.000344\n",
      "Iter 23445, training loss 0.000004, validation loss 0.000339\n",
      "Iter 23446, training loss 0.000003, validation loss 0.000344\n",
      "Iter 23447, training loss 0.000003, validation loss 0.000348\n",
      "Iter 23448, training loss 0.000003, validation loss 0.000349\n",
      "Iter 23449, training loss 0.000003, validation loss 0.000352\n",
      "Iter 23450, training loss 0.000002, validation loss 0.000344\n",
      "Iter 23451, training loss 0.000002, validation loss 0.000347\n",
      "Iter 23452, training loss 0.000002, validation loss 0.000344\n",
      "Iter 23453, training loss 0.000002, validation loss 0.000347\n",
      "Iter 23454, training loss 0.000001, validation loss 0.000350\n",
      "Iter 23455, training loss 0.000002, validation loss 0.000350\n",
      "Iter 23456, training loss 0.000002, validation loss 0.000352\n",
      "Iter 23457, training loss 0.000001, validation loss 0.000347\n",
      "Iter 23458, training loss 0.000001, validation loss 0.000347\n",
      "Iter 23459, training loss 0.000001, validation loss 0.000344\n",
      "Iter 23460, training loss 0.000001, validation loss 0.000344\n",
      "Iter 23461, training loss 0.000001, validation loss 0.000347\n",
      "Iter 23462, training loss 0.000001, validation loss 0.000346\n",
      "Iter 23463, training loss 0.000001, validation loss 0.000349\n",
      "Iter 23464, training loss 0.000001, validation loss 0.000346\n",
      "Iter 23465, training loss 0.000001, validation loss 0.000346\n",
      "Iter 23466, training loss 0.000001, validation loss 0.000345\n",
      "Iter 23467, training loss 0.000001, validation loss 0.000346\n",
      "Iter 23468, training loss 0.000001, validation loss 0.000347\n",
      "Iter 23469, training loss 0.000001, validation loss 0.000346\n",
      "Iter 23470, training loss 0.000001, validation loss 0.000348\n",
      "Iter 23471, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23472, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23473, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23474, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23475, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23476, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23477, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23478, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23479, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23480, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23481, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23482, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23483, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23484, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23485, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23486, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23487, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23488, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23489, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23490, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23491, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23492, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23493, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23494, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23495, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23496, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23497, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23498, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23499, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23500, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23501, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23502, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23503, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23504, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23505, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23506, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23507, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23508, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23509, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23510, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23511, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23512, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23513, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23514, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23515, training loss 0.000000, validation loss 0.000347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23516, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23517, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23518, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23519, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23520, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23521, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23522, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23523, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23524, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23525, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23526, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23527, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23528, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23529, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23530, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23531, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23532, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23533, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23534, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23535, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23536, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23537, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23538, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23539, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23540, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23541, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23542, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23543, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23544, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23545, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23546, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23547, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23548, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23549, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23550, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23551, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23552, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23553, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23554, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23555, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23556, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23557, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23558, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23559, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23560, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23561, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23562, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23563, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23564, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23565, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23566, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23567, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23568, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23569, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23570, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23571, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23572, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23573, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23574, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23575, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23576, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23577, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23578, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23579, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23580, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23581, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23582, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23583, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23584, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23585, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23586, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23587, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23588, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23589, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23590, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23591, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23592, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23593, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23594, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23595, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23596, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23597, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23598, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23599, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23600, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23601, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23602, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23603, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23604, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23605, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23606, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23607, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23608, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23609, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23610, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23611, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23612, training loss 0.000000, validation loss 0.000350\n",
      "Iter 23613, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23614, training loss 0.000000, validation loss 0.000350\n",
      "Iter 23615, training loss 0.000001, validation loss 0.000348\n",
      "Iter 23616, training loss 0.000001, validation loss 0.000352\n",
      "Iter 23617, training loss 0.000001, validation loss 0.000348\n",
      "Iter 23618, training loss 0.000002, validation loss 0.000354\n",
      "Iter 23619, training loss 0.000003, validation loss 0.000348\n",
      "Iter 23620, training loss 0.000005, validation loss 0.000360\n",
      "Iter 23621, training loss 0.000008, validation loss 0.000352\n",
      "Iter 23622, training loss 0.000013, validation loss 0.000371\n",
      "Iter 23623, training loss 0.000020, validation loss 0.000362\n",
      "Iter 23624, training loss 0.000031, validation loss 0.000396\n",
      "Iter 23625, training loss 0.000045, validation loss 0.000384\n",
      "Iter 23626, training loss 0.000061, validation loss 0.000435\n",
      "Iter 23627, training loss 0.000076, validation loss 0.000415\n",
      "Iter 23628, training loss 0.000081, validation loss 0.000458\n",
      "Iter 23629, training loss 0.000071, validation loss 0.000406\n",
      "Iter 23630, training loss 0.000046, validation loss 0.000413\n",
      "Iter 23631, training loss 0.000018, validation loss 0.000359\n",
      "Iter 23632, training loss 0.000002, validation loss 0.000354\n",
      "Iter 23633, training loss 0.000004, validation loss 0.000358\n",
      "Iter 23634, training loss 0.000017, validation loss 0.000355\n",
      "Iter 23635, training loss 0.000029, validation loss 0.000388\n",
      "Iter 23636, training loss 0.000028, validation loss 0.000362\n",
      "Iter 23637, training loss 0.000016, validation loss 0.000372\n",
      "Iter 23638, training loss 0.000004, validation loss 0.000349\n",
      "Iter 23639, training loss 0.000001, validation loss 0.000348\n",
      "Iter 23640, training loss 0.000006, validation loss 0.000358\n",
      "Iter 23641, training loss 0.000012, validation loss 0.000350\n",
      "Iter 23642, training loss 0.000014, validation loss 0.000366\n",
      "Iter 23643, training loss 0.000008, validation loss 0.000348\n",
      "Iter 23644, training loss 0.000003, validation loss 0.000353\n",
      "Iter 23645, training loss 0.000001, validation loss 0.000347\n",
      "Iter 23646, training loss 0.000003, validation loss 0.000344\n",
      "Iter 23647, training loss 0.000006, validation loss 0.000354\n",
      "Iter 23648, training loss 0.000006, validation loss 0.000346\n",
      "Iter 23649, training loss 0.000004, validation loss 0.000355\n",
      "Iter 23650, training loss 0.000001, validation loss 0.000346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23651, training loss 0.000001, validation loss 0.000345\n",
      "Iter 23652, training loss 0.000002, validation loss 0.000346\n",
      "Iter 23653, training loss 0.000003, validation loss 0.000342\n",
      "Iter 23654, training loss 0.000003, validation loss 0.000352\n",
      "Iter 23655, training loss 0.000002, validation loss 0.000346\n",
      "Iter 23656, training loss 0.000001, validation loss 0.000347\n",
      "Iter 23657, training loss 0.000000, validation loss 0.000344\n",
      "Iter 23658, training loss 0.000001, validation loss 0.000342\n",
      "Iter 23659, training loss 0.000002, validation loss 0.000348\n",
      "Iter 23660, training loss 0.000002, validation loss 0.000344\n",
      "Iter 23661, training loss 0.000001, validation loss 0.000348\n",
      "Iter 23662, training loss 0.000001, validation loss 0.000344\n",
      "Iter 23663, training loss 0.000000, validation loss 0.000345\n",
      "Iter 23664, training loss 0.000001, validation loss 0.000347\n",
      "Iter 23665, training loss 0.000001, validation loss 0.000344\n",
      "Iter 23666, training loss 0.000001, validation loss 0.000348\n",
      "Iter 23667, training loss 0.000001, validation loss 0.000344\n",
      "Iter 23668, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23669, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23670, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23671, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23672, training loss 0.000000, validation loss 0.000345\n",
      "Iter 23673, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23674, training loss 0.000000, validation loss 0.000345\n",
      "Iter 23675, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23676, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23677, training loss 0.000000, validation loss 0.000345\n",
      "Iter 23678, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23679, training loss 0.000000, validation loss 0.000345\n",
      "Iter 23680, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23681, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23682, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23683, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23684, training loss 0.000000, validation loss 0.000345\n",
      "Iter 23685, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23686, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23687, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23688, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23689, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23690, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23691, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23692, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23693, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23694, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23695, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23696, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23697, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23698, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23699, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23700, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23701, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23702, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23703, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23704, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23705, training loss 0.000000, validation loss 0.000346\n",
      "Iter 23706, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23707, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23708, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23709, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23710, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23711, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23712, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23713, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23714, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23715, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23716, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23717, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23718, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23719, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23720, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23721, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23722, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23723, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23724, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23725, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23726, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23727, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23728, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23729, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23730, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23731, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23732, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23733, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23734, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23735, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23736, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23737, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23738, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23739, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23740, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23741, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23742, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23743, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23744, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23745, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23746, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23747, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23748, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23749, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23750, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23751, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23752, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23753, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23754, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23755, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23756, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23757, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23758, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23759, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23760, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23761, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23762, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23763, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23764, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23765, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23766, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23767, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23768, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23769, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23770, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23771, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23772, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23773, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23774, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23775, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23776, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23777, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23778, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23779, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23780, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23781, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23782, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23783, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23784, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23785, training loss 0.000000, validation loss 0.000347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23786, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23787, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23788, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23789, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23790, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23791, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23792, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23793, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23794, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23795, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23796, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23797, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23798, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23799, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23800, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23801, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23802, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23803, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23804, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23805, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23806, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23807, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23808, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23809, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23810, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23811, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23812, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23813, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23814, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23815, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23816, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23817, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23818, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23819, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23820, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23821, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23822, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23823, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23824, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23825, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23826, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23827, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23828, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23829, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23830, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23831, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23832, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23833, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23834, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23835, training loss 0.000000, validation loss 0.000348\n",
      "Iter 23836, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23837, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23838, training loss 0.000000, validation loss 0.000347\n",
      "Iter 23839, training loss 0.000000, validation loss 0.000349\n",
      "Iter 23840, training loss 0.000001, validation loss 0.000347\n",
      "Iter 23841, training loss 0.000001, validation loss 0.000351\n",
      "Iter 23842, training loss 0.000002, validation loss 0.000347\n",
      "Iter 23843, training loss 0.000003, validation loss 0.000354\n",
      "Iter 23844, training loss 0.000004, validation loss 0.000348\n",
      "Iter 23845, training loss 0.000007, validation loss 0.000361\n",
      "Iter 23846, training loss 0.000011, validation loss 0.000353\n",
      "Iter 23847, training loss 0.000018, validation loss 0.000377\n",
      "Iter 23848, training loss 0.000028, validation loss 0.000368\n",
      "Iter 23849, training loss 0.000043, validation loss 0.000410\n",
      "Iter 23850, training loss 0.000063, validation loss 0.000400\n",
      "Iter 23851, training loss 0.000084, validation loss 0.000459\n",
      "Iter 23852, training loss 0.000098, validation loss 0.000437\n",
      "Iter 23853, training loss 0.000095, validation loss 0.000478\n",
      "Iter 23854, training loss 0.000073, validation loss 0.000410\n",
      "Iter 23855, training loss 0.000045, validation loss 0.000388\n",
      "Iter 23856, training loss 0.000022, validation loss 0.000347\n",
      "Iter 23857, training loss 0.000011, validation loss 0.000365\n",
      "Iter 23858, training loss 0.000007, validation loss 0.000357\n",
      "Iter 23859, training loss 0.000007, validation loss 0.000352\n",
      "Iter 23860, training loss 0.000009, validation loss 0.000343\n",
      "Iter 23861, training loss 0.000010, validation loss 0.000333\n",
      "Iter 23862, training loss 0.000008, validation loss 0.000354\n",
      "Iter 23863, training loss 0.000011, validation loss 0.000359\n",
      "Iter 23864, training loss 0.000008, validation loss 0.000362\n",
      "Iter 23865, training loss 0.000006, validation loss 0.000343\n",
      "Iter 23866, training loss 0.000006, validation loss 0.000341\n",
      "Iter 23867, training loss 0.000003, validation loss 0.000339\n",
      "Iter 23868, training loss 0.000003, validation loss 0.000343\n",
      "Iter 23869, training loss 0.000005, validation loss 0.000352\n",
      "Iter 23870, training loss 0.000005, validation loss 0.000345\n",
      "Iter 23871, training loss 0.000004, validation loss 0.000345\n",
      "Iter 23872, training loss 0.000003, validation loss 0.000335\n",
      "Iter 23873, training loss 0.000002, validation loss 0.000337\n",
      "Iter 23874, training loss 0.000002, validation loss 0.000337\n",
      "Iter 23875, training loss 0.000002, validation loss 0.000337\n",
      "Iter 23876, training loss 0.000002, validation loss 0.000341\n",
      "Iter 23877, training loss 0.000002, validation loss 0.000336\n",
      "Iter 23878, training loss 0.000002, validation loss 0.000341\n",
      "Iter 23879, training loss 0.000001, validation loss 0.000338\n",
      "Iter 23880, training loss 0.000001, validation loss 0.000340\n",
      "Iter 23881, training loss 0.000001, validation loss 0.000339\n",
      "Iter 23882, training loss 0.000001, validation loss 0.000335\n",
      "Iter 23883, training loss 0.000001, validation loss 0.000337\n",
      "Iter 23884, training loss 0.000001, validation loss 0.000334\n",
      "Iter 23885, training loss 0.000001, validation loss 0.000339\n",
      "Iter 23886, training loss 0.000001, validation loss 0.000339\n",
      "Iter 23887, training loss 0.000001, validation loss 0.000340\n",
      "Iter 23888, training loss 0.000001, validation loss 0.000340\n",
      "Iter 23889, training loss 0.000001, validation loss 0.000337\n",
      "Iter 23890, training loss 0.000001, validation loss 0.000339\n",
      "Iter 23891, training loss 0.000001, validation loss 0.000338\n",
      "Iter 23892, training loss 0.000001, validation loss 0.000341\n",
      "Iter 23893, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23894, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23895, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23896, training loss 0.000000, validation loss 0.000338\n",
      "Iter 23897, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23898, training loss 0.000000, validation loss 0.000338\n",
      "Iter 23899, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23900, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23901, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23902, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23903, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23904, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23905, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23906, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23907, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23908, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23909, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23910, training loss 0.000000, validation loss 0.000338\n",
      "Iter 23911, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23912, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23913, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23914, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23915, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23916, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23917, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23918, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23919, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23920, training loss 0.000000, validation loss 0.000340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 23921, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23922, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23923, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23924, training loss 0.000000, validation loss 0.000339\n",
      "Iter 23925, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23926, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23927, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23928, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23929, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23930, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23931, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23932, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23933, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23934, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23935, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23936, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23937, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23938, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23939, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23940, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23941, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23942, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23943, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23944, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23945, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23946, training loss 0.000000, validation loss 0.000340\n",
      "Iter 23947, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23948, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23949, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23950, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23951, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23952, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23953, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23954, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23955, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23956, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23957, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23958, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23959, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23960, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23961, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23962, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23963, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23964, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23965, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23966, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23967, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23968, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23969, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23970, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23971, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23972, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23973, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23974, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23975, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23976, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23977, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23978, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23979, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23980, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23981, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23982, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23983, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23984, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23985, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23986, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23987, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23988, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23989, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23990, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23991, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23992, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23993, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23994, training loss 0.000000, validation loss 0.000341\n",
      "Iter 23995, training loss 0.000000, validation loss 0.000342\n",
      "Iter 23996, training loss 0.000000, validation loss 0.000342\n",
      "Iter 23997, training loss 0.000000, validation loss 0.000342\n",
      "Iter 23998, training loss 0.000000, validation loss 0.000342\n",
      "Iter 23999, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24000, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24001, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24002, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24003, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24004, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24005, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24006, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24007, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24008, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24009, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24010, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24011, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24012, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24013, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24014, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24015, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24016, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24017, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24018, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24019, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24020, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24021, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24022, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24023, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24024, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24025, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24026, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24027, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24028, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24029, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24030, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24031, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24032, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24033, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24034, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24035, training loss 0.000000, validation loss 0.000343\n",
      "Iter 24036, training loss 0.000000, validation loss 0.000342\n",
      "Iter 24037, training loss 0.000000, validation loss 0.000343\n",
      "Iter 24038, training loss 0.000000, validation loss 0.000341\n",
      "Iter 24039, training loss 0.000000, validation loss 0.000344\n",
      "Iter 24040, training loss 0.000000, validation loss 0.000341\n",
      "Iter 24041, training loss 0.000001, validation loss 0.000345\n",
      "Iter 24042, training loss 0.000001, validation loss 0.000341\n",
      "Iter 24043, training loss 0.000001, validation loss 0.000347\n",
      "Iter 24044, training loss 0.000002, validation loss 0.000341\n",
      "Iter 24045, training loss 0.000004, validation loss 0.000351\n",
      "Iter 24046, training loss 0.000006, validation loss 0.000343\n",
      "Iter 24047, training loss 0.000010, validation loss 0.000362\n",
      "Iter 24048, training loss 0.000016, validation loss 0.000351\n",
      "Iter 24049, training loss 0.000026, validation loss 0.000384\n",
      "Iter 24050, training loss 0.000040, validation loss 0.000372\n",
      "Iter 24051, training loss 0.000058, validation loss 0.000426\n",
      "Iter 24052, training loss 0.000077, validation loss 0.000409\n",
      "Iter 24053, training loss 0.000091, validation loss 0.000463\n",
      "Iter 24054, training loss 0.000092, validation loss 0.000418\n",
      "Iter 24055, training loss 0.000078, validation loss 0.000451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24056, training loss 0.000070, validation loss 0.000393\n",
      "Iter 24057, training loss 0.000050, validation loss 0.000396\n",
      "Iter 24058, training loss 0.000053, validation loss 0.000377\n",
      "Iter 24059, training loss 0.000081, validation loss 0.000451\n",
      "Iter 24060, training loss 0.000117, validation loss 0.000414\n",
      "Iter 24061, training loss 0.000204, validation loss 0.000541\n",
      "Iter 24062, training loss 0.000353, validation loss 0.000754\n",
      "Iter 24063, training loss 0.000544, validation loss 0.000977\n",
      "Iter 24064, training loss 0.001046, validation loss 0.001309\n",
      "Iter 24065, training loss 0.001602, validation loss 0.002021\n",
      "Iter 24066, training loss 0.002301, validation loss 0.002888\n",
      "Iter 24067, training loss 0.003250, validation loss 0.003111\n",
      "Iter 24068, training loss 0.003162, validation loss 0.003220\n",
      "Iter 24069, training loss 0.000550, validation loss 0.000829\n",
      "Iter 24070, training loss 0.000709, validation loss 0.001002\n",
      "Iter 24071, training loss 0.001589, validation loss 0.001562\n",
      "Iter 24072, training loss 0.002063, validation loss 0.002073\n",
      "Iter 24073, training loss 0.000264, validation loss 0.000395\n",
      "Iter 24074, training loss 0.000306, validation loss 0.000422\n",
      "Iter 24075, training loss 0.000980, validation loss 0.001176\n",
      "Iter 24076, training loss 0.000635, validation loss 0.000860\n",
      "Iter 24077, training loss 0.000164, validation loss 0.000308\n",
      "Iter 24078, training loss 0.000514, validation loss 0.000638\n",
      "Iter 24079, training loss 0.000564, validation loss 0.000592\n",
      "Iter 24080, training loss 0.000131, validation loss 0.000212\n",
      "Iter 24081, training loss 0.000452, validation loss 0.000570\n",
      "Iter 24082, training loss 0.000317, validation loss 0.000374\n",
      "Iter 24083, training loss 0.000159, validation loss 0.000240\n",
      "Iter 24084, training loss 0.000386, validation loss 0.000513\n",
      "Iter 24085, training loss 0.000127, validation loss 0.000227\n",
      "Iter 24086, training loss 0.000260, validation loss 0.000341\n",
      "Iter 24087, training loss 0.000198, validation loss 0.000321\n",
      "Iter 24088, training loss 0.000144, validation loss 0.000260\n",
      "Iter 24089, training loss 0.000234, validation loss 0.000297\n",
      "Iter 24090, training loss 0.000101, validation loss 0.000202\n",
      "Iter 24091, training loss 0.000208, validation loss 0.000346\n",
      "Iter 24092, training loss 0.000108, validation loss 0.000202\n",
      "Iter 24093, training loss 0.000156, validation loss 0.000234\n",
      "Iter 24094, training loss 0.000132, validation loss 0.000260\n",
      "Iter 24095, training loss 0.000117, validation loss 0.000238\n",
      "Iter 24096, training loss 0.000142, validation loss 0.000228\n",
      "Iter 24097, training loss 0.000088, validation loss 0.000197\n",
      "Iter 24098, training loss 0.000134, validation loss 0.000276\n",
      "Iter 24099, training loss 0.000084, validation loss 0.000199\n",
      "Iter 24100, training loss 0.000113, validation loss 0.000215\n",
      "Iter 24101, training loss 0.000096, validation loss 0.000221\n",
      "Iter 24102, training loss 0.000089, validation loss 0.000230\n",
      "Iter 24103, training loss 0.000103, validation loss 0.000205\n",
      "Iter 24104, training loss 0.000076, validation loss 0.000201\n",
      "Iter 24105, training loss 0.000093, validation loss 0.000225\n",
      "Iter 24106, training loss 0.000082, validation loss 0.000209\n",
      "Iter 24107, training loss 0.000082, validation loss 0.000206\n",
      "Iter 24108, training loss 0.000076, validation loss 0.000213\n",
      "Iter 24109, training loss 0.000077, validation loss 0.000213\n",
      "Iter 24110, training loss 0.000076, validation loss 0.000198\n",
      "Iter 24111, training loss 0.000075, validation loss 0.000197\n",
      "Iter 24112, training loss 0.000076, validation loss 0.000208\n",
      "Iter 24113, training loss 0.000073, validation loss 0.000200\n",
      "Iter 24114, training loss 0.000075, validation loss 0.000195\n",
      "Iter 24115, training loss 0.000071, validation loss 0.000197\n",
      "Iter 24116, training loss 0.000072, validation loss 0.000201\n",
      "Iter 24117, training loss 0.000070, validation loss 0.000194\n",
      "Iter 24118, training loss 0.000069, validation loss 0.000195\n",
      "Iter 24119, training loss 0.000069, validation loss 0.000200\n",
      "Iter 24120, training loss 0.000067, validation loss 0.000197\n",
      "Iter 24121, training loss 0.000067, validation loss 0.000193\n",
      "Iter 24122, training loss 0.000065, validation loss 0.000193\n",
      "Iter 24123, training loss 0.000065, validation loss 0.000195\n",
      "Iter 24124, training loss 0.000063, validation loss 0.000191\n",
      "Iter 24125, training loss 0.000062, validation loss 0.000190\n",
      "Iter 24126, training loss 0.000062, validation loss 0.000192\n",
      "Iter 24127, training loss 0.000060, validation loss 0.000189\n",
      "Iter 24128, training loss 0.000059, validation loss 0.000187\n",
      "Iter 24129, training loss 0.000058, validation loss 0.000188\n",
      "Iter 24130, training loss 0.000057, validation loss 0.000189\n",
      "Iter 24131, training loss 0.000056, validation loss 0.000187\n",
      "Iter 24132, training loss 0.000055, validation loss 0.000186\n",
      "Iter 24133, training loss 0.000055, validation loss 0.000187\n",
      "Iter 24134, training loss 0.000053, validation loss 0.000184\n",
      "Iter 24135, training loss 0.000053, validation loss 0.000182\n",
      "Iter 24136, training loss 0.000052, validation loss 0.000183\n",
      "Iter 24137, training loss 0.000051, validation loss 0.000181\n",
      "Iter 24138, training loss 0.000050, validation loss 0.000179\n",
      "Iter 24139, training loss 0.000049, validation loss 0.000180\n",
      "Iter 24140, training loss 0.000048, validation loss 0.000179\n",
      "Iter 24141, training loss 0.000047, validation loss 0.000178\n",
      "Iter 24142, training loss 0.000046, validation loss 0.000179\n",
      "Iter 24143, training loss 0.000045, validation loss 0.000178\n",
      "Iter 24144, training loss 0.000044, validation loss 0.000178\n",
      "Iter 24145, training loss 0.000044, validation loss 0.000182\n",
      "Iter 24146, training loss 0.000044, validation loss 0.000185\n",
      "Iter 24147, training loss 0.000043, validation loss 0.000185\n",
      "Iter 24148, training loss 0.000042, validation loss 0.000184\n",
      "Iter 24149, training loss 0.000041, validation loss 0.000182\n",
      "Iter 24150, training loss 0.000041, validation loss 0.000180\n",
      "Iter 24151, training loss 0.000040, validation loss 0.000180\n",
      "Iter 24152, training loss 0.000039, validation loss 0.000179\n",
      "Iter 24153, training loss 0.000039, validation loss 0.000178\n",
      "Iter 24154, training loss 0.000038, validation loss 0.000178\n",
      "Iter 24155, training loss 0.000037, validation loss 0.000178\n",
      "Iter 24156, training loss 0.000036, validation loss 0.000179\n",
      "Iter 24157, training loss 0.000036, validation loss 0.000179\n",
      "Iter 24158, training loss 0.000035, validation loss 0.000180\n",
      "Iter 24159, training loss 0.000035, validation loss 0.000180\n",
      "Iter 24160, training loss 0.000034, validation loss 0.000179\n",
      "Iter 24161, training loss 0.000034, validation loss 0.000177\n",
      "Iter 24162, training loss 0.000033, validation loss 0.000176\n",
      "Iter 24163, training loss 0.000033, validation loss 0.000175\n",
      "Iter 24164, training loss 0.000033, validation loss 0.000175\n",
      "Iter 24165, training loss 0.000032, validation loss 0.000174\n",
      "Iter 24166, training loss 0.000032, validation loss 0.000174\n",
      "Iter 24167, training loss 0.000032, validation loss 0.000173\n",
      "Iter 24168, training loss 0.000031, validation loss 0.000173\n",
      "Iter 24169, training loss 0.000031, validation loss 0.000172\n",
      "Iter 24170, training loss 0.000031, validation loss 0.000172\n",
      "Iter 24171, training loss 0.000030, validation loss 0.000172\n",
      "Iter 24172, training loss 0.000030, validation loss 0.000172\n",
      "Iter 24173, training loss 0.000030, validation loss 0.000171\n",
      "Iter 24174, training loss 0.000031, validation loss 0.000172\n",
      "Iter 24175, training loss 0.000031, validation loss 0.000171\n",
      "Iter 24176, training loss 0.000032, validation loss 0.000173\n",
      "Iter 24177, training loss 0.000034, validation loss 0.000172\n",
      "Iter 24178, training loss 0.000036, validation loss 0.000176\n",
      "Iter 24179, training loss 0.000037, validation loss 0.000173\n",
      "Iter 24180, training loss 0.000034, validation loss 0.000175\n",
      "Iter 24181, training loss 0.000030, validation loss 0.000169\n",
      "Iter 24182, training loss 0.000028, validation loss 0.000168\n",
      "Iter 24183, training loss 0.000030, validation loss 0.000171\n",
      "Iter 24184, training loss 0.000032, validation loss 0.000169\n",
      "Iter 24185, training loss 0.000030, validation loss 0.000170\n",
      "Iter 24186, training loss 0.000028, validation loss 0.000166\n",
      "Iter 24187, training loss 0.000027, validation loss 0.000165\n",
      "Iter 24188, training loss 0.000029, validation loss 0.000167\n",
      "Iter 24189, training loss 0.000030, validation loss 0.000165\n",
      "Iter 24190, training loss 0.000029, validation loss 0.000166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24191, training loss 0.000027, validation loss 0.000163\n",
      "Iter 24192, training loss 0.000026, validation loss 0.000163\n",
      "Iter 24193, training loss 0.000027, validation loss 0.000164\n",
      "Iter 24194, training loss 0.000028, validation loss 0.000163\n",
      "Iter 24195, training loss 0.000027, validation loss 0.000164\n",
      "Iter 24196, training loss 0.000026, validation loss 0.000162\n",
      "Iter 24197, training loss 0.000026, validation loss 0.000161\n",
      "Iter 24198, training loss 0.000026, validation loss 0.000162\n",
      "Iter 24199, training loss 0.000026, validation loss 0.000161\n",
      "Iter 24200, training loss 0.000026, validation loss 0.000162\n",
      "Iter 24201, training loss 0.000026, validation loss 0.000160\n",
      "Iter 24202, training loss 0.000025, validation loss 0.000160\n",
      "Iter 24203, training loss 0.000025, validation loss 0.000160\n",
      "Iter 24204, training loss 0.000025, validation loss 0.000160\n",
      "Iter 24205, training loss 0.000025, validation loss 0.000161\n",
      "Iter 24206, training loss 0.000025, validation loss 0.000159\n",
      "Iter 24207, training loss 0.000024, validation loss 0.000160\n",
      "Iter 24208, training loss 0.000024, validation loss 0.000159\n",
      "Iter 24209, training loss 0.000024, validation loss 0.000159\n",
      "Iter 24210, training loss 0.000024, validation loss 0.000159\n",
      "Iter 24211, training loss 0.000024, validation loss 0.000158\n",
      "Iter 24212, training loss 0.000024, validation loss 0.000159\n",
      "Iter 24213, training loss 0.000024, validation loss 0.000158\n",
      "Iter 24214, training loss 0.000023, validation loss 0.000158\n",
      "Iter 24215, training loss 0.000023, validation loss 0.000158\n",
      "Iter 24216, training loss 0.000023, validation loss 0.000158\n",
      "Iter 24217, training loss 0.000023, validation loss 0.000158\n",
      "Iter 24218, training loss 0.000023, validation loss 0.000158\n",
      "Iter 24219, training loss 0.000023, validation loss 0.000158\n",
      "Iter 24220, training loss 0.000023, validation loss 0.000157\n",
      "Iter 24221, training loss 0.000023, validation loss 0.000158\n",
      "Iter 24222, training loss 0.000022, validation loss 0.000157\n",
      "Iter 24223, training loss 0.000022, validation loss 0.000157\n",
      "Iter 24224, training loss 0.000022, validation loss 0.000157\n",
      "Iter 24225, training loss 0.000022, validation loss 0.000157\n",
      "Iter 24226, training loss 0.000021, validation loss 0.000158\n",
      "Iter 24227, training loss 0.000021, validation loss 0.000158\n",
      "Iter 24228, training loss 0.000021, validation loss 0.000158\n",
      "Iter 24229, training loss 0.000021, validation loss 0.000158\n",
      "Iter 24230, training loss 0.000020, validation loss 0.000159\n",
      "Iter 24231, training loss 0.000020, validation loss 0.000158\n",
      "Iter 24232, training loss 0.000020, validation loss 0.000159\n",
      "Iter 24233, training loss 0.000020, validation loss 0.000158\n",
      "Iter 24234, training loss 0.000020, validation loss 0.000159\n",
      "Iter 24235, training loss 0.000020, validation loss 0.000159\n",
      "Iter 24236, training loss 0.000019, validation loss 0.000159\n",
      "Iter 24237, training loss 0.000019, validation loss 0.000159\n",
      "Iter 24238, training loss 0.000019, validation loss 0.000159\n",
      "Iter 24239, training loss 0.000019, validation loss 0.000158\n",
      "Iter 24240, training loss 0.000019, validation loss 0.000158\n",
      "Iter 24241, training loss 0.000019, validation loss 0.000158\n",
      "Iter 24242, training loss 0.000019, validation loss 0.000159\n",
      "Iter 24243, training loss 0.000019, validation loss 0.000158\n",
      "Iter 24244, training loss 0.000019, validation loss 0.000159\n",
      "Iter 24245, training loss 0.000019, validation loss 0.000158\n",
      "Iter 24246, training loss 0.000019, validation loss 0.000159\n",
      "Iter 24247, training loss 0.000019, validation loss 0.000158\n",
      "Iter 24248, training loss 0.000020, validation loss 0.000160\n",
      "Iter 24249, training loss 0.000020, validation loss 0.000160\n",
      "Iter 24250, training loss 0.000022, validation loss 0.000163\n",
      "Iter 24251, training loss 0.000024, validation loss 0.000162\n",
      "Iter 24252, training loss 0.000027, validation loss 0.000167\n",
      "Iter 24253, training loss 0.000028, validation loss 0.000164\n",
      "Iter 24254, training loss 0.000026, validation loss 0.000166\n",
      "Iter 24255, training loss 0.000021, validation loss 0.000160\n",
      "Iter 24256, training loss 0.000018, validation loss 0.000159\n",
      "Iter 24257, training loss 0.000019, validation loss 0.000160\n",
      "Iter 24258, training loss 0.000022, validation loss 0.000160\n",
      "Iter 24259, training loss 0.000025, validation loss 0.000164\n",
      "Iter 24260, training loss 0.000025, validation loss 0.000162\n",
      "Iter 24261, training loss 0.000023, validation loss 0.000164\n",
      "Iter 24262, training loss 0.000019, validation loss 0.000158\n",
      "Iter 24263, training loss 0.000018, validation loss 0.000157\n",
      "Iter 24264, training loss 0.000019, validation loss 0.000160\n",
      "Iter 24265, training loss 0.000021, validation loss 0.000159\n",
      "Iter 24266, training loss 0.000020, validation loss 0.000160\n",
      "Iter 24267, training loss 0.000018, validation loss 0.000158\n",
      "Iter 24268, training loss 0.000018, validation loss 0.000158\n",
      "Iter 24269, training loss 0.000018, validation loss 0.000159\n",
      "Iter 24270, training loss 0.000019, validation loss 0.000158\n",
      "Iter 24271, training loss 0.000019, validation loss 0.000159\n",
      "Iter 24272, training loss 0.000018, validation loss 0.000157\n",
      "Iter 24273, training loss 0.000018, validation loss 0.000156\n",
      "Iter 24274, training loss 0.000018, validation loss 0.000157\n",
      "Iter 24275, training loss 0.000018, validation loss 0.000157\n",
      "Iter 24276, training loss 0.000018, validation loss 0.000158\n",
      "Iter 24277, training loss 0.000017, validation loss 0.000157\n",
      "Iter 24278, training loss 0.000017, validation loss 0.000157\n",
      "Iter 24279, training loss 0.000017, validation loss 0.000157\n",
      "Iter 24280, training loss 0.000018, validation loss 0.000157\n",
      "Iter 24281, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24282, training loss 0.000017, validation loss 0.000157\n",
      "Iter 24283, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24284, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24285, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24286, training loss 0.000017, validation loss 0.000157\n",
      "Iter 24287, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24288, training loss 0.000017, validation loss 0.000157\n",
      "Iter 24289, training loss 0.000017, validation loss 0.000157\n",
      "Iter 24290, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24291, training loss 0.000017, validation loss 0.000157\n",
      "Iter 24292, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24293, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24294, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24295, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24296, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24297, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24298, training loss 0.000017, validation loss 0.000156\n",
      "Iter 24299, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24300, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24301, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24302, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24303, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24304, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24305, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24306, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24307, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24308, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24309, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24310, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24311, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24312, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24313, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24314, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24315, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24316, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24317, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24318, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24319, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24320, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24321, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24322, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24323, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24324, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24325, training loss 0.000016, validation loss 0.000156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24326, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24327, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24328, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24329, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24330, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24331, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24332, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24333, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24334, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24335, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24336, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24337, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24338, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24339, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24340, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24341, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24342, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24343, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24344, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24345, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24346, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24347, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24348, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24349, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24350, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24351, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24352, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24353, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24354, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24355, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24356, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24357, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24358, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24359, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24360, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24361, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24362, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24363, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24364, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24365, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24366, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24367, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24368, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24369, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24370, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24371, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24372, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24373, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24374, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24375, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24376, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24377, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24378, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24379, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24380, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24381, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24382, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24383, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24384, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24385, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24386, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24387, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24388, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24389, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24390, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24391, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24392, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24393, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24394, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24395, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24396, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24397, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24398, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24399, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24400, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24401, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24402, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24403, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24404, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24405, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24406, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24407, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24408, training loss 0.000016, validation loss 0.000156\n",
      "Iter 24409, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24410, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24411, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24412, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24413, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24414, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24415, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24416, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24417, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24418, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24419, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24420, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24421, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24422, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24423, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24424, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24425, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24426, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24427, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24428, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24429, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24430, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24431, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24432, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24433, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24434, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24435, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24436, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24437, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24438, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24439, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24440, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24441, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24442, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24443, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24444, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24445, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24446, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24447, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24448, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24449, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24450, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24451, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24452, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24453, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24454, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24455, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24456, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24457, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24458, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24459, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24460, training loss 0.000015, validation loss 0.000157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24461, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24462, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24463, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24464, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24465, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24466, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24467, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24468, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24469, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24470, training loss 0.000017, validation loss 0.000159\n",
      "Iter 24471, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24472, training loss 0.000017, validation loss 0.000160\n",
      "Iter 24473, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24474, training loss 0.000017, validation loss 0.000160\n",
      "Iter 24475, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24476, training loss 0.000017, validation loss 0.000159\n",
      "Iter 24477, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24478, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24479, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24480, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24481, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24482, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24483, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24484, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24485, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24486, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24487, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24488, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24489, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24490, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24491, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24492, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24493, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24494, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24495, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24496, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24497, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24498, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24499, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24500, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24501, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24502, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24503, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24504, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24505, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24506, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24507, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24508, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24509, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24510, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24511, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24512, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24513, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24514, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24515, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24516, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24517, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24518, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24519, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24520, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24521, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24522, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24523, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24524, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24525, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24526, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24527, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24528, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24529, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24530, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24531, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24532, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24533, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24534, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24535, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24536, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24537, training loss 0.000017, validation loss 0.000160\n",
      "Iter 24538, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24539, training loss 0.000018, validation loss 0.000161\n",
      "Iter 24540, training loss 0.000018, validation loss 0.000159\n",
      "Iter 24541, training loss 0.000018, validation loss 0.000161\n",
      "Iter 24542, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24543, training loss 0.000017, validation loss 0.000160\n",
      "Iter 24544, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24545, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24546, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24547, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24548, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24549, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24550, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24551, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24552, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24553, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24554, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24555, training loss 0.000016, validation loss 0.000157\n",
      "Iter 24556, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24557, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24558, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24559, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24560, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24561, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24562, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24563, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24564, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24565, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24566, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24567, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24568, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24569, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24570, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24571, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24572, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24573, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24574, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24575, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24576, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24577, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24578, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24579, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24580, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24581, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24582, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24583, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24584, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24585, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24586, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24587, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24588, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24589, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24590, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24591, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24592, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24593, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24594, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24595, training loss 0.000014, validation loss 0.000157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24596, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24597, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24598, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24599, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24600, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24601, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24602, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24603, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24604, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24605, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24606, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24607, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24608, training loss 0.000016, validation loss 0.000160\n",
      "Iter 24609, training loss 0.000017, validation loss 0.000158\n",
      "Iter 24610, training loss 0.000017, validation loss 0.000161\n",
      "Iter 24611, training loss 0.000018, validation loss 0.000159\n",
      "Iter 24612, training loss 0.000018, validation loss 0.000161\n",
      "Iter 24613, training loss 0.000018, validation loss 0.000159\n",
      "Iter 24614, training loss 0.000017, validation loss 0.000160\n",
      "Iter 24615, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24616, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24617, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24618, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24619, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24620, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24621, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24622, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24623, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24624, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24625, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24626, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24627, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24628, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24629, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24630, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24631, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24632, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24633, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24634, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24635, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24636, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24637, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24638, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24639, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24640, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24641, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24642, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24643, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24644, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24645, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24646, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24647, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24648, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24649, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24650, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24651, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24652, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24653, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24654, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24655, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24656, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24657, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24658, training loss 0.000015, validation loss 0.000157\n",
      "Iter 24659, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24660, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24661, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24662, training loss 0.000014, validation loss 0.000157\n",
      "Iter 24663, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24664, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24665, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24666, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24667, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24668, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24669, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24670, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24671, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24672, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24673, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24674, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24675, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24676, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24677, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24678, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24679, training loss 0.000017, validation loss 0.000161\n",
      "Iter 24680, training loss 0.000017, validation loss 0.000159\n",
      "Iter 24681, training loss 0.000018, validation loss 0.000162\n",
      "Iter 24682, training loss 0.000019, validation loss 0.000160\n",
      "Iter 24683, training loss 0.000019, validation loss 0.000163\n",
      "Iter 24684, training loss 0.000018, validation loss 0.000159\n",
      "Iter 24685, training loss 0.000017, validation loss 0.000161\n",
      "Iter 24686, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24687, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24688, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24689, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24690, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24691, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24692, training loss 0.000016, validation loss 0.000160\n",
      "Iter 24693, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24694, training loss 0.000016, validation loss 0.000160\n",
      "Iter 24695, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24696, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24697, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24698, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24699, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24700, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24701, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24702, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24703, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24704, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24705, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24706, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24707, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24708, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24709, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24710, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24711, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24712, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24713, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24714, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24715, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24716, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24717, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24718, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24719, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24720, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24721, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24722, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24723, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24724, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24725, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24726, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24727, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24728, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24729, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24730, training loss 0.000014, validation loss 0.000158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24731, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24732, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24733, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24734, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24735, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24736, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24737, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24738, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24739, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24740, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24741, training loss 0.000016, validation loss 0.000160\n",
      "Iter 24742, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24743, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24744, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24745, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24746, training loss 0.000016, validation loss 0.000158\n",
      "Iter 24747, training loss 0.000016, validation loss 0.000160\n",
      "Iter 24748, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24749, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24750, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24751, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24752, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24753, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24754, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24755, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24756, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24757, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24758, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24759, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24760, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24761, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24762, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24763, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24764, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24765, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24766, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24767, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24768, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24769, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24770, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24771, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24772, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24773, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24774, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24775, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24776, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24777, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24778, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24779, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24780, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24781, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24782, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24783, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24784, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24785, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24786, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24787, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24788, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24789, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24790, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24791, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24792, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24793, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24794, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24795, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24796, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24797, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24798, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24799, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24800, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24801, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24802, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24803, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24804, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24805, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24806, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24807, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24808, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24809, training loss 0.000016, validation loss 0.000160\n",
      "Iter 24810, training loss 0.000017, validation loss 0.000162\n",
      "Iter 24811, training loss 0.000017, validation loss 0.000160\n",
      "Iter 24812, training loss 0.000017, validation loss 0.000162\n",
      "Iter 24813, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24814, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24815, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24816, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24817, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24818, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24819, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24820, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24821, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24822, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24823, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24824, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24825, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24826, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24827, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24828, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24829, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24830, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24831, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24832, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24833, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24834, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24835, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24836, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24837, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24838, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24839, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24840, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24841, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24842, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24843, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24844, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24845, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24846, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24847, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24848, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24849, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24850, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24851, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24852, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24853, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24854, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24855, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24856, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24857, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24858, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24859, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24860, training loss 0.000016, validation loss 0.000160\n",
      "Iter 24861, training loss 0.000017, validation loss 0.000163\n",
      "Iter 24862, training loss 0.000018, validation loss 0.000160\n",
      "Iter 24863, training loss 0.000019, validation loss 0.000165\n",
      "Iter 24864, training loss 0.000019, validation loss 0.000161\n",
      "Iter 24865, training loss 0.000019, validation loss 0.000165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 24866, training loss 0.000018, validation loss 0.000161\n",
      "Iter 24867, training loss 0.000018, validation loss 0.000164\n",
      "Iter 24868, training loss 0.000017, validation loss 0.000160\n",
      "Iter 24869, training loss 0.000016, validation loss 0.000162\n",
      "Iter 24870, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24871, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24872, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24873, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24874, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24875, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24876, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24877, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24878, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24879, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24880, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24881, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24882, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24883, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24884, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24885, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24886, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24887, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24888, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24889, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24890, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24891, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24892, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24893, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24894, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24895, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24896, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24897, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24898, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24899, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24900, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24901, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24902, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24903, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24904, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24905, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24906, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24907, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24908, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24909, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24910, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24911, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24912, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24913, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24914, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24915, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24916, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24917, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24918, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24919, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24920, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24921, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24922, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24923, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24924, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24925, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24926, training loss 0.000016, validation loss 0.000162\n",
      "Iter 24927, training loss 0.000017, validation loss 0.000160\n",
      "Iter 24928, training loss 0.000018, validation loss 0.000164\n",
      "Iter 24929, training loss 0.000018, validation loss 0.000160\n",
      "Iter 24930, training loss 0.000019, validation loss 0.000165\n",
      "Iter 24931, training loss 0.000019, validation loss 0.000161\n",
      "Iter 24932, training loss 0.000019, validation loss 0.000166\n",
      "Iter 24933, training loss 0.000019, validation loss 0.000161\n",
      "Iter 24934, training loss 0.000018, validation loss 0.000164\n",
      "Iter 24935, training loss 0.000017, validation loss 0.000159\n",
      "Iter 24936, training loss 0.000016, validation loss 0.000162\n",
      "Iter 24937, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24938, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24939, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24940, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24941, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24942, training loss 0.000015, validation loss 0.000158\n",
      "Iter 24943, training loss 0.000016, validation loss 0.000162\n",
      "Iter 24944, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24945, training loss 0.000016, validation loss 0.000161\n",
      "Iter 24946, training loss 0.000016, validation loss 0.000159\n",
      "Iter 24947, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24948, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24949, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24950, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24951, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24952, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24953, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24954, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24955, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24956, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24957, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24958, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24959, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24960, training loss 0.000015, validation loss 0.000160\n",
      "Iter 24961, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24962, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24963, training loss 0.000014, validation loss 0.000158\n",
      "Iter 24964, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24965, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24966, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24967, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24968, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24969, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24970, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24971, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24972, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24973, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24974, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24975, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24976, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24977, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24978, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24979, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24980, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24981, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24982, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24983, training loss 0.000014, validation loss 0.000160\n",
      "Iter 24984, training loss 0.000014, validation loss 0.000159\n",
      "Iter 24985, training loss 0.000014, validation loss 0.000161\n",
      "Iter 24986, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24987, training loss 0.000015, validation loss 0.000161\n",
      "Iter 24988, training loss 0.000015, validation loss 0.000159\n",
      "Iter 24989, training loss 0.000016, validation loss 0.000163\n",
      "Iter 24990, training loss 0.000017, validation loss 0.000159\n",
      "Iter 24991, training loss 0.000017, validation loss 0.000164\n",
      "Iter 24992, training loss 0.000018, validation loss 0.000160\n",
      "Iter 24993, training loss 0.000018, validation loss 0.000165\n",
      "Iter 24994, training loss 0.000019, validation loss 0.000161\n",
      "Iter 24995, training loss 0.000019, validation loss 0.000167\n",
      "Iter 24996, training loss 0.000019, validation loss 0.000161\n",
      "Iter 24997, training loss 0.000019, validation loss 0.000165\n",
      "Iter 24998, training loss 0.000018, validation loss 0.000160\n",
      "Iter 24999, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25000, training loss 0.000015, validation loss 0.000159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25001, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25002, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25003, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25004, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25005, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25006, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25007, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25008, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25009, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25010, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25011, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25012, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25013, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25014, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25015, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25016, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25017, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25018, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25019, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25020, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25021, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25022, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25023, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25024, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25025, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25026, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25027, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25028, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25029, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25030, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25031, training loss 0.000015, validation loss 0.000160\n",
      "Iter 25032, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25033, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25034, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25035, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25036, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25037, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25038, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25039, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25040, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25041, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25042, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25043, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25044, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25045, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25046, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25047, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25048, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25049, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25050, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25051, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25052, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25053, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25054, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25055, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25056, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25057, training loss 0.000015, validation loss 0.000160\n",
      "Iter 25058, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25059, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25060, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25061, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25062, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25063, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25064, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25065, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25066, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25067, training loss 0.000016, validation loss 0.000162\n",
      "Iter 25068, training loss 0.000016, validation loss 0.000160\n",
      "Iter 25069, training loss 0.000017, validation loss 0.000164\n",
      "Iter 25070, training loss 0.000017, validation loss 0.000159\n",
      "Iter 25071, training loss 0.000017, validation loss 0.000163\n",
      "Iter 25072, training loss 0.000017, validation loss 0.000159\n",
      "Iter 25073, training loss 0.000017, validation loss 0.000163\n",
      "Iter 25074, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25075, training loss 0.000016, validation loss 0.000162\n",
      "Iter 25076, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25077, training loss 0.000016, validation loss 0.000162\n",
      "Iter 25078, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25079, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25080, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25081, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25082, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25083, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25084, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25085, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25086, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25087, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25088, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25089, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25090, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25091, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25092, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25093, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25094, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25095, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25096, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25097, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25098, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25099, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25100, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25101, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25102, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25103, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25104, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25105, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25106, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25107, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25108, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25109, training loss 0.000017, validation loss 0.000159\n",
      "Iter 25110, training loss 0.000017, validation loss 0.000164\n",
      "Iter 25111, training loss 0.000018, validation loss 0.000161\n",
      "Iter 25112, training loss 0.000019, validation loss 0.000166\n",
      "Iter 25113, training loss 0.000020, validation loss 0.000162\n",
      "Iter 25114, training loss 0.000021, validation loss 0.000169\n",
      "Iter 25115, training loss 0.000022, validation loss 0.000162\n",
      "Iter 25116, training loss 0.000022, validation loss 0.000169\n",
      "Iter 25117, training loss 0.000021, validation loss 0.000162\n",
      "Iter 25118, training loss 0.000020, validation loss 0.000168\n",
      "Iter 25119, training loss 0.000018, validation loss 0.000160\n",
      "Iter 25120, training loss 0.000016, validation loss 0.000162\n",
      "Iter 25121, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25122, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25123, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25124, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25125, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25126, training loss 0.000016, validation loss 0.000158\n",
      "Iter 25127, training loss 0.000017, validation loss 0.000163\n",
      "Iter 25128, training loss 0.000017, validation loss 0.000159\n",
      "Iter 25129, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25130, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25131, training loss 0.000015, validation loss 0.000160\n",
      "Iter 25132, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25133, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25134, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25135, training loss 0.000014, validation loss 0.000159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25136, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25137, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25138, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25139, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25140, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25141, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25142, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25143, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25144, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25145, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25146, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25147, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25148, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25149, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25150, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25151, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25152, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25153, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25154, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25155, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25156, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25157, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25158, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25159, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25160, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25161, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25162, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25163, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25164, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25165, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25166, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25167, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25168, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25169, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25170, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25171, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25172, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25173, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25174, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25175, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25176, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25177, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25178, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25179, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25180, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25181, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25182, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25183, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25184, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25185, training loss 0.000017, validation loss 0.000164\n",
      "Iter 25186, training loss 0.000017, validation loss 0.000160\n",
      "Iter 25187, training loss 0.000017, validation loss 0.000165\n",
      "Iter 25188, training loss 0.000018, validation loss 0.000160\n",
      "Iter 25189, training loss 0.000018, validation loss 0.000166\n",
      "Iter 25190, training loss 0.000018, validation loss 0.000160\n",
      "Iter 25191, training loss 0.000018, validation loss 0.000165\n",
      "Iter 25192, training loss 0.000017, validation loss 0.000160\n",
      "Iter 25193, training loss 0.000017, validation loss 0.000164\n",
      "Iter 25194, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25195, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25196, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25197, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25198, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25199, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25200, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25201, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25202, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25203, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25204, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25205, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25206, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25207, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25208, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25209, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25210, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25211, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25212, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25213, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25214, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25215, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25216, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25217, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25218, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25219, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25220, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25221, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25222, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25223, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25224, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25225, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25226, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25227, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25228, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25229, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25230, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25231, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25232, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25233, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25234, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25235, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25236, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25237, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25238, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25239, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25240, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25241, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25242, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25243, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25244, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25245, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25246, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25247, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25248, training loss 0.000017, validation loss 0.000164\n",
      "Iter 25249, training loss 0.000017, validation loss 0.000159\n",
      "Iter 25250, training loss 0.000018, validation loss 0.000165\n",
      "Iter 25251, training loss 0.000018, validation loss 0.000161\n",
      "Iter 25252, training loss 0.000019, validation loss 0.000168\n",
      "Iter 25253, training loss 0.000020, validation loss 0.000161\n",
      "Iter 25254, training loss 0.000021, validation loss 0.000168\n",
      "Iter 25255, training loss 0.000021, validation loss 0.000162\n",
      "Iter 25256, training loss 0.000022, validation loss 0.000169\n",
      "Iter 25257, training loss 0.000024, validation loss 0.000163\n",
      "Iter 25258, training loss 0.000026, validation loss 0.000174\n",
      "Iter 25259, training loss 0.000029, validation loss 0.000168\n",
      "Iter 25260, training loss 0.000030, validation loss 0.000178\n",
      "Iter 25261, training loss 0.000029, validation loss 0.000164\n",
      "Iter 25262, training loss 0.000028, validation loss 0.000172\n",
      "Iter 25263, training loss 0.000026, validation loss 0.000161\n",
      "Iter 25264, training loss 0.000024, validation loss 0.000170\n",
      "Iter 25265, training loss 0.000022, validation loss 0.000161\n",
      "Iter 25266, training loss 0.000018, validation loss 0.000162\n",
      "Iter 25267, training loss 0.000016, validation loss 0.000157\n",
      "Iter 25268, training loss 0.000016, validation loss 0.000157\n",
      "Iter 25269, training loss 0.000018, validation loss 0.000162\n",
      "Iter 25270, training loss 0.000021, validation loss 0.000158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25271, training loss 0.000023, validation loss 0.000167\n",
      "Iter 25272, training loss 0.000025, validation loss 0.000162\n",
      "Iter 25273, training loss 0.000024, validation loss 0.000171\n",
      "Iter 25274, training loss 0.000021, validation loss 0.000161\n",
      "Iter 25275, training loss 0.000017, validation loss 0.000163\n",
      "Iter 25276, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25277, training loss 0.000016, validation loss 0.000157\n",
      "Iter 25278, training loss 0.000018, validation loss 0.000162\n",
      "Iter 25279, training loss 0.000020, validation loss 0.000161\n",
      "Iter 25280, training loss 0.000020, validation loss 0.000166\n",
      "Iter 25281, training loss 0.000019, validation loss 0.000160\n",
      "Iter 25282, training loss 0.000016, validation loss 0.000162\n",
      "Iter 25283, training loss 0.000015, validation loss 0.000157\n",
      "Iter 25284, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25285, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25286, training loss 0.000017, validation loss 0.000158\n",
      "Iter 25287, training loss 0.000017, validation loss 0.000163\n",
      "Iter 25288, training loss 0.000017, validation loss 0.000158\n",
      "Iter 25289, training loss 0.000016, validation loss 0.000161\n",
      "Iter 25290, training loss 0.000015, validation loss 0.000157\n",
      "Iter 25291, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25292, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25293, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25294, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25295, training loss 0.000015, validation loss 0.000157\n",
      "Iter 25296, training loss 0.000015, validation loss 0.000160\n",
      "Iter 25297, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25298, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25299, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25300, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25301, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25302, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25303, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25304, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25305, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25306, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25307, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25308, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25309, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25310, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25311, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25312, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25313, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25314, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25315, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25316, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25317, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25318, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25319, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25320, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25321, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25322, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25323, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25324, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25325, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25326, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25327, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25328, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25329, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25330, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25331, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25332, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25333, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25334, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25335, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25336, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25337, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25338, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25339, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25340, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25341, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25342, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25343, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25344, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25345, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25346, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25347, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25348, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25349, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25350, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25351, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25352, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25353, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25354, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25355, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25356, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25357, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25358, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25359, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25360, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25361, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25362, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25363, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25364, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25365, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25366, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25367, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25368, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25369, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25370, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25371, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25372, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25373, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25374, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25375, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25376, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25377, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25378, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25379, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25380, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25381, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25382, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25383, training loss 0.000017, validation loss 0.000159\n",
      "Iter 25384, training loss 0.000018, validation loss 0.000166\n",
      "Iter 25385, training loss 0.000019, validation loss 0.000160\n",
      "Iter 25386, training loss 0.000020, validation loss 0.000167\n",
      "Iter 25387, training loss 0.000020, validation loss 0.000161\n",
      "Iter 25388, training loss 0.000020, validation loss 0.000167\n",
      "Iter 25389, training loss 0.000019, validation loss 0.000161\n",
      "Iter 25390, training loss 0.000019, validation loss 0.000166\n",
      "Iter 25391, training loss 0.000018, validation loss 0.000159\n",
      "Iter 25392, training loss 0.000017, validation loss 0.000163\n",
      "Iter 25393, training loss 0.000015, validation loss 0.000157\n",
      "Iter 25394, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25395, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25396, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25397, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25398, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25399, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25400, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25401, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25402, training loss 0.000016, validation loss 0.000158\n",
      "Iter 25403, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25404, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25405, training loss 0.000015, validation loss 0.000161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25406, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25407, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25408, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25409, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25410, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25411, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25412, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25413, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25414, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25415, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25416, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25417, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25418, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25419, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25420, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25421, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25422, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25423, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25424, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25425, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25426, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25427, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25428, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25429, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25430, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25431, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25432, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25433, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25434, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25435, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25436, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25437, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25438, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25439, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25440, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25441, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25442, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25443, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25444, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25445, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25446, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25447, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25448, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25449, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25450, training loss 0.000016, validation loss 0.000164\n",
      "Iter 25451, training loss 0.000017, validation loss 0.000160\n",
      "Iter 25452, training loss 0.000019, validation loss 0.000167\n",
      "Iter 25453, training loss 0.000020, validation loss 0.000161\n",
      "Iter 25454, training loss 0.000021, validation loss 0.000169\n",
      "Iter 25455, training loss 0.000022, validation loss 0.000163\n",
      "Iter 25456, training loss 0.000023, validation loss 0.000172\n",
      "Iter 25457, training loss 0.000024, validation loss 0.000165\n",
      "Iter 25458, training loss 0.000025, validation loss 0.000174\n",
      "Iter 25459, training loss 0.000024, validation loss 0.000164\n",
      "Iter 25460, training loss 0.000023, validation loss 0.000171\n",
      "Iter 25461, training loss 0.000020, validation loss 0.000162\n",
      "Iter 25462, training loss 0.000018, validation loss 0.000166\n",
      "Iter 25463, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25464, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25465, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25466, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25467, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25468, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25469, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25470, training loss 0.000016, validation loss 0.000158\n",
      "Iter 25471, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25472, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25473, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25474, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25475, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25476, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25477, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25478, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25479, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25480, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25481, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25482, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25483, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25484, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25485, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25486, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25487, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25488, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25489, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25490, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25491, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25492, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25493, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25494, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25495, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25496, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25497, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25498, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25499, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25500, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25501, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25502, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25503, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25504, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25505, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25506, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25507, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25508, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25509, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25510, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25511, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25512, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25513, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25514, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25515, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25516, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25517, training loss 0.000015, validation loss 0.000163\n",
      "Iter 25518, training loss 0.000016, validation loss 0.000160\n",
      "Iter 25519, training loss 0.000017, validation loss 0.000166\n",
      "Iter 25520, training loss 0.000017, validation loss 0.000160\n",
      "Iter 25521, training loss 0.000019, validation loss 0.000168\n",
      "Iter 25522, training loss 0.000020, validation loss 0.000162\n",
      "Iter 25523, training loss 0.000022, validation loss 0.000172\n",
      "Iter 25524, training loss 0.000023, validation loss 0.000164\n",
      "Iter 25525, training loss 0.000024, validation loss 0.000173\n",
      "Iter 25526, training loss 0.000024, validation loss 0.000164\n",
      "Iter 25527, training loss 0.000023, validation loss 0.000172\n",
      "Iter 25528, training loss 0.000022, validation loss 0.000163\n",
      "Iter 25529, training loss 0.000020, validation loss 0.000169\n",
      "Iter 25530, training loss 0.000019, validation loss 0.000159\n",
      "Iter 25531, training loss 0.000017, validation loss 0.000163\n",
      "Iter 25532, training loss 0.000015, validation loss 0.000157\n",
      "Iter 25533, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25534, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25535, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25536, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25537, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25538, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25539, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25540, training loss 0.000016, validation loss 0.000164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25541, training loss 0.000016, validation loss 0.000159\n",
      "Iter 25542, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25543, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25544, training loss 0.000015, validation loss 0.000163\n",
      "Iter 25545, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25546, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25547, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25548, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25549, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25550, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25551, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25552, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25553, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25554, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25555, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25556, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25557, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25558, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25559, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25560, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25561, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25562, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25563, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25564, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25565, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25566, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25567, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25568, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25569, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25570, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25571, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25572, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25573, training loss 0.000014, validation loss 0.000163\n",
      "Iter 25574, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25575, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25576, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25577, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25578, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25579, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25580, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25581, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25582, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25583, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25584, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25585, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25586, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25587, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25588, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25589, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25590, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25591, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25592, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25593, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25594, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25595, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25596, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25597, training loss 0.000013, validation loss 0.000162\n",
      "Iter 25598, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25599, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25600, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25601, training loss 0.000015, validation loss 0.000164\n",
      "Iter 25602, training loss 0.000016, validation loss 0.000160\n",
      "Iter 25603, training loss 0.000017, validation loss 0.000167\n",
      "Iter 25604, training loss 0.000018, validation loss 0.000161\n",
      "Iter 25605, training loss 0.000021, validation loss 0.000171\n",
      "Iter 25606, training loss 0.000024, validation loss 0.000165\n",
      "Iter 25607, training loss 0.000028, validation loss 0.000178\n",
      "Iter 25608, training loss 0.000031, validation loss 0.000169\n",
      "Iter 25609, training loss 0.000034, validation loss 0.000184\n",
      "Iter 25610, training loss 0.000035, validation loss 0.000173\n",
      "Iter 25611, training loss 0.000036, validation loss 0.000187\n",
      "Iter 25612, training loss 0.000036, validation loss 0.000174\n",
      "Iter 25613, training loss 0.000036, validation loss 0.000186\n",
      "Iter 25614, training loss 0.000032, validation loss 0.000169\n",
      "Iter 25615, training loss 0.000025, validation loss 0.000173\n",
      "Iter 25616, training loss 0.000018, validation loss 0.000159\n",
      "Iter 25617, training loss 0.000015, validation loss 0.000160\n",
      "Iter 25618, training loss 0.000015, validation loss 0.000160\n",
      "Iter 25619, training loss 0.000017, validation loss 0.000159\n",
      "Iter 25620, training loss 0.000020, validation loss 0.000167\n",
      "Iter 25621, training loss 0.000023, validation loss 0.000161\n",
      "Iter 25622, training loss 0.000024, validation loss 0.000174\n",
      "Iter 25623, training loss 0.000023, validation loss 0.000165\n",
      "Iter 25624, training loss 0.000019, validation loss 0.000168\n",
      "Iter 25625, training loss 0.000016, validation loss 0.000158\n",
      "Iter 25626, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25627, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25628, training loss 0.000015, validation loss 0.000157\n",
      "Iter 25629, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25630, training loss 0.000017, validation loss 0.000159\n",
      "Iter 25631, training loss 0.000017, validation loss 0.000164\n",
      "Iter 25632, training loss 0.000017, validation loss 0.000157\n",
      "Iter 25633, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25634, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25635, training loss 0.000014, validation loss 0.000159\n",
      "Iter 25636, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25637, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25638, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25639, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25640, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25641, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25642, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25643, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25644, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25645, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25646, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25647, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25648, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25649, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25650, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25651, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25652, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25653, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25654, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25655, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25656, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25657, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25658, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25659, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25660, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25661, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25662, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25663, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25664, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25665, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25666, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25667, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25668, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25669, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25670, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25671, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25672, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25673, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25674, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25675, training loss 0.000013, validation loss 0.000159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25676, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25677, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25678, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25679, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25680, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25681, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25682, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25683, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25684, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25685, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25686, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25687, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25688, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25689, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25690, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25691, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25692, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25693, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25694, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25695, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25696, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25697, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25698, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25699, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25700, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25701, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25702, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25703, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25704, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25705, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25706, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25707, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25708, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25709, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25710, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25711, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25712, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25713, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25714, training loss 0.000015, validation loss 0.000158\n",
      "Iter 25715, training loss 0.000016, validation loss 0.000164\n",
      "Iter 25716, training loss 0.000016, validation loss 0.000158\n",
      "Iter 25717, training loss 0.000017, validation loss 0.000165\n",
      "Iter 25718, training loss 0.000018, validation loss 0.000159\n",
      "Iter 25719, training loss 0.000019, validation loss 0.000168\n",
      "Iter 25720, training loss 0.000019, validation loss 0.000160\n",
      "Iter 25721, training loss 0.000020, validation loss 0.000169\n",
      "Iter 25722, training loss 0.000020, validation loss 0.000160\n",
      "Iter 25723, training loss 0.000021, validation loss 0.000170\n",
      "Iter 25724, training loss 0.000023, validation loss 0.000161\n",
      "Iter 25725, training loss 0.000024, validation loss 0.000174\n",
      "Iter 25726, training loss 0.000027, validation loss 0.000165\n",
      "Iter 25727, training loss 0.000029, validation loss 0.000180\n",
      "Iter 25728, training loss 0.000032, validation loss 0.000167\n",
      "Iter 25729, training loss 0.000033, validation loss 0.000185\n",
      "Iter 25730, training loss 0.000034, validation loss 0.000169\n",
      "Iter 25731, training loss 0.000030, validation loss 0.000179\n",
      "Iter 25732, training loss 0.000024, validation loss 0.000160\n",
      "Iter 25733, training loss 0.000020, validation loss 0.000167\n",
      "Iter 25734, training loss 0.000016, validation loss 0.000157\n",
      "Iter 25735, training loss 0.000014, validation loss 0.000160\n",
      "Iter 25736, training loss 0.000014, validation loss 0.000158\n",
      "Iter 25737, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25738, training loss 0.000015, validation loss 0.000159\n",
      "Iter 25739, training loss 0.000016, validation loss 0.000155\n",
      "Iter 25740, training loss 0.000017, validation loss 0.000164\n",
      "Iter 25741, training loss 0.000017, validation loss 0.000158\n",
      "Iter 25742, training loss 0.000017, validation loss 0.000165\n",
      "Iter 25743, training loss 0.000016, validation loss 0.000157\n",
      "Iter 25744, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25745, training loss 0.000013, validation loss 0.000155\n",
      "Iter 25746, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25747, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25748, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25749, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25750, training loss 0.000014, validation loss 0.000156\n",
      "Iter 25751, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25752, training loss 0.000014, validation loss 0.000157\n",
      "Iter 25753, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25754, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25755, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25756, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25757, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25758, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25759, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25760, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25761, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25762, training loss 0.000013, validation loss 0.000158\n",
      "Iter 25763, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25764, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25765, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25766, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25767, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25768, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25769, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25770, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25771, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25772, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25773, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25774, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25775, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25776, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25777, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25778, training loss 0.000011, validation loss 0.000155\n",
      "Iter 25779, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25780, training loss 0.000011, validation loss 0.000155\n",
      "Iter 25781, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25782, training loss 0.000011, validation loss 0.000155\n",
      "Iter 25783, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25784, training loss 0.000012, validation loss 0.000155\n",
      "Iter 25785, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25786, training loss 0.000012, validation loss 0.000155\n",
      "Iter 25787, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25788, training loss 0.000013, validation loss 0.000155\n",
      "Iter 25789, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25790, training loss 0.000013, validation loss 0.000155\n",
      "Iter 25791, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25792, training loss 0.000014, validation loss 0.000156\n",
      "Iter 25793, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25794, training loss 0.000015, validation loss 0.000156\n",
      "Iter 25795, training loss 0.000015, validation loss 0.000163\n",
      "Iter 25796, training loss 0.000015, validation loss 0.000156\n",
      "Iter 25797, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25798, training loss 0.000014, validation loss 0.000156\n",
      "Iter 25799, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25800, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25801, training loss 0.000012, validation loss 0.000159\n",
      "Iter 25802, training loss 0.000012, validation loss 0.000155\n",
      "Iter 25803, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25804, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25805, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25806, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25807, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25808, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25809, training loss 0.000011, validation loss 0.000155\n",
      "Iter 25810, training loss 0.000011, validation loss 0.000158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25811, training loss 0.000012, validation loss 0.000155\n",
      "Iter 25812, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25813, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25814, training loss 0.000012, validation loss 0.000159\n",
      "Iter 25815, training loss 0.000013, validation loss 0.000155\n",
      "Iter 25816, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25817, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25818, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25819, training loss 0.000014, validation loss 0.000156\n",
      "Iter 25820, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25821, training loss 0.000014, validation loss 0.000156\n",
      "Iter 25822, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25823, training loss 0.000014, validation loss 0.000155\n",
      "Iter 25824, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25825, training loss 0.000014, validation loss 0.000156\n",
      "Iter 25826, training loss 0.000014, validation loss 0.000161\n",
      "Iter 25827, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25828, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25829, training loss 0.000013, validation loss 0.000155\n",
      "Iter 25830, training loss 0.000012, validation loss 0.000159\n",
      "Iter 25831, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25832, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25833, training loss 0.000012, validation loss 0.000155\n",
      "Iter 25834, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25835, training loss 0.000011, validation loss 0.000155\n",
      "Iter 25836, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25837, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25838, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25839, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25840, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25841, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25842, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25843, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25844, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25845, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25846, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25847, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25848, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25849, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25850, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25851, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25852, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25853, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25854, training loss 0.000012, validation loss 0.000159\n",
      "Iter 25855, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25856, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25857, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25858, training loss 0.000015, validation loss 0.000163\n",
      "Iter 25859, training loss 0.000016, validation loss 0.000157\n",
      "Iter 25860, training loss 0.000018, validation loss 0.000168\n",
      "Iter 25861, training loss 0.000020, validation loss 0.000159\n",
      "Iter 25862, training loss 0.000023, validation loss 0.000173\n",
      "Iter 25863, training loss 0.000025, validation loss 0.000163\n",
      "Iter 25864, training loss 0.000029, validation loss 0.000180\n",
      "Iter 25865, training loss 0.000036, validation loss 0.000171\n",
      "Iter 25866, training loss 0.000042, validation loss 0.000194\n",
      "Iter 25867, training loss 0.000044, validation loss 0.000176\n",
      "Iter 25868, training loss 0.000040, validation loss 0.000191\n",
      "Iter 25869, training loss 0.000032, validation loss 0.000167\n",
      "Iter 25870, training loss 0.000023, validation loss 0.000171\n",
      "Iter 25871, training loss 0.000014, validation loss 0.000155\n",
      "Iter 25872, training loss 0.000012, validation loss 0.000155\n",
      "Iter 25873, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25874, training loss 0.000018, validation loss 0.000157\n",
      "Iter 25875, training loss 0.000022, validation loss 0.000170\n",
      "Iter 25876, training loss 0.000024, validation loss 0.000161\n",
      "Iter 25877, training loss 0.000022, validation loss 0.000171\n",
      "Iter 25878, training loss 0.000018, validation loss 0.000158\n",
      "Iter 25879, training loss 0.000015, validation loss 0.000161\n",
      "Iter 25880, training loss 0.000012, validation loss 0.000155\n",
      "Iter 25881, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25882, training loss 0.000013, validation loss 0.000159\n",
      "Iter 25883, training loss 0.000015, validation loss 0.000156\n",
      "Iter 25884, training loss 0.000016, validation loss 0.000163\n",
      "Iter 25885, training loss 0.000016, validation loss 0.000154\n",
      "Iter 25886, training loss 0.000015, validation loss 0.000162\n",
      "Iter 25887, training loss 0.000014, validation loss 0.000156\n",
      "Iter 25888, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25889, training loss 0.000011, validation loss 0.000154\n",
      "Iter 25890, training loss 0.000012, validation loss 0.000154\n",
      "Iter 25891, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25892, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25893, training loss 0.000013, validation loss 0.000160\n",
      "Iter 25894, training loss 0.000013, validation loss 0.000155\n",
      "Iter 25895, training loss 0.000012, validation loss 0.000158\n",
      "Iter 25896, training loss 0.000012, validation loss 0.000155\n",
      "Iter 25897, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25898, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25899, training loss 0.000011, validation loss 0.000155\n",
      "Iter 25900, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25901, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25902, training loss 0.000012, validation loss 0.000159\n",
      "Iter 25903, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25904, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25905, training loss 0.000011, validation loss 0.000155\n",
      "Iter 25906, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25907, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25908, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25909, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25910, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25911, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25912, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25913, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25914, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25915, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25916, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25917, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25918, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25919, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25920, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25921, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25922, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25923, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25924, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25925, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25926, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25927, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25928, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25929, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25930, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25931, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25932, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25933, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25934, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25935, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25936, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25937, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25938, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25939, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25940, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25941, training loss 0.000012, validation loss 0.000159\n",
      "Iter 25942, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25943, training loss 0.000011, validation loss 0.000159\n",
      "Iter 25944, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25945, training loss 0.000011, validation loss 0.000158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25946, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25947, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25948, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25949, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25950, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25951, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25952, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25953, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25954, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25955, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25956, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25957, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25958, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25959, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25960, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25961, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25962, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25963, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25964, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25965, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25966, training loss 0.000012, validation loss 0.000159\n",
      "Iter 25967, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25968, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25969, training loss 0.000013, validation loss 0.000157\n",
      "Iter 25970, training loss 0.000014, validation loss 0.000163\n",
      "Iter 25971, training loss 0.000015, validation loss 0.000157\n",
      "Iter 25972, training loss 0.000015, validation loss 0.000164\n",
      "Iter 25973, training loss 0.000016, validation loss 0.000157\n",
      "Iter 25974, training loss 0.000016, validation loss 0.000165\n",
      "Iter 25975, training loss 0.000017, validation loss 0.000158\n",
      "Iter 25976, training loss 0.000017, validation loss 0.000167\n",
      "Iter 25977, training loss 0.000018, validation loss 0.000158\n",
      "Iter 25978, training loss 0.000017, validation loss 0.000167\n",
      "Iter 25979, training loss 0.000017, validation loss 0.000158\n",
      "Iter 25980, training loss 0.000016, validation loss 0.000164\n",
      "Iter 25981, training loss 0.000015, validation loss 0.000157\n",
      "Iter 25982, training loss 0.000014, validation loss 0.000162\n",
      "Iter 25983, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25984, training loss 0.000012, validation loss 0.000160\n",
      "Iter 25985, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25986, training loss 0.000011, validation loss 0.000157\n",
      "Iter 25987, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25988, training loss 0.000011, validation loss 0.000156\n",
      "Iter 25989, training loss 0.000011, validation loss 0.000158\n",
      "Iter 25990, training loss 0.000011, validation loss 0.000155\n",
      "Iter 25991, training loss 0.000012, validation loss 0.000159\n",
      "Iter 25992, training loss 0.000012, validation loss 0.000156\n",
      "Iter 25993, training loss 0.000012, validation loss 0.000160\n",
      "Iter 25994, training loss 0.000012, validation loss 0.000157\n",
      "Iter 25995, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25996, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25997, training loss 0.000013, validation loss 0.000161\n",
      "Iter 25998, training loss 0.000013, validation loss 0.000156\n",
      "Iter 25999, training loss 0.000012, validation loss 0.000160\n",
      "Iter 26000, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26001, training loss 0.000012, validation loss 0.000160\n",
      "Iter 26002, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26003, training loss 0.000012, validation loss 0.000159\n",
      "Iter 26004, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26005, training loss 0.000011, validation loss 0.000158\n",
      "Iter 26006, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26007, training loss 0.000011, validation loss 0.000158\n",
      "Iter 26008, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26009, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26010, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26011, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26012, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26013, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26014, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26015, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26016, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26017, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26018, training loss 0.000011, validation loss 0.000158\n",
      "Iter 26019, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26020, training loss 0.000011, validation loss 0.000158\n",
      "Iter 26021, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26022, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26023, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26024, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26025, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26026, training loss 0.000011, validation loss 0.000158\n",
      "Iter 26027, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26028, training loss 0.000011, validation loss 0.000159\n",
      "Iter 26029, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26030, training loss 0.000012, validation loss 0.000161\n",
      "Iter 26031, training loss 0.000013, validation loss 0.000157\n",
      "Iter 26032, training loss 0.000014, validation loss 0.000164\n",
      "Iter 26033, training loss 0.000016, validation loss 0.000158\n",
      "Iter 26034, training loss 0.000018, validation loss 0.000169\n",
      "Iter 26035, training loss 0.000021, validation loss 0.000160\n",
      "Iter 26036, training loss 0.000024, validation loss 0.000175\n",
      "Iter 26037, training loss 0.000028, validation loss 0.000166\n",
      "Iter 26038, training loss 0.000034, validation loss 0.000187\n",
      "Iter 26039, training loss 0.000044, validation loss 0.000179\n",
      "Iter 26040, training loss 0.000054, validation loss 0.000207\n",
      "Iter 26041, training loss 0.000062, validation loss 0.000194\n",
      "Iter 26042, training loss 0.000067, validation loss 0.000224\n",
      "Iter 26043, training loss 0.000066, validation loss 0.000192\n",
      "Iter 26044, training loss 0.000052, validation loss 0.000208\n",
      "Iter 26045, training loss 0.000034, validation loss 0.000172\n",
      "Iter 26046, training loss 0.000018, validation loss 0.000168\n",
      "Iter 26047, training loss 0.000014, validation loss 0.000160\n",
      "Iter 26048, training loss 0.000019, validation loss 0.000159\n",
      "Iter 26049, training loss 0.000028, validation loss 0.000178\n",
      "Iter 26050, training loss 0.000034, validation loss 0.000167\n",
      "Iter 26051, training loss 0.000032, validation loss 0.000182\n",
      "Iter 26052, training loss 0.000024, validation loss 0.000161\n",
      "Iter 26053, training loss 0.000015, validation loss 0.000162\n",
      "Iter 26054, training loss 0.000013, validation loss 0.000156\n",
      "Iter 26055, training loss 0.000016, validation loss 0.000155\n",
      "Iter 26056, training loss 0.000020, validation loss 0.000168\n",
      "Iter 26057, training loss 0.000022, validation loss 0.000159\n",
      "Iter 26058, training loss 0.000020, validation loss 0.000167\n",
      "Iter 26059, training loss 0.000015, validation loss 0.000153\n",
      "Iter 26060, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26061, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26062, training loss 0.000015, validation loss 0.000154\n",
      "Iter 26063, training loss 0.000016, validation loss 0.000163\n",
      "Iter 26064, training loss 0.000016, validation loss 0.000154\n",
      "Iter 26065, training loss 0.000014, validation loss 0.000159\n",
      "Iter 26066, training loss 0.000012, validation loss 0.000153\n",
      "Iter 26067, training loss 0.000012, validation loss 0.000153\n",
      "Iter 26068, training loss 0.000013, validation loss 0.000156\n",
      "Iter 26069, training loss 0.000014, validation loss 0.000152\n",
      "Iter 26070, training loss 0.000014, validation loss 0.000158\n",
      "Iter 26071, training loss 0.000013, validation loss 0.000152\n",
      "Iter 26072, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26073, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26074, training loss 0.000012, validation loss 0.000153\n",
      "Iter 26075, training loss 0.000012, validation loss 0.000158\n",
      "Iter 26076, training loss 0.000013, validation loss 0.000154\n",
      "Iter 26077, training loss 0.000012, validation loss 0.000157\n",
      "Iter 26078, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26079, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26080, training loss 0.000011, validation loss 0.000156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26081, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26082, training loss 0.000012, validation loss 0.000157\n",
      "Iter 26083, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26084, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26085, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26086, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26087, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26088, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26089, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26090, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26091, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26092, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26093, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26094, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26095, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26096, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26097, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26098, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26099, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26100, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26101, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26102, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26103, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26104, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26105, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26106, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26107, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26108, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26109, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26110, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26111, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26112, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26113, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26114, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26115, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26116, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26117, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26118, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26119, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26120, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26121, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26122, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26123, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26124, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26125, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26126, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26127, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26128, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26129, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26130, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26131, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26132, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26133, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26134, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26135, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26136, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26137, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26138, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26139, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26140, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26141, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26142, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26143, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26144, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26145, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26146, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26147, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26148, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26149, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26150, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26151, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26152, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26153, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26154, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26155, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26156, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26157, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26158, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26159, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26160, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26161, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26162, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26163, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26164, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26165, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26166, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26167, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26168, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26169, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26170, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26171, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26172, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26173, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26174, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26175, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26176, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26177, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26178, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26179, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26180, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26181, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26182, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26183, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26184, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26185, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26186, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26187, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26188, training loss 0.000012, validation loss 0.000158\n",
      "Iter 26189, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26190, training loss 0.000012, validation loss 0.000159\n",
      "Iter 26191, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26192, training loss 0.000012, validation loss 0.000158\n",
      "Iter 26193, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26194, training loss 0.000012, validation loss 0.000159\n",
      "Iter 26195, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26196, training loss 0.000012, validation loss 0.000159\n",
      "Iter 26197, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26198, training loss 0.000012, validation loss 0.000158\n",
      "Iter 26199, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26200, training loss 0.000012, validation loss 0.000159\n",
      "Iter 26201, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26202, training loss 0.000011, validation loss 0.000158\n",
      "Iter 26203, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26204, training loss 0.000012, validation loss 0.000157\n",
      "Iter 26205, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26206, training loss 0.000012, validation loss 0.000158\n",
      "Iter 26207, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26208, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26209, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26210, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26211, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26212, training loss 0.000012, validation loss 0.000158\n",
      "Iter 26213, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26214, training loss 0.000012, validation loss 0.000158\n",
      "Iter 26215, training loss 0.000012, validation loss 0.000154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26216, training loss 0.000012, validation loss 0.000159\n",
      "Iter 26217, training loss 0.000013, validation loss 0.000154\n",
      "Iter 26218, training loss 0.000013, validation loss 0.000161\n",
      "Iter 26219, training loss 0.000014, validation loss 0.000154\n",
      "Iter 26220, training loss 0.000014, validation loss 0.000162\n",
      "Iter 26221, training loss 0.000014, validation loss 0.000154\n",
      "Iter 26222, training loss 0.000015, validation loss 0.000162\n",
      "Iter 26223, training loss 0.000015, validation loss 0.000155\n",
      "Iter 26224, training loss 0.000015, validation loss 0.000164\n",
      "Iter 26225, training loss 0.000016, validation loss 0.000155\n",
      "Iter 26226, training loss 0.000016, validation loss 0.000164\n",
      "Iter 26227, training loss 0.000016, validation loss 0.000154\n",
      "Iter 26228, training loss 0.000015, validation loss 0.000163\n",
      "Iter 26229, training loss 0.000015, validation loss 0.000155\n",
      "Iter 26230, training loss 0.000014, validation loss 0.000162\n",
      "Iter 26231, training loss 0.000014, validation loss 0.000154\n",
      "Iter 26232, training loss 0.000013, validation loss 0.000159\n",
      "Iter 26233, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26234, training loss 0.000012, validation loss 0.000157\n",
      "Iter 26235, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26236, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26237, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26238, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26239, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26240, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26241, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26242, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26243, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26244, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26245, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26246, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26247, training loss 0.000011, validation loss 0.000157\n",
      "Iter 26248, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26249, training loss 0.000011, validation loss 0.000158\n",
      "Iter 26250, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26251, training loss 0.000012, validation loss 0.000159\n",
      "Iter 26252, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26253, training loss 0.000013, validation loss 0.000160\n",
      "Iter 26254, training loss 0.000013, validation loss 0.000154\n",
      "Iter 26255, training loss 0.000014, validation loss 0.000162\n",
      "Iter 26256, training loss 0.000015, validation loss 0.000155\n",
      "Iter 26257, training loss 0.000016, validation loss 0.000165\n",
      "Iter 26258, training loss 0.000017, validation loss 0.000156\n",
      "Iter 26259, training loss 0.000019, validation loss 0.000168\n",
      "Iter 26260, training loss 0.000020, validation loss 0.000157\n",
      "Iter 26261, training loss 0.000022, validation loss 0.000171\n",
      "Iter 26262, training loss 0.000023, validation loss 0.000160\n",
      "Iter 26263, training loss 0.000024, validation loss 0.000176\n",
      "Iter 26264, training loss 0.000027, validation loss 0.000159\n",
      "Iter 26265, training loss 0.000030, validation loss 0.000180\n",
      "Iter 26266, training loss 0.000033, validation loss 0.000167\n",
      "Iter 26267, training loss 0.000036, validation loss 0.000188\n",
      "Iter 26268, training loss 0.000037, validation loss 0.000168\n",
      "Iter 26269, training loss 0.000035, validation loss 0.000187\n",
      "Iter 26270, training loss 0.000031, validation loss 0.000162\n",
      "Iter 26271, training loss 0.000026, validation loss 0.000173\n",
      "Iter 26272, training loss 0.000021, validation loss 0.000156\n",
      "Iter 26273, training loss 0.000018, validation loss 0.000162\n",
      "Iter 26274, training loss 0.000016, validation loss 0.000151\n",
      "Iter 26275, training loss 0.000013, validation loss 0.000155\n",
      "Iter 26276, training loss 0.000012, validation loss 0.000151\n",
      "Iter 26277, training loss 0.000012, validation loss 0.000152\n",
      "Iter 26278, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26279, training loss 0.000013, validation loss 0.000151\n",
      "Iter 26280, training loss 0.000014, validation loss 0.000158\n",
      "Iter 26281, training loss 0.000015, validation loss 0.000152\n",
      "Iter 26282, training loss 0.000015, validation loss 0.000160\n",
      "Iter 26283, training loss 0.000015, validation loss 0.000152\n",
      "Iter 26284, training loss 0.000013, validation loss 0.000157\n",
      "Iter 26285, training loss 0.000012, validation loss 0.000151\n",
      "Iter 26286, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26287, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26288, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26289, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26290, training loss 0.000012, validation loss 0.000151\n",
      "Iter 26291, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26292, training loss 0.000012, validation loss 0.000150\n",
      "Iter 26293, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26294, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26295, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26296, training loss 0.000011, validation loss 0.000152\n",
      "Iter 26297, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26298, training loss 0.000011, validation loss 0.000152\n",
      "Iter 26299, training loss 0.000011, validation loss 0.000152\n",
      "Iter 26300, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26301, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26302, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26303, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26304, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26305, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26306, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26307, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26308, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26309, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26310, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26311, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26312, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26313, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26314, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26315, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26316, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26317, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26318, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26319, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26320, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26321, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26322, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26323, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26324, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26325, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26326, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26327, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26328, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26329, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26330, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26331, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26332, training loss 0.000012, validation loss 0.000151\n",
      "Iter 26333, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26334, training loss 0.000013, validation loss 0.000150\n",
      "Iter 26335, training loss 0.000013, validation loss 0.000156\n",
      "Iter 26336, training loss 0.000012, validation loss 0.000150\n",
      "Iter 26337, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26338, training loss 0.000012, validation loss 0.000150\n",
      "Iter 26339, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26340, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26341, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26342, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26343, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26344, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26345, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26346, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26347, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26348, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26349, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26350, training loss 0.000010, validation loss 0.000151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26351, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26352, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26353, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26354, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26355, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26356, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26357, training loss 0.000011, validation loss 0.000151\n",
      "Iter 26358, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26359, training loss 0.000013, validation loss 0.000151\n",
      "Iter 26360, training loss 0.000014, validation loss 0.000158\n",
      "Iter 26361, training loss 0.000015, validation loss 0.000151\n",
      "Iter 26362, training loss 0.000015, validation loss 0.000160\n",
      "Iter 26363, training loss 0.000016, validation loss 0.000152\n",
      "Iter 26364, training loss 0.000017, validation loss 0.000162\n",
      "Iter 26365, training loss 0.000018, validation loss 0.000154\n",
      "Iter 26366, training loss 0.000020, validation loss 0.000167\n",
      "Iter 26367, training loss 0.000021, validation loss 0.000155\n",
      "Iter 26368, training loss 0.000021, validation loss 0.000167\n",
      "Iter 26369, training loss 0.000020, validation loss 0.000155\n",
      "Iter 26370, training loss 0.000019, validation loss 0.000165\n",
      "Iter 26371, training loss 0.000017, validation loss 0.000153\n",
      "Iter 26372, training loss 0.000016, validation loss 0.000159\n",
      "Iter 26373, training loss 0.000015, validation loss 0.000149\n",
      "Iter 26374, training loss 0.000013, validation loss 0.000157\n",
      "Iter 26375, training loss 0.000012, validation loss 0.000152\n",
      "Iter 26376, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26377, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26378, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26379, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26380, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26381, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26382, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26383, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26384, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26385, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26386, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26387, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26388, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26389, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26390, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26391, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26392, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26393, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26394, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26395, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26396, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26397, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26398, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26399, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26400, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26401, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26402, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26403, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26404, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26405, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26406, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26407, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26408, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26409, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26410, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26411, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26412, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26413, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26414, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26415, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26416, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26417, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26418, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26419, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26420, training loss 0.000013, validation loss 0.000150\n",
      "Iter 26421, training loss 0.000014, validation loss 0.000159\n",
      "Iter 26422, training loss 0.000016, validation loss 0.000151\n",
      "Iter 26423, training loss 0.000017, validation loss 0.000163\n",
      "Iter 26424, training loss 0.000019, validation loss 0.000154\n",
      "Iter 26425, training loss 0.000021, validation loss 0.000168\n",
      "Iter 26426, training loss 0.000023, validation loss 0.000154\n",
      "Iter 26427, training loss 0.000025, validation loss 0.000172\n",
      "Iter 26428, training loss 0.000027, validation loss 0.000160\n",
      "Iter 26429, training loss 0.000030, validation loss 0.000177\n",
      "Iter 26430, training loss 0.000033, validation loss 0.000160\n",
      "Iter 26431, training loss 0.000036, validation loss 0.000183\n",
      "Iter 26432, training loss 0.000040, validation loss 0.000169\n",
      "Iter 26433, training loss 0.000042, validation loss 0.000191\n",
      "Iter 26434, training loss 0.000040, validation loss 0.000171\n",
      "Iter 26435, training loss 0.000034, validation loss 0.000184\n",
      "Iter 26436, training loss 0.000025, validation loss 0.000156\n",
      "Iter 26437, training loss 0.000016, validation loss 0.000157\n",
      "Iter 26438, training loss 0.000012, validation loss 0.000146\n",
      "Iter 26439, training loss 0.000011, validation loss 0.000146\n",
      "Iter 26440, training loss 0.000013, validation loss 0.000153\n",
      "Iter 26441, training loss 0.000017, validation loss 0.000149\n",
      "Iter 26442, training loss 0.000020, validation loss 0.000163\n",
      "Iter 26443, training loss 0.000020, validation loss 0.000151\n",
      "Iter 26444, training loss 0.000018, validation loss 0.000161\n",
      "Iter 26445, training loss 0.000015, validation loss 0.000148\n",
      "Iter 26446, training loss 0.000012, validation loss 0.000151\n",
      "Iter 26447, training loss 0.000010, validation loss 0.000146\n",
      "Iter 26448, training loss 0.000010, validation loss 0.000145\n",
      "Iter 26449, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26450, training loss 0.000013, validation loss 0.000147\n",
      "Iter 26451, training loss 0.000014, validation loss 0.000155\n",
      "Iter 26452, training loss 0.000013, validation loss 0.000148\n",
      "Iter 26453, training loss 0.000012, validation loss 0.000152\n",
      "Iter 26454, training loss 0.000011, validation loss 0.000146\n",
      "Iter 26455, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26456, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26457, training loss 0.000010, validation loss 0.000146\n",
      "Iter 26458, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26459, training loss 0.000011, validation loss 0.000146\n",
      "Iter 26460, training loss 0.000011, validation loss 0.000152\n",
      "Iter 26461, training loss 0.000011, validation loss 0.000147\n",
      "Iter 26462, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26463, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26464, training loss 0.000009, validation loss 0.000147\n",
      "Iter 26465, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26466, training loss 0.000010, validation loss 0.000146\n",
      "Iter 26467, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26468, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26469, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26470, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26471, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26472, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26473, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26474, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26475, training loss 0.000009, validation loss 0.000147\n",
      "Iter 26476, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26477, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26478, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26479, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26480, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26481, training loss 0.000009, validation loss 0.000147\n",
      "Iter 26482, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26483, training loss 0.000009, validation loss 0.000147\n",
      "Iter 26484, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26485, training loss 0.000009, validation loss 0.000148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26486, training loss 0.000009, validation loss 0.000147\n",
      "Iter 26487, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26488, training loss 0.000009, validation loss 0.000147\n",
      "Iter 26489, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26490, training loss 0.000009, validation loss 0.000147\n",
      "Iter 26491, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26492, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26493, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26494, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26495, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26496, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26497, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26498, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26499, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26500, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26501, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26502, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26503, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26504, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26505, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26506, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26507, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26508, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26509, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26510, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26511, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26512, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26513, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26514, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26515, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26516, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26517, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26518, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26519, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26520, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26521, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26522, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26523, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26524, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26525, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26526, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26527, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26528, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26529, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26530, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26531, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26532, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26533, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26534, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26535, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26536, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26537, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26538, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26539, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26540, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26541, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26542, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26543, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26544, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26545, training loss 0.000012, validation loss 0.000148\n",
      "Iter 26546, training loss 0.000013, validation loss 0.000156\n",
      "Iter 26547, training loss 0.000014, validation loss 0.000150\n",
      "Iter 26548, training loss 0.000015, validation loss 0.000159\n",
      "Iter 26549, training loss 0.000016, validation loss 0.000151\n",
      "Iter 26550, training loss 0.000017, validation loss 0.000161\n",
      "Iter 26551, training loss 0.000017, validation loss 0.000149\n",
      "Iter 26552, training loss 0.000017, validation loss 0.000161\n",
      "Iter 26553, training loss 0.000018, validation loss 0.000151\n",
      "Iter 26554, training loss 0.000018, validation loss 0.000161\n",
      "Iter 26555, training loss 0.000017, validation loss 0.000149\n",
      "Iter 26556, training loss 0.000017, validation loss 0.000160\n",
      "Iter 26557, training loss 0.000017, validation loss 0.000151\n",
      "Iter 26558, training loss 0.000016, validation loss 0.000160\n",
      "Iter 26559, training loss 0.000016, validation loss 0.000148\n",
      "Iter 26560, training loss 0.000016, validation loss 0.000159\n",
      "Iter 26561, training loss 0.000015, validation loss 0.000150\n",
      "Iter 26562, training loss 0.000014, validation loss 0.000158\n",
      "Iter 26563, training loss 0.000013, validation loss 0.000149\n",
      "Iter 26564, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26565, training loss 0.000011, validation loss 0.000147\n",
      "Iter 26566, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26567, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26568, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26569, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26570, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26571, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26572, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26573, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26574, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26575, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26576, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26577, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26578, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26579, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26580, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26581, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26582, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26583, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26584, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26585, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26586, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26587, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26588, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26589, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26590, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26591, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26592, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26593, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26594, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26595, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26596, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26597, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26598, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26599, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26600, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26601, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26602, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26603, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26604, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26605, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26606, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26607, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26608, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26609, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26610, training loss 0.000011, validation loss 0.000154\n",
      "Iter 26611, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26612, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26613, training loss 0.000012, validation loss 0.000148\n",
      "Iter 26614, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26615, training loss 0.000012, validation loss 0.000148\n",
      "Iter 26616, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26617, training loss 0.000012, validation loss 0.000149\n",
      "Iter 26618, training loss 0.000013, validation loss 0.000157\n",
      "Iter 26619, training loss 0.000013, validation loss 0.000149\n",
      "Iter 26620, training loss 0.000014, validation loss 0.000158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26621, training loss 0.000014, validation loss 0.000150\n",
      "Iter 26622, training loss 0.000015, validation loss 0.000158\n",
      "Iter 26623, training loss 0.000016, validation loss 0.000149\n",
      "Iter 26624, training loss 0.000017, validation loss 0.000162\n",
      "Iter 26625, training loss 0.000020, validation loss 0.000155\n",
      "Iter 26626, training loss 0.000023, validation loss 0.000170\n",
      "Iter 26627, training loss 0.000028, validation loss 0.000159\n",
      "Iter 26628, training loss 0.000033, validation loss 0.000183\n",
      "Iter 26629, training loss 0.000041, validation loss 0.000166\n",
      "Iter 26630, training loss 0.000045, validation loss 0.000196\n",
      "Iter 26631, training loss 0.000047, validation loss 0.000175\n",
      "Iter 26632, training loss 0.000045, validation loss 0.000197\n",
      "Iter 26633, training loss 0.000037, validation loss 0.000165\n",
      "Iter 26634, training loss 0.000025, validation loss 0.000173\n",
      "Iter 26635, training loss 0.000015, validation loss 0.000150\n",
      "Iter 26636, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26637, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26638, training loss 0.000018, validation loss 0.000151\n",
      "Iter 26639, training loss 0.000023, validation loss 0.000169\n",
      "Iter 26640, training loss 0.000025, validation loss 0.000157\n",
      "Iter 26641, training loss 0.000023, validation loss 0.000170\n",
      "Iter 26642, training loss 0.000017, validation loss 0.000152\n",
      "Iter 26643, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26644, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26645, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26646, training loss 0.000014, validation loss 0.000157\n",
      "Iter 26647, training loss 0.000016, validation loss 0.000151\n",
      "Iter 26648, training loss 0.000016, validation loss 0.000161\n",
      "Iter 26649, training loss 0.000015, validation loss 0.000150\n",
      "Iter 26650, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26651, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26652, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26653, training loss 0.000011, validation loss 0.000152\n",
      "Iter 26654, training loss 0.000013, validation loss 0.000147\n",
      "Iter 26655, training loss 0.000013, validation loss 0.000155\n",
      "Iter 26656, training loss 0.000012, validation loss 0.000148\n",
      "Iter 26657, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26658, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26659, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26660, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26661, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26662, training loss 0.000011, validation loss 0.000152\n",
      "Iter 26663, training loss 0.000011, validation loss 0.000147\n",
      "Iter 26664, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26665, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26666, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26667, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26668, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26669, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26670, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26671, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26672, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26673, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26674, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26675, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26676, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26677, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26678, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26679, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26680, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26681, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26682, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26683, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26684, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26685, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26686, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26687, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26688, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26689, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26690, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26691, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26692, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26693, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26694, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26695, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26696, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26697, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26698, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26699, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26700, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26701, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26702, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26703, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26704, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26705, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26706, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26707, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26708, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26709, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26710, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26711, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26712, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26713, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26714, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26715, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26716, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26717, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26718, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26719, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26720, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26721, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26722, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26723, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26724, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26725, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26726, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26727, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26728, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26729, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26730, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26731, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26732, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26733, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26734, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26735, training loss 0.000012, validation loss 0.000149\n",
      "Iter 26736, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26737, training loss 0.000012, validation loss 0.000149\n",
      "Iter 26738, training loss 0.000013, validation loss 0.000155\n",
      "Iter 26739, training loss 0.000012, validation loss 0.000148\n",
      "Iter 26740, training loss 0.000013, validation loss 0.000156\n",
      "Iter 26741, training loss 0.000013, validation loss 0.000150\n",
      "Iter 26742, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26743, training loss 0.000013, validation loss 0.000148\n",
      "Iter 26744, training loss 0.000013, validation loss 0.000156\n",
      "Iter 26745, training loss 0.000013, validation loss 0.000150\n",
      "Iter 26746, training loss 0.000013, validation loss 0.000157\n",
      "Iter 26747, training loss 0.000012, validation loss 0.000149\n",
      "Iter 26748, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26749, training loss 0.000012, validation loss 0.000149\n",
      "Iter 26750, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26751, training loss 0.000012, validation loss 0.000150\n",
      "Iter 26752, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26753, training loss 0.000012, validation loss 0.000148\n",
      "Iter 26754, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26755, training loss 0.000012, validation loss 0.000150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26756, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26757, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26758, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26759, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26760, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26761, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26762, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26763, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26764, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26765, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26766, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26767, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26768, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26769, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26770, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26771, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26772, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26773, training loss 0.000009, validation loss 0.000152\n",
      "Iter 26774, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26775, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26776, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26777, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26778, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26779, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26780, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26781, training loss 0.000012, validation loss 0.000155\n",
      "Iter 26782, training loss 0.000012, validation loss 0.000150\n",
      "Iter 26783, training loss 0.000012, validation loss 0.000157\n",
      "Iter 26784, training loss 0.000013, validation loss 0.000150\n",
      "Iter 26785, training loss 0.000013, validation loss 0.000157\n",
      "Iter 26786, training loss 0.000013, validation loss 0.000150\n",
      "Iter 26787, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26788, training loss 0.000012, validation loss 0.000150\n",
      "Iter 26789, training loss 0.000012, validation loss 0.000156\n",
      "Iter 26790, training loss 0.000012, validation loss 0.000149\n",
      "Iter 26791, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26792, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26793, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26794, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26795, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26796, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26797, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26798, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26799, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26800, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26801, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26802, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26803, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26804, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26805, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26806, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26807, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26808, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26809, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26810, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26811, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26812, training loss 0.000009, validation loss 0.000152\n",
      "Iter 26813, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26814, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26815, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26816, training loss 0.000010, validation loss 0.000154\n",
      "Iter 26817, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26818, training loss 0.000011, validation loss 0.000155\n",
      "Iter 26819, training loss 0.000012, validation loss 0.000150\n",
      "Iter 26820, training loss 0.000013, validation loss 0.000157\n",
      "Iter 26821, training loss 0.000014, validation loss 0.000150\n",
      "Iter 26822, training loss 0.000015, validation loss 0.000160\n",
      "Iter 26823, training loss 0.000016, validation loss 0.000151\n",
      "Iter 26824, training loss 0.000017, validation loss 0.000163\n",
      "Iter 26825, training loss 0.000020, validation loss 0.000154\n",
      "Iter 26826, training loss 0.000023, validation loss 0.000171\n",
      "Iter 26827, training loss 0.000028, validation loss 0.000161\n",
      "Iter 26828, training loss 0.000034, validation loss 0.000184\n",
      "Iter 26829, training loss 0.000039, validation loss 0.000165\n",
      "Iter 26830, training loss 0.000041, validation loss 0.000190\n",
      "Iter 26831, training loss 0.000042, validation loss 0.000169\n",
      "Iter 26832, training loss 0.000041, validation loss 0.000193\n",
      "Iter 26833, training loss 0.000038, validation loss 0.000166\n",
      "Iter 26834, training loss 0.000032, validation loss 0.000182\n",
      "Iter 26835, training loss 0.000026, validation loss 0.000157\n",
      "Iter 26836, training loss 0.000019, validation loss 0.000162\n",
      "Iter 26837, training loss 0.000014, validation loss 0.000147\n",
      "Iter 26838, training loss 0.000011, validation loss 0.000149\n",
      "Iter 26839, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26840, training loss 0.000013, validation loss 0.000147\n",
      "Iter 26841, training loss 0.000016, validation loss 0.000157\n",
      "Iter 26842, training loss 0.000018, validation loss 0.000150\n",
      "Iter 26843, training loss 0.000019, validation loss 0.000164\n",
      "Iter 26844, training loss 0.000018, validation loss 0.000152\n",
      "Iter 26845, training loss 0.000016, validation loss 0.000160\n",
      "Iter 26846, training loss 0.000013, validation loss 0.000148\n",
      "Iter 26847, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26848, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26849, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26850, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26851, training loss 0.000013, validation loss 0.000149\n",
      "Iter 26852, training loss 0.000013, validation loss 0.000156\n",
      "Iter 26853, training loss 0.000013, validation loss 0.000148\n",
      "Iter 26854, training loss 0.000012, validation loss 0.000153\n",
      "Iter 26855, training loss 0.000011, validation loss 0.000147\n",
      "Iter 26856, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26857, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26858, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26859, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26860, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26861, training loss 0.000011, validation loss 0.000153\n",
      "Iter 26862, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26863, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26864, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26865, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26866, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26867, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26868, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26869, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26870, training loss 0.000010, validation loss 0.000152\n",
      "Iter 26871, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26872, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26873, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26874, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26875, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26876, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26877, training loss 0.000009, validation loss 0.000152\n",
      "Iter 26878, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26879, training loss 0.000009, validation loss 0.000152\n",
      "Iter 26880, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26881, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26882, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26883, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26884, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26885, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26886, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26887, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26888, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26889, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26890, training loss 0.000009, validation loss 0.000151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 26891, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26892, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26893, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26894, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26895, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26896, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26897, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26898, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26899, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26900, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26901, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26902, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26903, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26904, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26905, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26906, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26907, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26908, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26909, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26910, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26911, training loss 0.000009, validation loss 0.000151\n",
      "Iter 26912, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26913, training loss 0.000009, validation loss 0.000152\n",
      "Iter 26914, training loss 0.000009, validation loss 0.000149\n",
      "Iter 26915, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26916, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26917, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26918, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26919, training loss 0.000010, validation loss 0.000154\n",
      "Iter 26920, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26921, training loss 0.000010, validation loss 0.000154\n",
      "Iter 26922, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26923, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26924, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26925, training loss 0.000010, validation loss 0.000154\n",
      "Iter 26926, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26927, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26928, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26929, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26930, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26931, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26932, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26933, training loss 0.000009, validation loss 0.000152\n",
      "Iter 26934, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26935, training loss 0.000009, validation loss 0.000153\n",
      "Iter 26936, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26937, training loss 0.000009, validation loss 0.000152\n",
      "Iter 26938, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26939, training loss 0.000009, validation loss 0.000153\n",
      "Iter 26940, training loss 0.000009, validation loss 0.000150\n",
      "Iter 26941, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26942, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26943, training loss 0.000010, validation loss 0.000153\n",
      "Iter 26944, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26945, training loss 0.000010, validation loss 0.000154\n",
      "Iter 26946, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26947, training loss 0.000011, validation loss 0.000156\n",
      "Iter 26948, training loss 0.000011, validation loss 0.000150\n",
      "Iter 26949, training loss 0.000012, validation loss 0.000157\n",
      "Iter 26950, training loss 0.000012, validation loss 0.000151\n",
      "Iter 26951, training loss 0.000013, validation loss 0.000159\n",
      "Iter 26952, training loss 0.000014, validation loss 0.000151\n",
      "Iter 26953, training loss 0.000015, validation loss 0.000162\n",
      "Iter 26954, training loss 0.000016, validation loss 0.000152\n",
      "Iter 26955, training loss 0.000016, validation loss 0.000163\n",
      "Iter 26956, training loss 0.000017, validation loss 0.000153\n",
      "Iter 26957, training loss 0.000018, validation loss 0.000166\n",
      "Iter 26958, training loss 0.000019, validation loss 0.000152\n",
      "Iter 26959, training loss 0.000020, validation loss 0.000167\n",
      "Iter 26960, training loss 0.000023, validation loss 0.000157\n",
      "Iter 26961, training loss 0.000026, validation loss 0.000175\n",
      "Iter 26962, training loss 0.000028, validation loss 0.000158\n",
      "Iter 26963, training loss 0.000031, validation loss 0.000179\n",
      "Iter 26964, training loss 0.000034, validation loss 0.000164\n",
      "Iter 26965, training loss 0.000037, validation loss 0.000189\n",
      "Iter 26966, training loss 0.000038, validation loss 0.000168\n",
      "Iter 26967, training loss 0.000038, validation loss 0.000190\n",
      "Iter 26968, training loss 0.000035, validation loss 0.000163\n",
      "Iter 26969, training loss 0.000029, validation loss 0.000176\n",
      "Iter 26970, training loss 0.000021, validation loss 0.000152\n",
      "Iter 26971, training loss 0.000016, validation loss 0.000157\n",
      "Iter 26972, training loss 0.000012, validation loss 0.000148\n",
      "Iter 26973, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26974, training loss 0.000013, validation loss 0.000153\n",
      "Iter 26975, training loss 0.000015, validation loss 0.000150\n",
      "Iter 26976, training loss 0.000017, validation loss 0.000159\n",
      "Iter 26977, training loss 0.000018, validation loss 0.000152\n",
      "Iter 26978, training loss 0.000018, validation loss 0.000163\n",
      "Iter 26979, training loss 0.000016, validation loss 0.000152\n",
      "Iter 26980, training loss 0.000014, validation loss 0.000158\n",
      "Iter 26981, training loss 0.000012, validation loss 0.000150\n",
      "Iter 26982, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26983, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26984, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26985, training loss 0.000012, validation loss 0.000153\n",
      "Iter 26986, training loss 0.000012, validation loss 0.000147\n",
      "Iter 26987, training loss 0.000012, validation loss 0.000154\n",
      "Iter 26988, training loss 0.000011, validation loss 0.000148\n",
      "Iter 26989, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26990, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26991, training loss 0.000010, validation loss 0.000148\n",
      "Iter 26992, training loss 0.000010, validation loss 0.000149\n",
      "Iter 26993, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26994, training loss 0.000010, validation loss 0.000151\n",
      "Iter 26995, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26996, training loss 0.000010, validation loss 0.000150\n",
      "Iter 26997, training loss 0.000010, validation loss 0.000147\n",
      "Iter 26998, training loss 0.000009, validation loss 0.000148\n",
      "Iter 26999, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27000, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27001, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27002, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27003, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27004, training loss 0.000010, validation loss 0.000147\n",
      "Iter 27005, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27006, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27007, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27008, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27009, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27010, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27011, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27012, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27013, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27014, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27015, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27016, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27017, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27018, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27019, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27020, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27021, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27022, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27023, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27024, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27025, training loss 0.000009, validation loss 0.000147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27026, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27027, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27028, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27029, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27030, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27031, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27032, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27033, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27034, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27035, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27036, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27037, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27038, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27039, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27040, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27041, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27042, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27043, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27044, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27045, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27046, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27047, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27048, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27049, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27050, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27051, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27052, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27053, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27054, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27055, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27056, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27057, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27058, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27059, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27060, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27061, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27062, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27063, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27064, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27065, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27066, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27067, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27068, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27069, training loss 0.000010, validation loss 0.000151\n",
      "Iter 27070, training loss 0.000011, validation loss 0.000145\n",
      "Iter 27071, training loss 0.000011, validation loss 0.000151\n",
      "Iter 27072, training loss 0.000011, validation loss 0.000145\n",
      "Iter 27073, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27074, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27075, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27076, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27077, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27078, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27079, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27080, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27081, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27082, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27083, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27084, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27085, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27086, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27087, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27088, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27089, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27090, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27091, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27092, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27093, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27094, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27095, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27096, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27097, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27098, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27099, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27100, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27101, training loss 0.000011, validation loss 0.000152\n",
      "Iter 27102, training loss 0.000011, validation loss 0.000146\n",
      "Iter 27103, training loss 0.000012, validation loss 0.000154\n",
      "Iter 27104, training loss 0.000013, validation loss 0.000146\n",
      "Iter 27105, training loss 0.000013, validation loss 0.000155\n",
      "Iter 27106, training loss 0.000014, validation loss 0.000148\n",
      "Iter 27107, training loss 0.000017, validation loss 0.000160\n",
      "Iter 27108, training loss 0.000019, validation loss 0.000150\n",
      "Iter 27109, training loss 0.000022, validation loss 0.000167\n",
      "Iter 27110, training loss 0.000027, validation loss 0.000158\n",
      "Iter 27111, training loss 0.000034, validation loss 0.000183\n",
      "Iter 27112, training loss 0.000044, validation loss 0.000165\n",
      "Iter 27113, training loss 0.000053, validation loss 0.000202\n",
      "Iter 27114, training loss 0.000066, validation loss 0.000187\n",
      "Iter 27115, training loss 0.000074, validation loss 0.000229\n",
      "Iter 27116, training loss 0.000069, validation loss 0.000189\n",
      "Iter 27117, training loss 0.000055, validation loss 0.000201\n",
      "Iter 27118, training loss 0.000038, validation loss 0.000163\n",
      "Iter 27119, training loss 0.000022, validation loss 0.000163\n",
      "Iter 27120, training loss 0.000012, validation loss 0.000145\n",
      "Iter 27121, training loss 0.000013, validation loss 0.000144\n",
      "Iter 27122, training loss 0.000022, validation loss 0.000160\n",
      "Iter 27123, training loss 0.000031, validation loss 0.000159\n",
      "Iter 27124, training loss 0.000034, validation loss 0.000178\n",
      "Iter 27125, training loss 0.000027, validation loss 0.000154\n",
      "Iter 27126, training loss 0.000017, validation loss 0.000154\n",
      "Iter 27127, training loss 0.000011, validation loss 0.000143\n",
      "Iter 27128, training loss 0.000012, validation loss 0.000143\n",
      "Iter 27129, training loss 0.000017, validation loss 0.000156\n",
      "Iter 27130, training loss 0.000021, validation loss 0.000147\n",
      "Iter 27131, training loss 0.000019, validation loss 0.000156\n",
      "Iter 27132, training loss 0.000014, validation loss 0.000143\n",
      "Iter 27133, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27134, training loss 0.000011, validation loss 0.000146\n",
      "Iter 27135, training loss 0.000013, validation loss 0.000143\n",
      "Iter 27136, training loss 0.000015, validation loss 0.000153\n",
      "Iter 27137, training loss 0.000014, validation loss 0.000145\n",
      "Iter 27138, training loss 0.000012, validation loss 0.000149\n",
      "Iter 27139, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27140, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27141, training loss 0.000011, validation loss 0.000147\n",
      "Iter 27142, training loss 0.000012, validation loss 0.000143\n",
      "Iter 27143, training loss 0.000012, validation loss 0.000150\n",
      "Iter 27144, training loss 0.000011, validation loss 0.000144\n",
      "Iter 27145, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27146, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27147, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27148, training loss 0.000011, validation loss 0.000149\n",
      "Iter 27149, training loss 0.000011, validation loss 0.000145\n",
      "Iter 27150, training loss 0.000010, validation loss 0.000148\n",
      "Iter 27151, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27152, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27153, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27154, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27155, training loss 0.000010, validation loss 0.000148\n",
      "Iter 27156, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27157, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27158, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27159, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27160, training loss 0.000009, validation loss 0.000148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27161, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27162, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27163, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27164, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27165, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27166, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27167, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27168, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27169, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27170, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27171, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27172, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27173, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27174, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27175, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27176, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27177, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27178, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27179, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27180, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27181, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27182, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27183, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27184, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27185, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27186, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27187, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27188, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27189, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27190, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27191, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27192, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27193, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27194, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27195, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27196, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27197, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27198, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27199, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27200, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27201, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27202, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27203, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27204, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27205, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27206, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27207, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27208, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27209, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27210, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27211, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27212, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27213, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27214, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27215, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27216, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27217, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27218, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27219, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27220, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27221, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27222, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27223, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27224, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27225, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27226, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27227, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27228, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27229, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27230, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27231, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27232, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27233, training loss 0.000008, validation loss 0.000146\n",
      "Iter 27234, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27235, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27236, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27237, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27238, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27239, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27240, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27241, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27242, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27243, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27244, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27245, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27246, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27247, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27248, training loss 0.000008, validation loss 0.000147\n",
      "Iter 27249, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27250, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27251, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27252, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27253, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27254, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27255, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27256, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27257, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27258, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27259, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27260, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27261, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27262, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27263, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27264, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27265, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27266, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27267, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27268, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27269, training loss 0.000009, validation loss 0.000150\n",
      "Iter 27270, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27271, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27272, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27273, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27274, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27275, training loss 0.000010, validation loss 0.000151\n",
      "Iter 27276, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27277, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27278, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27279, training loss 0.000010, validation loss 0.000151\n",
      "Iter 27280, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27281, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27282, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27283, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27284, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27285, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27286, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27287, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27288, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27289, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27290, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27291, training loss 0.000009, validation loss 0.000149\n",
      "Iter 27292, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27293, training loss 0.000009, validation loss 0.000150\n",
      "Iter 27294, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27295, training loss 0.000009, validation loss 0.000150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27296, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27297, training loss 0.000010, validation loss 0.000151\n",
      "Iter 27298, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27299, training loss 0.000010, validation loss 0.000152\n",
      "Iter 27300, training loss 0.000011, validation loss 0.000146\n",
      "Iter 27301, training loss 0.000011, validation loss 0.000152\n",
      "Iter 27302, training loss 0.000012, validation loss 0.000147\n",
      "Iter 27303, training loss 0.000013, validation loss 0.000155\n",
      "Iter 27304, training loss 0.000013, validation loss 0.000148\n",
      "Iter 27305, training loss 0.000015, validation loss 0.000157\n",
      "Iter 27306, training loss 0.000018, validation loss 0.000149\n",
      "Iter 27307, training loss 0.000021, validation loss 0.000166\n",
      "Iter 27308, training loss 0.000024, validation loss 0.000155\n",
      "Iter 27309, training loss 0.000026, validation loss 0.000172\n",
      "Iter 27310, training loss 0.000027, validation loss 0.000154\n",
      "Iter 27311, training loss 0.000027, validation loss 0.000171\n",
      "Iter 27312, training loss 0.000029, validation loss 0.000159\n",
      "Iter 27313, training loss 0.000029, validation loss 0.000175\n",
      "Iter 27314, training loss 0.000028, validation loss 0.000156\n",
      "Iter 27315, training loss 0.000026, validation loss 0.000169\n",
      "Iter 27316, training loss 0.000022, validation loss 0.000153\n",
      "Iter 27317, training loss 0.000018, validation loss 0.000159\n",
      "Iter 27318, training loss 0.000013, validation loss 0.000144\n",
      "Iter 27319, training loss 0.000010, validation loss 0.000147\n",
      "Iter 27320, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27321, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27322, training loss 0.000012, validation loss 0.000152\n",
      "Iter 27323, training loss 0.000014, validation loss 0.000146\n",
      "Iter 27324, training loss 0.000015, validation loss 0.000156\n",
      "Iter 27325, training loss 0.000015, validation loss 0.000147\n",
      "Iter 27326, training loss 0.000015, validation loss 0.000157\n",
      "Iter 27327, training loss 0.000013, validation loss 0.000145\n",
      "Iter 27328, training loss 0.000011, validation loss 0.000149\n",
      "Iter 27329, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27330, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27331, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27332, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27333, training loss 0.000011, validation loss 0.000150\n",
      "Iter 27334, training loss 0.000012, validation loss 0.000143\n",
      "Iter 27335, training loss 0.000011, validation loss 0.000151\n",
      "Iter 27336, training loss 0.000011, validation loss 0.000144\n",
      "Iter 27337, training loss 0.000010, validation loss 0.000148\n",
      "Iter 27338, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27339, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27340, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27341, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27342, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27343, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27344, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27345, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27346, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27347, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27348, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27349, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27350, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27351, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27352, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27353, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27354, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27355, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27356, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27357, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27358, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27359, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27360, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27361, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27362, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27363, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27364, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27365, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27366, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27367, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27368, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27369, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27370, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27371, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27372, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27373, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27374, training loss 0.000008, validation loss 0.000146\n",
      "Iter 27375, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27376, training loss 0.000008, validation loss 0.000146\n",
      "Iter 27377, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27378, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27379, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27380, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27381, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27382, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27383, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27384, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27385, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27386, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27387, training loss 0.000008, validation loss 0.000146\n",
      "Iter 27388, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27389, training loss 0.000008, validation loss 0.000146\n",
      "Iter 27390, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27391, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27392, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27393, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27394, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27395, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27396, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27397, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27398, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27399, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27400, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27401, training loss 0.000011, validation loss 0.000150\n",
      "Iter 27402, training loss 0.000011, validation loss 0.000144\n",
      "Iter 27403, training loss 0.000011, validation loss 0.000150\n",
      "Iter 27404, training loss 0.000011, validation loss 0.000145\n",
      "Iter 27405, training loss 0.000011, validation loss 0.000152\n",
      "Iter 27406, training loss 0.000012, validation loss 0.000144\n",
      "Iter 27407, training loss 0.000012, validation loss 0.000153\n",
      "Iter 27408, training loss 0.000012, validation loss 0.000145\n",
      "Iter 27409, training loss 0.000013, validation loss 0.000153\n",
      "Iter 27410, training loss 0.000013, validation loss 0.000145\n",
      "Iter 27411, training loss 0.000013, validation loss 0.000154\n",
      "Iter 27412, training loss 0.000013, validation loss 0.000146\n",
      "Iter 27413, training loss 0.000014, validation loss 0.000154\n",
      "Iter 27414, training loss 0.000015, validation loss 0.000146\n",
      "Iter 27415, training loss 0.000016, validation loss 0.000159\n",
      "Iter 27416, training loss 0.000017, validation loss 0.000149\n",
      "Iter 27417, training loss 0.000018, validation loss 0.000161\n",
      "Iter 27418, training loss 0.000018, validation loss 0.000146\n",
      "Iter 27419, training loss 0.000017, validation loss 0.000159\n",
      "Iter 27420, training loss 0.000017, validation loss 0.000148\n",
      "Iter 27421, training loss 0.000017, validation loss 0.000159\n",
      "Iter 27422, training loss 0.000016, validation loss 0.000144\n",
      "Iter 27423, training loss 0.000014, validation loss 0.000155\n",
      "Iter 27424, training loss 0.000013, validation loss 0.000145\n",
      "Iter 27425, training loss 0.000012, validation loss 0.000152\n",
      "Iter 27426, training loss 0.000011, validation loss 0.000143\n",
      "Iter 27427, training loss 0.000010, validation loss 0.000147\n",
      "Iter 27428, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27429, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27430, training loss 0.000009, validation loss 0.000144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27431, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27432, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27433, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27434, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27435, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27436, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27437, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27438, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27439, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27440, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27441, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27442, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27443, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27444, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27445, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27446, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27447, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27448, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27449, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27450, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27451, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27452, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27453, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27454, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27455, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27456, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27457, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27458, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27459, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27460, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27461, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27462, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27463, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27464, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27465, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27466, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27467, training loss 0.000009, validation loss 0.000148\n",
      "Iter 27468, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27469, training loss 0.000010, validation loss 0.000149\n",
      "Iter 27470, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27471, training loss 0.000010, validation loss 0.000150\n",
      "Iter 27472, training loss 0.000011, validation loss 0.000145\n",
      "Iter 27473, training loss 0.000012, validation loss 0.000152\n",
      "Iter 27474, training loss 0.000012, validation loss 0.000145\n",
      "Iter 27475, training loss 0.000014, validation loss 0.000155\n",
      "Iter 27476, training loss 0.000014, validation loss 0.000145\n",
      "Iter 27477, training loss 0.000015, validation loss 0.000157\n",
      "Iter 27478, training loss 0.000017, validation loss 0.000148\n",
      "Iter 27479, training loss 0.000017, validation loss 0.000161\n",
      "Iter 27480, training loss 0.000018, validation loss 0.000146\n",
      "Iter 27481, training loss 0.000018, validation loss 0.000161\n",
      "Iter 27482, training loss 0.000018, validation loss 0.000148\n",
      "Iter 27483, training loss 0.000017, validation loss 0.000159\n",
      "Iter 27484, training loss 0.000016, validation loss 0.000144\n",
      "Iter 27485, training loss 0.000015, validation loss 0.000155\n",
      "Iter 27486, training loss 0.000014, validation loss 0.000147\n",
      "Iter 27487, training loss 0.000014, validation loss 0.000156\n",
      "Iter 27488, training loss 0.000015, validation loss 0.000145\n",
      "Iter 27489, training loss 0.000015, validation loss 0.000156\n",
      "Iter 27490, training loss 0.000016, validation loss 0.000146\n",
      "Iter 27491, training loss 0.000016, validation loss 0.000158\n",
      "Iter 27492, training loss 0.000017, validation loss 0.000146\n",
      "Iter 27493, training loss 0.000017, validation loss 0.000158\n",
      "Iter 27494, training loss 0.000017, validation loss 0.000143\n",
      "Iter 27495, training loss 0.000015, validation loss 0.000154\n",
      "Iter 27496, training loss 0.000013, validation loss 0.000144\n",
      "Iter 27497, training loss 0.000012, validation loss 0.000151\n",
      "Iter 27498, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27499, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27500, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27501, training loss 0.000009, validation loss 0.000142\n",
      "Iter 27502, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27503, training loss 0.000010, validation loss 0.000141\n",
      "Iter 27504, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27505, training loss 0.000011, validation loss 0.000142\n",
      "Iter 27506, training loss 0.000011, validation loss 0.000148\n",
      "Iter 27507, training loss 0.000011, validation loss 0.000142\n",
      "Iter 27508, training loss 0.000011, validation loss 0.000149\n",
      "Iter 27509, training loss 0.000011, validation loss 0.000142\n",
      "Iter 27510, training loss 0.000010, validation loss 0.000148\n",
      "Iter 27511, training loss 0.000010, validation loss 0.000142\n",
      "Iter 27512, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27513, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27514, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27515, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27516, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27517, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27518, training loss 0.000009, validation loss 0.000142\n",
      "Iter 27519, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27520, training loss 0.000009, validation loss 0.000142\n",
      "Iter 27521, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27522, training loss 0.000010, validation loss 0.000142\n",
      "Iter 27523, training loss 0.000010, validation loss 0.000147\n",
      "Iter 27524, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27525, training loss 0.000009, validation loss 0.000147\n",
      "Iter 27526, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27527, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27528, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27529, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27530, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27531, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27532, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27533, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27534, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27535, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27536, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27537, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27538, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27539, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27540, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27541, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27542, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27543, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27544, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27545, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27546, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27547, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27548, training loss 0.000010, validation loss 0.000147\n",
      "Iter 27549, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27550, training loss 0.000010, validation loss 0.000148\n",
      "Iter 27551, training loss 0.000010, validation loss 0.000142\n",
      "Iter 27552, training loss 0.000011, validation loss 0.000149\n",
      "Iter 27553, training loss 0.000011, validation loss 0.000143\n",
      "Iter 27554, training loss 0.000012, validation loss 0.000150\n",
      "Iter 27555, training loss 0.000012, validation loss 0.000144\n",
      "Iter 27556, training loss 0.000014, validation loss 0.000154\n",
      "Iter 27557, training loss 0.000015, validation loss 0.000143\n",
      "Iter 27558, training loss 0.000016, validation loss 0.000157\n",
      "Iter 27559, training loss 0.000020, validation loss 0.000150\n",
      "Iter 27560, training loss 0.000023, validation loss 0.000167\n",
      "Iter 27561, training loss 0.000028, validation loss 0.000150\n",
      "Iter 27562, training loss 0.000036, validation loss 0.000180\n",
      "Iter 27563, training loss 0.000049, validation loss 0.000172\n",
      "Iter 27564, training loss 0.000063, validation loss 0.000213\n",
      "Iter 27565, training loss 0.000072, validation loss 0.000190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27566, training loss 0.000079, validation loss 0.000234\n",
      "Iter 27567, training loss 0.000077, validation loss 0.000189\n",
      "Iter 27568, training loss 0.000055, validation loss 0.000207\n",
      "Iter 27569, training loss 0.000027, validation loss 0.000153\n",
      "Iter 27570, training loss 0.000011, validation loss 0.000145\n",
      "Iter 27571, training loss 0.000014, validation loss 0.000150\n",
      "Iter 27572, training loss 0.000029, validation loss 0.000153\n",
      "Iter 27573, training loss 0.000041, validation loss 0.000191\n",
      "Iter 27574, training loss 0.000042, validation loss 0.000165\n",
      "Iter 27575, training loss 0.000028, validation loss 0.000173\n",
      "Iter 27576, training loss 0.000014, validation loss 0.000143\n",
      "Iter 27577, training loss 0.000010, validation loss 0.000141\n",
      "Iter 27578, training loss 0.000017, validation loss 0.000156\n",
      "Iter 27579, training loss 0.000025, validation loss 0.000149\n",
      "Iter 27580, training loss 0.000026, validation loss 0.000169\n",
      "Iter 27581, training loss 0.000019, validation loss 0.000145\n",
      "Iter 27582, training loss 0.000011, validation loss 0.000147\n",
      "Iter 27583, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27584, training loss 0.000014, validation loss 0.000142\n",
      "Iter 27585, training loss 0.000017, validation loss 0.000156\n",
      "Iter 27586, training loss 0.000017, validation loss 0.000143\n",
      "Iter 27587, training loss 0.000012, validation loss 0.000149\n",
      "Iter 27588, training loss 0.000009, validation loss 0.000140\n",
      "Iter 27589, training loss 0.000010, validation loss 0.000139\n",
      "Iter 27590, training loss 0.000013, validation loss 0.000149\n",
      "Iter 27591, training loss 0.000014, validation loss 0.000142\n",
      "Iter 27592, training loss 0.000013, validation loss 0.000151\n",
      "Iter 27593, training loss 0.000010, validation loss 0.000141\n",
      "Iter 27594, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27595, training loss 0.000010, validation loss 0.000144\n",
      "Iter 27596, training loss 0.000011, validation loss 0.000140\n",
      "Iter 27597, training loss 0.000011, validation loss 0.000148\n",
      "Iter 27598, training loss 0.000010, validation loss 0.000140\n",
      "Iter 27599, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27600, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27601, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27602, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27603, training loss 0.000010, validation loss 0.000140\n",
      "Iter 27604, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27605, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27606, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27607, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27608, training loss 0.000009, validation loss 0.000140\n",
      "Iter 27609, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27610, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27611, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27612, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27613, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27614, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27615, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27616, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27617, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27618, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27619, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27620, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27621, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27622, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27623, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27624, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27625, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27626, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27627, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27628, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27629, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27630, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27631, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27632, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27633, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27634, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27635, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27636, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27637, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27638, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27639, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27640, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27641, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27642, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27643, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27644, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27645, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27646, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27647, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27648, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27649, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27650, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27651, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27652, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27653, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27654, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27655, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27656, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27657, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27658, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27659, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27660, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27661, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27662, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27663, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27664, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27665, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27666, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27667, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27668, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27669, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27670, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27671, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27672, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27673, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27674, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27675, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27676, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27677, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27678, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27679, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27680, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27681, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27682, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27683, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27684, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27685, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27686, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27687, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27688, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27689, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27690, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27691, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27692, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27693, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27694, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27695, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27696, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27697, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27698, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27699, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27700, training loss 0.000008, validation loss 0.000143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27701, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27702, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27703, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27704, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27705, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27706, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27707, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27708, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27709, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27710, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27711, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27712, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27713, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27714, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27715, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27716, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27717, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27718, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27719, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27720, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27721, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27722, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27723, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27724, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27725, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27726, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27727, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27728, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27729, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27730, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27731, training loss 0.000009, validation loss 0.000142\n",
      "Iter 27732, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27733, training loss 0.000009, validation loss 0.000142\n",
      "Iter 27734, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27735, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27736, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27737, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27738, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27739, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27740, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27741, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27742, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27743, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27744, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27745, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27746, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27747, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27748, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27749, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27750, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27751, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27752, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27753, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27754, training loss 0.000008, validation loss 0.000144\n",
      "Iter 27755, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27756, training loss 0.000008, validation loss 0.000145\n",
      "Iter 27757, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27758, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27759, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27760, training loss 0.000009, validation loss 0.000146\n",
      "Iter 27761, training loss 0.000009, validation loss 0.000142\n",
      "Iter 27762, training loss 0.000010, validation loss 0.000148\n",
      "Iter 27763, training loss 0.000010, validation loss 0.000142\n",
      "Iter 27764, training loss 0.000010, validation loss 0.000148\n",
      "Iter 27765, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27766, training loss 0.000011, validation loss 0.000149\n",
      "Iter 27767, training loss 0.000011, validation loss 0.000142\n",
      "Iter 27768, training loss 0.000012, validation loss 0.000152\n",
      "Iter 27769, training loss 0.000012, validation loss 0.000142\n",
      "Iter 27770, training loss 0.000013, validation loss 0.000152\n",
      "Iter 27771, training loss 0.000013, validation loss 0.000144\n",
      "Iter 27772, training loss 0.000014, validation loss 0.000155\n",
      "Iter 27773, training loss 0.000014, validation loss 0.000144\n",
      "Iter 27774, training loss 0.000014, validation loss 0.000155\n",
      "Iter 27775, training loss 0.000015, validation loss 0.000145\n",
      "Iter 27776, training loss 0.000016, validation loss 0.000159\n",
      "Iter 27777, training loss 0.000016, validation loss 0.000148\n",
      "Iter 27778, training loss 0.000019, validation loss 0.000161\n",
      "Iter 27779, training loss 0.000021, validation loss 0.000147\n",
      "Iter 27780, training loss 0.000025, validation loss 0.000171\n",
      "Iter 27781, training loss 0.000028, validation loss 0.000154\n",
      "Iter 27782, training loss 0.000029, validation loss 0.000177\n",
      "Iter 27783, training loss 0.000028, validation loss 0.000150\n",
      "Iter 27784, training loss 0.000024, validation loss 0.000167\n",
      "Iter 27785, training loss 0.000018, validation loss 0.000147\n",
      "Iter 27786, training loss 0.000013, validation loss 0.000154\n",
      "Iter 27787, training loss 0.000010, validation loss 0.000142\n",
      "Iter 27788, training loss 0.000009, validation loss 0.000142\n",
      "Iter 27789, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27790, training loss 0.000012, validation loss 0.000142\n",
      "Iter 27791, training loss 0.000015, validation loss 0.000154\n",
      "Iter 27792, training loss 0.000017, validation loss 0.000143\n",
      "Iter 27793, training loss 0.000018, validation loss 0.000157\n",
      "Iter 27794, training loss 0.000017, validation loss 0.000144\n",
      "Iter 27795, training loss 0.000015, validation loss 0.000156\n",
      "Iter 27796, training loss 0.000013, validation loss 0.000142\n",
      "Iter 27797, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27798, training loss 0.000009, validation loss 0.000139\n",
      "Iter 27799, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27800, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27801, training loss 0.000010, validation loss 0.000140\n",
      "Iter 27802, training loss 0.000011, validation loss 0.000148\n",
      "Iter 27803, training loss 0.000012, validation loss 0.000140\n",
      "Iter 27804, training loss 0.000012, validation loss 0.000150\n",
      "Iter 27805, training loss 0.000011, validation loss 0.000141\n",
      "Iter 27806, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27807, training loss 0.000009, validation loss 0.000140\n",
      "Iter 27808, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27809, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27810, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27811, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27812, training loss 0.000009, validation loss 0.000140\n",
      "Iter 27813, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27814, training loss 0.000009, validation loss 0.000140\n",
      "Iter 27815, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27816, training loss 0.000009, validation loss 0.000139\n",
      "Iter 27817, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27818, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27819, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27820, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27821, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27822, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27823, training loss 0.000008, validation loss 0.000139\n",
      "Iter 27824, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27825, training loss 0.000008, validation loss 0.000139\n",
      "Iter 27826, training loss 0.000008, validation loss 0.000143\n",
      "Iter 27827, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27828, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27829, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27830, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27831, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27832, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27833, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27834, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27835, training loss 0.000008, validation loss 0.000141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27836, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27837, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27838, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27839, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27840, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27841, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27842, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27843, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27844, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27845, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27846, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27847, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27848, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27849, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27850, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27851, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27852, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27853, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27854, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27855, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27856, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27857, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27858, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27859, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27860, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27861, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27862, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27863, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27864, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27865, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27866, training loss 0.000008, validation loss 0.000139\n",
      "Iter 27867, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27868, training loss 0.000009, validation loss 0.000140\n",
      "Iter 27869, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27870, training loss 0.000010, validation loss 0.000139\n",
      "Iter 27871, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27872, training loss 0.000010, validation loss 0.000139\n",
      "Iter 27873, training loss 0.000010, validation loss 0.000147\n",
      "Iter 27874, training loss 0.000011, validation loss 0.000141\n",
      "Iter 27875, training loss 0.000011, validation loss 0.000147\n",
      "Iter 27876, training loss 0.000011, validation loss 0.000140\n",
      "Iter 27877, training loss 0.000012, validation loss 0.000149\n",
      "Iter 27878, training loss 0.000012, validation loss 0.000140\n",
      "Iter 27879, training loss 0.000011, validation loss 0.000147\n",
      "Iter 27880, training loss 0.000011, validation loss 0.000140\n",
      "Iter 27881, training loss 0.000011, validation loss 0.000148\n",
      "Iter 27882, training loss 0.000010, validation loss 0.000140\n",
      "Iter 27883, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27884, training loss 0.000009, validation loss 0.000140\n",
      "Iter 27885, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27886, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27887, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27888, training loss 0.000009, validation loss 0.000139\n",
      "Iter 27889, training loss 0.000009, validation loss 0.000145\n",
      "Iter 27890, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27891, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27892, training loss 0.000010, validation loss 0.000138\n",
      "Iter 27893, training loss 0.000010, validation loss 0.000146\n",
      "Iter 27894, training loss 0.000011, validation loss 0.000141\n",
      "Iter 27895, training loss 0.000011, validation loss 0.000148\n",
      "Iter 27896, training loss 0.000012, validation loss 0.000139\n",
      "Iter 27897, training loss 0.000013, validation loss 0.000150\n",
      "Iter 27898, training loss 0.000013, validation loss 0.000142\n",
      "Iter 27899, training loss 0.000013, validation loss 0.000152\n",
      "Iter 27900, training loss 0.000013, validation loss 0.000139\n",
      "Iter 27901, training loss 0.000013, validation loss 0.000150\n",
      "Iter 27902, training loss 0.000013, validation loss 0.000142\n",
      "Iter 27903, training loss 0.000013, validation loss 0.000150\n",
      "Iter 27904, training loss 0.000013, validation loss 0.000137\n",
      "Iter 27905, training loss 0.000014, validation loss 0.000147\n",
      "Iter 27906, training loss 0.000016, validation loss 0.000142\n",
      "Iter 27907, training loss 0.000019, validation loss 0.000160\n",
      "Iter 27908, training loss 0.000025, validation loss 0.000151\n",
      "Iter 27909, training loss 0.000031, validation loss 0.000172\n",
      "Iter 27910, training loss 0.000038, validation loss 0.000159\n",
      "Iter 27911, training loss 0.000043, validation loss 0.000190\n",
      "Iter 27912, training loss 0.000047, validation loss 0.000168\n",
      "Iter 27913, training loss 0.000042, validation loss 0.000186\n",
      "Iter 27914, training loss 0.000030, validation loss 0.000147\n",
      "Iter 27915, training loss 0.000019, validation loss 0.000151\n",
      "Iter 27916, training loss 0.000012, validation loss 0.000139\n",
      "Iter 27917, training loss 0.000010, validation loss 0.000143\n",
      "Iter 27918, training loss 0.000011, validation loss 0.000146\n",
      "Iter 27919, training loss 0.000015, validation loss 0.000141\n",
      "Iter 27920, training loss 0.000020, validation loss 0.000158\n",
      "Iter 27921, training loss 0.000023, validation loss 0.000149\n",
      "Iter 27922, training loss 0.000022, validation loss 0.000163\n",
      "Iter 27923, training loss 0.000017, validation loss 0.000145\n",
      "Iter 27924, training loss 0.000013, validation loss 0.000146\n",
      "Iter 27925, training loss 0.000009, validation loss 0.000140\n",
      "Iter 27926, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27927, training loss 0.000010, validation loss 0.000147\n",
      "Iter 27928, training loss 0.000011, validation loss 0.000141\n",
      "Iter 27929, training loss 0.000013, validation loss 0.000149\n",
      "Iter 27930, training loss 0.000014, validation loss 0.000139\n",
      "Iter 27931, training loss 0.000013, validation loss 0.000150\n",
      "Iter 27932, training loss 0.000011, validation loss 0.000140\n",
      "Iter 27933, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27934, training loss 0.000008, validation loss 0.000139\n",
      "Iter 27935, training loss 0.000008, validation loss 0.000139\n",
      "Iter 27936, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27937, training loss 0.000010, validation loss 0.000139\n",
      "Iter 27938, training loss 0.000010, validation loss 0.000145\n",
      "Iter 27939, training loss 0.000010, validation loss 0.000138\n",
      "Iter 27940, training loss 0.000009, validation loss 0.000143\n",
      "Iter 27941, training loss 0.000008, validation loss 0.000139\n",
      "Iter 27942, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27943, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27944, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27945, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27946, training loss 0.000009, validation loss 0.000141\n",
      "Iter 27947, training loss 0.000009, validation loss 0.000144\n",
      "Iter 27948, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27949, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27950, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27951, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27952, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27953, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27954, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27955, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27956, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27957, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27958, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27959, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27960, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27961, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27962, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27963, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27964, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27965, training loss 0.000008, validation loss 0.000142\n",
      "Iter 27966, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27967, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27968, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27969, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27970, training loss 0.000008, validation loss 0.000140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 27971, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27972, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27973, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27974, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27975, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27976, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27977, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27978, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27979, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27980, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27981, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27982, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27983, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27984, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27985, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27986, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27987, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27988, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27989, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27990, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27991, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27992, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27993, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27994, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27995, training loss 0.000008, validation loss 0.000140\n",
      "Iter 27996, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27997, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27998, training loss 0.000008, validation loss 0.000141\n",
      "Iter 27999, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28000, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28001, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28002, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28003, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28004, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28005, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28006, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28007, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28008, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28009, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28010, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28011, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28012, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28013, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28014, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28015, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28016, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28017, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28018, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28019, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28020, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28021, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28022, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28023, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28024, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28025, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28026, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28027, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28028, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28029, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28030, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28031, training loss 0.000009, validation loss 0.000140\n",
      "Iter 28032, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28033, training loss 0.000009, validation loss 0.000140\n",
      "Iter 28034, training loss 0.000009, validation loss 0.000146\n",
      "Iter 28035, training loss 0.000010, validation loss 0.000140\n",
      "Iter 28036, training loss 0.000010, validation loss 0.000146\n",
      "Iter 28037, training loss 0.000010, validation loss 0.000141\n",
      "Iter 28038, training loss 0.000010, validation loss 0.000146\n",
      "Iter 28039, training loss 0.000011, validation loss 0.000141\n",
      "Iter 28040, training loss 0.000011, validation loss 0.000149\n",
      "Iter 28041, training loss 0.000012, validation loss 0.000142\n",
      "Iter 28042, training loss 0.000014, validation loss 0.000150\n",
      "Iter 28043, training loss 0.000016, validation loss 0.000142\n",
      "Iter 28044, training loss 0.000017, validation loss 0.000158\n",
      "Iter 28045, training loss 0.000019, validation loss 0.000148\n",
      "Iter 28046, training loss 0.000022, validation loss 0.000160\n",
      "Iter 28047, training loss 0.000024, validation loss 0.000148\n",
      "Iter 28048, training loss 0.000024, validation loss 0.000167\n",
      "Iter 28049, training loss 0.000024, validation loss 0.000153\n",
      "Iter 28050, training loss 0.000025, validation loss 0.000164\n",
      "Iter 28051, training loss 0.000027, validation loss 0.000148\n",
      "Iter 28052, training loss 0.000027, validation loss 0.000169\n",
      "Iter 28053, training loss 0.000024, validation loss 0.000151\n",
      "Iter 28054, training loss 0.000020, validation loss 0.000157\n",
      "Iter 28055, training loss 0.000015, validation loss 0.000139\n",
      "Iter 28056, training loss 0.000011, validation loss 0.000145\n",
      "Iter 28057, training loss 0.000009, validation loss 0.000141\n",
      "Iter 28058, training loss 0.000009, validation loss 0.000140\n",
      "Iter 28059, training loss 0.000010, validation loss 0.000143\n",
      "Iter 28060, training loss 0.000012, validation loss 0.000139\n",
      "Iter 28061, training loss 0.000014, validation loss 0.000149\n",
      "Iter 28062, training loss 0.000014, validation loss 0.000142\n",
      "Iter 28063, training loss 0.000014, validation loss 0.000152\n",
      "Iter 28064, training loss 0.000012, validation loss 0.000141\n",
      "Iter 28065, training loss 0.000010, validation loss 0.000146\n",
      "Iter 28066, training loss 0.000009, validation loss 0.000139\n",
      "Iter 28067, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28068, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28069, training loss 0.000009, validation loss 0.000139\n",
      "Iter 28070, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28071, training loss 0.000010, validation loss 0.000139\n",
      "Iter 28072, training loss 0.000010, validation loss 0.000145\n",
      "Iter 28073, training loss 0.000010, validation loss 0.000140\n",
      "Iter 28074, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28075, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28076, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28077, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28078, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28079, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28080, training loss 0.000009, validation loss 0.000140\n",
      "Iter 28081, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28082, training loss 0.000009, validation loss 0.000139\n",
      "Iter 28083, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28084, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28085, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28086, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28087, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28088, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28089, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28090, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28091, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28092, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28093, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28094, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28095, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28096, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28097, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28098, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28099, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28100, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28101, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28102, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28103, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28104, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28105, training loss 0.000008, validation loss 0.000141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28106, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28107, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28108, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28109, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28110, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28111, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28112, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28113, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28114, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28115, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28116, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28117, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28118, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28119, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28120, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28121, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28122, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28123, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28124, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28125, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28126, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28127, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28128, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28129, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28130, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28131, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28132, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28133, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28134, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28135, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28136, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28137, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28138, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28139, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28140, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28141, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28142, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28143, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28144, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28145, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28146, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28147, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28148, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28149, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28150, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28151, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28152, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28153, training loss 0.000009, validation loss 0.000139\n",
      "Iter 28154, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28155, training loss 0.000009, validation loss 0.000140\n",
      "Iter 28156, training loss 0.000010, validation loss 0.000146\n",
      "Iter 28157, training loss 0.000010, validation loss 0.000139\n",
      "Iter 28158, training loss 0.000011, validation loss 0.000148\n",
      "Iter 28159, training loss 0.000011, validation loss 0.000140\n",
      "Iter 28160, training loss 0.000012, validation loss 0.000149\n",
      "Iter 28161, training loss 0.000013, validation loss 0.000142\n",
      "Iter 28162, training loss 0.000014, validation loss 0.000152\n",
      "Iter 28163, training loss 0.000015, validation loss 0.000142\n",
      "Iter 28164, training loss 0.000016, validation loss 0.000155\n",
      "Iter 28165, training loss 0.000018, validation loss 0.000145\n",
      "Iter 28166, training loss 0.000021, validation loss 0.000160\n",
      "Iter 28167, training loss 0.000027, validation loss 0.000150\n",
      "Iter 28168, training loss 0.000032, validation loss 0.000177\n",
      "Iter 28169, training loss 0.000044, validation loss 0.000173\n",
      "Iter 28170, training loss 0.000069, validation loss 0.000218\n",
      "Iter 28171, training loss 0.000105, validation loss 0.000215\n",
      "Iter 28172, training loss 0.000132, validation loss 0.000296\n",
      "Iter 28173, training loss 0.000130, validation loss 0.000241\n",
      "Iter 28174, training loss 0.000082, validation loss 0.000229\n",
      "Iter 28175, training loss 0.000029, validation loss 0.000146\n",
      "Iter 28176, training loss 0.000013, validation loss 0.000137\n",
      "Iter 28177, training loss 0.000033, validation loss 0.000171\n",
      "Iter 28178, training loss 0.000058, validation loss 0.000182\n",
      "Iter 28179, training loss 0.000062, validation loss 0.000210\n",
      "Iter 28180, training loss 0.000040, validation loss 0.000159\n",
      "Iter 28181, training loss 0.000016, validation loss 0.000150\n",
      "Iter 28182, training loss 0.000014, validation loss 0.000148\n",
      "Iter 28183, training loss 0.000031, validation loss 0.000158\n",
      "Iter 28184, training loss 0.000041, validation loss 0.000187\n",
      "Iter 28185, training loss 0.000030, validation loss 0.000158\n",
      "Iter 28186, training loss 0.000013, validation loss 0.000150\n",
      "Iter 28187, training loss 0.000013, validation loss 0.000149\n",
      "Iter 28188, training loss 0.000024, validation loss 0.000150\n",
      "Iter 28189, training loss 0.000029, validation loss 0.000174\n",
      "Iter 28190, training loss 0.000021, validation loss 0.000148\n",
      "Iter 28191, training loss 0.000010, validation loss 0.000146\n",
      "Iter 28192, training loss 0.000013, validation loss 0.000148\n",
      "Iter 28193, training loss 0.000019, validation loss 0.000145\n",
      "Iter 28194, training loss 0.000019, validation loss 0.000160\n",
      "Iter 28195, training loss 0.000013, validation loss 0.000143\n",
      "Iter 28196, training loss 0.000009, validation loss 0.000143\n",
      "Iter 28197, training loss 0.000013, validation loss 0.000150\n",
      "Iter 28198, training loss 0.000016, validation loss 0.000145\n",
      "Iter 28199, training loss 0.000014, validation loss 0.000152\n",
      "Iter 28200, training loss 0.000009, validation loss 0.000142\n",
      "Iter 28201, training loss 0.000009, validation loss 0.000142\n",
      "Iter 28202, training loss 0.000012, validation loss 0.000150\n",
      "Iter 28203, training loss 0.000013, validation loss 0.000142\n",
      "Iter 28204, training loss 0.000010, validation loss 0.000147\n",
      "Iter 28205, training loss 0.000009, validation loss 0.000142\n",
      "Iter 28206, training loss 0.000010, validation loss 0.000140\n",
      "Iter 28207, training loss 0.000011, validation loss 0.000147\n",
      "Iter 28208, training loss 0.000010, validation loss 0.000141\n",
      "Iter 28209, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28210, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28211, training loss 0.000009, validation loss 0.000142\n",
      "Iter 28212, training loss 0.000010, validation loss 0.000147\n",
      "Iter 28213, training loss 0.000009, validation loss 0.000142\n",
      "Iter 28214, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28215, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28216, training loss 0.000009, validation loss 0.000141\n",
      "Iter 28217, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28218, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28219, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28220, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28221, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28222, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28223, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28224, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28225, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28226, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28227, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28228, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28229, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28230, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28231, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28232, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28233, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28234, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28235, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28236, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28237, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28238, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28239, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28240, training loss 0.000007, validation loss 0.000140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28241, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28242, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28243, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28244, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28245, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28246, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28247, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28248, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28249, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28250, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28251, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28252, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28253, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28254, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28255, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28256, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28257, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28258, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28259, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28260, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28261, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28262, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28263, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28264, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28265, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28266, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28267, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28268, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28269, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28270, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28271, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28272, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28273, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28274, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28275, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28276, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28277, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28278, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28279, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28280, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28281, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28282, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28283, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28284, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28285, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28286, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28287, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28288, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28289, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28290, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28291, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28292, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28293, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28294, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28295, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28296, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28297, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28298, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28299, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28300, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28301, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28302, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28303, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28304, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28305, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28306, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28307, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28308, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28309, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28310, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28311, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28312, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28313, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28314, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28315, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28316, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28317, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28318, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28319, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28320, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28321, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28322, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28323, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28324, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28325, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28326, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28327, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28328, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28329, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28330, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28331, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28332, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28333, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28334, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28335, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28336, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28337, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28338, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28339, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28340, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28341, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28342, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28343, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28344, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28345, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28346, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28347, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28348, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28349, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28350, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28351, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28352, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28353, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28354, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28355, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28356, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28357, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28358, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28359, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28360, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28361, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28362, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28363, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28364, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28365, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28366, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28367, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28368, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28369, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28370, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28371, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28372, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28373, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28374, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28375, training loss 0.000007, validation loss 0.000142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28376, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28377, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28378, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28379, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28380, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28381, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28382, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28383, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28384, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28385, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28386, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28387, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28388, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28389, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28390, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28391, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28392, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28393, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28394, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28395, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28396, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28397, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28398, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28399, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28400, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28401, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28402, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28403, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28404, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28405, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28406, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28407, training loss 0.000007, validation loss 0.000144\n",
      "Iter 28408, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28409, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28410, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28411, training loss 0.000008, validation loss 0.000145\n",
      "Iter 28412, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28413, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28414, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28415, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28416, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28417, training loss 0.000007, validation loss 0.000144\n",
      "Iter 28418, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28419, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28420, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28421, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28422, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28423, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28424, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28425, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28426, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28427, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28428, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28429, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28430, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28431, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28432, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28433, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28434, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28435, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28436, training loss 0.000009, validation loss 0.000147\n",
      "Iter 28437, training loss 0.000009, validation loss 0.000140\n",
      "Iter 28438, training loss 0.000009, validation loss 0.000145\n",
      "Iter 28439, training loss 0.000009, validation loss 0.000142\n",
      "Iter 28440, training loss 0.000009, validation loss 0.000146\n",
      "Iter 28441, training loss 0.000009, validation loss 0.000141\n",
      "Iter 28442, training loss 0.000009, validation loss 0.000148\n",
      "Iter 28443, training loss 0.000009, validation loss 0.000141\n",
      "Iter 28444, training loss 0.000009, validation loss 0.000145\n",
      "Iter 28445, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28446, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28447, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28448, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28449, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28450, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28451, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28452, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28453, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28454, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28455, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28456, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28457, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28458, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28459, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28460, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28461, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28462, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28463, training loss 0.000008, validation loss 0.000145\n",
      "Iter 28464, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28465, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28466, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28467, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28468, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28469, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28470, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28471, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28472, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28473, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28474, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28475, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28476, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28477, training loss 0.000008, validation loss 0.000145\n",
      "Iter 28478, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28479, training loss 0.000008, validation loss 0.000145\n",
      "Iter 28480, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28481, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28482, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28483, training loss 0.000009, validation loss 0.000146\n",
      "Iter 28484, training loss 0.000009, validation loss 0.000140\n",
      "Iter 28485, training loss 0.000009, validation loss 0.000146\n",
      "Iter 28486, training loss 0.000009, validation loss 0.000141\n",
      "Iter 28487, training loss 0.000009, validation loss 0.000146\n",
      "Iter 28488, training loss 0.000009, validation loss 0.000141\n",
      "Iter 28489, training loss 0.000010, validation loss 0.000148\n",
      "Iter 28490, training loss 0.000010, validation loss 0.000141\n",
      "Iter 28491, training loss 0.000010, validation loss 0.000148\n",
      "Iter 28492, training loss 0.000010, validation loss 0.000141\n",
      "Iter 28493, training loss 0.000011, validation loss 0.000150\n",
      "Iter 28494, training loss 0.000010, validation loss 0.000141\n",
      "Iter 28495, training loss 0.000010, validation loss 0.000148\n",
      "Iter 28496, training loss 0.000010, validation loss 0.000142\n",
      "Iter 28497, training loss 0.000010, validation loss 0.000148\n",
      "Iter 28498, training loss 0.000009, validation loss 0.000140\n",
      "Iter 28499, training loss 0.000009, validation loss 0.000146\n",
      "Iter 28500, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28501, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28502, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28503, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28504, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28505, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28506, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28507, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28508, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28509, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28510, training loss 0.000007, validation loss 0.000141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28511, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28512, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28513, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28514, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28515, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28516, training loss 0.000007, validation loss 0.000142\n",
      "Iter 28517, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28518, training loss 0.000007, validation loss 0.000143\n",
      "Iter 28519, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28520, training loss 0.000008, validation loss 0.000144\n",
      "Iter 28521, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28522, training loss 0.000009, validation loss 0.000146\n",
      "Iter 28523, training loss 0.000010, validation loss 0.000141\n",
      "Iter 28524, training loss 0.000010, validation loss 0.000149\n",
      "Iter 28525, training loss 0.000011, validation loss 0.000140\n",
      "Iter 28526, training loss 0.000012, validation loss 0.000151\n",
      "Iter 28527, training loss 0.000013, validation loss 0.000142\n",
      "Iter 28528, training loss 0.000014, validation loss 0.000153\n",
      "Iter 28529, training loss 0.000016, validation loss 0.000144\n",
      "Iter 28530, training loss 0.000019, validation loss 0.000160\n",
      "Iter 28531, training loss 0.000022, validation loss 0.000150\n",
      "Iter 28532, training loss 0.000031, validation loss 0.000173\n",
      "Iter 28533, training loss 0.000045, validation loss 0.000162\n",
      "Iter 28534, training loss 0.000062, validation loss 0.000212\n",
      "Iter 28535, training loss 0.000076, validation loss 0.000194\n",
      "Iter 28536, training loss 0.000081, validation loss 0.000236\n",
      "Iter 28537, training loss 0.000068, validation loss 0.000183\n",
      "Iter 28538, training loss 0.000038, validation loss 0.000184\n",
      "Iter 28539, training loss 0.000014, validation loss 0.000141\n",
      "Iter 28540, training loss 0.000010, validation loss 0.000137\n",
      "Iter 28541, training loss 0.000023, validation loss 0.000160\n",
      "Iter 28542, training loss 0.000040, validation loss 0.000160\n",
      "Iter 28543, training loss 0.000045, validation loss 0.000191\n",
      "Iter 28544, training loss 0.000033, validation loss 0.000151\n",
      "Iter 28545, training loss 0.000016, validation loss 0.000151\n",
      "Iter 28546, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28547, training loss 0.000014, validation loss 0.000140\n",
      "Iter 28548, training loss 0.000024, validation loss 0.000161\n",
      "Iter 28549, training loss 0.000027, validation loss 0.000148\n",
      "Iter 28550, training loss 0.000021, validation loss 0.000160\n",
      "Iter 28551, training loss 0.000013, validation loss 0.000139\n",
      "Iter 28552, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28553, training loss 0.000011, validation loss 0.000145\n",
      "Iter 28554, training loss 0.000016, validation loss 0.000139\n",
      "Iter 28555, training loss 0.000017, validation loss 0.000152\n",
      "Iter 28556, training loss 0.000013, validation loss 0.000138\n",
      "Iter 28557, training loss 0.000009, validation loss 0.000141\n",
      "Iter 28558, training loss 0.000008, validation loss 0.000138\n",
      "Iter 28559, training loss 0.000011, validation loss 0.000135\n",
      "Iter 28560, training loss 0.000013, validation loss 0.000147\n",
      "Iter 28561, training loss 0.000012, validation loss 0.000138\n",
      "Iter 28562, training loss 0.000010, validation loss 0.000143\n",
      "Iter 28563, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28564, training loss 0.000008, validation loss 0.000135\n",
      "Iter 28565, training loss 0.000010, validation loss 0.000143\n",
      "Iter 28566, training loss 0.000011, validation loss 0.000138\n",
      "Iter 28567, training loss 0.000010, validation loss 0.000144\n",
      "Iter 28568, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28569, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28570, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28571, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28572, training loss 0.000009, validation loss 0.000143\n",
      "Iter 28573, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28574, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28575, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28576, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28577, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28578, training loss 0.000008, validation loss 0.000136\n",
      "Iter 28579, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28580, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28581, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28582, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28583, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28584, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28585, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28586, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28587, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28588, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28589, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28590, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28591, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28592, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28593, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28594, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28595, training loss 0.000007, validation loss 0.000137\n",
      "Iter 28596, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28597, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28598, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28599, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28600, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28601, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28602, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28603, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28604, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28605, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28606, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28607, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28608, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28609, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28610, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28611, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28612, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28613, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28614, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28615, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28616, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28617, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28618, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28619, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28620, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28621, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28622, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28623, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28624, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28625, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28626, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28627, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28628, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28629, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28630, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28631, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28632, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28633, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28634, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28635, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28636, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28637, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28638, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28639, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28640, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28641, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28642, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28643, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28644, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28645, training loss 0.000007, validation loss 0.000138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28646, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28647, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28648, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28649, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28650, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28651, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28652, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28653, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28654, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28655, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28656, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28657, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28658, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28659, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28660, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28661, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28662, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28663, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28664, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28665, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28666, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28667, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28668, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28669, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28670, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28671, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28672, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28673, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28674, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28675, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28676, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28677, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28678, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28679, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28680, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28681, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28682, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28683, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28684, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28685, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28686, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28687, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28688, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28689, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28690, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28691, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28692, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28693, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28694, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28695, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28696, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28697, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28698, training loss 0.000007, validation loss 0.000137\n",
      "Iter 28699, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28700, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28701, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28702, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28703, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28704, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28705, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28706, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28707, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28708, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28709, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28710, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28711, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28712, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28713, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28714, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28715, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28716, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28717, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28718, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28719, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28720, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28721, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28722, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28723, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28724, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28725, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28726, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28727, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28728, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28729, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28730, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28731, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28732, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28733, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28734, training loss 0.000009, validation loss 0.000145\n",
      "Iter 28735, training loss 0.000009, validation loss 0.000136\n",
      "Iter 28736, training loss 0.000009, validation loss 0.000142\n",
      "Iter 28737, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28738, training loss 0.000009, validation loss 0.000143\n",
      "Iter 28739, training loss 0.000009, validation loss 0.000136\n",
      "Iter 28740, training loss 0.000009, validation loss 0.000143\n",
      "Iter 28741, training loss 0.000009, validation loss 0.000139\n",
      "Iter 28742, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28743, training loss 0.000009, validation loss 0.000137\n",
      "Iter 28744, training loss 0.000009, validation loss 0.000143\n",
      "Iter 28745, training loss 0.000009, validation loss 0.000139\n",
      "Iter 28746, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28747, training loss 0.000009, validation loss 0.000136\n",
      "Iter 28748, training loss 0.000009, validation loss 0.000143\n",
      "Iter 28749, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28750, training loss 0.000009, validation loss 0.000143\n",
      "Iter 28751, training loss 0.000008, validation loss 0.000136\n",
      "Iter 28752, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28753, training loss 0.000008, validation loss 0.000138\n",
      "Iter 28754, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28755, training loss 0.000007, validation loss 0.000136\n",
      "Iter 28756, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28757, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28758, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28759, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28760, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28761, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28762, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28763, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28764, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28765, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28766, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28767, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28768, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28769, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28770, training loss 0.000007, validation loss 0.000137\n",
      "Iter 28771, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28772, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28773, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28774, training loss 0.000007, validation loss 0.000137\n",
      "Iter 28775, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28776, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28777, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28778, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28779, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28780, training loss 0.000009, validation loss 0.000136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28781, training loss 0.000009, validation loss 0.000143\n",
      "Iter 28782, training loss 0.000009, validation loss 0.000137\n",
      "Iter 28783, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28784, training loss 0.000009, validation loss 0.000137\n",
      "Iter 28785, training loss 0.000010, validation loss 0.000145\n",
      "Iter 28786, training loss 0.000010, validation loss 0.000136\n",
      "Iter 28787, training loss 0.000010, validation loss 0.000144\n",
      "Iter 28788, training loss 0.000011, validation loss 0.000139\n",
      "Iter 28789, training loss 0.000012, validation loss 0.000148\n",
      "Iter 28790, training loss 0.000014, validation loss 0.000137\n",
      "Iter 28791, training loss 0.000015, validation loss 0.000153\n",
      "Iter 28792, training loss 0.000018, validation loss 0.000143\n",
      "Iter 28793, training loss 0.000021, validation loss 0.000161\n",
      "Iter 28794, training loss 0.000027, validation loss 0.000146\n",
      "Iter 28795, training loss 0.000032, validation loss 0.000173\n",
      "Iter 28796, training loss 0.000033, validation loss 0.000152\n",
      "Iter 28797, training loss 0.000030, validation loss 0.000172\n",
      "Iter 28798, training loss 0.000025, validation loss 0.000147\n",
      "Iter 28799, training loss 0.000019, validation loss 0.000157\n",
      "Iter 28800, training loss 0.000013, validation loss 0.000138\n",
      "Iter 28801, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28802, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28803, training loss 0.000008, validation loss 0.000138\n",
      "Iter 28804, training loss 0.000010, validation loss 0.000143\n",
      "Iter 28805, training loss 0.000012, validation loss 0.000139\n",
      "Iter 28806, training loss 0.000014, validation loss 0.000153\n",
      "Iter 28807, training loss 0.000015, validation loss 0.000143\n",
      "Iter 28808, training loss 0.000015, validation loss 0.000154\n",
      "Iter 28809, training loss 0.000014, validation loss 0.000140\n",
      "Iter 28810, training loss 0.000012, validation loss 0.000148\n",
      "Iter 28811, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28812, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28813, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28814, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28815, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28816, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28817, training loss 0.000010, validation loss 0.000145\n",
      "Iter 28818, training loss 0.000010, validation loss 0.000138\n",
      "Iter 28819, training loss 0.000009, validation loss 0.000145\n",
      "Iter 28820, training loss 0.000009, validation loss 0.000138\n",
      "Iter 28821, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28822, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28823, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28824, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28825, training loss 0.000008, validation loss 0.000138\n",
      "Iter 28826, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28827, training loss 0.000008, validation loss 0.000138\n",
      "Iter 28828, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28829, training loss 0.000008, validation loss 0.000138\n",
      "Iter 28830, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28831, training loss 0.000007, validation loss 0.000137\n",
      "Iter 28832, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28833, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28834, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28835, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28836, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28837, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28838, training loss 0.000008, validation loss 0.000137\n",
      "Iter 28839, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28840, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28841, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28842, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28843, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28844, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28845, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28846, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28847, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28848, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28849, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28850, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28851, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28852, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28853, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28854, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28855, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28856, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28857, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28858, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28859, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28860, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28861, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28862, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28863, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28864, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28865, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28866, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28867, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28868, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28869, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28870, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28871, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28872, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28873, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28874, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28875, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28876, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28877, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28878, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28879, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28880, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28881, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28882, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28883, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28884, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28885, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28886, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28887, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28888, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28889, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28890, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28891, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28892, training loss 0.000007, validation loss 0.000138\n",
      "Iter 28893, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28894, training loss 0.000008, validation loss 0.000138\n",
      "Iter 28895, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28896, training loss 0.000008, validation loss 0.000138\n",
      "Iter 28897, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28898, training loss 0.000009, validation loss 0.000137\n",
      "Iter 28899, training loss 0.000009, validation loss 0.000145\n",
      "Iter 28900, training loss 0.000011, validation loss 0.000140\n",
      "Iter 28901, training loss 0.000012, validation loss 0.000149\n",
      "Iter 28902, training loss 0.000013, validation loss 0.000138\n",
      "Iter 28903, training loss 0.000015, validation loss 0.000153\n",
      "Iter 28904, training loss 0.000019, validation loss 0.000147\n",
      "Iter 28905, training loss 0.000027, validation loss 0.000170\n",
      "Iter 28906, training loss 0.000037, validation loss 0.000154\n",
      "Iter 28907, training loss 0.000046, validation loss 0.000193\n",
      "Iter 28908, training loss 0.000058, validation loss 0.000181\n",
      "Iter 28909, training loss 0.000072, validation loss 0.000225\n",
      "Iter 28910, training loss 0.000082, validation loss 0.000194\n",
      "Iter 28911, training loss 0.000075, validation loss 0.000230\n",
      "Iter 28912, training loss 0.000048, validation loss 0.000164\n",
      "Iter 28913, training loss 0.000022, validation loss 0.000155\n",
      "Iter 28914, training loss 0.000011, validation loss 0.000137\n",
      "Iter 28915, training loss 0.000016, validation loss 0.000144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 28916, training loss 0.000029, validation loss 0.000174\n",
      "Iter 28917, training loss 0.000040, validation loss 0.000162\n",
      "Iter 28918, training loss 0.000038, validation loss 0.000183\n",
      "Iter 28919, training loss 0.000024, validation loss 0.000147\n",
      "Iter 28920, training loss 0.000011, validation loss 0.000145\n",
      "Iter 28921, training loss 0.000011, validation loss 0.000144\n",
      "Iter 28922, training loss 0.000018, validation loss 0.000145\n",
      "Iter 28923, training loss 0.000026, validation loss 0.000164\n",
      "Iter 28924, training loss 0.000026, validation loss 0.000150\n",
      "Iter 28925, training loss 0.000017, validation loss 0.000156\n",
      "Iter 28926, training loss 0.000010, validation loss 0.000139\n",
      "Iter 28927, training loss 0.000010, validation loss 0.000139\n",
      "Iter 28928, training loss 0.000014, validation loss 0.000151\n",
      "Iter 28929, training loss 0.000018, validation loss 0.000144\n",
      "Iter 28930, training loss 0.000016, validation loss 0.000152\n",
      "Iter 28931, training loss 0.000011, validation loss 0.000138\n",
      "Iter 28932, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28933, training loss 0.000009, validation loss 0.000145\n",
      "Iter 28934, training loss 0.000012, validation loss 0.000142\n",
      "Iter 28935, training loss 0.000013, validation loss 0.000150\n",
      "Iter 28936, training loss 0.000011, validation loss 0.000139\n",
      "Iter 28937, training loss 0.000009, validation loss 0.000141\n",
      "Iter 28938, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28939, training loss 0.000009, validation loss 0.000139\n",
      "Iter 28940, training loss 0.000010, validation loss 0.000147\n",
      "Iter 28941, training loss 0.000010, validation loss 0.000139\n",
      "Iter 28942, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28943, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28944, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28945, training loss 0.000009, validation loss 0.000144\n",
      "Iter 28946, training loss 0.000009, validation loss 0.000139\n",
      "Iter 28947, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28948, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28949, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28950, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28951, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28952, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28953, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28954, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28955, training loss 0.000008, validation loss 0.000141\n",
      "Iter 28956, training loss 0.000008, validation loss 0.000139\n",
      "Iter 28957, training loss 0.000008, validation loss 0.000143\n",
      "Iter 28958, training loss 0.000008, validation loss 0.000140\n",
      "Iter 28959, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28960, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28961, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28962, training loss 0.000008, validation loss 0.000142\n",
      "Iter 28963, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28964, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28965, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28966, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28967, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28968, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28969, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28970, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28971, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28972, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28973, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28974, training loss 0.000007, validation loss 0.000141\n",
      "Iter 28975, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28976, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28977, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28978, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28979, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28980, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28981, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28982, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28983, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28984, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28985, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28986, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28987, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28988, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28989, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28990, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28991, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28992, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28993, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28994, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28995, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28996, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28997, training loss 0.000007, validation loss 0.000140\n",
      "Iter 28998, training loss 0.000007, validation loss 0.000139\n",
      "Iter 28999, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29000, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29001, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29002, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29003, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29004, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29005, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29006, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29007, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29008, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29009, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29010, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29011, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29012, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29013, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29014, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29015, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29016, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29017, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29018, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29019, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29020, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29021, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29022, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29023, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29024, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29025, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29026, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29027, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29028, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29029, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29030, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29031, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29032, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29033, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29034, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29035, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29036, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29037, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29038, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29039, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29040, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29041, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29042, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29043, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29044, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29045, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29046, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29047, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29048, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29049, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29050, training loss 0.000007, validation loss 0.000140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29051, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29052, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29053, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29054, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29055, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29056, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29057, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29058, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29059, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29060, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29061, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29062, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29063, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29064, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29065, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29066, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29067, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29068, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29069, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29070, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29071, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29072, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29073, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29074, training loss 0.000007, validation loss 0.000142\n",
      "Iter 29075, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29076, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29077, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29078, training loss 0.000007, validation loss 0.000142\n",
      "Iter 29079, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29080, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29081, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29082, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29083, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29084, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29085, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29086, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29087, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29088, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29089, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29090, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29091, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29092, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29093, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29094, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29095, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29096, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29097, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29098, training loss 0.000007, validation loss 0.000142\n",
      "Iter 29099, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29100, training loss 0.000008, validation loss 0.000142\n",
      "Iter 29101, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29102, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29103, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29104, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29105, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29106, training loss 0.000008, validation loss 0.000142\n",
      "Iter 29107, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29108, training loss 0.000008, validation loss 0.000144\n",
      "Iter 29109, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29110, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29111, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29112, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29113, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29114, training loss 0.000008, validation loss 0.000144\n",
      "Iter 29115, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29116, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29117, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29118, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29119, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29120, training loss 0.000008, validation loss 0.000144\n",
      "Iter 29121, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29122, training loss 0.000008, validation loss 0.000142\n",
      "Iter 29123, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29124, training loss 0.000007, validation loss 0.000143\n",
      "Iter 29125, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29126, training loss 0.000007, validation loss 0.000143\n",
      "Iter 29127, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29128, training loss 0.000007, validation loss 0.000142\n",
      "Iter 29129, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29130, training loss 0.000007, validation loss 0.000142\n",
      "Iter 29131, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29132, training loss 0.000007, validation loss 0.000142\n",
      "Iter 29133, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29134, training loss 0.000007, validation loss 0.000142\n",
      "Iter 29135, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29136, training loss 0.000007, validation loss 0.000142\n",
      "Iter 29137, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29138, training loss 0.000007, validation loss 0.000143\n",
      "Iter 29139, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29140, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29141, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29142, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29143, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29144, training loss 0.000009, validation loss 0.000145\n",
      "Iter 29145, training loss 0.000009, validation loss 0.000138\n",
      "Iter 29146, training loss 0.000009, validation loss 0.000146\n",
      "Iter 29147, training loss 0.000009, validation loss 0.000140\n",
      "Iter 29148, training loss 0.000010, validation loss 0.000147\n",
      "Iter 29149, training loss 0.000011, validation loss 0.000140\n",
      "Iter 29150, training loss 0.000012, validation loss 0.000151\n",
      "Iter 29151, training loss 0.000013, validation loss 0.000140\n",
      "Iter 29152, training loss 0.000014, validation loss 0.000153\n",
      "Iter 29153, training loss 0.000015, validation loss 0.000142\n",
      "Iter 29154, training loss 0.000016, validation loss 0.000156\n",
      "Iter 29155, training loss 0.000017, validation loss 0.000142\n",
      "Iter 29156, training loss 0.000018, validation loss 0.000160\n",
      "Iter 29157, training loss 0.000020, validation loss 0.000146\n",
      "Iter 29158, training loss 0.000026, validation loss 0.000167\n",
      "Iter 29159, training loss 0.000035, validation loss 0.000152\n",
      "Iter 29160, training loss 0.000044, validation loss 0.000194\n",
      "Iter 29161, training loss 0.000048, validation loss 0.000165\n",
      "Iter 29162, training loss 0.000049, validation loss 0.000196\n",
      "Iter 29163, training loss 0.000048, validation loss 0.000163\n",
      "Iter 29164, training loss 0.000042, validation loss 0.000192\n",
      "Iter 29165, training loss 0.000028, validation loss 0.000143\n",
      "Iter 29166, training loss 0.000015, validation loss 0.000149\n",
      "Iter 29167, training loss 0.000010, validation loss 0.000136\n",
      "Iter 29168, training loss 0.000011, validation loss 0.000139\n",
      "Iter 29169, training loss 0.000017, validation loss 0.000156\n",
      "Iter 29170, training loss 0.000022, validation loss 0.000145\n",
      "Iter 29171, training loss 0.000024, validation loss 0.000165\n",
      "Iter 29172, training loss 0.000022, validation loss 0.000144\n",
      "Iter 29173, training loss 0.000016, validation loss 0.000156\n",
      "Iter 29174, training loss 0.000011, validation loss 0.000137\n",
      "Iter 29175, training loss 0.000009, validation loss 0.000137\n",
      "Iter 29176, training loss 0.000011, validation loss 0.000144\n",
      "Iter 29177, training loss 0.000015, validation loss 0.000138\n",
      "Iter 29178, training loss 0.000017, validation loss 0.000158\n",
      "Iter 29179, training loss 0.000017, validation loss 0.000140\n",
      "Iter 29180, training loss 0.000013, validation loss 0.000151\n",
      "Iter 29181, training loss 0.000009, validation loss 0.000137\n",
      "Iter 29182, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29183, training loss 0.000009, validation loss 0.000142\n",
      "Iter 29184, training loss 0.000010, validation loss 0.000137\n",
      "Iter 29185, training loss 0.000011, validation loss 0.000147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29186, training loss 0.000011, validation loss 0.000137\n",
      "Iter 29187, training loss 0.000010, validation loss 0.000145\n",
      "Iter 29188, training loss 0.000008, validation loss 0.000137\n",
      "Iter 29189, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29190, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29191, training loss 0.000008, validation loss 0.000136\n",
      "Iter 29192, training loss 0.000009, validation loss 0.000144\n",
      "Iter 29193, training loss 0.000009, validation loss 0.000137\n",
      "Iter 29194, training loss 0.000009, validation loss 0.000143\n",
      "Iter 29195, training loss 0.000008, validation loss 0.000136\n",
      "Iter 29196, training loss 0.000007, validation loss 0.000138\n",
      "Iter 29197, training loss 0.000007, validation loss 0.000138\n",
      "Iter 29198, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29199, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29200, training loss 0.000008, validation loss 0.000136\n",
      "Iter 29201, training loss 0.000008, validation loss 0.000142\n",
      "Iter 29202, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29203, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29204, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29205, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29206, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29207, training loss 0.000007, validation loss 0.000138\n",
      "Iter 29208, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29209, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29210, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29211, training loss 0.000007, validation loss 0.000138\n",
      "Iter 29212, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29213, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29214, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29215, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29216, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29217, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29218, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29219, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29220, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29221, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29222, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29223, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29224, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29225, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29226, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29227, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29228, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29229, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29230, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29231, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29232, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29233, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29234, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29235, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29236, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29237, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29238, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29239, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29240, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29241, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29242, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29243, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29244, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29245, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29246, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29247, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29248, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29249, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29250, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29251, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29252, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29253, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29254, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29255, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29256, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29257, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29258, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29259, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29260, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29261, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29262, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29263, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29264, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29265, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29266, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29267, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29268, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29269, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29270, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29271, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29272, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29273, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29274, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29275, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29276, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29277, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29278, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29279, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29280, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29281, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29282, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29283, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29284, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29285, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29286, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29287, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29288, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29289, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29290, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29291, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29292, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29293, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29294, training loss 0.000007, validation loss 0.000135\n",
      "Iter 29295, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29296, training loss 0.000007, validation loss 0.000135\n",
      "Iter 29297, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29298, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29299, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29300, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29301, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29302, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29303, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29304, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29305, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29306, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29307, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29308, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29309, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29310, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29311, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29312, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29313, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29314, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29315, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29316, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29317, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29318, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29319, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29320, training loss 0.000006, validation loss 0.000136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29321, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29322, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29323, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29324, training loss 0.000008, validation loss 0.000136\n",
      "Iter 29325, training loss 0.000009, validation loss 0.000144\n",
      "Iter 29326, training loss 0.000010, validation loss 0.000136\n",
      "Iter 29327, training loss 0.000011, validation loss 0.000148\n",
      "Iter 29328, training loss 0.000012, validation loss 0.000136\n",
      "Iter 29329, training loss 0.000013, validation loss 0.000149\n",
      "Iter 29330, training loss 0.000014, validation loss 0.000140\n",
      "Iter 29331, training loss 0.000016, validation loss 0.000155\n",
      "Iter 29332, training loss 0.000018, validation loss 0.000138\n",
      "Iter 29333, training loss 0.000020, validation loss 0.000158\n",
      "Iter 29334, training loss 0.000026, validation loss 0.000149\n",
      "Iter 29335, training loss 0.000033, validation loss 0.000178\n",
      "Iter 29336, training loss 0.000038, validation loss 0.000154\n",
      "Iter 29337, training loss 0.000040, validation loss 0.000183\n",
      "Iter 29338, training loss 0.000042, validation loss 0.000160\n",
      "Iter 29339, training loss 0.000037, validation loss 0.000183\n",
      "Iter 29340, training loss 0.000024, validation loss 0.000144\n",
      "Iter 29341, training loss 0.000015, validation loss 0.000148\n",
      "Iter 29342, training loss 0.000008, validation loss 0.000135\n",
      "Iter 29343, training loss 0.000008, validation loss 0.000138\n",
      "Iter 29344, training loss 0.000011, validation loss 0.000149\n",
      "Iter 29345, training loss 0.000016, validation loss 0.000138\n",
      "Iter 29346, training loss 0.000020, validation loss 0.000157\n",
      "Iter 29347, training loss 0.000021, validation loss 0.000142\n",
      "Iter 29348, training loss 0.000020, validation loss 0.000159\n",
      "Iter 29349, training loss 0.000014, validation loss 0.000135\n",
      "Iter 29350, training loss 0.000009, validation loss 0.000138\n",
      "Iter 29351, training loss 0.000007, validation loss 0.000132\n",
      "Iter 29352, training loss 0.000008, validation loss 0.000133\n",
      "Iter 29353, training loss 0.000010, validation loss 0.000143\n",
      "Iter 29354, training loss 0.000012, validation loss 0.000136\n",
      "Iter 29355, training loss 0.000012, validation loss 0.000147\n",
      "Iter 29356, training loss 0.000011, validation loss 0.000136\n",
      "Iter 29357, training loss 0.000008, validation loss 0.000142\n",
      "Iter 29358, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29359, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29360, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29361, training loss 0.000009, validation loss 0.000135\n",
      "Iter 29362, training loss 0.000009, validation loss 0.000144\n",
      "Iter 29363, training loss 0.000009, validation loss 0.000136\n",
      "Iter 29364, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29365, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29366, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29367, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29368, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29369, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29370, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29371, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29372, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29373, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29374, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29375, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29376, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29377, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29378, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29379, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29380, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29381, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29382, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29383, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29384, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29385, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29386, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29387, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29388, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29389, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29390, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29391, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29392, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29393, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29394, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29395, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29396, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29397, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29398, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29399, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29400, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29401, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29402, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29403, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29404, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29405, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29406, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29407, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29408, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29409, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29410, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29411, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29412, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29413, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29414, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29415, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29416, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29417, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29418, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29419, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29420, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29421, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29422, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29423, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29424, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29425, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29426, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29427, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29428, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29429, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29430, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29431, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29432, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29433, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29434, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29435, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29436, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29437, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29438, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29439, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29440, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29441, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29442, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29443, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29444, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29445, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29446, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29447, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29448, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29449, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29450, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29451, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29452, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29453, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29454, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29455, training loss 0.000006, validation loss 0.000137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29456, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29457, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29458, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29459, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29460, training loss 0.000006, validation loss 0.000140\n",
      "Iter 29461, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29462, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29463, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29464, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29465, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29466, training loss 0.000007, validation loss 0.000141\n",
      "Iter 29467, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29468, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29469, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29470, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29471, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29472, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29473, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29474, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29475, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29476, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29477, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29478, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29479, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29480, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29481, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29482, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29483, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29484, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29485, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29486, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29487, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29488, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29489, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29490, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29491, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29492, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29493, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29494, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29495, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29496, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29497, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29498, training loss 0.000009, validation loss 0.000137\n",
      "Iter 29499, training loss 0.000010, validation loss 0.000146\n",
      "Iter 29500, training loss 0.000011, validation loss 0.000139\n",
      "Iter 29501, training loss 0.000014, validation loss 0.000151\n",
      "Iter 29502, training loss 0.000017, validation loss 0.000143\n",
      "Iter 29503, training loss 0.000022, validation loss 0.000163\n",
      "Iter 29504, training loss 0.000025, validation loss 0.000146\n",
      "Iter 29505, training loss 0.000027, validation loss 0.000168\n",
      "Iter 29506, training loss 0.000027, validation loss 0.000149\n",
      "Iter 29507, training loss 0.000029, validation loss 0.000168\n",
      "Iter 29508, training loss 0.000033, validation loss 0.000150\n",
      "Iter 29509, training loss 0.000032, validation loss 0.000176\n",
      "Iter 29510, training loss 0.000029, validation loss 0.000150\n",
      "Iter 29511, training loss 0.000025, validation loss 0.000162\n",
      "Iter 29512, training loss 0.000020, validation loss 0.000144\n",
      "Iter 29513, training loss 0.000016, validation loss 0.000154\n",
      "Iter 29514, training loss 0.000010, validation loss 0.000136\n",
      "Iter 29515, training loss 0.000008, validation loss 0.000138\n",
      "Iter 29516, training loss 0.000007, validation loss 0.000136\n",
      "Iter 29517, training loss 0.000008, validation loss 0.000136\n",
      "Iter 29518, training loss 0.000009, validation loss 0.000144\n",
      "Iter 29519, training loss 0.000011, validation loss 0.000136\n",
      "Iter 29520, training loss 0.000013, validation loss 0.000147\n",
      "Iter 29521, training loss 0.000014, validation loss 0.000139\n",
      "Iter 29522, training loss 0.000014, validation loss 0.000152\n",
      "Iter 29523, training loss 0.000012, validation loss 0.000138\n",
      "Iter 29524, training loss 0.000010, validation loss 0.000144\n",
      "Iter 29525, training loss 0.000008, validation loss 0.000135\n",
      "Iter 29526, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29527, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29528, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29529, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29530, training loss 0.000008, validation loss 0.000136\n",
      "Iter 29531, training loss 0.000009, validation loss 0.000143\n",
      "Iter 29532, training loss 0.000008, validation loss 0.000136\n",
      "Iter 29533, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29534, training loss 0.000007, validation loss 0.000134\n",
      "Iter 29535, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29536, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29537, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29538, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29539, training loss 0.000007, validation loss 0.000134\n",
      "Iter 29540, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29541, training loss 0.000007, validation loss 0.000135\n",
      "Iter 29542, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29543, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29544, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29545, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29546, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29547, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29548, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29549, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29550, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29551, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29552, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29553, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29554, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29555, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29556, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29557, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29558, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29559, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29560, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29561, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29562, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29563, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29564, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29565, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29566, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29567, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29568, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29569, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29570, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29571, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29572, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29573, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29574, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29575, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29576, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29577, training loss 0.000005, validation loss 0.000137\n",
      "Iter 29578, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29579, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29580, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29581, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29582, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29583, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29584, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29585, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29586, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29587, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29588, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29589, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29590, training loss 0.000006, validation loss 0.000135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29591, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29592, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29593, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29594, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29595, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29596, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29597, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29598, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29599, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29600, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29601, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29602, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29603, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29604, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29605, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29606, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29607, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29608, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29609, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29610, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29611, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29612, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29613, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29614, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29615, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29616, training loss 0.000007, validation loss 0.000134\n",
      "Iter 29617, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29618, training loss 0.000007, validation loss 0.000135\n",
      "Iter 29619, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29620, training loss 0.000007, validation loss 0.000134\n",
      "Iter 29621, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29622, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29623, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29624, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29625, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29626, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29627, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29628, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29629, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29630, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29631, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29632, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29633, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29634, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29635, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29636, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29637, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29638, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29639, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29640, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29641, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29642, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29643, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29644, training loss 0.000005, validation loss 0.000136\n",
      "Iter 29645, training loss 0.000005, validation loss 0.000135\n",
      "Iter 29646, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29647, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29648, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29649, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29650, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29651, training loss 0.000007, validation loss 0.000135\n",
      "Iter 29652, training loss 0.000008, validation loss 0.000142\n",
      "Iter 29653, training loss 0.000009, validation loss 0.000136\n",
      "Iter 29654, training loss 0.000011, validation loss 0.000146\n",
      "Iter 29655, training loss 0.000012, validation loss 0.000137\n",
      "Iter 29656, training loss 0.000014, validation loss 0.000151\n",
      "Iter 29657, training loss 0.000016, validation loss 0.000137\n",
      "Iter 29658, training loss 0.000017, validation loss 0.000154\n",
      "Iter 29659, training loss 0.000021, validation loss 0.000147\n",
      "Iter 29660, training loss 0.000029, validation loss 0.000172\n",
      "Iter 29661, training loss 0.000041, validation loss 0.000158\n",
      "Iter 29662, training loss 0.000049, validation loss 0.000196\n",
      "Iter 29663, training loss 0.000052, validation loss 0.000170\n",
      "Iter 29664, training loss 0.000051, validation loss 0.000195\n",
      "Iter 29665, training loss 0.000048, validation loss 0.000164\n",
      "Iter 29666, training loss 0.000034, validation loss 0.000177\n",
      "Iter 29667, training loss 0.000021, validation loss 0.000153\n",
      "Iter 29668, training loss 0.000015, validation loss 0.000151\n",
      "Iter 29669, training loss 0.000011, validation loss 0.000141\n",
      "Iter 29670, training loss 0.000009, validation loss 0.000145\n",
      "Iter 29671, training loss 0.000008, validation loss 0.000143\n",
      "Iter 29672, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29673, training loss 0.000009, validation loss 0.000142\n",
      "Iter 29674, training loss 0.000010, validation loss 0.000138\n",
      "Iter 29675, training loss 0.000011, validation loss 0.000145\n",
      "Iter 29676, training loss 0.000011, validation loss 0.000136\n",
      "Iter 29677, training loss 0.000009, validation loss 0.000142\n",
      "Iter 29678, training loss 0.000008, validation loss 0.000136\n",
      "Iter 29679, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29680, training loss 0.000006, validation loss 0.000139\n",
      "Iter 29681, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29682, training loss 0.000007, validation loss 0.000139\n",
      "Iter 29683, training loss 0.000008, validation loss 0.000135\n",
      "Iter 29684, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29685, training loss 0.000008, validation loss 0.000137\n",
      "Iter 29686, training loss 0.000007, validation loss 0.000140\n",
      "Iter 29687, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29688, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29689, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29690, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29691, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29692, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29693, training loss 0.000006, validation loss 0.000138\n",
      "Iter 29694, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29695, training loss 0.000006, validation loss 0.000137\n",
      "Iter 29696, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29697, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29698, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29699, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29700, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29701, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29702, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29703, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29704, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29705, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29706, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29707, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29708, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29709, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29710, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29711, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29712, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29713, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29714, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29715, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29716, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29717, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29718, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29719, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29720, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29721, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29722, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29723, training loss 0.000005, validation loss 0.000131\n",
      "Iter 29724, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29725, training loss 0.000005, validation loss 0.000132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29726, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29727, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29728, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29729, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29730, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29731, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29732, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29733, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29734, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29735, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29736, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29737, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29738, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29739, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29740, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29741, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29742, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29743, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29744, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29745, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29746, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29747, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29748, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29749, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29750, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29751, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29752, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29753, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29754, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29755, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29756, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29757, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29758, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29759, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29760, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29761, training loss 0.000005, validation loss 0.000133\n",
      "Iter 29762, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29763, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29764, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29765, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29766, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29767, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29768, training loss 0.000005, validation loss 0.000132\n",
      "Iter 29769, training loss 0.000005, validation loss 0.000134\n",
      "Iter 29770, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29771, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29772, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29773, training loss 0.000006, validation loss 0.000134\n",
      "Iter 29774, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29775, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29776, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29777, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29778, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29779, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29780, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29781, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29782, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29783, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29784, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29785, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29786, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29787, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29788, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29789, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29790, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29791, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29792, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29793, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29794, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29795, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29796, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29797, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29798, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29799, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29800, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29801, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29802, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29803, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29804, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29805, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29806, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29807, training loss 0.000006, validation loss 0.000135\n",
      "Iter 29808, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29809, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29810, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29811, training loss 0.000006, validation loss 0.000136\n",
      "Iter 29812, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29813, training loss 0.000007, validation loss 0.000137\n",
      "Iter 29814, training loss 0.000007, validation loss 0.000131\n",
      "Iter 29815, training loss 0.000008, validation loss 0.000139\n",
      "Iter 29816, training loss 0.000008, validation loss 0.000131\n",
      "Iter 29817, training loss 0.000008, validation loss 0.000140\n",
      "Iter 29818, training loss 0.000009, validation loss 0.000133\n",
      "Iter 29819, training loss 0.000010, validation loss 0.000142\n",
      "Iter 29820, training loss 0.000012, validation loss 0.000134\n",
      "Iter 29821, training loss 0.000014, validation loss 0.000149\n",
      "Iter 29822, training loss 0.000017, validation loss 0.000134\n",
      "Iter 29823, training loss 0.000018, validation loss 0.000154\n",
      "Iter 29824, training loss 0.000019, validation loss 0.000140\n",
      "Iter 29825, training loss 0.000024, validation loss 0.000159\n",
      "Iter 29826, training loss 0.000032, validation loss 0.000147\n",
      "Iter 29827, training loss 0.000038, validation loss 0.000180\n",
      "Iter 29828, training loss 0.000052, validation loss 0.000173\n",
      "Iter 29829, training loss 0.000086, validation loss 0.000240\n",
      "Iter 29830, training loss 0.000123, validation loss 0.000227\n",
      "Iter 29831, training loss 0.000129, validation loss 0.000300\n",
      "Iter 29832, training loss 0.000087, validation loss 0.000184\n",
      "Iter 29833, training loss 0.000027, validation loss 0.000160\n",
      "Iter 29834, training loss 0.000012, validation loss 0.000136\n",
      "Iter 29835, training loss 0.000041, validation loss 0.000160\n",
      "Iter 29836, training loss 0.000080, validation loss 0.000221\n",
      "Iter 29837, training loss 0.000080, validation loss 0.000196\n",
      "Iter 29838, training loss 0.000042, validation loss 0.000180\n",
      "Iter 29839, training loss 0.000013, validation loss 0.000135\n",
      "Iter 29840, training loss 0.000020, validation loss 0.000138\n",
      "Iter 29841, training loss 0.000044, validation loss 0.000180\n",
      "Iter 29842, training loss 0.000052, validation loss 0.000161\n",
      "Iter 29843, training loss 0.000031, validation loss 0.000167\n",
      "Iter 29844, training loss 0.000013, validation loss 0.000136\n",
      "Iter 29845, training loss 0.000014, validation loss 0.000137\n",
      "Iter 29846, training loss 0.000026, validation loss 0.000169\n",
      "Iter 29847, training loss 0.000027, validation loss 0.000142\n",
      "Iter 29848, training loss 0.000015, validation loss 0.000143\n",
      "Iter 29849, training loss 0.000009, validation loss 0.000133\n",
      "Iter 29850, training loss 0.000016, validation loss 0.000136\n",
      "Iter 29851, training loss 0.000021, validation loss 0.000152\n",
      "Iter 29852, training loss 0.000016, validation loss 0.000137\n",
      "Iter 29853, training loss 0.000009, validation loss 0.000135\n",
      "Iter 29854, training loss 0.000011, validation loss 0.000136\n",
      "Iter 29855, training loss 0.000016, validation loss 0.000132\n",
      "Iter 29856, training loss 0.000015, validation loss 0.000140\n",
      "Iter 29857, training loss 0.000010, validation loss 0.000131\n",
      "Iter 29858, training loss 0.000008, validation loss 0.000132\n",
      "Iter 29859, training loss 0.000010, validation loss 0.000138\n",
      "Iter 29860, training loss 0.000012, validation loss 0.000134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29861, training loss 0.000010, validation loss 0.000137\n",
      "Iter 29862, training loss 0.000008, validation loss 0.000131\n",
      "Iter 29863, training loss 0.000008, validation loss 0.000131\n",
      "Iter 29864, training loss 0.000009, validation loss 0.000136\n",
      "Iter 29865, training loss 0.000010, validation loss 0.000131\n",
      "Iter 29866, training loss 0.000008, validation loss 0.000133\n",
      "Iter 29867, training loss 0.000007, validation loss 0.000131\n",
      "Iter 29868, training loss 0.000008, validation loss 0.000130\n",
      "Iter 29869, training loss 0.000008, validation loss 0.000134\n",
      "Iter 29870, training loss 0.000008, validation loss 0.000129\n",
      "Iter 29871, training loss 0.000007, validation loss 0.000132\n",
      "Iter 29872, training loss 0.000007, validation loss 0.000132\n",
      "Iter 29873, training loss 0.000007, validation loss 0.000131\n",
      "Iter 29874, training loss 0.000008, validation loss 0.000134\n",
      "Iter 29875, training loss 0.000007, validation loss 0.000130\n",
      "Iter 29876, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29877, training loss 0.000007, validation loss 0.000134\n",
      "Iter 29878, training loss 0.000007, validation loss 0.000132\n",
      "Iter 29879, training loss 0.000007, validation loss 0.000134\n",
      "Iter 29880, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29881, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29882, training loss 0.000007, validation loss 0.000133\n",
      "Iter 29883, training loss 0.000007, validation loss 0.000131\n",
      "Iter 29884, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29885, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29886, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29887, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29888, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29889, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29890, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29891, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29892, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29893, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29894, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29895, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29896, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29897, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29898, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29899, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29900, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29901, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29902, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29903, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29904, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29905, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29906, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29907, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29908, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29909, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29910, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29911, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29912, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29913, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29914, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29915, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29916, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29917, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29918, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29919, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29920, training loss 0.000006, validation loss 0.000131\n",
      "Iter 29921, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29922, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29923, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29924, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29925, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29926, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29927, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29928, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29929, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29930, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29931, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29932, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29933, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29934, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29935, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29936, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29937, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29938, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29939, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29940, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29941, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29942, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29943, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29944, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29945, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29946, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29947, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29948, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29949, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29950, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29951, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29952, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29953, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29954, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29955, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29956, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29957, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29958, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29959, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29960, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29961, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29962, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29963, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29964, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29965, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29966, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29967, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29968, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29969, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29970, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29971, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29972, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29973, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29974, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29975, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29976, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29977, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29978, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29979, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29980, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29981, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29982, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29983, training loss 0.000006, validation loss 0.000132\n",
      "Iter 29984, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29985, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29986, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29987, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29988, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29989, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29990, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29991, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29992, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29993, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29994, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29995, training loss 0.000006, validation loss 0.000133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29996, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29997, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29998, training loss 0.000006, validation loss 0.000133\n",
      "Iter 29999, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30000, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30001, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30002, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30003, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30004, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30005, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30006, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30007, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30008, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30009, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30010, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30011, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30012, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30013, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30014, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30015, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30016, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30017, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30018, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30019, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30020, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30021, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30022, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30023, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30024, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30025, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30026, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30027, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30028, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30029, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30030, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30031, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30032, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30033, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30034, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30035, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30036, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30037, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30038, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30039, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30040, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30041, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30042, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30043, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30044, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30045, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30046, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30047, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30048, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30049, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30050, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30051, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30052, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30053, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30054, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30055, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30056, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30057, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30058, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30059, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30060, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30061, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30062, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30063, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30064, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30065, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30066, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30067, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30068, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30069, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30070, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30071, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30072, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30073, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30074, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30075, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30076, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30077, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30078, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30079, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30080, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30081, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30082, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30083, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30084, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30085, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30086, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30087, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30088, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30089, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30090, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30091, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30092, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30093, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30094, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30095, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30096, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30097, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30098, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30099, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30100, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30101, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30102, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30103, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30104, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30105, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30106, training loss 0.000006, validation loss 0.000135\n",
      "Iter 30107, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30108, training loss 0.000007, validation loss 0.000136\n",
      "Iter 30109, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30110, training loss 0.000008, validation loss 0.000137\n",
      "Iter 30111, training loss 0.000008, validation loss 0.000132\n",
      "Iter 30112, training loss 0.000007, validation loss 0.000137\n",
      "Iter 30113, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30114, training loss 0.000007, validation loss 0.000135\n",
      "Iter 30115, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30116, training loss 0.000006, validation loss 0.000135\n",
      "Iter 30117, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30118, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30119, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30120, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30121, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30122, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30123, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30124, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30125, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30126, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30127, training loss 0.000006, validation loss 0.000136\n",
      "Iter 30128, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30129, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30130, training loss 0.000006, validation loss 0.000132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30131, training loss 0.000006, validation loss 0.000136\n",
      "Iter 30132, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30133, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30134, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30135, training loss 0.000006, validation loss 0.000135\n",
      "Iter 30136, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30137, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30138, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30139, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30140, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30141, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30142, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30143, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30144, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30145, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30146, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30147, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30148, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30149, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30150, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30151, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30152, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30153, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30154, training loss 0.000006, validation loss 0.000135\n",
      "Iter 30155, training loss 0.000007, validation loss 0.000132\n",
      "Iter 30156, training loss 0.000007, validation loss 0.000136\n",
      "Iter 30157, training loss 0.000007, validation loss 0.000132\n",
      "Iter 30158, training loss 0.000007, validation loss 0.000136\n",
      "Iter 30159, training loss 0.000008, validation loss 0.000133\n",
      "Iter 30160, training loss 0.000008, validation loss 0.000138\n",
      "Iter 30161, training loss 0.000009, validation loss 0.000133\n",
      "Iter 30162, training loss 0.000009, validation loss 0.000140\n",
      "Iter 30163, training loss 0.000009, validation loss 0.000133\n",
      "Iter 30164, training loss 0.000009, validation loss 0.000139\n",
      "Iter 30165, training loss 0.000010, validation loss 0.000134\n",
      "Iter 30166, training loss 0.000010, validation loss 0.000141\n",
      "Iter 30167, training loss 0.000010, validation loss 0.000133\n",
      "Iter 30168, training loss 0.000010, validation loss 0.000141\n",
      "Iter 30169, training loss 0.000009, validation loss 0.000134\n",
      "Iter 30170, training loss 0.000009, validation loss 0.000137\n",
      "Iter 30171, training loss 0.000008, validation loss 0.000132\n",
      "Iter 30172, training loss 0.000008, validation loss 0.000139\n",
      "Iter 30173, training loss 0.000008, validation loss 0.000134\n",
      "Iter 30174, training loss 0.000008, validation loss 0.000137\n",
      "Iter 30175, training loss 0.000008, validation loss 0.000132\n",
      "Iter 30176, training loss 0.000008, validation loss 0.000138\n",
      "Iter 30177, training loss 0.000007, validation loss 0.000134\n",
      "Iter 30178, training loss 0.000007, validation loss 0.000136\n",
      "Iter 30179, training loss 0.000007, validation loss 0.000132\n",
      "Iter 30180, training loss 0.000006, validation loss 0.000136\n",
      "Iter 30181, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30182, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30183, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30184, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30185, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30186, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30187, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30188, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30189, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30190, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30191, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30192, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30193, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30194, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30195, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30196, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30197, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30198, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30199, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30200, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30201, training loss 0.000007, validation loss 0.000136\n",
      "Iter 30202, training loss 0.000007, validation loss 0.000132\n",
      "Iter 30203, training loss 0.000008, validation loss 0.000137\n",
      "Iter 30204, training loss 0.000009, validation loss 0.000133\n",
      "Iter 30205, training loss 0.000010, validation loss 0.000141\n",
      "Iter 30206, training loss 0.000010, validation loss 0.000133\n",
      "Iter 30207, training loss 0.000010, validation loss 0.000140\n",
      "Iter 30208, training loss 0.000011, validation loss 0.000135\n",
      "Iter 30209, training loss 0.000012, validation loss 0.000143\n",
      "Iter 30210, training loss 0.000013, validation loss 0.000135\n",
      "Iter 30211, training loss 0.000014, validation loss 0.000146\n",
      "Iter 30212, training loss 0.000014, validation loss 0.000137\n",
      "Iter 30213, training loss 0.000016, validation loss 0.000147\n",
      "Iter 30214, training loss 0.000021, validation loss 0.000141\n",
      "Iter 30215, training loss 0.000025, validation loss 0.000160\n",
      "Iter 30216, training loss 0.000028, validation loss 0.000149\n",
      "Iter 30217, training loss 0.000036, validation loss 0.000171\n",
      "Iter 30218, training loss 0.000047, validation loss 0.000162\n",
      "Iter 30219, training loss 0.000049, validation loss 0.000188\n",
      "Iter 30220, training loss 0.000036, validation loss 0.000152\n",
      "Iter 30221, training loss 0.000023, validation loss 0.000151\n",
      "Iter 30222, training loss 0.000012, validation loss 0.000132\n",
      "Iter 30223, training loss 0.000008, validation loss 0.000134\n",
      "Iter 30224, training loss 0.000008, validation loss 0.000135\n",
      "Iter 30225, training loss 0.000014, validation loss 0.000132\n",
      "Iter 30226, training loss 0.000019, validation loss 0.000146\n",
      "Iter 30227, training loss 0.000022, validation loss 0.000140\n",
      "Iter 30228, training loss 0.000020, validation loss 0.000154\n",
      "Iter 30229, training loss 0.000013, validation loss 0.000132\n",
      "Iter 30230, training loss 0.000008, validation loss 0.000133\n",
      "Iter 30231, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30232, training loss 0.000009, validation loss 0.000132\n",
      "Iter 30233, training loss 0.000011, validation loss 0.000140\n",
      "Iter 30234, training loss 0.000012, validation loss 0.000133\n",
      "Iter 30235, training loss 0.000011, validation loss 0.000139\n",
      "Iter 30236, training loss 0.000009, validation loss 0.000131\n",
      "Iter 30237, training loss 0.000007, validation loss 0.000135\n",
      "Iter 30238, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30239, training loss 0.000007, validation loss 0.000129\n",
      "Iter 30240, training loss 0.000008, validation loss 0.000134\n",
      "Iter 30241, training loss 0.000009, validation loss 0.000130\n",
      "Iter 30242, training loss 0.000009, validation loss 0.000137\n",
      "Iter 30243, training loss 0.000007, validation loss 0.000129\n",
      "Iter 30244, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30245, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30246, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30247, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30248, training loss 0.000007, validation loss 0.000128\n",
      "Iter 30249, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30250, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30251, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30252, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30253, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30254, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30255, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30256, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30257, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30258, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30259, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30260, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30261, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30262, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30263, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30264, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30265, training loss 0.000005, validation loss 0.000131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30266, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30267, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30268, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30269, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30270, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30271, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30272, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30273, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30274, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30275, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30276, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30277, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30278, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30279, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30280, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30281, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30282, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30283, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30284, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30285, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30286, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30287, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30288, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30289, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30290, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30291, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30292, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30293, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30294, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30295, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30296, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30297, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30298, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30299, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30300, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30301, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30302, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30303, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30304, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30305, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30306, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30307, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30308, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30309, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30310, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30311, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30312, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30313, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30314, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30315, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30316, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30317, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30318, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30319, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30320, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30321, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30322, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30323, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30324, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30325, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30326, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30327, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30328, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30329, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30330, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30331, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30332, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30333, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30334, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30335, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30336, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30337, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30338, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30339, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30340, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30341, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30342, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30343, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30344, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30345, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30346, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30347, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30348, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30349, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30350, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30351, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30352, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30353, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30354, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30355, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30356, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30357, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30358, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30359, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30360, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30361, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30362, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30363, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30364, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30365, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30366, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30367, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30368, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30369, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30370, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30371, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30372, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30373, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30374, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30375, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30376, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30377, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30378, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30379, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30380, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30381, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30382, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30383, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30384, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30385, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30386, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30387, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30388, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30389, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30390, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30391, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30392, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30393, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30394, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30395, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30396, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30397, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30398, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30399, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30400, training loss 0.000006, validation loss 0.000130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30401, training loss 0.000006, validation loss 0.000134\n",
      "Iter 30402, training loss 0.000007, validation loss 0.000131\n",
      "Iter 30403, training loss 0.000008, validation loss 0.000137\n",
      "Iter 30404, training loss 0.000009, validation loss 0.000130\n",
      "Iter 30405, training loss 0.000010, validation loss 0.000140\n",
      "Iter 30406, training loss 0.000010, validation loss 0.000131\n",
      "Iter 30407, training loss 0.000011, validation loss 0.000140\n",
      "Iter 30408, training loss 0.000012, validation loss 0.000134\n",
      "Iter 30409, training loss 0.000015, validation loss 0.000147\n",
      "Iter 30410, training loss 0.000018, validation loss 0.000138\n",
      "Iter 30411, training loss 0.000021, validation loss 0.000156\n",
      "Iter 30412, training loss 0.000022, validation loss 0.000141\n",
      "Iter 30413, training loss 0.000021, validation loss 0.000154\n",
      "Iter 30414, training loss 0.000021, validation loss 0.000141\n",
      "Iter 30415, training loss 0.000023, validation loss 0.000156\n",
      "Iter 30416, training loss 0.000025, validation loss 0.000141\n",
      "Iter 30417, training loss 0.000024, validation loss 0.000158\n",
      "Iter 30418, training loss 0.000021, validation loss 0.000141\n",
      "Iter 30419, training loss 0.000020, validation loss 0.000152\n",
      "Iter 30420, training loss 0.000019, validation loss 0.000136\n",
      "Iter 30421, training loss 0.000015, validation loss 0.000148\n",
      "Iter 30422, training loss 0.000011, validation loss 0.000133\n",
      "Iter 30423, training loss 0.000007, validation loss 0.000135\n",
      "Iter 30424, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30425, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30426, training loss 0.000009, validation loss 0.000138\n",
      "Iter 30427, training loss 0.000012, validation loss 0.000133\n",
      "Iter 30428, training loss 0.000014, validation loss 0.000145\n",
      "Iter 30429, training loss 0.000015, validation loss 0.000134\n",
      "Iter 30430, training loss 0.000013, validation loss 0.000144\n",
      "Iter 30431, training loss 0.000010, validation loss 0.000132\n",
      "Iter 30432, training loss 0.000007, validation loss 0.000134\n",
      "Iter 30433, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30434, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30435, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30436, training loss 0.000008, validation loss 0.000129\n",
      "Iter 30437, training loss 0.000009, validation loss 0.000137\n",
      "Iter 30438, training loss 0.000009, validation loss 0.000130\n",
      "Iter 30439, training loss 0.000008, validation loss 0.000135\n",
      "Iter 30440, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30441, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30442, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30443, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30444, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30445, training loss 0.000007, validation loss 0.000129\n",
      "Iter 30446, training loss 0.000007, validation loss 0.000134\n",
      "Iter 30447, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30448, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30449, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30450, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30451, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30452, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30453, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30454, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30455, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30456, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30457, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30458, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30459, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30460, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30461, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30462, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30463, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30464, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30465, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30466, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30467, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30468, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30469, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30470, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30471, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30472, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30473, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30474, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30475, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30476, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30477, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30478, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30479, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30480, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30481, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30482, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30483, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30484, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30485, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30486, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30487, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30488, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30489, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30490, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30491, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30492, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30493, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30494, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30495, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30496, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30497, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30498, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30499, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30500, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30501, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30502, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30503, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30504, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30505, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30506, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30507, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30508, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30509, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30510, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30511, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30512, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30513, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30514, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30515, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30516, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30517, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30518, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30519, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30520, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30521, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30522, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30523, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30524, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30525, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30526, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30527, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30528, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30529, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30530, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30531, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30532, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30533, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30534, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30535, training loss 0.000005, validation loss 0.000131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30536, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30537, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30538, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30539, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30540, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30541, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30542, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30543, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30544, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30545, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30546, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30547, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30548, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30549, training loss 0.000005, validation loss 0.000132\n",
      "Iter 30550, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30551, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30552, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30553, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30554, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30555, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30556, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30557, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30558, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30559, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30560, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30561, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30562, training loss 0.000007, validation loss 0.000129\n",
      "Iter 30563, training loss 0.000007, validation loss 0.000135\n",
      "Iter 30564, training loss 0.000007, validation loss 0.000128\n",
      "Iter 30565, training loss 0.000007, validation loss 0.000135\n",
      "Iter 30566, training loss 0.000007, validation loss 0.000129\n",
      "Iter 30567, training loss 0.000008, validation loss 0.000136\n",
      "Iter 30568, training loss 0.000008, validation loss 0.000130\n",
      "Iter 30569, training loss 0.000009, validation loss 0.000138\n",
      "Iter 30570, training loss 0.000009, validation loss 0.000129\n",
      "Iter 30571, training loss 0.000009, validation loss 0.000138\n",
      "Iter 30572, training loss 0.000009, validation loss 0.000130\n",
      "Iter 30573, training loss 0.000009, validation loss 0.000138\n",
      "Iter 30574, training loss 0.000010, validation loss 0.000131\n",
      "Iter 30575, training loss 0.000011, validation loss 0.000141\n",
      "Iter 30576, training loss 0.000012, validation loss 0.000132\n",
      "Iter 30577, training loss 0.000015, validation loss 0.000145\n",
      "Iter 30578, training loss 0.000021, validation loss 0.000139\n",
      "Iter 30579, training loss 0.000029, validation loss 0.000165\n",
      "Iter 30580, training loss 0.000034, validation loss 0.000146\n",
      "Iter 30581, training loss 0.000035, validation loss 0.000171\n",
      "Iter 30582, training loss 0.000042, validation loss 0.000161\n",
      "Iter 30583, training loss 0.000053, validation loss 0.000195\n",
      "Iter 30584, training loss 0.000061, validation loss 0.000170\n",
      "Iter 30585, training loss 0.000061, validation loss 0.000205\n",
      "Iter 30586, training loss 0.000052, validation loss 0.000162\n",
      "Iter 30587, training loss 0.000034, validation loss 0.000168\n",
      "Iter 30588, training loss 0.000014, validation loss 0.000132\n",
      "Iter 30589, training loss 0.000009, validation loss 0.000127\n",
      "Iter 30590, training loss 0.000015, validation loss 0.000137\n",
      "Iter 30591, training loss 0.000025, validation loss 0.000138\n",
      "Iter 30592, training loss 0.000032, validation loss 0.000167\n",
      "Iter 30593, training loss 0.000029, validation loss 0.000137\n",
      "Iter 30594, training loss 0.000019, validation loss 0.000152\n",
      "Iter 30595, training loss 0.000012, validation loss 0.000131\n",
      "Iter 30596, training loss 0.000008, validation loss 0.000133\n",
      "Iter 30597, training loss 0.000009, validation loss 0.000136\n",
      "Iter 30598, training loss 0.000013, validation loss 0.000135\n",
      "Iter 30599, training loss 0.000017, validation loss 0.000148\n",
      "Iter 30600, training loss 0.000018, validation loss 0.000137\n",
      "Iter 30601, training loss 0.000014, validation loss 0.000146\n",
      "Iter 30602, training loss 0.000008, validation loss 0.000126\n",
      "Iter 30603, training loss 0.000006, validation loss 0.000124\n",
      "Iter 30604, training loss 0.000009, validation loss 0.000130\n",
      "Iter 30605, training loss 0.000012, validation loss 0.000126\n",
      "Iter 30606, training loss 0.000011, validation loss 0.000136\n",
      "Iter 30607, training loss 0.000008, validation loss 0.000123\n",
      "Iter 30608, training loss 0.000006, validation loss 0.000125\n",
      "Iter 30609, training loss 0.000006, validation loss 0.000127\n",
      "Iter 30610, training loss 0.000008, validation loss 0.000125\n",
      "Iter 30611, training loss 0.000009, validation loss 0.000134\n",
      "Iter 30612, training loss 0.000008, validation loss 0.000125\n",
      "Iter 30613, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30614, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30615, training loss 0.000006, validation loss 0.000127\n",
      "Iter 30616, training loss 0.000007, validation loss 0.000132\n",
      "Iter 30617, training loss 0.000007, validation loss 0.000127\n",
      "Iter 30618, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30619, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30620, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30621, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30622, training loss 0.000006, validation loss 0.000125\n",
      "Iter 30623, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30624, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30625, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30626, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30627, training loss 0.000005, validation loss 0.000125\n",
      "Iter 30628, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30629, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30630, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30631, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30632, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30633, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30634, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30635, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30636, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30637, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30638, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30639, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30640, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30641, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30642, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30643, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30644, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30645, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30646, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30647, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30648, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30649, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30650, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30651, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30652, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30653, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30654, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30655, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30656, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30657, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30658, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30659, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30660, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30661, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30662, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30663, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30664, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30665, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30666, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30667, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30668, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30669, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30670, training loss 0.000005, validation loss 0.000127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30671, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30672, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30673, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30674, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30675, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30676, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30677, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30678, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30679, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30680, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30681, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30682, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30683, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30684, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30685, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30686, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30687, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30688, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30689, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30690, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30691, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30692, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30693, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30694, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30695, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30696, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30697, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30698, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30699, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30700, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30701, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30702, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30703, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30704, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30705, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30706, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30707, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30708, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30709, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30710, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30711, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30712, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30713, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30714, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30715, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30716, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30717, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30718, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30719, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30720, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30721, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30722, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30723, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30724, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30725, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30726, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30727, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30728, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30729, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30730, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30731, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30732, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30733, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30734, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30735, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30736, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30737, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30738, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30739, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30740, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30741, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30742, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30743, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30744, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30745, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30746, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30747, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30748, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30749, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30750, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30751, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30752, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30753, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30754, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30755, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30756, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30757, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30758, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30759, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30760, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30761, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30762, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30763, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30764, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30765, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30766, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30767, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30768, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30769, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30770, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30771, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30772, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30773, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30774, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30775, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30776, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30777, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30778, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30779, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30780, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30781, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30782, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30783, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30784, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30785, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30786, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30787, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30788, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30789, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30790, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30791, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30792, training loss 0.000005, validation loss 0.000126\n",
      "Iter 30793, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30794, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30795, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30796, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30797, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30798, training loss 0.000006, validation loss 0.000126\n",
      "Iter 30799, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30800, training loss 0.000006, validation loss 0.000127\n",
      "Iter 30801, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30802, training loss 0.000006, validation loss 0.000126\n",
      "Iter 30803, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30804, training loss 0.000006, validation loss 0.000126\n",
      "Iter 30805, training loss 0.000006, validation loss 0.000132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30806, training loss 0.000006, validation loss 0.000127\n",
      "Iter 30807, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30808, training loss 0.000007, validation loss 0.000126\n",
      "Iter 30809, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30810, training loss 0.000006, validation loss 0.000126\n",
      "Iter 30811, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30812, training loss 0.000006, validation loss 0.000127\n",
      "Iter 30813, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30814, training loss 0.000006, validation loss 0.000126\n",
      "Iter 30815, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30816, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30817, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30818, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30819, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30820, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30821, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30822, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30823, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30824, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30825, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30826, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30827, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30828, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30829, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30830, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30831, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30832, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30833, training loss 0.000005, validation loss 0.000127\n",
      "Iter 30834, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30835, training loss 0.000006, validation loss 0.000127\n",
      "Iter 30836, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30837, training loss 0.000006, validation loss 0.000127\n",
      "Iter 30838, training loss 0.000006, validation loss 0.000133\n",
      "Iter 30839, training loss 0.000007, validation loss 0.000126\n",
      "Iter 30840, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30841, training loss 0.000007, validation loss 0.000127\n",
      "Iter 30842, training loss 0.000007, validation loss 0.000133\n",
      "Iter 30843, training loss 0.000008, validation loss 0.000128\n",
      "Iter 30844, training loss 0.000009, validation loss 0.000137\n",
      "Iter 30845, training loss 0.000010, validation loss 0.000128\n",
      "Iter 30846, training loss 0.000012, validation loss 0.000141\n",
      "Iter 30847, training loss 0.000013, validation loss 0.000129\n",
      "Iter 30848, training loss 0.000014, validation loss 0.000143\n",
      "Iter 30849, training loss 0.000017, validation loss 0.000135\n",
      "Iter 30850, training loss 0.000023, validation loss 0.000157\n",
      "Iter 30851, training loss 0.000033, validation loss 0.000145\n",
      "Iter 30852, training loss 0.000042, validation loss 0.000181\n",
      "Iter 30853, training loss 0.000047, validation loss 0.000159\n",
      "Iter 30854, training loss 0.000051, validation loss 0.000186\n",
      "Iter 30855, training loss 0.000056, validation loss 0.000167\n",
      "Iter 30856, training loss 0.000047, validation loss 0.000185\n",
      "Iter 30857, training loss 0.000026, validation loss 0.000147\n",
      "Iter 30858, training loss 0.000014, validation loss 0.000139\n",
      "Iter 30859, training loss 0.000007, validation loss 0.000130\n",
      "Iter 30860, training loss 0.000010, validation loss 0.000133\n",
      "Iter 30861, training loss 0.000015, validation loss 0.000148\n",
      "Iter 30862, training loss 0.000020, validation loss 0.000134\n",
      "Iter 30863, training loss 0.000021, validation loss 0.000153\n",
      "Iter 30864, training loss 0.000018, validation loss 0.000136\n",
      "Iter 30865, training loss 0.000014, validation loss 0.000144\n",
      "Iter 30866, training loss 0.000007, validation loss 0.000128\n",
      "Iter 30867, training loss 0.000008, validation loss 0.000127\n",
      "Iter 30868, training loss 0.000008, validation loss 0.000133\n",
      "Iter 30869, training loss 0.000012, validation loss 0.000133\n",
      "Iter 30870, training loss 0.000013, validation loss 0.000144\n",
      "Iter 30871, training loss 0.000011, validation loss 0.000131\n",
      "Iter 30872, training loss 0.000009, validation loss 0.000135\n",
      "Iter 30873, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30874, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30875, training loss 0.000007, validation loss 0.000135\n",
      "Iter 30876, training loss 0.000008, validation loss 0.000130\n",
      "Iter 30877, training loss 0.000008, validation loss 0.000135\n",
      "Iter 30878, training loss 0.000007, validation loss 0.000130\n",
      "Iter 30879, training loss 0.000007, validation loss 0.000135\n",
      "Iter 30880, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30881, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30882, training loss 0.000006, validation loss 0.000132\n",
      "Iter 30883, training loss 0.000006, validation loss 0.000129\n",
      "Iter 30884, training loss 0.000007, validation loss 0.000134\n",
      "Iter 30885, training loss 0.000006, validation loss 0.000128\n",
      "Iter 30886, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30887, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30888, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30889, training loss 0.000006, validation loss 0.000131\n",
      "Iter 30890, training loss 0.000006, validation loss 0.000127\n",
      "Iter 30891, training loss 0.000006, validation loss 0.000130\n",
      "Iter 30892, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30893, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30894, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30895, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30896, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30897, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30898, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30899, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30900, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30901, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30902, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30903, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30904, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30905, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30906, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30907, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30908, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30909, training loss 0.000005, validation loss 0.000128\n",
      "Iter 30910, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30911, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30912, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30913, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30914, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30915, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30916, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30917, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30918, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30919, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30920, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30921, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30922, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30923, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30924, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30925, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30926, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30927, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30928, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30929, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30930, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30931, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30932, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30933, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30934, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30935, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30936, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30937, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30938, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30939, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30940, training loss 0.000005, validation loss 0.000129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30941, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30942, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30943, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30944, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30945, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30946, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30947, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30948, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30949, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30950, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30951, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30952, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30953, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30954, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30955, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30956, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30957, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30958, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30959, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30960, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30961, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30962, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30963, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30964, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30965, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30966, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30967, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30968, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30969, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30970, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30971, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30972, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30973, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30974, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30975, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30976, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30977, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30978, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30979, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30980, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30981, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30982, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30983, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30984, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30985, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30986, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30987, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30988, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30989, training loss 0.000005, validation loss 0.000131\n",
      "Iter 30990, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30991, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30992, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30993, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30994, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30995, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30996, training loss 0.000005, validation loss 0.000129\n",
      "Iter 30997, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30998, training loss 0.000005, validation loss 0.000130\n",
      "Iter 30999, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31000, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31001, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31002, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31003, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31004, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31005, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31006, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31007, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31008, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31009, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31010, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31011, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31012, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31013, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31014, training loss 0.000005, validation loss 0.000132\n",
      "Iter 31015, training loss 0.000006, validation loss 0.000129\n",
      "Iter 31016, training loss 0.000006, validation loss 0.000133\n",
      "Iter 31017, training loss 0.000006, validation loss 0.000129\n",
      "Iter 31018, training loss 0.000006, validation loss 0.000133\n",
      "Iter 31019, training loss 0.000006, validation loss 0.000129\n",
      "Iter 31020, training loss 0.000006, validation loss 0.000134\n",
      "Iter 31021, training loss 0.000006, validation loss 0.000128\n",
      "Iter 31022, training loss 0.000006, validation loss 0.000134\n",
      "Iter 31023, training loss 0.000006, validation loss 0.000129\n",
      "Iter 31024, training loss 0.000006, validation loss 0.000133\n",
      "Iter 31025, training loss 0.000006, validation loss 0.000129\n",
      "Iter 31026, training loss 0.000005, validation loss 0.000133\n",
      "Iter 31027, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31028, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31029, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31030, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31031, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31032, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31033, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31034, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31035, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31036, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31037, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31038, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31039, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31040, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31041, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31042, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31043, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31044, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31045, training loss 0.000005, validation loss 0.000132\n",
      "Iter 31046, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31047, training loss 0.000006, validation loss 0.000133\n",
      "Iter 31048, training loss 0.000006, validation loss 0.000129\n",
      "Iter 31049, training loss 0.000006, validation loss 0.000134\n",
      "Iter 31050, training loss 0.000006, validation loss 0.000128\n",
      "Iter 31051, training loss 0.000006, validation loss 0.000134\n",
      "Iter 31052, training loss 0.000007, validation loss 0.000129\n",
      "Iter 31053, training loss 0.000007, validation loss 0.000135\n",
      "Iter 31054, training loss 0.000008, validation loss 0.000129\n",
      "Iter 31055, training loss 0.000008, validation loss 0.000138\n",
      "Iter 31056, training loss 0.000009, validation loss 0.000129\n",
      "Iter 31057, training loss 0.000009, validation loss 0.000139\n",
      "Iter 31058, training loss 0.000010, validation loss 0.000131\n",
      "Iter 31059, training loss 0.000012, validation loss 0.000142\n",
      "Iter 31060, training loss 0.000015, validation loss 0.000133\n",
      "Iter 31061, training loss 0.000018, validation loss 0.000152\n",
      "Iter 31062, training loss 0.000022, validation loss 0.000139\n",
      "Iter 31063, training loss 0.000029, validation loss 0.000163\n",
      "Iter 31064, training loss 0.000041, validation loss 0.000153\n",
      "Iter 31065, training loss 0.000052, validation loss 0.000194\n",
      "Iter 31066, training loss 0.000052, validation loss 0.000161\n",
      "Iter 31067, training loss 0.000045, validation loss 0.000181\n",
      "Iter 31068, training loss 0.000035, validation loss 0.000151\n",
      "Iter 31069, training loss 0.000024, validation loss 0.000158\n",
      "Iter 31070, training loss 0.000011, validation loss 0.000130\n",
      "Iter 31071, training loss 0.000007, validation loss 0.000128\n",
      "Iter 31072, training loss 0.000009, validation loss 0.000134\n",
      "Iter 31073, training loss 0.000015, validation loss 0.000133\n",
      "Iter 31074, training loss 0.000019, validation loss 0.000151\n",
      "Iter 31075, training loss 0.000019, validation loss 0.000136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 31076, training loss 0.000015, validation loss 0.000145\n",
      "Iter 31077, training loss 0.000009, validation loss 0.000131\n",
      "Iter 31078, training loss 0.000006, validation loss 0.000133\n",
      "Iter 31079, training loss 0.000006, validation loss 0.000131\n",
      "Iter 31080, training loss 0.000008, validation loss 0.000127\n",
      "Iter 31081, training loss 0.000011, validation loss 0.000137\n",
      "Iter 31082, training loss 0.000012, validation loss 0.000131\n",
      "Iter 31083, training loss 0.000012, validation loss 0.000141\n",
      "Iter 31084, training loss 0.000008, validation loss 0.000127\n",
      "Iter 31085, training loss 0.000006, validation loss 0.000129\n",
      "Iter 31086, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31087, training loss 0.000006, validation loss 0.000128\n",
      "Iter 31088, training loss 0.000007, validation loss 0.000135\n",
      "Iter 31089, training loss 0.000008, validation loss 0.000128\n",
      "Iter 31090, training loss 0.000007, validation loss 0.000133\n",
      "Iter 31091, training loss 0.000006, validation loss 0.000127\n",
      "Iter 31092, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31093, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31094, training loss 0.000006, validation loss 0.000128\n",
      "Iter 31095, training loss 0.000006, validation loss 0.000132\n",
      "Iter 31096, training loss 0.000006, validation loss 0.000128\n",
      "Iter 31097, training loss 0.000006, validation loss 0.000133\n",
      "Iter 31098, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31099, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31100, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31101, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31102, training loss 0.000006, validation loss 0.000132\n",
      "Iter 31103, training loss 0.000005, validation loss 0.000127\n",
      "Iter 31104, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31105, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31106, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31107, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31108, training loss 0.000005, validation loss 0.000127\n",
      "Iter 31109, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31110, training loss 0.000005, validation loss 0.000127\n",
      "Iter 31111, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31112, training loss 0.000004, validation loss 0.000128\n",
      "Iter 31113, training loss 0.000004, validation loss 0.000128\n",
      "Iter 31114, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31115, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31116, training loss 0.000005, validation loss 0.000130\n",
      "Iter 31117, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31118, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31119, training loss 0.000004, validation loss 0.000128\n",
      "Iter 31120, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31121, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31122, training loss 0.000004, validation loss 0.000128\n",
      "Iter 31123, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31124, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31125, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31126, training loss 0.000004, validation loss 0.000128\n",
      "Iter 31127, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31128, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31129, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31130, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31131, training loss 0.000004, validation loss 0.000128\n",
      "Iter 31132, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31133, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31134, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31135, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31136, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31137, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31138, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31139, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31140, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31141, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31142, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31143, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31144, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31145, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31146, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31147, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31148, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31149, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31150, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31151, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31152, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31153, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31154, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31155, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31156, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31157, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31158, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31159, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31160, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31161, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31162, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31163, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31164, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31165, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31166, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31167, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31168, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31169, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31170, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31171, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31172, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31173, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31174, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31175, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31176, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31177, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31178, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31179, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31180, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31181, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31182, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31183, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31184, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31185, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31186, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31187, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31188, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31189, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31190, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31191, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31192, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31193, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31194, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31195, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31196, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31197, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31198, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31199, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31200, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31201, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31202, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31203, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31204, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31205, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31206, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31207, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31208, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31209, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31210, training loss 0.000004, validation loss 0.000129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 31211, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31212, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31213, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31214, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31215, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31216, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31217, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31218, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31219, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31220, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31221, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31222, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31223, training loss 0.000005, validation loss 0.000132\n",
      "Iter 31224, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31225, training loss 0.000005, validation loss 0.000132\n",
      "Iter 31226, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31227, training loss 0.000005, validation loss 0.000132\n",
      "Iter 31228, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31229, training loss 0.000005, validation loss 0.000133\n",
      "Iter 31230, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31231, training loss 0.000005, validation loss 0.000133\n",
      "Iter 31232, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31233, training loss 0.000005, validation loss 0.000132\n",
      "Iter 31234, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31235, training loss 0.000005, validation loss 0.000133\n",
      "Iter 31236, training loss 0.000005, validation loss 0.000128\n",
      "Iter 31237, training loss 0.000005, validation loss 0.000132\n",
      "Iter 31238, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31239, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31240, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31241, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31242, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31243, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31244, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31245, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31246, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31247, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31248, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31249, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31250, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31251, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31252, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31253, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31254, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31255, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31256, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31257, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31258, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31259, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31260, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31261, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31262, training loss 0.000004, validation loss 0.000130\n",
      "Iter 31263, training loss 0.000004, validation loss 0.000129\n",
      "Iter 31264, training loss 0.000005, validation loss 0.000131\n",
      "Iter 31265, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31266, training loss 0.000005, validation loss 0.000132\n",
      "Iter 31267, training loss 0.000005, validation loss 0.000129\n",
      "Iter 31268, training loss 0.000006, validation loss 0.000133\n",
      "Iter 31269, training loss 0.000006, validation loss 0.000129\n",
      "Iter 31270, training loss 0.000007, validation loss 0.000136\n",
      "Iter 31271, training loss 0.000009, validation loss 0.000131\n",
      "Iter 31272, training loss 0.000012, validation loss 0.000143\n",
      "Iter 31273, training loss 0.000017, validation loss 0.000137\n",
      "Iter 31274, training loss 0.000027, validation loss 0.000162\n",
      "Iter 31275, training loss 0.000046, validation loss 0.000160\n",
      "Iter 31276, training loss 0.000071, validation loss 0.000217\n",
      "Iter 31277, training loss 0.000099, validation loss 0.000209\n",
      "Iter 31278, training loss 0.000123, validation loss 0.000280\n",
      "Iter 31279, training loss 0.000117, validation loss 0.000220\n",
      "Iter 31280, training loss 0.000056, validation loss 0.000195\n",
      "Iter 31281, training loss 0.000016, validation loss 0.000137\n",
      "Iter 31282, training loss 0.000016, validation loss 0.000135\n",
      "Iter 31283, training loss 0.000044, validation loss 0.000169\n",
      "Iter 31284, training loss 0.000086, validation loss 0.000199\n",
      "Iter 31285, training loss 0.000117, validation loss 0.000279\n",
      "Iter 31286, training loss 0.000075, validation loss 0.000168\n",
      "Iter 31287, training loss 0.000028, validation loss 0.000154\n",
      "Iter 31288, training loss 0.000017, validation loss 0.000136\n",
      "Iter 31289, training loss 0.000035, validation loss 0.000143\n",
      "Iter 31290, training loss 0.000056, validation loss 0.000177\n",
      "Iter 31291, training loss 0.000052, validation loss 0.000160\n",
      "Iter 31292, training loss 0.000029, validation loss 0.000151\n",
      "Iter 31293, training loss 0.000012, validation loss 0.000130\n",
      "Iter 31294, training loss 0.000017, validation loss 0.000133\n",
      "Iter 31295, training loss 0.000029, validation loss 0.000157\n",
      "Iter 31296, training loss 0.000027, validation loss 0.000142\n",
      "Iter 31297, training loss 0.000016, validation loss 0.000130\n",
      "Iter 31298, training loss 0.000011, validation loss 0.000124\n",
      "Iter 31299, training loss 0.000016, validation loss 0.000129\n",
      "Iter 31300, training loss 0.000020, validation loss 0.000144\n",
      "Iter 31301, training loss 0.000015, validation loss 0.000128\n",
      "Iter 31302, training loss 0.000009, validation loss 0.000126\n",
      "Iter 31303, training loss 0.000010, validation loss 0.000128\n",
      "Iter 31304, training loss 0.000014, validation loss 0.000126\n",
      "Iter 31305, training loss 0.000012, validation loss 0.000129\n",
      "Iter 31306, training loss 0.000008, validation loss 0.000120\n",
      "Iter 31307, training loss 0.000007, validation loss 0.000120\n",
      "Iter 31308, training loss 0.000010, validation loss 0.000126\n",
      "Iter 31309, training loss 0.000011, validation loss 0.000122\n",
      "Iter 31310, training loss 0.000008, validation loss 0.000124\n",
      "Iter 31311, training loss 0.000006, validation loss 0.000121\n",
      "Iter 31312, training loss 0.000008, validation loss 0.000120\n",
      "Iter 31313, training loss 0.000009, validation loss 0.000125\n",
      "Iter 31314, training loss 0.000007, validation loss 0.000121\n",
      "Iter 31315, training loss 0.000006, validation loss 0.000122\n",
      "Iter 31316, training loss 0.000006, validation loss 0.000124\n",
      "Iter 31317, training loss 0.000007, validation loss 0.000121\n",
      "Iter 31318, training loss 0.000007, validation loss 0.000124\n",
      "Iter 31319, training loss 0.000006, validation loss 0.000121\n",
      "Iter 31320, training loss 0.000006, validation loss 0.000121\n",
      "Iter 31321, training loss 0.000006, validation loss 0.000123\n",
      "Iter 31322, training loss 0.000006, validation loss 0.000120\n",
      "Iter 31323, training loss 0.000005, validation loss 0.000121\n",
      "Iter 31324, training loss 0.000005, validation loss 0.000121\n",
      "Iter 31325, training loss 0.000006, validation loss 0.000120\n",
      "Iter 31326, training loss 0.000006, validation loss 0.000122\n",
      "Iter 31327, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31328, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31329, training loss 0.000005, validation loss 0.000120\n",
      "Iter 31330, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31331, training loss 0.000005, validation loss 0.000121\n",
      "Iter 31332, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31333, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31334, training loss 0.000005, validation loss 0.000120\n",
      "Iter 31335, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31336, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31337, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31338, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31339, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31340, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31341, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31342, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31343, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31344, training loss 0.000005, validation loss 0.000119\n",
      "Iter 31345, training loss 0.000005, validation loss 0.000118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 31346, training loss 0.000004, validation loss 0.000118\n",
      "Iter 31347, training loss 0.000004, validation loss 0.000118\n",
      "Iter 31348, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31349, training loss 0.000004, validation loss 0.000118\n",
      "Iter 31350, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31351, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31352, training loss 0.000004, validation loss 0.000118\n",
      "Iter 31353, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31354, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31355, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31356, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31357, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31358, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31359, training loss 0.000004, validation loss 0.000118\n",
      "Iter 31360, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31361, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31362, training loss 0.000004, validation loss 0.000118\n",
      "Iter 31363, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31364, training loss 0.000004, validation loss 0.000118\n",
      "Iter 31365, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31366, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31367, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31368, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31369, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31370, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31371, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31372, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31373, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31374, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31375, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31376, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31377, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31378, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31379, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31380, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31381, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31382, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31383, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31384, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31385, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31386, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31387, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31388, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31389, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31390, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31391, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31392, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31393, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31394, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31395, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31396, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31397, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31398, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31399, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31400, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31401, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31402, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31403, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31404, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31405, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31406, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31407, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31408, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31409, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31410, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31411, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31412, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31413, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31414, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31415, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31416, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31417, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31418, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31419, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31420, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31421, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31422, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31423, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31424, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31425, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31426, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31427, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31428, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31429, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31430, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31431, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31432, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31433, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31434, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31435, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31436, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31437, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31438, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31439, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31440, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31441, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31442, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31443, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31444, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31445, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31446, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31447, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31448, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31449, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31450, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31451, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31452, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31453, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31454, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31455, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31456, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31457, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31458, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31459, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31460, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31461, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31462, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31463, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31464, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31465, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31466, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31467, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31468, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31469, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31470, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31471, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31472, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31473, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31474, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31475, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31476, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31477, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31478, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31479, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31480, training loss 0.000004, validation loss 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 31481, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31482, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31483, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31484, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31485, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31486, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31487, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31488, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31489, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31490, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31491, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31492, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31493, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31494, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31495, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31496, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31497, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31498, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31499, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31500, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31501, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31502, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31503, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31504, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31505, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31506, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31507, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31508, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31509, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31510, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31511, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31512, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31513, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31514, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31515, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31516, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31517, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31518, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31519, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31520, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31521, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31522, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31523, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31524, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31525, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31526, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31527, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31528, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31529, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31530, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31531, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31532, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31533, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31534, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31535, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31536, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31537, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31538, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31539, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31540, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31541, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31542, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31543, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31544, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31545, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31546, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31547, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31548, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31549, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31550, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31551, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31552, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31553, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31554, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31555, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31556, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31557, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31558, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31559, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31560, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31561, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31562, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31563, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31564, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31565, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31566, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31567, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31568, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31569, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31570, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31571, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31572, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31573, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31574, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31575, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31576, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31577, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31578, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31579, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31580, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31581, training loss 0.000005, validation loss 0.000116\n",
      "Iter 31582, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31583, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31584, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31585, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31586, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31587, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31588, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31589, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31590, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31591, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31592, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31593, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31594, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31595, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31596, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31597, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31598, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31599, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31600, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31601, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31602, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31603, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31604, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31605, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31606, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31607, training loss 0.000005, validation loss 0.000114\n",
      "Iter 31608, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31609, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31610, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31611, training loss 0.000005, validation loss 0.000114\n",
      "Iter 31612, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31613, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31614, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31615, training loss 0.000005, validation loss 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 31616, training loss 0.000005, validation loss 0.000116\n",
      "Iter 31617, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31618, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31619, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31620, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31621, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31622, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31623, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31624, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31625, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31626, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31627, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31628, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31629, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31630, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31631, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31632, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31633, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31634, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31635, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31636, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31637, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31638, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31639, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31640, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31641, training loss 0.000005, validation loss 0.000114\n",
      "Iter 31642, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31643, training loss 0.000006, validation loss 0.000114\n",
      "Iter 31644, training loss 0.000006, validation loss 0.000119\n",
      "Iter 31645, training loss 0.000006, validation loss 0.000114\n",
      "Iter 31646, training loss 0.000007, validation loss 0.000120\n",
      "Iter 31647, training loss 0.000007, validation loss 0.000113\n",
      "Iter 31648, training loss 0.000006, validation loss 0.000120\n",
      "Iter 31649, training loss 0.000006, validation loss 0.000114\n",
      "Iter 31650, training loss 0.000006, validation loss 0.000120\n",
      "Iter 31651, training loss 0.000007, validation loss 0.000113\n",
      "Iter 31652, training loss 0.000008, validation loss 0.000121\n",
      "Iter 31653, training loss 0.000008, validation loss 0.000115\n",
      "Iter 31654, training loss 0.000009, validation loss 0.000122\n",
      "Iter 31655, training loss 0.000011, validation loss 0.000116\n",
      "Iter 31656, training loss 0.000012, validation loss 0.000128\n",
      "Iter 31657, training loss 0.000014, validation loss 0.000118\n",
      "Iter 31658, training loss 0.000016, validation loss 0.000131\n",
      "Iter 31659, training loss 0.000019, validation loss 0.000122\n",
      "Iter 31660, training loss 0.000021, validation loss 0.000139\n",
      "Iter 31661, training loss 0.000019, validation loss 0.000121\n",
      "Iter 31662, training loss 0.000016, validation loss 0.000130\n",
      "Iter 31663, training loss 0.000013, validation loss 0.000117\n",
      "Iter 31664, training loss 0.000009, validation loss 0.000123\n",
      "Iter 31665, training loss 0.000006, validation loss 0.000112\n",
      "Iter 31666, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31667, training loss 0.000005, validation loss 0.000116\n",
      "Iter 31668, training loss 0.000006, validation loss 0.000115\n",
      "Iter 31669, training loss 0.000008, validation loss 0.000123\n",
      "Iter 31670, training loss 0.000010, validation loss 0.000115\n",
      "Iter 31671, training loss 0.000011, validation loss 0.000126\n",
      "Iter 31672, training loss 0.000010, validation loss 0.000117\n",
      "Iter 31673, training loss 0.000009, validation loss 0.000125\n",
      "Iter 31674, training loss 0.000006, validation loss 0.000113\n",
      "Iter 31675, training loss 0.000005, validation loss 0.000116\n",
      "Iter 31676, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31677, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31678, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31679, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31680, training loss 0.000006, validation loss 0.000119\n",
      "Iter 31681, training loss 0.000006, validation loss 0.000114\n",
      "Iter 31682, training loss 0.000006, validation loss 0.000120\n",
      "Iter 31683, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31684, training loss 0.000005, validation loss 0.000116\n",
      "Iter 31685, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31686, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31687, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31688, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31689, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31690, training loss 0.000005, validation loss 0.000114\n",
      "Iter 31691, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31692, training loss 0.000005, validation loss 0.000114\n",
      "Iter 31693, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31694, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31695, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31696, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31697, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31698, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31699, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31700, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31701, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31702, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31703, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31704, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31705, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31706, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31707, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31708, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31709, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31710, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31711, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31712, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31713, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31714, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31715, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31716, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31717, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31718, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31719, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31720, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31721, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31722, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31723, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31724, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31725, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31726, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31727, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31728, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31729, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31730, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31731, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31732, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31733, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31734, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31735, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31736, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31737, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31738, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31739, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31740, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31741, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31742, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31743, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31744, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31745, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31746, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31747, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31748, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31749, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31750, training loss 0.000005, validation loss 0.000113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 31751, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31752, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31753, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31754, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31755, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31756, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31757, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31758, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31759, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31760, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31761, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31762, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31763, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31764, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31765, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31766, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31767, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31768, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31769, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31770, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31771, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31772, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31773, training loss 0.000004, validation loss 0.000113\n",
      "Iter 31774, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31775, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31776, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31777, training loss 0.000005, validation loss 0.000114\n",
      "Iter 31778, training loss 0.000006, validation loss 0.000120\n",
      "Iter 31779, training loss 0.000007, validation loss 0.000115\n",
      "Iter 31780, training loss 0.000009, validation loss 0.000124\n",
      "Iter 31781, training loss 0.000011, validation loss 0.000116\n",
      "Iter 31782, training loss 0.000012, validation loss 0.000128\n",
      "Iter 31783, training loss 0.000015, validation loss 0.000120\n",
      "Iter 31784, training loss 0.000020, validation loss 0.000140\n",
      "Iter 31785, training loss 0.000028, validation loss 0.000130\n",
      "Iter 31786, training loss 0.000037, validation loss 0.000161\n",
      "Iter 31787, training loss 0.000045, validation loss 0.000145\n",
      "Iter 31788, training loss 0.000048, validation loss 0.000173\n",
      "Iter 31789, training loss 0.000044, validation loss 0.000141\n",
      "Iter 31790, training loss 0.000029, validation loss 0.000148\n",
      "Iter 31791, training loss 0.000012, validation loss 0.000111\n",
      "Iter 31792, training loss 0.000005, validation loss 0.000110\n",
      "Iter 31793, training loss 0.000010, validation loss 0.000121\n",
      "Iter 31794, training loss 0.000020, validation loss 0.000122\n",
      "Iter 31795, training loss 0.000026, validation loss 0.000145\n",
      "Iter 31796, training loss 0.000023, validation loss 0.000123\n",
      "Iter 31797, training loss 0.000014, validation loss 0.000128\n",
      "Iter 31798, training loss 0.000006, validation loss 0.000112\n",
      "Iter 31799, training loss 0.000005, validation loss 0.000112\n",
      "Iter 31800, training loss 0.000009, validation loss 0.000120\n",
      "Iter 31801, training loss 0.000013, validation loss 0.000115\n",
      "Iter 31802, training loss 0.000015, validation loss 0.000128\n",
      "Iter 31803, training loss 0.000012, validation loss 0.000117\n",
      "Iter 31804, training loss 0.000007, validation loss 0.000121\n",
      "Iter 31805, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31806, training loss 0.000006, validation loss 0.000113\n",
      "Iter 31807, training loss 0.000009, validation loss 0.000122\n",
      "Iter 31808, training loss 0.000010, validation loss 0.000116\n",
      "Iter 31809, training loss 0.000008, validation loss 0.000124\n",
      "Iter 31810, training loss 0.000005, validation loss 0.000114\n",
      "Iter 31811, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31812, training loss 0.000006, validation loss 0.000119\n",
      "Iter 31813, training loss 0.000007, validation loss 0.000116\n",
      "Iter 31814, training loss 0.000008, validation loss 0.000123\n",
      "Iter 31815, training loss 0.000007, validation loss 0.000114\n",
      "Iter 31816, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31817, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31818, training loss 0.000005, validation loss 0.000115\n",
      "Iter 31819, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31820, training loss 0.000006, validation loss 0.000113\n",
      "Iter 31821, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31822, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31823, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31824, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31825, training loss 0.000005, validation loss 0.000113\n",
      "Iter 31826, training loss 0.000005, validation loss 0.000117\n",
      "Iter 31827, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31828, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31829, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31830, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31831, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31832, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31833, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31834, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31835, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31836, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31837, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31838, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31839, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31840, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31841, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31842, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31843, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31844, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31845, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31846, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31847, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31848, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31849, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31850, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31851, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31852, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31853, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31854, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31855, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31856, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31857, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31858, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31859, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31860, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31861, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31862, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31863, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31864, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31865, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31866, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31867, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31868, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31869, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31870, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31871, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31872, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31873, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31874, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31875, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31876, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31877, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31878, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31879, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31880, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31881, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31882, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31883, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31884, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31885, training loss 0.000004, validation loss 0.000115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 31886, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31887, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31888, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31889, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31890, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31891, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31892, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31893, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31894, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31895, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31896, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31897, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31898, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31899, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31900, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31901, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31902, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31903, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31904, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31905, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31906, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31907, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31908, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31909, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31910, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31911, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31912, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31913, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31914, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31915, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31916, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31917, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31918, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31919, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31920, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31921, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31922, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31923, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31924, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31925, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31926, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31927, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31928, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31929, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31930, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31931, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31932, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31933, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31934, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31935, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31936, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31937, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31938, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31939, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31940, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31941, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31942, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31943, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31944, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31945, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31946, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31947, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31948, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31949, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31950, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31951, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31952, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31953, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31954, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31955, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31956, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31957, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31958, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31959, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31960, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31961, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31962, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31963, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31964, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31965, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31966, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31967, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31968, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31969, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31970, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31971, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31972, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31973, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31974, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31975, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31976, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31977, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31978, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31979, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31980, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31981, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31982, training loss 0.000004, validation loss 0.000115\n",
      "Iter 31983, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31984, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31985, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31986, training loss 0.000004, validation loss 0.000116\n",
      "Iter 31987, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31988, training loss 0.000004, validation loss 0.000117\n",
      "Iter 31989, training loss 0.000004, validation loss 0.000114\n",
      "Iter 31990, training loss 0.000005, validation loss 0.000118\n",
      "Iter 31991, training loss 0.000005, validation loss 0.000114\n",
      "Iter 31992, training loss 0.000005, validation loss 0.000120\n",
      "Iter 31993, training loss 0.000006, validation loss 0.000114\n",
      "Iter 31994, training loss 0.000006, validation loss 0.000121\n",
      "Iter 31995, training loss 0.000007, validation loss 0.000114\n",
      "Iter 31996, training loss 0.000007, validation loss 0.000123\n",
      "Iter 31997, training loss 0.000008, validation loss 0.000116\n",
      "Iter 31998, training loss 0.000010, validation loss 0.000126\n",
      "Iter 31999, training loss 0.000012, validation loss 0.000118\n",
      "Iter 32000, training loss 0.000015, validation loss 0.000133\n",
      "Iter 32001, training loss 0.000017, validation loss 0.000120\n",
      "Iter 32002, training loss 0.000019, validation loss 0.000137\n",
      "Iter 32003, training loss 0.000023, validation loss 0.000128\n",
      "Iter 32004, training loss 0.000029, validation loss 0.000153\n",
      "Iter 32005, training loss 0.000034, validation loss 0.000133\n",
      "Iter 32006, training loss 0.000035, validation loss 0.000158\n",
      "Iter 32007, training loss 0.000031, validation loss 0.000132\n",
      "Iter 32008, training loss 0.000025, validation loss 0.000144\n",
      "Iter 32009, training loss 0.000020, validation loss 0.000121\n",
      "Iter 32010, training loss 0.000013, validation loss 0.000129\n",
      "Iter 32011, training loss 0.000008, validation loss 0.000116\n",
      "Iter 32012, training loss 0.000005, validation loss 0.000115\n",
      "Iter 32013, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32014, training loss 0.000007, validation loss 0.000112\n",
      "Iter 32015, training loss 0.000008, validation loss 0.000120\n",
      "Iter 32016, training loss 0.000010, validation loss 0.000116\n",
      "Iter 32017, training loss 0.000011, validation loss 0.000127\n",
      "Iter 32018, training loss 0.000010, validation loss 0.000116\n",
      "Iter 32019, training loss 0.000009, validation loss 0.000123\n",
      "Iter 32020, training loss 0.000007, validation loss 0.000112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32021, training loss 0.000005, validation loss 0.000116\n",
      "Iter 32022, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32023, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32024, training loss 0.000005, validation loss 0.000116\n",
      "Iter 32025, training loss 0.000006, validation loss 0.000112\n",
      "Iter 32026, training loss 0.000006, validation loss 0.000118\n",
      "Iter 32027, training loss 0.000006, validation loss 0.000114\n",
      "Iter 32028, training loss 0.000005, validation loss 0.000118\n",
      "Iter 32029, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32030, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32031, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32032, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32033, training loss 0.000005, validation loss 0.000118\n",
      "Iter 32034, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32035, training loss 0.000005, validation loss 0.000117\n",
      "Iter 32036, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32037, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32038, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32039, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32040, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32041, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32042, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32043, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32044, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32045, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32046, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32047, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32048, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32049, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32050, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32051, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32052, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32053, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32054, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32055, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32056, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32057, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32058, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32059, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32060, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32061, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32062, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32063, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32064, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32065, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32066, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32067, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32068, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32069, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32070, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32071, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32072, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32073, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32074, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32075, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32076, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32077, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32078, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32079, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32080, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32081, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32082, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32083, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32084, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32085, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32086, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32087, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32088, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32089, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32090, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32091, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32092, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32093, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32094, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32095, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32096, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32097, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32098, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32099, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32100, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32101, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32102, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32103, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32104, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32105, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32106, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32107, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32108, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32109, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32110, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32111, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32112, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32113, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32114, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32115, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32116, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32117, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32118, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32119, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32120, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32121, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32122, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32123, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32124, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32125, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32126, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32127, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32128, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32129, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32130, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32131, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32132, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32133, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32134, training loss 0.000005, validation loss 0.000118\n",
      "Iter 32135, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32136, training loss 0.000005, validation loss 0.000119\n",
      "Iter 32137, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32138, training loss 0.000005, validation loss 0.000119\n",
      "Iter 32139, training loss 0.000006, validation loss 0.000114\n",
      "Iter 32140, training loss 0.000006, validation loss 0.000121\n",
      "Iter 32141, training loss 0.000006, validation loss 0.000114\n",
      "Iter 32142, training loss 0.000006, validation loss 0.000120\n",
      "Iter 32143, training loss 0.000007, validation loss 0.000115\n",
      "Iter 32144, training loss 0.000007, validation loss 0.000121\n",
      "Iter 32145, training loss 0.000007, validation loss 0.000114\n",
      "Iter 32146, training loss 0.000007, validation loss 0.000122\n",
      "Iter 32147, training loss 0.000007, validation loss 0.000114\n",
      "Iter 32148, training loss 0.000007, validation loss 0.000121\n",
      "Iter 32149, training loss 0.000006, validation loss 0.000114\n",
      "Iter 32150, training loss 0.000006, validation loss 0.000120\n",
      "Iter 32151, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32152, training loss 0.000005, validation loss 0.000118\n",
      "Iter 32153, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32154, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32155, training loss 0.000004, validation loss 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32156, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32157, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32158, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32159, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32160, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32161, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32162, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32163, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32164, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32165, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32166, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32167, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32168, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32169, training loss 0.000005, validation loss 0.000118\n",
      "Iter 32170, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32171, training loss 0.000005, validation loss 0.000119\n",
      "Iter 32172, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32173, training loss 0.000006, validation loss 0.000120\n",
      "Iter 32174, training loss 0.000006, validation loss 0.000115\n",
      "Iter 32175, training loss 0.000006, validation loss 0.000121\n",
      "Iter 32176, training loss 0.000007, validation loss 0.000114\n",
      "Iter 32177, training loss 0.000007, validation loss 0.000122\n",
      "Iter 32178, training loss 0.000008, validation loss 0.000116\n",
      "Iter 32179, training loss 0.000009, validation loss 0.000125\n",
      "Iter 32180, training loss 0.000010, validation loss 0.000115\n",
      "Iter 32181, training loss 0.000011, validation loss 0.000127\n",
      "Iter 32182, training loss 0.000012, validation loss 0.000118\n",
      "Iter 32183, training loss 0.000014, validation loss 0.000130\n",
      "Iter 32184, training loss 0.000018, validation loss 0.000122\n",
      "Iter 32185, training loss 0.000022, validation loss 0.000142\n",
      "Iter 32186, training loss 0.000024, validation loss 0.000128\n",
      "Iter 32187, training loss 0.000028, validation loss 0.000146\n",
      "Iter 32188, training loss 0.000034, validation loss 0.000134\n",
      "Iter 32189, training loss 0.000034, validation loss 0.000157\n",
      "Iter 32190, training loss 0.000026, validation loss 0.000128\n",
      "Iter 32191, training loss 0.000019, validation loss 0.000131\n",
      "Iter 32192, training loss 0.000012, validation loss 0.000116\n",
      "Iter 32193, training loss 0.000008, validation loss 0.000121\n",
      "Iter 32194, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32195, training loss 0.000006, validation loss 0.000112\n",
      "Iter 32196, training loss 0.000009, validation loss 0.000120\n",
      "Iter 32197, training loss 0.000013, validation loss 0.000118\n",
      "Iter 32198, training loss 0.000015, validation loss 0.000133\n",
      "Iter 32199, training loss 0.000013, validation loss 0.000117\n",
      "Iter 32200, training loss 0.000011, validation loss 0.000123\n",
      "Iter 32201, training loss 0.000008, validation loss 0.000116\n",
      "Iter 32202, training loss 0.000006, validation loss 0.000120\n",
      "Iter 32203, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32204, training loss 0.000005, validation loss 0.000112\n",
      "Iter 32205, training loss 0.000005, validation loss 0.000116\n",
      "Iter 32206, training loss 0.000006, validation loss 0.000114\n",
      "Iter 32207, training loss 0.000007, validation loss 0.000121\n",
      "Iter 32208, training loss 0.000006, validation loss 0.000113\n",
      "Iter 32209, training loss 0.000005, validation loss 0.000116\n",
      "Iter 32210, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32211, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32212, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32213, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32214, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32215, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32216, training loss 0.000004, validation loss 0.000119\n",
      "Iter 32217, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32218, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32219, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32220, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32221, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32222, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32223, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32224, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32225, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32226, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32227, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32228, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32229, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32230, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32231, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32232, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32233, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32234, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32235, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32236, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32237, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32238, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32239, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32240, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32241, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32242, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32243, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32244, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32245, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32246, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32247, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32248, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32249, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32250, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32251, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32252, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32253, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32254, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32255, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32256, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32257, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32258, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32259, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32260, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32261, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32262, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32263, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32264, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32265, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32266, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32267, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32268, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32269, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32270, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32271, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32272, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32273, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32274, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32275, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32276, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32277, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32278, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32279, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32280, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32281, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32282, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32283, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32284, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32285, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32286, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32287, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32288, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32289, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32290, training loss 0.000003, validation loss 0.000116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32291, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32292, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32293, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32294, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32295, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32296, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32297, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32298, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32299, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32300, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32301, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32302, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32303, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32304, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32305, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32306, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32307, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32308, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32309, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32310, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32311, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32312, training loss 0.000003, validation loss 0.000118\n",
      "Iter 32313, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32314, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32315, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32316, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32317, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32318, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32319, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32320, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32321, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32322, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32323, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32324, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32325, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32326, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32327, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32328, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32329, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32330, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32331, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32332, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32333, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32334, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32335, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32336, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32337, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32338, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32339, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32340, training loss 0.000003, validation loss 0.000117\n",
      "Iter 32341, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32342, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32343, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32344, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32345, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32346, training loss 0.000004, validation loss 0.000119\n",
      "Iter 32347, training loss 0.000005, validation loss 0.000115\n",
      "Iter 32348, training loss 0.000005, validation loss 0.000121\n",
      "Iter 32349, training loss 0.000005, validation loss 0.000115\n",
      "Iter 32350, training loss 0.000006, validation loss 0.000121\n",
      "Iter 32351, training loss 0.000006, validation loss 0.000116\n",
      "Iter 32352, training loss 0.000008, validation loss 0.000125\n",
      "Iter 32353, training loss 0.000010, validation loss 0.000119\n",
      "Iter 32354, training loss 0.000014, validation loss 0.000133\n",
      "Iter 32355, training loss 0.000019, validation loss 0.000124\n",
      "Iter 32356, training loss 0.000027, validation loss 0.000149\n",
      "Iter 32357, training loss 0.000041, validation loss 0.000145\n",
      "Iter 32358, training loss 0.000058, validation loss 0.000188\n",
      "Iter 32359, training loss 0.000069, validation loss 0.000169\n",
      "Iter 32360, training loss 0.000064, validation loss 0.000196\n",
      "Iter 32361, training loss 0.000041, validation loss 0.000143\n",
      "Iter 32362, training loss 0.000015, validation loss 0.000129\n",
      "Iter 32363, training loss 0.000005, validation loss 0.000115\n",
      "Iter 32364, training loss 0.000016, validation loss 0.000121\n",
      "Iter 32365, training loss 0.000033, validation loss 0.000155\n",
      "Iter 32366, training loss 0.000041, validation loss 0.000145\n",
      "Iter 32367, training loss 0.000031, validation loss 0.000163\n",
      "Iter 32368, training loss 0.000013, validation loss 0.000121\n",
      "Iter 32369, training loss 0.000005, validation loss 0.000117\n",
      "Iter 32370, training loss 0.000012, validation loss 0.000126\n",
      "Iter 32371, training loss 0.000022, validation loss 0.000128\n",
      "Iter 32372, training loss 0.000024, validation loss 0.000144\n",
      "Iter 32373, training loss 0.000016, validation loss 0.000122\n",
      "Iter 32374, training loss 0.000007, validation loss 0.000121\n",
      "Iter 32375, training loss 0.000006, validation loss 0.000119\n",
      "Iter 32376, training loss 0.000011, validation loss 0.000118\n",
      "Iter 32377, training loss 0.000016, validation loss 0.000132\n",
      "Iter 32378, training loss 0.000013, validation loss 0.000118\n",
      "Iter 32379, training loss 0.000007, validation loss 0.000119\n",
      "Iter 32380, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32381, training loss 0.000008, validation loss 0.000115\n",
      "Iter 32382, training loss 0.000011, validation loss 0.000126\n",
      "Iter 32383, training loss 0.000009, validation loss 0.000116\n",
      "Iter 32384, training loss 0.000005, validation loss 0.000119\n",
      "Iter 32385, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32386, training loss 0.000006, validation loss 0.000115\n",
      "Iter 32387, training loss 0.000008, validation loss 0.000123\n",
      "Iter 32388, training loss 0.000007, validation loss 0.000115\n",
      "Iter 32389, training loss 0.000005, validation loss 0.000117\n",
      "Iter 32390, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32391, training loss 0.000005, validation loss 0.000113\n",
      "Iter 32392, training loss 0.000006, validation loss 0.000119\n",
      "Iter 32393, training loss 0.000006, validation loss 0.000113\n",
      "Iter 32394, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32395, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32396, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32397, training loss 0.000005, validation loss 0.000118\n",
      "Iter 32398, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32399, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32400, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32401, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32402, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32403, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32404, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32405, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32406, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32407, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32408, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32409, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32410, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32411, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32412, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32413, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32414, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32415, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32416, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32417, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32418, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32419, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32420, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32421, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32422, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32423, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32424, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32425, training loss 0.000003, validation loss 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32426, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32427, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32428, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32429, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32430, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32431, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32432, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32433, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32434, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32435, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32436, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32437, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32438, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32439, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32440, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32441, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32442, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32443, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32444, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32445, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32446, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32447, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32448, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32449, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32450, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32451, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32452, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32453, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32454, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32455, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32456, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32457, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32458, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32459, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32460, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32461, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32462, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32463, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32464, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32465, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32466, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32467, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32468, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32469, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32470, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32471, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32472, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32473, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32474, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32475, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32476, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32477, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32478, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32479, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32480, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32481, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32482, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32483, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32484, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32485, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32486, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32487, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32488, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32489, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32490, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32491, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32492, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32493, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32494, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32495, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32496, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32497, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32498, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32499, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32500, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32501, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32502, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32503, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32504, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32505, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32506, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32507, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32508, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32509, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32510, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32511, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32512, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32513, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32514, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32515, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32516, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32517, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32518, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32519, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32520, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32521, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32522, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32523, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32524, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32525, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32526, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32527, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32528, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32529, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32530, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32531, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32532, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32533, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32534, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32535, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32536, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32537, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32538, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32539, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32540, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32541, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32542, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32543, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32544, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32545, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32546, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32547, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32548, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32549, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32550, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32551, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32552, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32553, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32554, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32555, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32556, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32557, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32558, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32559, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32560, training loss 0.000003, validation loss 0.000115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32561, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32562, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32563, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32564, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32565, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32566, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32567, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32568, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32569, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32570, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32571, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32572, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32573, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32574, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32575, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32576, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32577, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32578, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32579, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32580, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32581, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32582, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32583, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32584, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32585, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32586, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32587, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32588, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32589, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32590, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32591, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32592, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32593, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32594, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32595, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32596, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32597, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32598, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32599, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32600, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32601, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32602, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32603, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32604, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32605, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32606, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32607, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32608, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32609, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32610, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32611, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32612, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32613, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32614, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32615, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32616, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32617, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32618, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32619, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32620, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32621, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32622, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32623, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32624, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32625, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32626, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32627, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32628, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32629, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32630, training loss 0.000004, validation loss 0.000116\n",
      "Iter 32631, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32632, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32633, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32634, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32635, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32636, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32637, training loss 0.000003, validation loss 0.000114\n",
      "Iter 32638, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32639, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32640, training loss 0.000003, validation loss 0.000116\n",
      "Iter 32641, training loss 0.000003, validation loss 0.000115\n",
      "Iter 32642, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32643, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32644, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32645, training loss 0.000004, validation loss 0.000115\n",
      "Iter 32646, training loss 0.000004, validation loss 0.000117\n",
      "Iter 32647, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32648, training loss 0.000004, validation loss 0.000118\n",
      "Iter 32649, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32650, training loss 0.000005, validation loss 0.000118\n",
      "Iter 32651, training loss 0.000005, validation loss 0.000115\n",
      "Iter 32652, training loss 0.000006, validation loss 0.000120\n",
      "Iter 32653, training loss 0.000007, validation loss 0.000116\n",
      "Iter 32654, training loss 0.000008, validation loss 0.000125\n",
      "Iter 32655, training loss 0.000010, validation loss 0.000118\n",
      "Iter 32656, training loss 0.000013, validation loss 0.000131\n",
      "Iter 32657, training loss 0.000019, validation loss 0.000124\n",
      "Iter 32658, training loss 0.000025, validation loss 0.000146\n",
      "Iter 32659, training loss 0.000029, validation loss 0.000126\n",
      "Iter 32660, training loss 0.000036, validation loss 0.000154\n",
      "Iter 32661, training loss 0.000056, validation loss 0.000159\n",
      "Iter 32662, training loss 0.000080, validation loss 0.000213\n",
      "Iter 32663, training loss 0.000090, validation loss 0.000180\n",
      "Iter 32664, training loss 0.000101, validation loss 0.000225\n",
      "Iter 32665, training loss 0.000099, validation loss 0.000184\n",
      "Iter 32666, training loss 0.000062, validation loss 0.000190\n",
      "Iter 32667, training loss 0.000025, validation loss 0.000128\n",
      "Iter 32668, training loss 0.000023, validation loss 0.000122\n",
      "Iter 32669, training loss 0.000014, validation loss 0.000119\n",
      "Iter 32670, training loss 0.000013, validation loss 0.000123\n",
      "Iter 32671, training loss 0.000020, validation loss 0.000134\n",
      "Iter 32672, training loss 0.000017, validation loss 0.000124\n",
      "Iter 32673, training loss 0.000022, validation loss 0.000131\n",
      "Iter 32674, training loss 0.000021, validation loss 0.000123\n",
      "Iter 32675, training loss 0.000015, validation loss 0.000131\n",
      "Iter 32676, training loss 0.000013, validation loss 0.000126\n",
      "Iter 32677, training loss 0.000008, validation loss 0.000125\n",
      "Iter 32678, training loss 0.000009, validation loss 0.000126\n",
      "Iter 32679, training loss 0.000013, validation loss 0.000129\n",
      "Iter 32680, training loss 0.000015, validation loss 0.000131\n",
      "Iter 32681, training loss 0.000019, validation loss 0.000127\n",
      "Iter 32682, training loss 0.000012, validation loss 0.000126\n",
      "Iter 32683, training loss 0.000010, validation loss 0.000113\n",
      "Iter 32684, training loss 0.000008, validation loss 0.000112\n",
      "Iter 32685, training loss 0.000009, validation loss 0.000114\n",
      "Iter 32686, training loss 0.000012, validation loss 0.000116\n",
      "Iter 32687, training loss 0.000013, validation loss 0.000122\n",
      "Iter 32688, training loss 0.000010, validation loss 0.000112\n",
      "Iter 32689, training loss 0.000009, validation loss 0.000111\n",
      "Iter 32690, training loss 0.000007, validation loss 0.000108\n",
      "Iter 32691, training loss 0.000007, validation loss 0.000109\n",
      "Iter 32692, training loss 0.000008, validation loss 0.000115\n",
      "Iter 32693, training loss 0.000008, validation loss 0.000110\n",
      "Iter 32694, training loss 0.000007, validation loss 0.000113\n",
      "Iter 32695, training loss 0.000006, validation loss 0.000109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32696, training loss 0.000005, validation loss 0.000110\n",
      "Iter 32697, training loss 0.000006, validation loss 0.000112\n",
      "Iter 32698, training loss 0.000006, validation loss 0.000109\n",
      "Iter 32699, training loss 0.000006, validation loss 0.000113\n",
      "Iter 32700, training loss 0.000006, validation loss 0.000110\n",
      "Iter 32701, training loss 0.000005, validation loss 0.000111\n",
      "Iter 32702, training loss 0.000005, validation loss 0.000110\n",
      "Iter 32703, training loss 0.000005, validation loss 0.000109\n",
      "Iter 32704, training loss 0.000005, validation loss 0.000111\n",
      "Iter 32705, training loss 0.000005, validation loss 0.000109\n",
      "Iter 32706, training loss 0.000005, validation loss 0.000110\n",
      "Iter 32707, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32708, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32709, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32710, training loss 0.000005, validation loss 0.000109\n",
      "Iter 32711, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32712, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32713, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32714, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32715, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32716, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32717, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32718, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32719, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32720, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32721, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32722, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32723, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32724, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32725, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32726, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32727, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32728, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32729, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32730, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32731, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32732, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32733, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32734, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32735, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32736, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32737, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32738, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32739, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32740, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32741, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32742, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32743, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32744, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32745, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32746, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32747, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32748, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32749, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32750, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32751, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32752, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32753, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32754, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32755, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32756, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32757, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32758, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32759, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32760, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32761, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32762, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32763, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32764, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32765, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32766, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32767, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32768, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32769, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32770, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32771, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32772, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32773, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32774, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32775, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32776, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32777, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32778, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32779, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32780, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32781, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32782, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32783, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32784, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32785, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32786, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32787, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32788, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32789, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32790, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32791, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32792, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32793, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32794, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32795, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32796, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32797, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32798, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32799, training loss 0.000003, validation loss 0.000109\n",
      "Iter 32800, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32801, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32802, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32803, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32804, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32805, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32806, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32807, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32808, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32809, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32810, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32811, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32812, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32813, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32814, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32815, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32816, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32817, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32818, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32819, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32820, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32821, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32822, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32823, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32824, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32825, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32826, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32827, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32828, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32829, training loss 0.000003, validation loss 0.000110\n",
      "Iter 32830, training loss 0.000004, validation loss 0.000110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32831, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32832, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32833, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32834, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32835, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32836, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32837, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32838, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32839, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32840, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32841, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32842, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32843, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32844, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32845, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32846, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32847, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32848, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32849, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32850, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32851, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32852, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32853, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32854, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32855, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32856, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32857, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32858, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32859, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32860, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32861, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32862, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32863, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32864, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32865, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32866, training loss 0.000005, validation loss 0.000112\n",
      "Iter 32867, training loss 0.000005, validation loss 0.000110\n",
      "Iter 32868, training loss 0.000005, validation loss 0.000113\n",
      "Iter 32869, training loss 0.000006, validation loss 0.000111\n",
      "Iter 32870, training loss 0.000006, validation loss 0.000114\n",
      "Iter 32871, training loss 0.000006, validation loss 0.000110\n",
      "Iter 32872, training loss 0.000007, validation loss 0.000115\n",
      "Iter 32873, training loss 0.000007, validation loss 0.000111\n",
      "Iter 32874, training loss 0.000007, validation loss 0.000115\n",
      "Iter 32875, training loss 0.000007, validation loss 0.000110\n",
      "Iter 32876, training loss 0.000007, validation loss 0.000115\n",
      "Iter 32877, training loss 0.000007, validation loss 0.000111\n",
      "Iter 32878, training loss 0.000007, validation loss 0.000117\n",
      "Iter 32879, training loss 0.000007, validation loss 0.000111\n",
      "Iter 32880, training loss 0.000007, validation loss 0.000114\n",
      "Iter 32881, training loss 0.000007, validation loss 0.000110\n",
      "Iter 32882, training loss 0.000007, validation loss 0.000115\n",
      "Iter 32883, training loss 0.000006, validation loss 0.000111\n",
      "Iter 32884, training loss 0.000006, validation loss 0.000114\n",
      "Iter 32885, training loss 0.000005, validation loss 0.000111\n",
      "Iter 32886, training loss 0.000005, validation loss 0.000113\n",
      "Iter 32887, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32888, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32889, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32890, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32891, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32892, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32893, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32894, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32895, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32896, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32897, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32898, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32899, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32900, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32901, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32902, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32903, training loss 0.000003, validation loss 0.000111\n",
      "Iter 32904, training loss 0.000003, validation loss 0.000111\n",
      "Iter 32905, training loss 0.000003, validation loss 0.000111\n",
      "Iter 32906, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32907, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32908, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32909, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32910, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32911, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32912, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32913, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32914, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32915, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32916, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32917, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32918, training loss 0.000003, validation loss 0.000111\n",
      "Iter 32919, training loss 0.000003, validation loss 0.000111\n",
      "Iter 32920, training loss 0.000003, validation loss 0.000112\n",
      "Iter 32921, training loss 0.000003, validation loss 0.000112\n",
      "Iter 32922, training loss 0.000003, validation loss 0.000111\n",
      "Iter 32923, training loss 0.000003, validation loss 0.000112\n",
      "Iter 32924, training loss 0.000003, validation loss 0.000112\n",
      "Iter 32925, training loss 0.000003, validation loss 0.000112\n",
      "Iter 32926, training loss 0.000003, validation loss 0.000111\n",
      "Iter 32927, training loss 0.000003, validation loss 0.000112\n",
      "Iter 32928, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32929, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32930, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32931, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32932, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32933, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32934, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32935, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32936, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32937, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32938, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32939, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32940, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32941, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32942, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32943, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32944, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32945, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32946, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32947, training loss 0.000004, validation loss 0.000113\n",
      "Iter 32948, training loss 0.000004, validation loss 0.000111\n",
      "Iter 32949, training loss 0.000004, validation loss 0.000114\n",
      "Iter 32950, training loss 0.000004, validation loss 0.000112\n",
      "Iter 32951, training loss 0.000005, validation loss 0.000114\n",
      "Iter 32952, training loss 0.000005, validation loss 0.000112\n",
      "Iter 32953, training loss 0.000006, validation loss 0.000116\n",
      "Iter 32954, training loss 0.000007, validation loss 0.000112\n",
      "Iter 32955, training loss 0.000008, validation loss 0.000118\n",
      "Iter 32956, training loss 0.000011, validation loss 0.000116\n",
      "Iter 32957, training loss 0.000015, validation loss 0.000128\n",
      "Iter 32958, training loss 0.000021, validation loss 0.000122\n",
      "Iter 32959, training loss 0.000029, validation loss 0.000143\n",
      "Iter 32960, training loss 0.000039, validation loss 0.000142\n",
      "Iter 32961, training loss 0.000054, validation loss 0.000173\n",
      "Iter 32962, training loss 0.000062, validation loss 0.000157\n",
      "Iter 32963, training loss 0.000052, validation loss 0.000173\n",
      "Iter 32964, training loss 0.000027, validation loss 0.000126\n",
      "Iter 32965, training loss 0.000008, validation loss 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32966, training loss 0.000006, validation loss 0.000111\n",
      "Iter 32967, training loss 0.000017, validation loss 0.000116\n",
      "Iter 32968, training loss 0.000028, validation loss 0.000141\n",
      "Iter 32969, training loss 0.000029, validation loss 0.000128\n",
      "Iter 32970, training loss 0.000019, validation loss 0.000133\n",
      "Iter 32971, training loss 0.000007, validation loss 0.000108\n",
      "Iter 32972, training loss 0.000006, validation loss 0.000106\n",
      "Iter 32973, training loss 0.000014, validation loss 0.000122\n",
      "Iter 32974, training loss 0.000021, validation loss 0.000119\n",
      "Iter 32975, training loss 0.000018, validation loss 0.000130\n",
      "Iter 32976, training loss 0.000010, validation loss 0.000110\n",
      "Iter 32977, training loss 0.000004, validation loss 0.000110\n",
      "Iter 32978, training loss 0.000007, validation loss 0.000115\n",
      "Iter 32979, training loss 0.000013, validation loss 0.000112\n",
      "Iter 32980, training loss 0.000015, validation loss 0.000126\n",
      "Iter 32981, training loss 0.000012, validation loss 0.000109\n",
      "Iter 32982, training loss 0.000006, validation loss 0.000112\n",
      "Iter 32983, training loss 0.000005, validation loss 0.000109\n",
      "Iter 32984, training loss 0.000008, validation loss 0.000108\n",
      "Iter 32985, training loss 0.000011, validation loss 0.000119\n",
      "Iter 32986, training loss 0.000009, validation loss 0.000108\n",
      "Iter 32987, training loss 0.000005, validation loss 0.000110\n",
      "Iter 32988, training loss 0.000004, validation loss 0.000109\n",
      "Iter 32989, training loss 0.000006, validation loss 0.000108\n",
      "Iter 32990, training loss 0.000008, validation loss 0.000116\n",
      "Iter 32991, training loss 0.000007, validation loss 0.000108\n",
      "Iter 32992, training loss 0.000005, validation loss 0.000111\n",
      "Iter 32993, training loss 0.000004, validation loss 0.000107\n",
      "Iter 32994, training loss 0.000005, validation loss 0.000106\n",
      "Iter 32995, training loss 0.000006, validation loss 0.000113\n",
      "Iter 32996, training loss 0.000006, validation loss 0.000108\n",
      "Iter 32997, training loss 0.000005, validation loss 0.000111\n",
      "Iter 32998, training loss 0.000004, validation loss 0.000107\n",
      "Iter 32999, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33000, training loss 0.000005, validation loss 0.000111\n",
      "Iter 33001, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33002, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33003, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33004, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33005, training loss 0.000005, validation loss 0.000110\n",
      "Iter 33006, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33007, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33008, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33009, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33010, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33011, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33012, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33013, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33014, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33015, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33016, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33017, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33018, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33019, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33020, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33021, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33022, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33023, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33024, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33025, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33026, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33027, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33028, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33029, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33030, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33031, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33032, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33033, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33034, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33035, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33036, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33037, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33038, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33039, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33040, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33041, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33042, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33043, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33044, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33045, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33046, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33047, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33048, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33049, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33050, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33051, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33052, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33053, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33054, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33055, training loss 0.000003, validation loss 0.000108\n",
      "Iter 33056, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33057, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33058, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33059, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33060, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33061, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33062, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33063, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33064, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33065, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33066, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33067, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33068, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33069, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33070, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33071, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33072, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33073, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33074, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33075, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33076, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33077, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33078, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33079, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33080, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33081, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33082, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33083, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33084, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33085, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33086, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33087, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33088, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33089, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33090, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33091, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33092, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33093, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33094, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33095, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33096, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33097, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33098, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33099, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33100, training loss 0.000003, validation loss 0.000109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 33101, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33102, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33103, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33104, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33105, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33106, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33107, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33108, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33109, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33110, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33111, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33112, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33113, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33114, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33115, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33116, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33117, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33118, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33119, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33120, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33121, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33122, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33123, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33124, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33125, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33126, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33127, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33128, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33129, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33130, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33131, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33132, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33133, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33134, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33135, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33136, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33137, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33138, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33139, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33140, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33141, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33142, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33143, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33144, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33145, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33146, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33147, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33148, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33149, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33150, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33151, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33152, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33153, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33154, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33155, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33156, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33157, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33158, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33159, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33160, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33161, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33162, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33163, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33164, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33165, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33166, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33167, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33168, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33169, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33170, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33171, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33172, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33173, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33174, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33175, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33176, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33177, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33178, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33179, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33180, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33181, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33182, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33183, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33184, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33185, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33186, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33187, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33188, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33189, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33190, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33191, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33192, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33193, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33194, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33195, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33196, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33197, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33198, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33199, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33200, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33201, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33202, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33203, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33204, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33205, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33206, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33207, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33208, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33209, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33210, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33211, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33212, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33213, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33214, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33215, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33216, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33217, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33218, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33219, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33220, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33221, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33222, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33223, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33224, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33225, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33226, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33227, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33228, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33229, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33230, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33231, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33232, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33233, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33234, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33235, training loss 0.000006, validation loss 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 33236, training loss 0.000006, validation loss 0.000109\n",
      "Iter 33237, training loss 0.000006, validation loss 0.000115\n",
      "Iter 33238, training loss 0.000007, validation loss 0.000110\n",
      "Iter 33239, training loss 0.000007, validation loss 0.000116\n",
      "Iter 33240, training loss 0.000009, validation loss 0.000111\n",
      "Iter 33241, training loss 0.000010, validation loss 0.000120\n",
      "Iter 33242, training loss 0.000011, validation loss 0.000111\n",
      "Iter 33243, training loss 0.000012, validation loss 0.000122\n",
      "Iter 33244, training loss 0.000013, validation loss 0.000114\n",
      "Iter 33245, training loss 0.000016, validation loss 0.000127\n",
      "Iter 33246, training loss 0.000021, validation loss 0.000120\n",
      "Iter 33247, training loss 0.000026, validation loss 0.000142\n",
      "Iter 33248, training loss 0.000028, validation loss 0.000128\n",
      "Iter 33249, training loss 0.000027, validation loss 0.000142\n",
      "Iter 33250, training loss 0.000028, validation loss 0.000128\n",
      "Iter 33251, training loss 0.000027, validation loss 0.000144\n",
      "Iter 33252, training loss 0.000024, validation loss 0.000116\n",
      "Iter 33253, training loss 0.000015, validation loss 0.000125\n",
      "Iter 33254, training loss 0.000009, validation loss 0.000109\n",
      "Iter 33255, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33256, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33257, training loss 0.000008, validation loss 0.000106\n",
      "Iter 33258, training loss 0.000010, validation loss 0.000116\n",
      "Iter 33259, training loss 0.000012, validation loss 0.000111\n",
      "Iter 33260, training loss 0.000012, validation loss 0.000121\n",
      "Iter 33261, training loss 0.000010, validation loss 0.000111\n",
      "Iter 33262, training loss 0.000007, validation loss 0.000116\n",
      "Iter 33263, training loss 0.000005, validation loss 0.000110\n",
      "Iter 33264, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33265, training loss 0.000006, validation loss 0.000113\n",
      "Iter 33266, training loss 0.000007, validation loss 0.000110\n",
      "Iter 33267, training loss 0.000008, validation loss 0.000117\n",
      "Iter 33268, training loss 0.000007, validation loss 0.000111\n",
      "Iter 33269, training loss 0.000006, validation loss 0.000114\n",
      "Iter 33270, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33271, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33272, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33273, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33274, training loss 0.000006, validation loss 0.000113\n",
      "Iter 33275, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33276, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33277, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33278, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33279, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33280, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33281, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33282, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33283, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33284, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33285, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33286, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33287, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33288, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33289, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33290, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33291, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33292, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33293, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33294, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33295, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33296, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33297, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33298, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33299, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33300, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33301, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33302, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33303, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33304, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33305, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33306, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33307, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33308, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33309, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33310, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33311, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33312, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33313, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33314, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33315, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33316, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33317, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33318, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33319, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33320, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33321, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33322, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33323, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33324, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33325, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33326, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33327, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33328, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33329, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33330, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33331, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33332, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33333, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33334, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33335, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33336, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33337, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33338, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33339, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33340, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33341, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33342, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33343, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33344, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33345, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33346, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33347, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33348, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33349, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33350, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33351, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33352, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33353, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33354, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33355, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33356, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33357, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33358, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33359, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33360, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33361, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33362, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33363, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33364, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33365, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33366, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33367, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33368, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33369, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33370, training loss 0.000004, validation loss 0.000109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 33371, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33372, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33373, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33374, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33375, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33376, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33377, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33378, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33379, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33380, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33381, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33382, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33383, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33384, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33385, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33386, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33387, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33388, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33389, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33390, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33391, training loss 0.000003, validation loss 0.000110\n",
      "Iter 33392, training loss 0.000003, validation loss 0.000109\n",
      "Iter 33393, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33394, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33395, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33396, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33397, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33398, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33399, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33400, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33401, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33402, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33403, training loss 0.000004, validation loss 0.000112\n",
      "Iter 33404, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33405, training loss 0.000005, validation loss 0.000113\n",
      "Iter 33406, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33407, training loss 0.000006, validation loss 0.000114\n",
      "Iter 33408, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33409, training loss 0.000007, validation loss 0.000115\n",
      "Iter 33410, training loss 0.000008, validation loss 0.000112\n",
      "Iter 33411, training loss 0.000011, validation loss 0.000121\n",
      "Iter 33412, training loss 0.000015, validation loss 0.000115\n",
      "Iter 33413, training loss 0.000020, validation loss 0.000132\n",
      "Iter 33414, training loss 0.000022, validation loss 0.000119\n",
      "Iter 33415, training loss 0.000025, validation loss 0.000137\n",
      "Iter 33416, training loss 0.000034, validation loss 0.000133\n",
      "Iter 33417, training loss 0.000048, validation loss 0.000168\n",
      "Iter 33418, training loss 0.000055, validation loss 0.000141\n",
      "Iter 33419, training loss 0.000059, validation loss 0.000173\n",
      "Iter 33420, training loss 0.000066, validation loss 0.000162\n",
      "Iter 33421, training loss 0.000054, validation loss 0.000178\n",
      "Iter 33422, training loss 0.000027, validation loss 0.000127\n",
      "Iter 33423, training loss 0.000016, validation loss 0.000119\n",
      "Iter 33424, training loss 0.000007, validation loss 0.000112\n",
      "Iter 33425, training loss 0.000015, validation loss 0.000117\n",
      "Iter 33426, training loss 0.000020, validation loss 0.000129\n",
      "Iter 33427, training loss 0.000021, validation loss 0.000118\n",
      "Iter 33428, training loss 0.000021, validation loss 0.000123\n",
      "Iter 33429, training loss 0.000014, validation loss 0.000115\n",
      "Iter 33430, training loss 0.000011, validation loss 0.000121\n",
      "Iter 33431, training loss 0.000006, validation loss 0.000110\n",
      "Iter 33432, training loss 0.000008, validation loss 0.000105\n",
      "Iter 33433, training loss 0.000010, validation loss 0.000109\n",
      "Iter 33434, training loss 0.000011, validation loss 0.000110\n",
      "Iter 33435, training loss 0.000012, validation loss 0.000118\n",
      "Iter 33436, training loss 0.000007, validation loss 0.000105\n",
      "Iter 33437, training loss 0.000007, validation loss 0.000104\n",
      "Iter 33438, training loss 0.000006, validation loss 0.000105\n",
      "Iter 33439, training loss 0.000007, validation loss 0.000107\n",
      "Iter 33440, training loss 0.000008, validation loss 0.000115\n",
      "Iter 33441, training loss 0.000007, validation loss 0.000108\n",
      "Iter 33442, training loss 0.000006, validation loss 0.000107\n",
      "Iter 33443, training loss 0.000005, validation loss 0.000105\n",
      "Iter 33444, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33445, training loss 0.000006, validation loss 0.000110\n",
      "Iter 33446, training loss 0.000005, validation loss 0.000106\n",
      "Iter 33447, training loss 0.000006, validation loss 0.000107\n",
      "Iter 33448, training loss 0.000005, validation loss 0.000105\n",
      "Iter 33449, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33450, training loss 0.000005, validation loss 0.000106\n",
      "Iter 33451, training loss 0.000004, validation loss 0.000105\n",
      "Iter 33452, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33453, training loss 0.000005, validation loss 0.000106\n",
      "Iter 33454, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33455, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33456, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33457, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33458, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33459, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33460, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33461, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33462, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33463, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33464, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33465, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33466, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33467, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33468, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33469, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33470, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33471, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33472, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33473, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33474, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33475, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33476, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33477, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33478, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33479, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33480, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33481, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33482, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33483, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33484, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33485, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33486, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33487, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33488, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33489, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33490, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33491, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33492, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33493, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33494, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33495, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33496, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33497, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33498, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33499, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33500, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33501, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33502, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33503, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33504, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33505, training loss 0.000004, validation loss 0.000109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 33506, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33507, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33508, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33509, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33510, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33511, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33512, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33513, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33514, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33515, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33516, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33517, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33518, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33519, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33520, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33521, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33522, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33523, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33524, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33525, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33526, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33527, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33528, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33529, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33530, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33531, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33532, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33533, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33534, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33535, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33536, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33537, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33538, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33539, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33540, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33541, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33542, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33543, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33544, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33545, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33546, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33547, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33548, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33549, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33550, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33551, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33552, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33553, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33554, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33555, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33556, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33557, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33558, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33559, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33560, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33561, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33562, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33563, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33564, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33565, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33566, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33567, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33568, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33569, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33570, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33571, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33572, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33573, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33574, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33575, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33576, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33577, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33578, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33579, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33580, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33581, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33582, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33583, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33584, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33585, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33586, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33587, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33588, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33589, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33590, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33591, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33592, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33593, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33594, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33595, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33596, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33597, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33598, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33599, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33600, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33601, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33602, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33603, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33604, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33605, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33606, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33607, training loss 0.000004, validation loss 0.000112\n",
      "Iter 33608, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33609, training loss 0.000005, validation loss 0.000113\n",
      "Iter 33610, training loss 0.000005, validation loss 0.000110\n",
      "Iter 33611, training loss 0.000005, validation loss 0.000114\n",
      "Iter 33612, training loss 0.000006, validation loss 0.000110\n",
      "Iter 33613, training loss 0.000006, validation loss 0.000115\n",
      "Iter 33614, training loss 0.000007, validation loss 0.000111\n",
      "Iter 33615, training loss 0.000007, validation loss 0.000115\n",
      "Iter 33616, training loss 0.000007, validation loss 0.000110\n",
      "Iter 33617, training loss 0.000007, validation loss 0.000115\n",
      "Iter 33618, training loss 0.000007, validation loss 0.000110\n",
      "Iter 33619, training loss 0.000006, validation loss 0.000115\n",
      "Iter 33620, training loss 0.000006, validation loss 0.000109\n",
      "Iter 33621, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33622, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33623, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33624, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33625, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33626, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33627, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33628, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33629, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33630, training loss 0.000005, validation loss 0.000113\n",
      "Iter 33631, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33632, training loss 0.000005, validation loss 0.000113\n",
      "Iter 33633, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33634, training loss 0.000004, validation loss 0.000112\n",
      "Iter 33635, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33636, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33637, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33638, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33639, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33640, training loss 0.000004, validation loss 0.000108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 33641, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33642, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33643, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33644, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33645, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33646, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33647, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33648, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33649, training loss 0.000005, validation loss 0.000113\n",
      "Iter 33650, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33651, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33652, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33653, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33654, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33655, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33656, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33657, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33658, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33659, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33660, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33661, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33662, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33663, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33664, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33665, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33666, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33667, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33668, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33669, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33670, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33671, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33672, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33673, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33674, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33675, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33676, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33677, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33678, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33679, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33680, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33681, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33682, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33683, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33684, training loss 0.000004, validation loss 0.000112\n",
      "Iter 33685, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33686, training loss 0.000005, validation loss 0.000111\n",
      "Iter 33687, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33688, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33689, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33690, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33691, training loss 0.000006, validation loss 0.000109\n",
      "Iter 33692, training loss 0.000007, validation loss 0.000115\n",
      "Iter 33693, training loss 0.000007, validation loss 0.000110\n",
      "Iter 33694, training loss 0.000010, validation loss 0.000118\n",
      "Iter 33695, training loss 0.000014, validation loss 0.000115\n",
      "Iter 33696, training loss 0.000020, validation loss 0.000133\n",
      "Iter 33697, training loss 0.000025, validation loss 0.000123\n",
      "Iter 33698, training loss 0.000035, validation loss 0.000147\n",
      "Iter 33699, training loss 0.000053, validation loss 0.000150\n",
      "Iter 33700, training loss 0.000076, validation loss 0.000203\n",
      "Iter 33701, training loss 0.000075, validation loss 0.000167\n",
      "Iter 33702, training loss 0.000051, validation loss 0.000170\n",
      "Iter 33703, training loss 0.000024, validation loss 0.000122\n",
      "Iter 33704, training loss 0.000009, validation loss 0.000116\n",
      "Iter 33705, training loss 0.000007, validation loss 0.000111\n",
      "Iter 33706, training loss 0.000016, validation loss 0.000115\n",
      "Iter 33707, training loss 0.000026, validation loss 0.000134\n",
      "Iter 33708, training loss 0.000027, validation loss 0.000125\n",
      "Iter 33709, training loss 0.000018, validation loss 0.000130\n",
      "Iter 33710, training loss 0.000008, validation loss 0.000109\n",
      "Iter 33711, training loss 0.000008, validation loss 0.000106\n",
      "Iter 33712, training loss 0.000014, validation loss 0.000120\n",
      "Iter 33713, training loss 0.000020, validation loss 0.000118\n",
      "Iter 33714, training loss 0.000018, validation loss 0.000130\n",
      "Iter 33715, training loss 0.000010, validation loss 0.000110\n",
      "Iter 33716, training loss 0.000006, validation loss 0.000110\n",
      "Iter 33717, training loss 0.000007, validation loss 0.000114\n",
      "Iter 33718, training loss 0.000011, validation loss 0.000112\n",
      "Iter 33719, training loss 0.000012, validation loss 0.000123\n",
      "Iter 33720, training loss 0.000009, validation loss 0.000107\n",
      "Iter 33721, training loss 0.000006, validation loss 0.000107\n",
      "Iter 33722, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33723, training loss 0.000007, validation loss 0.000108\n",
      "Iter 33724, training loss 0.000008, validation loss 0.000114\n",
      "Iter 33725, training loss 0.000007, validation loss 0.000106\n",
      "Iter 33726, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33727, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33728, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33729, training loss 0.000006, validation loss 0.000112\n",
      "Iter 33730, training loss 0.000006, validation loss 0.000107\n",
      "Iter 33731, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33732, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33733, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33734, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33735, training loss 0.000005, validation loss 0.000106\n",
      "Iter 33736, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33737, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33738, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33739, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33740, training loss 0.000004, validation loss 0.000106\n",
      "Iter 33741, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33742, training loss 0.000004, validation loss 0.000106\n",
      "Iter 33743, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33744, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33745, training loss 0.000004, validation loss 0.000106\n",
      "Iter 33746, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33747, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33748, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33749, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33750, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33751, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33752, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33753, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33754, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33755, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33756, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33757, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33758, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33759, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33760, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33761, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33762, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33763, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33764, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33765, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33766, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33767, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33768, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33769, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33770, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33771, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33772, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33773, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33774, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33775, training loss 0.000004, validation loss 0.000107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 33776, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33777, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33778, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33779, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33780, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33781, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33782, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33783, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33784, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33785, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33786, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33787, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33788, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33789, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33790, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33791, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33792, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33793, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33794, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33795, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33796, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33797, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33798, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33799, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33800, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33801, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33802, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33803, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33804, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33805, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33806, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33807, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33808, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33809, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33810, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33811, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33812, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33813, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33814, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33815, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33816, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33817, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33818, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33819, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33820, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33821, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33822, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33823, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33824, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33825, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33826, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33827, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33828, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33829, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33830, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33831, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33832, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33833, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33834, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33835, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33836, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33837, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33838, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33839, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33840, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33841, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33842, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33843, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33844, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33845, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33846, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33847, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33848, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33849, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33850, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33851, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33852, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33853, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33854, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33855, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33856, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33857, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33858, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33859, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33860, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33861, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33862, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33863, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33864, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33865, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33866, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33867, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33868, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33869, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33870, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33871, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33872, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33873, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33874, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33875, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33876, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33877, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33878, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33879, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33880, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33881, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33882, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33883, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33884, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33885, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33886, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33887, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33888, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33889, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33890, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33891, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33892, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33893, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33894, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33895, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33896, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33897, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33898, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33899, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33900, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33901, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33902, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33903, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33904, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33905, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33906, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33907, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33908, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33909, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33910, training loss 0.000004, validation loss 0.000109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 33911, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33912, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33913, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33914, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33915, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33916, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33917, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33918, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33919, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33920, training loss 0.000005, validation loss 0.000110\n",
      "Iter 33921, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33922, training loss 0.000005, validation loss 0.000110\n",
      "Iter 33923, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33924, training loss 0.000004, validation loss 0.000111\n",
      "Iter 33925, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33926, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33927, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33928, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33929, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33930, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33931, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33932, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33933, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33934, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33935, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33936, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33937, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33938, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33939, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33940, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33941, training loss 0.000005, validation loss 0.000111\n",
      "Iter 33942, training loss 0.000005, validation loss 0.000107\n",
      "Iter 33943, training loss 0.000005, validation loss 0.000111\n",
      "Iter 33944, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33945, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33946, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33947, training loss 0.000005, validation loss 0.000112\n",
      "Iter 33948, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33949, training loss 0.000006, validation loss 0.000113\n",
      "Iter 33950, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33951, training loss 0.000006, validation loss 0.000114\n",
      "Iter 33952, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33953, training loss 0.000006, validation loss 0.000113\n",
      "Iter 33954, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33955, training loss 0.000006, validation loss 0.000114\n",
      "Iter 33956, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33957, training loss 0.000006, validation loss 0.000113\n",
      "Iter 33958, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33959, training loss 0.000005, validation loss 0.000111\n",
      "Iter 33960, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33961, training loss 0.000004, validation loss 0.000110\n",
      "Iter 33962, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33963, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33964, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33965, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33966, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33967, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33968, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33969, training loss 0.000004, validation loss 0.000108\n",
      "Iter 33970, training loss 0.000005, validation loss 0.000111\n",
      "Iter 33971, training loss 0.000005, validation loss 0.000108\n",
      "Iter 33972, training loss 0.000006, validation loss 0.000112\n",
      "Iter 33973, training loss 0.000006, validation loss 0.000108\n",
      "Iter 33974, training loss 0.000007, validation loss 0.000114\n",
      "Iter 33975, training loss 0.000008, validation loss 0.000109\n",
      "Iter 33976, training loss 0.000008, validation loss 0.000117\n",
      "Iter 33977, training loss 0.000010, validation loss 0.000111\n",
      "Iter 33978, training loss 0.000011, validation loss 0.000121\n",
      "Iter 33979, training loss 0.000014, validation loss 0.000114\n",
      "Iter 33980, training loss 0.000016, validation loss 0.000128\n",
      "Iter 33981, training loss 0.000020, validation loss 0.000119\n",
      "Iter 33982, training loss 0.000023, validation loss 0.000137\n",
      "Iter 33983, training loss 0.000025, validation loss 0.000122\n",
      "Iter 33984, training loss 0.000022, validation loss 0.000136\n",
      "Iter 33985, training loss 0.000015, validation loss 0.000112\n",
      "Iter 33986, training loss 0.000008, validation loss 0.000115\n",
      "Iter 33987, training loss 0.000004, validation loss 0.000107\n",
      "Iter 33988, training loss 0.000005, validation loss 0.000106\n",
      "Iter 33989, training loss 0.000009, validation loss 0.000118\n",
      "Iter 33990, training loss 0.000014, validation loss 0.000113\n",
      "Iter 33991, training loss 0.000016, validation loss 0.000130\n",
      "Iter 33992, training loss 0.000014, validation loss 0.000115\n",
      "Iter 33993, training loss 0.000009, validation loss 0.000120\n",
      "Iter 33994, training loss 0.000005, validation loss 0.000109\n",
      "Iter 33995, training loss 0.000004, validation loss 0.000109\n",
      "Iter 33996, training loss 0.000005, validation loss 0.000113\n",
      "Iter 33997, training loss 0.000008, validation loss 0.000111\n",
      "Iter 33998, training loss 0.000008, validation loss 0.000120\n",
      "Iter 33999, training loss 0.000008, validation loss 0.000111\n",
      "Iter 34000, training loss 0.000006, validation loss 0.000115\n",
      "Iter 34001, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34002, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34003, training loss 0.000005, validation loss 0.000113\n",
      "Iter 34004, training loss 0.000006, validation loss 0.000110\n",
      "Iter 34005, training loss 0.000007, validation loss 0.000118\n",
      "Iter 34006, training loss 0.000006, validation loss 0.000111\n",
      "Iter 34007, training loss 0.000005, validation loss 0.000115\n",
      "Iter 34008, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34009, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34010, training loss 0.000004, validation loss 0.000112\n",
      "Iter 34011, training loss 0.000005, validation loss 0.000109\n",
      "Iter 34012, training loss 0.000005, validation loss 0.000114\n",
      "Iter 34013, training loss 0.000005, validation loss 0.000109\n",
      "Iter 34014, training loss 0.000005, validation loss 0.000113\n",
      "Iter 34015, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34016, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34017, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34018, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34019, training loss 0.000004, validation loss 0.000112\n",
      "Iter 34020, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34021, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34022, training loss 0.000004, validation loss 0.000108\n",
      "Iter 34023, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34024, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34025, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34026, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34027, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34028, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34029, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34030, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34031, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34032, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34033, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34034, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34035, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34036, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34037, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34038, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34039, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34040, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34041, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34042, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34043, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34044, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34045, training loss 0.000003, validation loss 0.000110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34046, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34047, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34048, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34049, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34050, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34051, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34052, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34053, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34054, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34055, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34056, training loss 0.000003, validation loss 0.000109\n",
      "Iter 34057, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34058, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34059, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34060, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34061, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34062, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34063, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34064, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34065, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34066, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34067, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34068, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34069, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34070, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34071, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34072, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34073, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34074, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34075, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34076, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34077, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34078, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34079, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34080, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34081, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34082, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34083, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34084, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34085, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34086, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34087, training loss 0.000003, validation loss 0.000109\n",
      "Iter 34088, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34089, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34090, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34091, training loss 0.000003, validation loss 0.000109\n",
      "Iter 34092, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34093, training loss 0.000003, validation loss 0.000109\n",
      "Iter 34094, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34095, training loss 0.000003, validation loss 0.000109\n",
      "Iter 34096, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34097, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34098, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34099, training loss 0.000004, validation loss 0.000109\n",
      "Iter 34100, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34101, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34102, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34103, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34104, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34105, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34106, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34107, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34108, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34109, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34110, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34111, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34112, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34113, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34114, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34115, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34116, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34117, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34118, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34119, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34120, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34121, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34122, training loss 0.000004, validation loss 0.000113\n",
      "Iter 34123, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34124, training loss 0.000004, validation loss 0.000113\n",
      "Iter 34125, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34126, training loss 0.000004, validation loss 0.000113\n",
      "Iter 34127, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34128, training loss 0.000004, validation loss 0.000114\n",
      "Iter 34129, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34130, training loss 0.000004, validation loss 0.000114\n",
      "Iter 34131, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34132, training loss 0.000004, validation loss 0.000114\n",
      "Iter 34133, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34134, training loss 0.000004, validation loss 0.000115\n",
      "Iter 34135, training loss 0.000005, validation loss 0.000110\n",
      "Iter 34136, training loss 0.000005, validation loss 0.000115\n",
      "Iter 34137, training loss 0.000005, validation loss 0.000110\n",
      "Iter 34138, training loss 0.000004, validation loss 0.000114\n",
      "Iter 34139, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34140, training loss 0.000004, validation loss 0.000115\n",
      "Iter 34141, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34142, training loss 0.000004, validation loss 0.000114\n",
      "Iter 34143, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34144, training loss 0.000004, validation loss 0.000114\n",
      "Iter 34145, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34146, training loss 0.000004, validation loss 0.000113\n",
      "Iter 34147, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34148, training loss 0.000004, validation loss 0.000112\n",
      "Iter 34149, training loss 0.000003, validation loss 0.000110\n",
      "Iter 34150, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34151, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34152, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34153, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34154, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34155, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34156, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34157, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34158, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34159, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34160, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34161, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34162, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34163, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34164, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34165, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34166, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34167, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34168, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34169, training loss 0.000003, validation loss 0.000111\n",
      "Iter 34170, training loss 0.000003, validation loss 0.000112\n",
      "Iter 34171, training loss 0.000004, validation loss 0.000111\n",
      "Iter 34172, training loss 0.000004, validation loss 0.000113\n",
      "Iter 34173, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34174, training loss 0.000004, validation loss 0.000114\n",
      "Iter 34175, training loss 0.000004, validation loss 0.000110\n",
      "Iter 34176, training loss 0.000005, validation loss 0.000116\n",
      "Iter 34177, training loss 0.000006, validation loss 0.000111\n",
      "Iter 34178, training loss 0.000007, validation loss 0.000120\n",
      "Iter 34179, training loss 0.000009, validation loss 0.000111\n",
      "Iter 34180, training loss 0.000011, validation loss 0.000125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34181, training loss 0.000016, validation loss 0.000118\n",
      "Iter 34182, training loss 0.000028, validation loss 0.000149\n",
      "Iter 34183, training loss 0.000057, validation loss 0.000152\n",
      "Iter 34184, training loss 0.000103, validation loss 0.000244\n",
      "Iter 34185, training loss 0.000154, validation loss 0.000241\n",
      "Iter 34186, training loss 0.000172, validation loss 0.000329\n",
      "Iter 34187, training loss 0.000107, validation loss 0.000190\n",
      "Iter 34188, training loss 0.000024, validation loss 0.000130\n",
      "Iter 34189, training loss 0.000062, validation loss 0.000152\n",
      "Iter 34190, training loss 0.000065, validation loss 0.000203\n",
      "Iter 34191, training loss 0.000171, validation loss 0.000253\n",
      "Iter 34192, training loss 0.000214, validation loss 0.000277\n",
      "Iter 34193, training loss 0.000206, validation loss 0.000348\n",
      "Iter 34194, training loss 0.000151, validation loss 0.000213\n",
      "Iter 34195, training loss 0.000049, validation loss 0.000154\n",
      "Iter 34196, training loss 0.000059, validation loss 0.000151\n",
      "Iter 34197, training loss 0.000148, validation loss 0.000248\n",
      "Iter 34198, training loss 0.000241, validation loss 0.000328\n",
      "Iter 34199, training loss 0.000177, validation loss 0.000243\n",
      "Iter 34200, training loss 0.000096, validation loss 0.000204\n",
      "Iter 34201, training loss 0.000058, validation loss 0.000160\n",
      "Iter 34202, training loss 0.000074, validation loss 0.000129\n",
      "Iter 34203, training loss 0.000082, validation loss 0.000154\n",
      "Iter 34204, training loss 0.000034, validation loss 0.000101\n",
      "Iter 34205, training loss 0.000057, validation loss 0.000120\n",
      "Iter 34206, training loss 0.000075, validation loss 0.000165\n",
      "Iter 34207, training loss 0.000039, validation loss 0.000105\n",
      "Iter 34208, training loss 0.000036, validation loss 0.000100\n",
      "Iter 34209, training loss 0.000054, validation loss 0.000131\n",
      "Iter 34210, training loss 0.000046, validation loss 0.000110\n",
      "Iter 34211, training loss 0.000032, validation loss 0.000117\n",
      "Iter 34212, training loss 0.000036, validation loss 0.000123\n",
      "Iter 34213, training loss 0.000042, validation loss 0.000104\n",
      "Iter 34214, training loss 0.000025, validation loss 0.000100\n",
      "Iter 34215, training loss 0.000025, validation loss 0.000107\n",
      "Iter 34216, training loss 0.000034, validation loss 0.000109\n",
      "Iter 34217, training loss 0.000023, validation loss 0.000104\n",
      "Iter 34218, training loss 0.000022, validation loss 0.000102\n",
      "Iter 34219, training loss 0.000028, validation loss 0.000101\n",
      "Iter 34220, training loss 0.000020, validation loss 0.000101\n",
      "Iter 34221, training loss 0.000020, validation loss 0.000105\n",
      "Iter 34222, training loss 0.000023, validation loss 0.000102\n",
      "Iter 34223, training loss 0.000017, validation loss 0.000099\n",
      "Iter 34224, training loss 0.000017, validation loss 0.000096\n",
      "Iter 34225, training loss 0.000020, validation loss 0.000096\n",
      "Iter 34226, training loss 0.000017, validation loss 0.000099\n",
      "Iter 34227, training loss 0.000015, validation loss 0.000097\n",
      "Iter 34228, training loss 0.000016, validation loss 0.000095\n",
      "Iter 34229, training loss 0.000016, validation loss 0.000096\n",
      "Iter 34230, training loss 0.000013, validation loss 0.000093\n",
      "Iter 34231, training loss 0.000013, validation loss 0.000095\n",
      "Iter 34232, training loss 0.000014, validation loss 0.000100\n",
      "Iter 34233, training loss 0.000012, validation loss 0.000095\n",
      "Iter 34234, training loss 0.000009, validation loss 0.000093\n",
      "Iter 34235, training loss 0.000011, validation loss 0.000093\n",
      "Iter 34236, training loss 0.000009, validation loss 0.000092\n",
      "Iter 34237, training loss 0.000009, validation loss 0.000094\n",
      "Iter 34238, training loss 0.000009, validation loss 0.000097\n",
      "Iter 34239, training loss 0.000009, validation loss 0.000094\n",
      "Iter 34240, training loss 0.000008, validation loss 0.000093\n",
      "Iter 34241, training loss 0.000008, validation loss 0.000093\n",
      "Iter 34242, training loss 0.000008, validation loss 0.000093\n",
      "Iter 34243, training loss 0.000008, validation loss 0.000096\n",
      "Iter 34244, training loss 0.000008, validation loss 0.000097\n",
      "Iter 34245, training loss 0.000008, validation loss 0.000094\n",
      "Iter 34246, training loss 0.000008, validation loss 0.000093\n",
      "Iter 34247, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34248, training loss 0.000008, validation loss 0.000093\n",
      "Iter 34249, training loss 0.000007, validation loss 0.000095\n",
      "Iter 34250, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34251, training loss 0.000007, validation loss 0.000092\n",
      "Iter 34252, training loss 0.000007, validation loss 0.000092\n",
      "Iter 34253, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34254, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34255, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34256, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34257, training loss 0.000007, validation loss 0.000092\n",
      "Iter 34258, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34259, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34260, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34261, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34262, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34263, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34264, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34265, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34266, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34267, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34268, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34269, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34270, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34271, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34272, training loss 0.000007, validation loss 0.000094\n",
      "Iter 34273, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34274, training loss 0.000007, validation loss 0.000093\n",
      "Iter 34275, training loss 0.000006, validation loss 0.000094\n",
      "Iter 34276, training loss 0.000006, validation loss 0.000094\n",
      "Iter 34277, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34278, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34279, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34280, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34281, training loss 0.000006, validation loss 0.000094\n",
      "Iter 34282, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34283, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34284, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34285, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34286, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34287, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34288, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34289, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34290, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34291, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34292, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34293, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34294, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34295, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34296, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34297, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34298, training loss 0.000006, validation loss 0.000093\n",
      "Iter 34299, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34300, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34301, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34302, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34303, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34304, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34305, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34306, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34307, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34308, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34309, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34310, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34311, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34312, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34313, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34314, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34315, training loss 0.000006, validation loss 0.000092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34316, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34317, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34318, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34319, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34320, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34321, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34322, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34323, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34324, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34325, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34326, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34327, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34328, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34329, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34330, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34331, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34332, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34333, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34334, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34335, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34336, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34337, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34338, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34339, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34340, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34341, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34342, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34343, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34344, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34345, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34346, training loss 0.000006, validation loss 0.000092\n",
      "Iter 34347, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34348, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34349, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34350, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34351, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34352, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34353, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34354, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34355, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34356, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34357, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34358, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34359, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34360, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34361, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34362, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34363, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34364, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34365, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34366, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34367, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34368, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34369, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34370, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34371, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34372, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34373, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34374, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34375, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34376, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34377, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34378, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34379, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34380, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34381, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34382, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34383, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34384, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34385, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34386, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34387, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34388, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34389, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34390, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34391, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34392, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34393, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34394, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34395, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34396, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34397, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34398, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34399, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34400, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34401, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34402, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34403, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34404, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34405, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34406, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34407, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34408, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34409, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34410, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34411, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34412, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34413, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34414, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34415, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34416, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34417, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34418, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34419, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34420, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34421, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34422, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34423, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34424, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34425, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34426, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34427, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34428, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34429, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34430, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34431, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34432, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34433, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34434, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34435, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34436, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34437, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34438, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34439, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34440, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34441, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34442, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34443, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34444, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34445, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34446, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34447, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34448, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34449, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34450, training loss 0.000006, validation loss 0.000091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34451, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34452, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34453, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34454, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34455, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34456, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34457, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34458, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34459, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34460, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34461, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34462, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34463, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34464, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34465, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34466, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34467, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34468, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34469, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34470, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34471, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34472, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34473, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34474, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34475, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34476, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34477, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34478, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34479, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34480, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34481, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34482, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34483, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34484, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34485, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34486, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34487, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34488, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34489, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34490, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34491, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34492, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34493, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34494, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34495, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34496, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34497, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34498, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34499, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34500, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34501, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34502, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34503, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34504, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34505, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34506, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34507, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34508, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34509, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34510, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34511, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34512, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34513, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34514, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34515, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34516, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34517, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34518, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34519, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34520, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34521, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34522, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34523, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34524, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34525, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34526, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34527, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34528, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34529, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34530, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34531, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34532, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34533, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34534, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34535, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34536, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34537, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34538, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34539, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34540, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34541, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34542, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34543, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34544, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34545, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34546, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34547, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34548, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34549, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34550, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34551, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34552, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34553, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34554, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34555, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34556, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34557, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34558, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34559, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34560, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34561, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34562, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34563, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34564, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34565, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34566, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34567, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34568, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34569, training loss 0.000005, validation loss 0.000091\n",
      "Iter 34570, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34571, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34572, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34573, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34574, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34575, training loss 0.000005, validation loss 0.000091\n",
      "Iter 34576, training loss 0.000005, validation loss 0.000091\n",
      "Iter 34577, training loss 0.000005, validation loss 0.000091\n",
      "Iter 34578, training loss 0.000005, validation loss 0.000091\n",
      "Iter 34579, training loss 0.000005, validation loss 0.000091\n",
      "Iter 34580, training loss 0.000005, validation loss 0.000091\n",
      "Iter 34581, training loss 0.000005, validation loss 0.000091\n",
      "Iter 34582, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34583, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34584, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34585, training loss 0.000005, validation loss 0.000090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34586, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34587, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34588, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34589, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34590, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34591, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34592, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34593, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34594, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34595, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34596, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34597, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34598, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34599, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34600, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34601, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34602, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34603, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34604, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34605, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34606, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34607, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34608, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34609, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34610, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34611, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34612, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34613, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34614, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34615, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34616, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34617, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34618, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34619, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34620, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34621, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34622, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34623, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34624, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34625, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34626, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34627, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34628, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34629, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34630, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34631, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34632, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34633, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34634, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34635, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34636, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34637, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34638, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34639, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34640, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34641, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34642, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34643, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34644, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34645, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34646, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34647, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34648, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34649, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34650, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34651, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34652, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34653, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34654, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34655, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34656, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34657, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34658, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34659, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34660, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34661, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34662, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34663, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34664, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34665, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34666, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34667, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34668, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34669, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34670, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34671, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34672, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34673, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34674, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34675, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34676, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34677, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34678, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34679, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34680, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34681, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34682, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34683, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34684, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34685, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34686, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34687, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34688, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34689, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34690, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34691, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34692, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34693, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34694, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34695, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34696, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34697, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34698, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34699, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34700, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34701, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34702, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34703, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34704, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34705, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34706, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34707, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34708, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34709, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34710, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34711, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34712, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34713, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34714, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34715, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34716, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34717, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34718, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34719, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34720, training loss 0.000005, validation loss 0.000090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34721, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34722, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34723, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34724, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34725, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34726, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34727, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34728, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34729, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34730, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34731, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34732, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34733, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34734, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34735, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34736, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34737, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34738, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34739, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34740, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34741, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34742, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34743, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34744, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34745, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34746, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34747, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34748, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34749, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34750, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34751, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34752, training loss 0.000005, validation loss 0.000089\n",
      "Iter 34753, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34754, training loss 0.000005, validation loss 0.000089\n",
      "Iter 34755, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34756, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34757, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34758, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34759, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34760, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34761, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34762, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34763, training loss 0.000005, validation loss 0.000090\n",
      "Iter 34764, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34765, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34766, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34767, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34768, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34769, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34770, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34771, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34772, training loss 0.000006, validation loss 0.000091\n",
      "Iter 34773, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34774, training loss 0.000007, validation loss 0.000091\n",
      "Iter 34775, training loss 0.000007, validation loss 0.000089\n",
      "Iter 34776, training loss 0.000007, validation loss 0.000092\n",
      "Iter 34777, training loss 0.000007, validation loss 0.000089\n",
      "Iter 34778, training loss 0.000007, validation loss 0.000092\n",
      "Iter 34779, training loss 0.000007, validation loss 0.000089\n",
      "Iter 34780, training loss 0.000007, validation loss 0.000091\n",
      "Iter 34781, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34782, training loss 0.000006, validation loss 0.000090\n",
      "Iter 34783, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34784, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34785, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34786, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34787, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34788, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34789, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34790, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34791, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34792, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34793, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34794, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34795, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34796, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34797, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34798, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34799, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34800, training loss 0.000006, validation loss 0.000087\n",
      "Iter 34801, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34802, training loss 0.000006, validation loss 0.000087\n",
      "Iter 34803, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34804, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34805, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34806, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34807, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34808, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34809, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34810, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34811, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34812, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34813, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34814, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34815, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34816, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34817, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34818, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34819, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34820, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34821, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34822, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34823, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34824, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34825, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34826, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34827, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34828, training loss 0.000005, validation loss 0.000088\n",
      "Iter 34829, training loss 0.000005, validation loss 0.000087\n",
      "Iter 34830, training loss 0.000006, validation loss 0.000088\n",
      "Iter 34831, training loss 0.000006, validation loss 0.000087\n",
      "Iter 34832, training loss 0.000006, validation loss 0.000089\n",
      "Iter 34833, training loss 0.000006, validation loss 0.000087\n",
      "Iter 34834, training loss 0.000007, validation loss 0.000090\n",
      "Iter 34835, training loss 0.000007, validation loss 0.000088\n",
      "Iter 34836, training loss 0.000008, validation loss 0.000092\n",
      "Iter 34837, training loss 0.000010, validation loss 0.000089\n",
      "Iter 34838, training loss 0.000012, validation loss 0.000097\n",
      "Iter 34839, training loss 0.000015, validation loss 0.000093\n",
      "Iter 34840, training loss 0.000018, validation loss 0.000103\n",
      "Iter 34841, training loss 0.000024, validation loss 0.000099\n",
      "Iter 34842, training loss 0.000028, validation loss 0.000115\n",
      "Iter 34843, training loss 0.000031, validation loss 0.000105\n",
      "Iter 34844, training loss 0.000029, validation loss 0.000116\n",
      "Iter 34845, training loss 0.000022, validation loss 0.000096\n",
      "Iter 34846, training loss 0.000011, validation loss 0.000094\n",
      "Iter 34847, training loss 0.000006, validation loss 0.000084\n",
      "Iter 34848, training loss 0.000008, validation loss 0.000085\n",
      "Iter 34849, training loss 0.000014, validation loss 0.000097\n",
      "Iter 34850, training loss 0.000017, validation loss 0.000092\n",
      "Iter 34851, training loss 0.000015, validation loss 0.000100\n",
      "Iter 34852, training loss 0.000010, validation loss 0.000086\n",
      "Iter 34853, training loss 0.000006, validation loss 0.000085\n",
      "Iter 34854, training loss 0.000007, validation loss 0.000087\n",
      "Iter 34855, training loss 0.000010, validation loss 0.000086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34856, training loss 0.000012, validation loss 0.000094\n",
      "Iter 34857, training loss 0.000011, validation loss 0.000086\n",
      "Iter 34858, training loss 0.000007, validation loss 0.000088\n",
      "Iter 34859, training loss 0.000006, validation loss 0.000084\n",
      "Iter 34860, training loss 0.000006, validation loss 0.000084\n",
      "Iter 34861, training loss 0.000008, validation loss 0.000089\n",
      "Iter 34862, training loss 0.000009, validation loss 0.000085\n",
      "Iter 34863, training loss 0.000008, validation loss 0.000088\n",
      "Iter 34864, training loss 0.000006, validation loss 0.000083\n",
      "Iter 34865, training loss 0.000006, validation loss 0.000084\n",
      "Iter 34866, training loss 0.000006, validation loss 0.000086\n",
      "Iter 34867, training loss 0.000007, validation loss 0.000084\n",
      "Iter 34868, training loss 0.000007, validation loss 0.000088\n",
      "Iter 34869, training loss 0.000007, validation loss 0.000084\n",
      "Iter 34870, training loss 0.000006, validation loss 0.000085\n",
      "Iter 34871, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34872, training loss 0.000006, validation loss 0.000083\n",
      "Iter 34873, training loss 0.000007, validation loss 0.000086\n",
      "Iter 34874, training loss 0.000007, validation loss 0.000084\n",
      "Iter 34875, training loss 0.000006, validation loss 0.000085\n",
      "Iter 34876, training loss 0.000005, validation loss 0.000083\n",
      "Iter 34877, training loss 0.000005, validation loss 0.000083\n",
      "Iter 34878, training loss 0.000006, validation loss 0.000085\n",
      "Iter 34879, training loss 0.000006, validation loss 0.000083\n",
      "Iter 34880, training loss 0.000006, validation loss 0.000085\n",
      "Iter 34881, training loss 0.000006, validation loss 0.000083\n",
      "Iter 34882, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34883, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34884, training loss 0.000006, validation loss 0.000084\n",
      "Iter 34885, training loss 0.000006, validation loss 0.000085\n",
      "Iter 34886, training loss 0.000006, validation loss 0.000083\n",
      "Iter 34887, training loss 0.000006, validation loss 0.000085\n",
      "Iter 34888, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34889, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34890, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34891, training loss 0.000006, validation loss 0.000083\n",
      "Iter 34892, training loss 0.000006, validation loss 0.000085\n",
      "Iter 34893, training loss 0.000005, validation loss 0.000083\n",
      "Iter 34894, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34895, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34896, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34897, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34898, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34899, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34900, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34901, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34902, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34903, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34904, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34905, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34906, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34907, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34908, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34909, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34910, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34911, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34912, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34913, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34914, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34915, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34916, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34917, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34918, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34919, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34920, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34921, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34922, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34923, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34924, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34925, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34926, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34927, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34928, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34929, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34930, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34931, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34932, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34933, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34934, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34935, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34936, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34937, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34938, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34939, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34940, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34941, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34942, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34943, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34944, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34945, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34946, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34947, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34948, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34949, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34950, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34951, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34952, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34953, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34954, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34955, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34956, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34957, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34958, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34959, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34960, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34961, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34962, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34963, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34964, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34965, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34966, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34967, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34968, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34969, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34970, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34971, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34972, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34973, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34974, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34975, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34976, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34977, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34978, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34979, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34980, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34981, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34982, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34983, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34984, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34985, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34986, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34987, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34988, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34989, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34990, training loss 0.000005, validation loss 0.000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34991, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34992, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34993, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34994, training loss 0.000005, validation loss 0.000083\n",
      "Iter 34995, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34996, training loss 0.000005, validation loss 0.000083\n",
      "Iter 34997, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34998, training loss 0.000005, validation loss 0.000084\n",
      "Iter 34999, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35000, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35001, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35002, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35003, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35004, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35005, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35006, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35007, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35008, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35009, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35010, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35011, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35012, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35013, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35014, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35015, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35016, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35017, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35018, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35019, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35020, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35021, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35022, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35023, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35024, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35025, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35026, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35027, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35028, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35029, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35030, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35031, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35032, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35033, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35034, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35035, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35036, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35037, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35038, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35039, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35040, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35041, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35042, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35043, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35044, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35045, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35046, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35047, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35048, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35049, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35050, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35051, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35052, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35053, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35054, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35055, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35056, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35057, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35058, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35059, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35060, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35061, training loss 0.000007, validation loss 0.000087\n",
      "Iter 35062, training loss 0.000007, validation loss 0.000084\n",
      "Iter 35063, training loss 0.000008, validation loss 0.000089\n",
      "Iter 35064, training loss 0.000009, validation loss 0.000085\n",
      "Iter 35065, training loss 0.000011, validation loss 0.000092\n",
      "Iter 35066, training loss 0.000013, validation loss 0.000089\n",
      "Iter 35067, training loss 0.000017, validation loss 0.000100\n",
      "Iter 35068, training loss 0.000021, validation loss 0.000095\n",
      "Iter 35069, training loss 0.000027, validation loss 0.000112\n",
      "Iter 35070, training loss 0.000034, validation loss 0.000106\n",
      "Iter 35071, training loss 0.000039, validation loss 0.000127\n",
      "Iter 35072, training loss 0.000039, validation loss 0.000110\n",
      "Iter 35073, training loss 0.000029, validation loss 0.000115\n",
      "Iter 35074, training loss 0.000015, validation loss 0.000089\n",
      "Iter 35075, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35076, training loss 0.000008, validation loss 0.000088\n",
      "Iter 35077, training loss 0.000016, validation loss 0.000091\n",
      "Iter 35078, training loss 0.000022, validation loss 0.000106\n",
      "Iter 35079, training loss 0.000020, validation loss 0.000095\n",
      "Iter 35080, training loss 0.000012, validation loss 0.000095\n",
      "Iter 35081, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35082, training loss 0.000008, validation loss 0.000084\n",
      "Iter 35083, training loss 0.000013, validation loss 0.000096\n",
      "Iter 35084, training loss 0.000016, validation loss 0.000091\n",
      "Iter 35085, training loss 0.000013, validation loss 0.000096\n",
      "Iter 35086, training loss 0.000008, validation loss 0.000084\n",
      "Iter 35087, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35088, training loss 0.000008, validation loss 0.000090\n",
      "Iter 35089, training loss 0.000011, validation loss 0.000087\n",
      "Iter 35090, training loss 0.000011, validation loss 0.000094\n",
      "Iter 35091, training loss 0.000008, validation loss 0.000084\n",
      "Iter 35092, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35093, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35094, training loss 0.000008, validation loss 0.000085\n",
      "Iter 35095, training loss 0.000009, validation loss 0.000090\n",
      "Iter 35096, training loss 0.000008, validation loss 0.000084\n",
      "Iter 35097, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35098, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35099, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35100, training loss 0.000007, validation loss 0.000087\n",
      "Iter 35101, training loss 0.000007, validation loss 0.000083\n",
      "Iter 35102, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35103, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35104, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35105, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35106, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35107, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35108, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35109, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35110, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35111, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35112, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35113, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35114, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35115, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35116, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35117, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35118, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35119, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35120, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35121, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35122, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35123, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35124, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35125, training loss 0.000005, validation loss 0.000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35126, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35127, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35128, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35129, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35130, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35131, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35132, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35133, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35134, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35135, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35136, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35137, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35138, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35139, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35140, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35141, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35142, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35143, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35144, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35145, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35146, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35147, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35148, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35149, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35150, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35151, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35152, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35153, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35154, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35155, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35156, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35157, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35158, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35159, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35160, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35161, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35162, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35163, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35164, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35165, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35166, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35167, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35168, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35169, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35170, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35171, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35172, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35173, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35174, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35175, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35176, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35177, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35178, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35179, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35180, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35181, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35182, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35183, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35184, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35185, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35186, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35187, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35188, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35189, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35190, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35191, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35192, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35193, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35194, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35195, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35196, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35197, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35198, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35199, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35200, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35201, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35202, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35203, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35204, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35205, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35206, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35207, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35208, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35209, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35210, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35211, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35212, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35213, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35214, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35215, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35216, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35217, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35218, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35219, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35220, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35221, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35222, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35223, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35224, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35225, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35226, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35227, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35228, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35229, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35230, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35231, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35232, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35233, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35234, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35235, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35236, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35237, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35238, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35239, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35240, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35241, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35242, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35243, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35244, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35245, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35246, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35247, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35248, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35249, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35250, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35251, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35252, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35253, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35254, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35255, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35256, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35257, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35258, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35259, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35260, training loss 0.000005, validation loss 0.000085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35261, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35262, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35263, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35264, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35265, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35266, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35267, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35268, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35269, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35270, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35271, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35272, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35273, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35274, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35275, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35276, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35277, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35278, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35279, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35280, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35281, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35282, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35283, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35284, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35285, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35286, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35287, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35288, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35289, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35290, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35291, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35292, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35293, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35294, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35295, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35296, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35297, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35298, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35299, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35300, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35301, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35302, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35303, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35304, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35305, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35306, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35307, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35308, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35309, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35310, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35311, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35312, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35313, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35314, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35315, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35316, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35317, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35318, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35319, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35320, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35321, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35322, training loss 0.000007, validation loss 0.000085\n",
      "Iter 35323, training loss 0.000007, validation loss 0.000089\n",
      "Iter 35324, training loss 0.000008, validation loss 0.000085\n",
      "Iter 35325, training loss 0.000008, validation loss 0.000090\n",
      "Iter 35326, training loss 0.000008, validation loss 0.000086\n",
      "Iter 35327, training loss 0.000009, validation loss 0.000091\n",
      "Iter 35328, training loss 0.000009, validation loss 0.000086\n",
      "Iter 35329, training loss 0.000009, validation loss 0.000092\n",
      "Iter 35330, training loss 0.000010, validation loss 0.000087\n",
      "Iter 35331, training loss 0.000010, validation loss 0.000094\n",
      "Iter 35332, training loss 0.000010, validation loss 0.000087\n",
      "Iter 35333, training loss 0.000010, validation loss 0.000092\n",
      "Iter 35334, training loss 0.000009, validation loss 0.000086\n",
      "Iter 35335, training loss 0.000009, validation loss 0.000091\n",
      "Iter 35336, training loss 0.000008, validation loss 0.000085\n",
      "Iter 35337, training loss 0.000007, validation loss 0.000089\n",
      "Iter 35338, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35339, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35340, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35341, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35342, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35343, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35344, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35345, training loss 0.000007, validation loss 0.000085\n",
      "Iter 35346, training loss 0.000007, validation loss 0.000089\n",
      "Iter 35347, training loss 0.000007, validation loss 0.000085\n",
      "Iter 35348, training loss 0.000007, validation loss 0.000089\n",
      "Iter 35349, training loss 0.000007, validation loss 0.000084\n",
      "Iter 35350, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35351, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35352, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35353, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35354, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35355, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35356, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35357, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35358, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35359, training loss 0.000007, validation loss 0.000088\n",
      "Iter 35360, training loss 0.000007, validation loss 0.000085\n",
      "Iter 35361, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35362, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35363, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35364, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35365, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35366, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35367, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35368, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35369, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35370, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35371, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35372, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35373, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35374, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35375, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35376, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35377, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35378, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35379, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35380, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35381, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35382, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35383, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35384, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35385, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35386, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35387, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35388, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35389, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35390, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35391, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35392, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35393, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35394, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35395, training loss 0.000005, validation loss 0.000086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35396, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35397, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35398, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35399, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35400, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35401, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35402, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35403, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35404, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35405, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35406, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35407, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35408, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35409, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35410, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35411, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35412, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35413, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35414, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35415, training loss 0.000007, validation loss 0.000089\n",
      "Iter 35416, training loss 0.000007, validation loss 0.000085\n",
      "Iter 35417, training loss 0.000007, validation loss 0.000089\n",
      "Iter 35418, training loss 0.000007, validation loss 0.000085\n",
      "Iter 35419, training loss 0.000008, validation loss 0.000090\n",
      "Iter 35420, training loss 0.000008, validation loss 0.000086\n",
      "Iter 35421, training loss 0.000009, validation loss 0.000092\n",
      "Iter 35422, training loss 0.000010, validation loss 0.000087\n",
      "Iter 35423, training loss 0.000010, validation loss 0.000094\n",
      "Iter 35424, training loss 0.000011, validation loss 0.000088\n",
      "Iter 35425, training loss 0.000011, validation loss 0.000095\n",
      "Iter 35426, training loss 0.000012, validation loss 0.000089\n",
      "Iter 35427, training loss 0.000014, validation loss 0.000099\n",
      "Iter 35428, training loss 0.000017, validation loss 0.000093\n",
      "Iter 35429, training loss 0.000019, validation loss 0.000106\n",
      "Iter 35430, training loss 0.000022, validation loss 0.000097\n",
      "Iter 35431, training loss 0.000027, validation loss 0.000114\n",
      "Iter 35432, training loss 0.000038, validation loss 0.000110\n",
      "Iter 35433, training loss 0.000046, validation loss 0.000139\n",
      "Iter 35434, training loss 0.000051, validation loss 0.000122\n",
      "Iter 35435, training loss 0.000048, validation loss 0.000138\n",
      "Iter 35436, training loss 0.000039, validation loss 0.000109\n",
      "Iter 35437, training loss 0.000019, validation loss 0.000107\n",
      "Iter 35438, training loss 0.000007, validation loss 0.000084\n",
      "Iter 35439, training loss 0.000012, validation loss 0.000086\n",
      "Iter 35440, training loss 0.000025, validation loss 0.000110\n",
      "Iter 35441, training loss 0.000033, validation loss 0.000106\n",
      "Iter 35442, training loss 0.000026, validation loss 0.000116\n",
      "Iter 35443, training loss 0.000013, validation loss 0.000087\n",
      "Iter 35444, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35445, training loss 0.000011, validation loss 0.000093\n",
      "Iter 35446, training loss 0.000020, validation loss 0.000095\n",
      "Iter 35447, training loss 0.000021, validation loss 0.000107\n",
      "Iter 35448, training loss 0.000013, validation loss 0.000086\n",
      "Iter 35449, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35450, training loss 0.000008, validation loss 0.000088\n",
      "Iter 35451, training loss 0.000014, validation loss 0.000088\n",
      "Iter 35452, training loss 0.000016, validation loss 0.000099\n",
      "Iter 35453, training loss 0.000011, validation loss 0.000084\n",
      "Iter 35454, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35455, training loss 0.000007, validation loss 0.000087\n",
      "Iter 35456, training loss 0.000010, validation loss 0.000086\n",
      "Iter 35457, training loss 0.000011, validation loss 0.000095\n",
      "Iter 35458, training loss 0.000009, validation loss 0.000085\n",
      "Iter 35459, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35460, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35461, training loss 0.000008, validation loss 0.000086\n",
      "Iter 35462, training loss 0.000008, validation loss 0.000091\n",
      "Iter 35463, training loss 0.000007, validation loss 0.000084\n",
      "Iter 35464, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35465, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35466, training loss 0.000007, validation loss 0.000085\n",
      "Iter 35467, training loss 0.000007, validation loss 0.000089\n",
      "Iter 35468, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35469, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35470, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35471, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35472, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35473, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35474, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35475, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35476, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35477, training loss 0.000006, validation loss 0.000086\n",
      "Iter 35478, training loss 0.000005, validation loss 0.000083\n",
      "Iter 35479, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35480, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35481, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35482, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35483, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35484, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35485, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35486, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35487, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35488, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35489, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35490, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35491, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35492, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35493, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35494, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35495, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35496, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35497, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35498, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35499, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35500, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35501, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35502, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35503, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35504, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35505, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35506, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35507, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35508, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35509, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35510, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35511, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35512, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35513, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35514, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35515, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35516, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35517, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35518, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35519, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35520, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35521, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35522, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35523, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35524, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35525, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35526, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35527, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35528, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35529, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35530, training loss 0.000005, validation loss 0.000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35531, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35532, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35533, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35534, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35535, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35536, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35537, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35538, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35539, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35540, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35541, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35542, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35543, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35544, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35545, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35546, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35547, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35548, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35549, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35550, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35551, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35552, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35553, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35554, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35555, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35556, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35557, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35558, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35559, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35560, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35561, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35562, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35563, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35564, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35565, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35566, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35567, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35568, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35569, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35570, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35571, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35572, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35573, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35574, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35575, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35576, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35577, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35578, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35579, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35580, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35581, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35582, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35583, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35584, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35585, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35586, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35587, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35588, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35589, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35590, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35591, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35592, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35593, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35594, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35595, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35596, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35597, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35598, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35599, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35600, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35601, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35602, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35603, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35604, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35605, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35606, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35607, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35608, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35609, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35610, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35611, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35612, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35613, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35614, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35615, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35616, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35617, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35618, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35619, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35620, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35621, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35622, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35623, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35624, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35625, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35626, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35627, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35628, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35629, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35630, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35631, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35632, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35633, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35634, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35635, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35636, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35637, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35638, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35639, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35640, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35641, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35642, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35643, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35644, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35645, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35646, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35647, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35648, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35649, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35650, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35651, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35652, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35653, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35654, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35655, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35656, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35657, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35658, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35659, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35660, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35661, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35662, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35663, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35664, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35665, training loss 0.000005, validation loss 0.000085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35666, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35667, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35668, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35669, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35670, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35671, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35672, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35673, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35674, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35675, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35676, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35677, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35678, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35679, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35680, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35681, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35682, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35683, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35684, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35685, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35686, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35687, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35688, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35689, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35690, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35691, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35692, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35693, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35694, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35695, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35696, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35697, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35698, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35699, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35700, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35701, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35702, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35703, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35704, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35705, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35706, training loss 0.000006, validation loss 0.000089\n",
      "Iter 35707, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35708, training loss 0.000006, validation loss 0.000089\n",
      "Iter 35709, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35710, training loss 0.000006, validation loss 0.000089\n",
      "Iter 35711, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35712, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35713, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35714, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35715, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35716, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35717, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35718, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35719, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35720, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35721, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35722, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35723, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35724, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35725, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35726, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35727, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35728, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35729, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35730, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35731, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35732, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35733, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35734, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35735, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35736, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35737, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35738, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35739, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35740, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35741, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35742, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35743, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35744, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35745, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35746, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35747, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35748, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35749, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35750, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35751, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35752, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35753, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35754, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35755, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35756, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35757, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35758, training loss 0.000007, validation loss 0.000090\n",
      "Iter 35759, training loss 0.000008, validation loss 0.000085\n",
      "Iter 35760, training loss 0.000009, validation loss 0.000093\n",
      "Iter 35761, training loss 0.000010, validation loss 0.000087\n",
      "Iter 35762, training loss 0.000011, validation loss 0.000096\n",
      "Iter 35763, training loss 0.000012, validation loss 0.000088\n",
      "Iter 35764, training loss 0.000015, validation loss 0.000101\n",
      "Iter 35765, training loss 0.000021, validation loss 0.000095\n",
      "Iter 35766, training loss 0.000028, validation loss 0.000119\n",
      "Iter 35767, training loss 0.000038, validation loss 0.000110\n",
      "Iter 35768, training loss 0.000057, validation loss 0.000150\n",
      "Iter 35769, training loss 0.000088, validation loss 0.000153\n",
      "Iter 35770, training loss 0.000092, validation loss 0.000196\n",
      "Iter 35771, training loss 0.000067, validation loss 0.000137\n",
      "Iter 35772, training loss 0.000028, validation loss 0.000116\n",
      "Iter 35773, training loss 0.000008, validation loss 0.000085\n",
      "Iter 35774, training loss 0.000020, validation loss 0.000092\n",
      "Iter 35775, training loss 0.000040, validation loss 0.000129\n",
      "Iter 35776, training loss 0.000038, validation loss 0.000107\n",
      "Iter 35777, training loss 0.000017, validation loss 0.000104\n",
      "Iter 35778, training loss 0.000007, validation loss 0.000086\n",
      "Iter 35779, training loss 0.000017, validation loss 0.000089\n",
      "Iter 35780, training loss 0.000028, validation loss 0.000117\n",
      "Iter 35781, training loss 0.000023, validation loss 0.000098\n",
      "Iter 35782, training loss 0.000010, validation loss 0.000092\n",
      "Iter 35783, training loss 0.000007, validation loss 0.000086\n",
      "Iter 35784, training loss 0.000016, validation loss 0.000088\n",
      "Iter 35785, training loss 0.000020, validation loss 0.000107\n",
      "Iter 35786, training loss 0.000013, validation loss 0.000088\n",
      "Iter 35787, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35788, training loss 0.000010, validation loss 0.000090\n",
      "Iter 35789, training loss 0.000015, validation loss 0.000086\n",
      "Iter 35790, training loss 0.000013, validation loss 0.000098\n",
      "Iter 35791, training loss 0.000007, validation loss 0.000085\n",
      "Iter 35792, training loss 0.000007, validation loss 0.000082\n",
      "Iter 35793, training loss 0.000012, validation loss 0.000094\n",
      "Iter 35794, training loss 0.000012, validation loss 0.000086\n",
      "Iter 35795, training loss 0.000007, validation loss 0.000092\n",
      "Iter 35796, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35797, training loss 0.000009, validation loss 0.000084\n",
      "Iter 35798, training loss 0.000010, validation loss 0.000093\n",
      "Iter 35799, training loss 0.000007, validation loss 0.000086\n",
      "Iter 35800, training loss 0.000006, validation loss 0.000087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35801, training loss 0.000007, validation loss 0.000090\n",
      "Iter 35802, training loss 0.000009, validation loss 0.000084\n",
      "Iter 35803, training loss 0.000007, validation loss 0.000089\n",
      "Iter 35804, training loss 0.000005, validation loss 0.000087\n",
      "Iter 35805, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35806, training loss 0.000007, validation loss 0.000090\n",
      "Iter 35807, training loss 0.000006, validation loss 0.000083\n",
      "Iter 35808, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35809, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35810, training loss 0.000006, validation loss 0.000085\n",
      "Iter 35811, training loss 0.000006, validation loss 0.000087\n",
      "Iter 35812, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35813, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35814, training loss 0.000006, validation loss 0.000088\n",
      "Iter 35815, training loss 0.000006, validation loss 0.000084\n",
      "Iter 35816, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35817, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35818, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35819, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35820, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35821, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35822, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35823, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35824, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35825, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35826, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35827, training loss 0.000005, validation loss 0.000086\n",
      "Iter 35828, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35829, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35830, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35831, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35832, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35833, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35834, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35835, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35836, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35837, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35838, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35839, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35840, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35841, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35842, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35843, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35844, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35845, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35846, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35847, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35848, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35849, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35850, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35851, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35852, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35853, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35854, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35855, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35856, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35857, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35858, training loss 0.000005, validation loss 0.000085\n",
      "Iter 35859, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35860, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35861, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35862, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35863, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35864, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35865, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35866, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35867, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35868, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35869, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35870, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35871, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35872, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35873, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35874, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35875, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35876, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35877, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35878, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35879, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35880, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35881, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35882, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35883, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35884, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35885, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35886, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35887, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35888, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35889, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35890, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35891, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35892, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35893, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35894, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35895, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35896, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35897, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35898, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35899, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35900, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35901, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35902, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35903, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35904, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35905, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35906, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35907, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35908, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35909, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35910, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35911, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35912, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35913, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35914, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35915, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35916, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35917, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35918, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35919, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35920, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35921, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35922, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35923, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35924, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35925, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35926, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35927, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35928, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35929, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35930, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35931, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35932, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35933, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35934, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35935, training loss 0.000005, validation loss 0.000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35936, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35937, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35938, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35939, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35940, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35941, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35942, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35943, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35944, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35945, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35946, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35947, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35948, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35949, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35950, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35951, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35952, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35953, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35954, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35955, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35956, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35957, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35958, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35959, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35960, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35961, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35962, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35963, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35964, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35965, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35966, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35967, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35968, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35969, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35970, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35971, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35972, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35973, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35974, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35975, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35976, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35977, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35978, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35979, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35980, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35981, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35982, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35983, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35984, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35985, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35986, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35987, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35988, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35989, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35990, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35991, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35992, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35993, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35994, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35995, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35996, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35997, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35998, training loss 0.000005, validation loss 0.000084\n",
      "Iter 35999, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36000, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36001, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36002, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36003, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36004, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36005, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36006, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36007, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36008, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36009, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36010, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36011, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36012, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36013, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36014, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36015, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36016, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36017, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36018, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36019, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36020, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36021, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36022, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36023, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36024, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36025, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36026, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36027, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36028, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36029, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36030, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36031, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36032, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36033, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36034, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36035, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36036, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36037, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36038, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36039, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36040, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36041, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36042, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36043, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36044, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36045, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36046, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36047, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36048, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36049, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36050, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36051, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36052, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36053, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36054, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36055, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36056, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36057, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36058, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36059, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36060, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36061, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36062, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36063, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36064, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36065, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36066, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36067, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36068, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36069, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36070, training loss 0.000005, validation loss 0.000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36071, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36072, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36073, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36074, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36075, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36076, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36077, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36078, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36079, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36080, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36081, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36082, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36083, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36084, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36085, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36086, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36087, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36088, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36089, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36090, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36091, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36092, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36093, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36094, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36095, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36096, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36097, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36098, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36099, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36100, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36101, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36102, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36103, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36104, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36105, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36106, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36107, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36108, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36109, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36110, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36111, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36112, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36113, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36114, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36115, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36116, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36117, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36118, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36119, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36120, training loss 0.000006, validation loss 0.000085\n",
      "Iter 36121, training loss 0.000006, validation loss 0.000082\n",
      "Iter 36122, training loss 0.000006, validation loss 0.000085\n",
      "Iter 36123, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36124, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36125, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36126, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36127, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36128, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36129, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36130, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36131, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36132, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36133, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36134, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36135, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36136, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36137, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36138, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36139, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36140, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36141, training loss 0.000006, validation loss 0.000085\n",
      "Iter 36142, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36143, training loss 0.000006, validation loss 0.000086\n",
      "Iter 36144, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36145, training loss 0.000006, validation loss 0.000087\n",
      "Iter 36146, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36147, training loss 0.000006, validation loss 0.000086\n",
      "Iter 36148, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36149, training loss 0.000006, validation loss 0.000086\n",
      "Iter 36150, training loss 0.000006, validation loss 0.000082\n",
      "Iter 36151, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36152, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36153, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36154, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36155, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36156, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36157, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36158, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36159, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36160, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36161, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36162, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36163, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36164, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36165, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36166, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36167, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36168, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36169, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36170, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36171, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36172, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36173, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36174, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36175, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36176, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36177, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36178, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36179, training loss 0.000006, validation loss 0.000086\n",
      "Iter 36180, training loss 0.000007, validation loss 0.000083\n",
      "Iter 36181, training loss 0.000008, validation loss 0.000089\n",
      "Iter 36182, training loss 0.000009, validation loss 0.000084\n",
      "Iter 36183, training loss 0.000010, validation loss 0.000093\n",
      "Iter 36184, training loss 0.000012, validation loss 0.000085\n",
      "Iter 36185, training loss 0.000013, validation loss 0.000096\n",
      "Iter 36186, training loss 0.000015, validation loss 0.000089\n",
      "Iter 36187, training loss 0.000021, validation loss 0.000108\n",
      "Iter 36188, training loss 0.000032, validation loss 0.000101\n",
      "Iter 36189, training loss 0.000049, validation loss 0.000140\n",
      "Iter 36190, training loss 0.000074, validation loss 0.000137\n",
      "Iter 36191, training loss 0.000085, validation loss 0.000186\n",
      "Iter 36192, training loss 0.000072, validation loss 0.000135\n",
      "Iter 36193, training loss 0.000039, validation loss 0.000128\n",
      "Iter 36194, training loss 0.000011, validation loss 0.000085\n",
      "Iter 36195, training loss 0.000012, validation loss 0.000086\n",
      "Iter 36196, training loss 0.000033, validation loss 0.000119\n",
      "Iter 36197, training loss 0.000046, validation loss 0.000115\n",
      "Iter 36198, training loss 0.000033, validation loss 0.000123\n",
      "Iter 36199, training loss 0.000011, validation loss 0.000083\n",
      "Iter 36200, training loss 0.000008, validation loss 0.000080\n",
      "Iter 36201, training loss 0.000023, validation loss 0.000108\n",
      "Iter 36202, training loss 0.000030, validation loss 0.000098\n",
      "Iter 36203, training loss 0.000018, validation loss 0.000104\n",
      "Iter 36204, training loss 0.000006, validation loss 0.000082\n",
      "Iter 36205, training loss 0.000011, validation loss 0.000082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36206, training loss 0.000020, validation loss 0.000107\n",
      "Iter 36207, training loss 0.000019, validation loss 0.000091\n",
      "Iter 36208, training loss 0.000009, validation loss 0.000092\n",
      "Iter 36209, training loss 0.000007, validation loss 0.000087\n",
      "Iter 36210, training loss 0.000013, validation loss 0.000087\n",
      "Iter 36211, training loss 0.000015, validation loss 0.000103\n",
      "Iter 36212, training loss 0.000010, validation loss 0.000091\n",
      "Iter 36213, training loss 0.000006, validation loss 0.000090\n",
      "Iter 36214, training loss 0.000009, validation loss 0.000095\n",
      "Iter 36215, training loss 0.000013, validation loss 0.000091\n",
      "Iter 36216, training loss 0.000010, validation loss 0.000096\n",
      "Iter 36217, training loss 0.000006, validation loss 0.000088\n",
      "Iter 36218, training loss 0.000007, validation loss 0.000087\n",
      "Iter 36219, training loss 0.000010, validation loss 0.000093\n",
      "Iter 36220, training loss 0.000009, validation loss 0.000085\n",
      "Iter 36221, training loss 0.000006, validation loss 0.000086\n",
      "Iter 36222, training loss 0.000006, validation loss 0.000086\n",
      "Iter 36223, training loss 0.000008, validation loss 0.000082\n",
      "Iter 36224, training loss 0.000008, validation loss 0.000086\n",
      "Iter 36225, training loss 0.000006, validation loss 0.000081\n",
      "Iter 36226, training loss 0.000006, validation loss 0.000082\n",
      "Iter 36227, training loss 0.000007, validation loss 0.000086\n",
      "Iter 36228, training loss 0.000007, validation loss 0.000081\n",
      "Iter 36229, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36230, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36231, training loss 0.000006, validation loss 0.000081\n",
      "Iter 36232, training loss 0.000006, validation loss 0.000084\n",
      "Iter 36233, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36234, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36235, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36236, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36237, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36238, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36239, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36240, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36241, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36242, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36243, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36244, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36245, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36246, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36247, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36248, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36249, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36250, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36251, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36252, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36253, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36254, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36255, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36256, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36257, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36258, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36259, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36260, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36261, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36262, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36263, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36264, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36265, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36266, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36267, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36268, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36269, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36270, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36271, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36272, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36273, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36274, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36275, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36276, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36277, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36278, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36279, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36280, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36281, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36282, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36283, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36284, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36285, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36286, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36287, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36288, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36289, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36290, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36291, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36292, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36293, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36294, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36295, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36296, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36297, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36298, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36299, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36300, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36301, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36302, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36303, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36304, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36305, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36306, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36307, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36308, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36309, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36310, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36311, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36312, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36313, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36314, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36315, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36316, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36317, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36318, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36319, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36320, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36321, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36322, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36323, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36324, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36325, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36326, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36327, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36328, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36329, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36330, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36331, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36332, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36333, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36334, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36335, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36336, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36337, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36338, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36339, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36340, training loss 0.000004, validation loss 0.000081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36341, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36342, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36343, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36344, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36345, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36346, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36347, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36348, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36349, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36350, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36351, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36352, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36353, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36354, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36355, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36356, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36357, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36358, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36359, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36360, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36361, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36362, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36363, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36364, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36365, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36366, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36367, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36368, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36369, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36370, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36371, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36372, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36373, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36374, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36375, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36376, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36377, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36378, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36379, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36380, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36381, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36382, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36383, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36384, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36385, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36386, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36387, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36388, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36389, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36390, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36391, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36392, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36393, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36394, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36395, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36396, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36397, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36398, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36399, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36400, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36401, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36402, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36403, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36404, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36405, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36406, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36407, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36408, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36409, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36410, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36411, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36412, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36413, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36414, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36415, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36416, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36417, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36418, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36419, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36420, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36421, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36422, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36423, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36424, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36425, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36426, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36427, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36428, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36429, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36430, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36431, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36432, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36433, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36434, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36435, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36436, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36437, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36438, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36439, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36440, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36441, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36442, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36443, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36444, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36445, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36446, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36447, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36448, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36449, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36450, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36451, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36452, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36453, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36454, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36455, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36456, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36457, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36458, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36459, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36460, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36461, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36462, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36463, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36464, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36465, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36466, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36467, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36468, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36469, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36470, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36471, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36472, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36473, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36474, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36475, training loss 0.000004, validation loss 0.000081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36476, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36477, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36478, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36479, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36480, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36481, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36482, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36483, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36484, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36485, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36486, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36487, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36488, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36489, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36490, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36491, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36492, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36493, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36494, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36495, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36496, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36497, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36498, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36499, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36500, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36501, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36502, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36503, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36504, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36505, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36506, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36507, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36508, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36509, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36510, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36511, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36512, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36513, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36514, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36515, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36516, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36517, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36518, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36519, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36520, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36521, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36522, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36523, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36524, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36525, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36526, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36527, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36528, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36529, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36530, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36531, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36532, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36533, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36534, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36535, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36536, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36537, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36538, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36539, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36540, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36541, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36542, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36543, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36544, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36545, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36546, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36547, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36548, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36549, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36550, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36551, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36552, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36553, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36554, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36555, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36556, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36557, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36558, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36559, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36560, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36561, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36562, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36563, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36564, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36565, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36566, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36567, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36568, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36569, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36570, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36571, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36572, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36573, training loss 0.000006, validation loss 0.000080\n",
      "Iter 36574, training loss 0.000006, validation loss 0.000084\n",
      "Iter 36575, training loss 0.000006, validation loss 0.000080\n",
      "Iter 36576, training loss 0.000007, validation loss 0.000085\n",
      "Iter 36577, training loss 0.000007, validation loss 0.000080\n",
      "Iter 36578, training loss 0.000007, validation loss 0.000085\n",
      "Iter 36579, training loss 0.000007, validation loss 0.000081\n",
      "Iter 36580, training loss 0.000008, validation loss 0.000086\n",
      "Iter 36581, training loss 0.000008, validation loss 0.000081\n",
      "Iter 36582, training loss 0.000008, validation loss 0.000087\n",
      "Iter 36583, training loss 0.000008, validation loss 0.000082\n",
      "Iter 36584, training loss 0.000009, validation loss 0.000087\n",
      "Iter 36585, training loss 0.000009, validation loss 0.000081\n",
      "Iter 36586, training loss 0.000010, validation loss 0.000089\n",
      "Iter 36587, training loss 0.000009, validation loss 0.000081\n",
      "Iter 36588, training loss 0.000008, validation loss 0.000086\n",
      "Iter 36589, training loss 0.000008, validation loss 0.000081\n",
      "Iter 36590, training loss 0.000008, validation loss 0.000087\n",
      "Iter 36591, training loss 0.000007, validation loss 0.000080\n",
      "Iter 36592, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36593, training loss 0.000006, validation loss 0.000079\n",
      "Iter 36594, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36595, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36596, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36597, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36598, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36599, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36600, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36601, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36602, training loss 0.000006, validation loss 0.000080\n",
      "Iter 36603, training loss 0.000006, validation loss 0.000084\n",
      "Iter 36604, training loss 0.000006, validation loss 0.000080\n",
      "Iter 36605, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36606, training loss 0.000006, validation loss 0.000079\n",
      "Iter 36607, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36608, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36609, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36610, training loss 0.000005, validation loss 0.000079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36611, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36612, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36613, training loss 0.000004, validation loss 0.000079\n",
      "Iter 36614, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36615, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36616, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36617, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36618, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36619, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36620, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36621, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36622, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36623, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36624, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36625, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36626, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36627, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36628, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36629, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36630, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36631, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36632, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36633, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36634, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36635, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36636, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36637, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36638, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36639, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36640, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36641, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36642, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36643, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36644, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36645, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36646, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36647, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36648, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36649, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36650, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36651, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36652, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36653, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36654, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36655, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36656, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36657, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36658, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36659, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36660, training loss 0.000004, validation loss 0.000080\n",
      "Iter 36661, training loss 0.000004, validation loss 0.000081\n",
      "Iter 36662, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36663, training loss 0.000005, validation loss 0.000081\n",
      "Iter 36664, training loss 0.000005, validation loss 0.000079\n",
      "Iter 36665, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36666, training loss 0.000005, validation loss 0.000080\n",
      "Iter 36667, training loss 0.000005, validation loss 0.000082\n",
      "Iter 36668, training loss 0.000006, validation loss 0.000080\n",
      "Iter 36669, training loss 0.000006, validation loss 0.000084\n",
      "Iter 36670, training loss 0.000006, validation loss 0.000080\n",
      "Iter 36671, training loss 0.000007, validation loss 0.000085\n",
      "Iter 36672, training loss 0.000008, validation loss 0.000081\n",
      "Iter 36673, training loss 0.000010, validation loss 0.000089\n",
      "Iter 36674, training loss 0.000013, validation loss 0.000085\n",
      "Iter 36675, training loss 0.000019, validation loss 0.000103\n",
      "Iter 36676, training loss 0.000029, validation loss 0.000099\n",
      "Iter 36677, training loss 0.000050, validation loss 0.000139\n",
      "Iter 36678, training loss 0.000084, validation loss 0.000145\n",
      "Iter 36679, training loss 0.000099, validation loss 0.000200\n",
      "Iter 36680, training loss 0.000087, validation loss 0.000149\n",
      "Iter 36681, training loss 0.000052, validation loss 0.000140\n",
      "Iter 36682, training loss 0.000015, validation loss 0.000082\n",
      "Iter 36683, training loss 0.000012, validation loss 0.000081\n",
      "Iter 36684, training loss 0.000040, validation loss 0.000121\n",
      "Iter 36685, training loss 0.000060, validation loss 0.000122\n",
      "Iter 36686, training loss 0.000038, validation loss 0.000128\n",
      "Iter 36687, training loss 0.000009, validation loss 0.000081\n",
      "Iter 36688, training loss 0.000019, validation loss 0.000086\n",
      "Iter 36689, training loss 0.000042, validation loss 0.000130\n",
      "Iter 36690, training loss 0.000036, validation loss 0.000105\n",
      "Iter 36691, training loss 0.000011, validation loss 0.000093\n",
      "Iter 36692, training loss 0.000013, validation loss 0.000093\n",
      "Iter 36693, training loss 0.000033, validation loss 0.000095\n",
      "Iter 36694, training loss 0.000028, validation loss 0.000114\n",
      "Iter 36695, training loss 0.000011, validation loss 0.000086\n",
      "Iter 36696, training loss 0.000011, validation loss 0.000084\n",
      "Iter 36697, training loss 0.000023, validation loss 0.000104\n",
      "Iter 36698, training loss 0.000019, validation loss 0.000086\n",
      "Iter 36699, training loss 0.000007, validation loss 0.000086\n",
      "Iter 36700, training loss 0.000010, validation loss 0.000091\n",
      "Iter 36701, training loss 0.000016, validation loss 0.000085\n",
      "Iter 36702, training loss 0.000012, validation loss 0.000091\n",
      "Iter 36703, training loss 0.000006, validation loss 0.000081\n",
      "Iter 36704, training loss 0.000008, validation loss 0.000083\n",
      "Iter 36705, training loss 0.000011, validation loss 0.000096\n",
      "Iter 36706, training loss 0.000008, validation loss 0.000082\n",
      "Iter 36707, training loss 0.000006, validation loss 0.000082\n",
      "Iter 36708, training loss 0.000008, validation loss 0.000088\n",
      "Iter 36709, training loss 0.000009, validation loss 0.000085\n",
      "Iter 36710, training loss 0.000007, validation loss 0.000089\n",
      "Iter 36711, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36712, training loss 0.000008, validation loss 0.000080\n",
      "Iter 36713, training loss 0.000008, validation loss 0.000088\n",
      "Iter 36714, training loss 0.000007, validation loss 0.000083\n",
      "Iter 36715, training loss 0.000006, validation loss 0.000084\n",
      "Iter 36716, training loss 0.000006, validation loss 0.000086\n",
      "Iter 36717, training loss 0.000007, validation loss 0.000082\n",
      "Iter 36718, training loss 0.000006, validation loss 0.000087\n",
      "Iter 36719, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36720, training loss 0.000006, validation loss 0.000084\n",
      "Iter 36721, training loss 0.000006, validation loss 0.000088\n",
      "Iter 36722, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36723, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36724, training loss 0.000006, validation loss 0.000087\n",
      "Iter 36725, training loss 0.000006, validation loss 0.000083\n",
      "Iter 36726, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36727, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36728, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36729, training loss 0.000005, validation loss 0.000087\n",
      "Iter 36730, training loss 0.000005, validation loss 0.000083\n",
      "Iter 36731, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36732, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36733, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36734, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36735, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36736, training loss 0.000005, validation loss 0.000084\n",
      "Iter 36737, training loss 0.000005, validation loss 0.000087\n",
      "Iter 36738, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36739, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36740, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36741, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36742, training loss 0.000005, validation loss 0.000087\n",
      "Iter 36743, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36744, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36745, training loss 0.000005, validation loss 0.000086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36746, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36747, training loss 0.000005, validation loss 0.000087\n",
      "Iter 36748, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36749, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36750, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36751, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36752, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36753, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36754, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36755, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36756, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36757, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36758, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36759, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36760, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36761, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36762, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36763, training loss 0.000005, validation loss 0.000086\n",
      "Iter 36764, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36765, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36766, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36767, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36768, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36769, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36770, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36771, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36772, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36773, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36774, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36775, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36776, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36777, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36778, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36779, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36780, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36781, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36782, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36783, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36784, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36785, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36786, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36787, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36788, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36789, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36790, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36791, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36792, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36793, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36794, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36795, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36796, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36797, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36798, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36799, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36800, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36801, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36802, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36803, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36804, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36805, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36806, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36807, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36808, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36809, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36810, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36811, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36812, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36813, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36814, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36815, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36816, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36817, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36818, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36819, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36820, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36821, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36822, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36823, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36824, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36825, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36826, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36827, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36828, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36829, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36830, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36831, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36832, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36833, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36834, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36835, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36836, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36837, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36838, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36839, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36840, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36841, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36842, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36843, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36844, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36845, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36846, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36847, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36848, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36849, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36850, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36851, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36852, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36853, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36854, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36855, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36856, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36857, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36858, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36859, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36860, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36861, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36862, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36863, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36864, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36865, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36866, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36867, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36868, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36869, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36870, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36871, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36872, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36873, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36874, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36875, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36876, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36877, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36878, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36879, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36880, training loss 0.000004, validation loss 0.000086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36881, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36882, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36883, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36884, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36885, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36886, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36887, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36888, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36889, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36890, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36891, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36892, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36893, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36894, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36895, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36896, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36897, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36898, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36899, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36900, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36901, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36902, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36903, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36904, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36905, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36906, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36907, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36908, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36909, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36910, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36911, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36912, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36913, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36914, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36915, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36916, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36917, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36918, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36919, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36920, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36921, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36922, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36923, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36924, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36925, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36926, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36927, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36928, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36929, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36930, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36931, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36932, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36933, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36934, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36935, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36936, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36937, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36938, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36939, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36940, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36941, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36942, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36943, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36944, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36945, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36946, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36947, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36948, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36949, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36950, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36951, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36952, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36953, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36954, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36955, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36956, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36957, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36958, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36959, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36960, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36961, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36962, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36963, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36964, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36965, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36966, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36967, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36968, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36969, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36970, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36971, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36972, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36973, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36974, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36975, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36976, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36977, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36978, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36979, training loss 0.000004, validation loss 0.000087\n",
      "Iter 36980, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36981, training loss 0.000004, validation loss 0.000087\n",
      "Iter 36982, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36983, training loss 0.000004, validation loss 0.000087\n",
      "Iter 36984, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36985, training loss 0.000005, validation loss 0.000087\n",
      "Iter 36986, training loss 0.000005, validation loss 0.000085\n",
      "Iter 36987, training loss 0.000005, validation loss 0.000087\n",
      "Iter 36988, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36989, training loss 0.000004, validation loss 0.000087\n",
      "Iter 36990, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36991, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36992, training loss 0.000004, validation loss 0.000085\n",
      "Iter 36993, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36994, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36995, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36996, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36997, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36998, training loss 0.000004, validation loss 0.000086\n",
      "Iter 36999, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37000, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37001, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37002, training loss 0.000004, validation loss 0.000087\n",
      "Iter 37003, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37004, training loss 0.000005, validation loss 0.000088\n",
      "Iter 37005, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37006, training loss 0.000005, validation loss 0.000088\n",
      "Iter 37007, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37008, training loss 0.000005, validation loss 0.000089\n",
      "Iter 37009, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37010, training loss 0.000005, validation loss 0.000088\n",
      "Iter 37011, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37012, training loss 0.000005, validation loss 0.000087\n",
      "Iter 37013, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37014, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37015, training loss 0.000004, validation loss 0.000085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 37016, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37017, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37018, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37019, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37020, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37021, training loss 0.000004, validation loss 0.000087\n",
      "Iter 37022, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37023, training loss 0.000005, validation loss 0.000087\n",
      "Iter 37024, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37025, training loss 0.000005, validation loss 0.000088\n",
      "Iter 37026, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37027, training loss 0.000005, validation loss 0.000088\n",
      "Iter 37028, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37029, training loss 0.000005, validation loss 0.000087\n",
      "Iter 37030, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37031, training loss 0.000004, validation loss 0.000087\n",
      "Iter 37032, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37033, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37034, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37035, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37036, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37037, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37038, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37039, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37040, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37041, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37042, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37043, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37044, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37045, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37046, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37047, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37048, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37049, training loss 0.000004, validation loss 0.000086\n",
      "Iter 37050, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37051, training loss 0.000004, validation loss 0.000087\n",
      "Iter 37052, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37053, training loss 0.000005, validation loss 0.000088\n",
      "Iter 37054, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37055, training loss 0.000005, validation loss 0.000089\n",
      "Iter 37056, training loss 0.000006, validation loss 0.000085\n",
      "Iter 37057, training loss 0.000006, validation loss 0.000090\n",
      "Iter 37058, training loss 0.000007, validation loss 0.000085\n",
      "Iter 37059, training loss 0.000007, validation loss 0.000091\n",
      "Iter 37060, training loss 0.000008, validation loss 0.000085\n",
      "Iter 37061, training loss 0.000008, validation loss 0.000093\n",
      "Iter 37062, training loss 0.000009, validation loss 0.000085\n",
      "Iter 37063, training loss 0.000008, validation loss 0.000093\n",
      "Iter 37064, training loss 0.000008, validation loss 0.000084\n",
      "Iter 37065, training loss 0.000007, validation loss 0.000090\n",
      "Iter 37066, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37067, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37068, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37069, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37070, training loss 0.000005, validation loss 0.000087\n",
      "Iter 37071, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37072, training loss 0.000006, validation loss 0.000088\n",
      "Iter 37073, training loss 0.000006, validation loss 0.000084\n",
      "Iter 37074, training loss 0.000007, validation loss 0.000091\n",
      "Iter 37075, training loss 0.000007, validation loss 0.000085\n",
      "Iter 37076, training loss 0.000007, validation loss 0.000091\n",
      "Iter 37077, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37078, training loss 0.000005, validation loss 0.000087\n",
      "Iter 37079, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37080, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37081, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37082, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37083, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37084, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37085, training loss 0.000006, validation loss 0.000089\n",
      "Iter 37086, training loss 0.000007, validation loss 0.000083\n",
      "Iter 37087, training loss 0.000008, validation loss 0.000091\n",
      "Iter 37088, training loss 0.000007, validation loss 0.000083\n",
      "Iter 37089, training loss 0.000007, validation loss 0.000089\n",
      "Iter 37090, training loss 0.000006, validation loss 0.000082\n",
      "Iter 37091, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37092, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37093, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37094, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37095, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37096, training loss 0.000006, validation loss 0.000087\n",
      "Iter 37097, training loss 0.000006, validation loss 0.000082\n",
      "Iter 37098, training loss 0.000007, validation loss 0.000089\n",
      "Iter 37099, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37100, training loss 0.000006, validation loss 0.000088\n",
      "Iter 37101, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37102, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37103, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37104, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37105, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37106, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37107, training loss 0.000005, validation loss 0.000087\n",
      "Iter 37108, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37109, training loss 0.000006, validation loss 0.000087\n",
      "Iter 37110, training loss 0.000006, validation loss 0.000082\n",
      "Iter 37111, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37112, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37113, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37114, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37115, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37116, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37117, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37118, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37119, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37120, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37121, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37122, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37123, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37124, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37125, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37126, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37127, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37128, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37129, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37130, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37131, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37132, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37133, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37134, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37135, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37136, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37137, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37138, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37139, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37140, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37141, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37142, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37143, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37144, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37145, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37146, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37147, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37148, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37149, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37150, training loss 0.000004, validation loss 0.000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 37151, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37152, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37153, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37154, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37155, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37156, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37157, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37158, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37159, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37160, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37161, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37162, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37163, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37164, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37165, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37166, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37167, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37168, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37169, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37170, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37171, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37172, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37173, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37174, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37175, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37176, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37177, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37178, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37179, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37180, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37181, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37182, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37183, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37184, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37185, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37186, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37187, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37188, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37189, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37190, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37191, training loss 0.000006, validation loss 0.000087\n",
      "Iter 37192, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37193, training loss 0.000006, validation loss 0.000089\n",
      "Iter 37194, training loss 0.000007, validation loss 0.000083\n",
      "Iter 37195, training loss 0.000007, validation loss 0.000089\n",
      "Iter 37196, training loss 0.000008, validation loss 0.000084\n",
      "Iter 37197, training loss 0.000009, validation loss 0.000092\n",
      "Iter 37198, training loss 0.000010, validation loss 0.000084\n",
      "Iter 37199, training loss 0.000011, validation loss 0.000095\n",
      "Iter 37200, training loss 0.000012, validation loss 0.000087\n",
      "Iter 37201, training loss 0.000014, validation loss 0.000099\n",
      "Iter 37202, training loss 0.000018, validation loss 0.000091\n",
      "Iter 37203, training loss 0.000023, validation loss 0.000111\n",
      "Iter 37204, training loss 0.000031, validation loss 0.000101\n",
      "Iter 37205, training loss 0.000037, validation loss 0.000128\n",
      "Iter 37206, training loss 0.000041, validation loss 0.000108\n",
      "Iter 37207, training loss 0.000032, validation loss 0.000122\n",
      "Iter 37208, training loss 0.000017, validation loss 0.000088\n",
      "Iter 37209, training loss 0.000007, validation loss 0.000084\n",
      "Iter 37210, training loss 0.000007, validation loss 0.000086\n",
      "Iter 37211, training loss 0.000016, validation loss 0.000087\n",
      "Iter 37212, training loss 0.000022, validation loss 0.000109\n",
      "Iter 37213, training loss 0.000021, validation loss 0.000091\n",
      "Iter 37214, training loss 0.000013, validation loss 0.000095\n",
      "Iter 37215, training loss 0.000006, validation loss 0.000081\n",
      "Iter 37216, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37217, training loss 0.000009, validation loss 0.000088\n",
      "Iter 37218, training loss 0.000013, validation loss 0.000085\n",
      "Iter 37219, training loss 0.000014, validation loss 0.000095\n",
      "Iter 37220, training loss 0.000010, validation loss 0.000083\n",
      "Iter 37221, training loss 0.000006, validation loss 0.000084\n",
      "Iter 37222, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37223, training loss 0.000008, validation loss 0.000081\n",
      "Iter 37224, training loss 0.000010, validation loss 0.000092\n",
      "Iter 37225, training loss 0.000010, validation loss 0.000084\n",
      "Iter 37226, training loss 0.000008, validation loss 0.000088\n",
      "Iter 37227, training loss 0.000005, validation loss 0.000080\n",
      "Iter 37228, training loss 0.000005, validation loss 0.000080\n",
      "Iter 37229, training loss 0.000007, validation loss 0.000086\n",
      "Iter 37230, training loss 0.000008, validation loss 0.000082\n",
      "Iter 37231, training loss 0.000007, validation loss 0.000087\n",
      "Iter 37232, training loss 0.000006, validation loss 0.000081\n",
      "Iter 37233, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37234, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37235, training loss 0.000006, validation loss 0.000082\n",
      "Iter 37236, training loss 0.000006, validation loss 0.000085\n",
      "Iter 37237, training loss 0.000006, validation loss 0.000081\n",
      "Iter 37238, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37239, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37240, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37241, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37242, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37243, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37244, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37245, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37246, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37247, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37248, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37249, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37250, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37251, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37252, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37253, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37254, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37255, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37256, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37257, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37258, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37259, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37260, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37261, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37262, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37263, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37264, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37265, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37266, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37267, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37268, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37269, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37270, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37271, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37272, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37273, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37274, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37275, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37276, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37277, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37278, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37279, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37280, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37281, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37282, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37283, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37284, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37285, training loss 0.000004, validation loss 0.000081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 37286, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37287, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37288, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37289, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37290, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37291, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37292, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37293, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37294, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37295, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37296, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37297, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37298, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37299, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37300, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37301, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37302, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37303, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37304, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37305, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37306, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37307, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37308, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37309, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37310, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37311, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37312, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37313, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37314, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37315, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37316, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37317, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37318, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37319, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37320, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37321, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37322, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37323, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37324, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37325, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37326, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37327, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37328, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37329, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37330, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37331, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37332, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37333, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37334, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37335, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37336, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37337, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37338, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37339, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37340, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37341, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37342, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37343, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37344, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37345, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37346, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37347, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37348, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37349, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37350, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37351, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37352, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37353, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37354, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37355, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37356, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37357, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37358, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37359, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37360, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37361, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37362, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37363, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37364, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37365, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37366, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37367, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37368, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37369, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37370, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37371, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37372, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37373, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37374, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37375, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37376, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37377, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37378, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37379, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37380, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37381, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37382, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37383, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37384, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37385, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37386, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37387, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37388, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37389, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37390, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37391, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37392, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37393, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37394, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37395, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37396, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37397, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37398, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37399, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37400, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37401, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37402, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37403, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37404, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37405, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37406, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37407, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37408, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37409, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37410, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37411, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37412, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37413, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37414, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37415, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37416, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37417, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37418, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37419, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37420, training loss 0.000004, validation loss 0.000081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 37421, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37422, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37423, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37424, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37425, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37426, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37427, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37428, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37429, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37430, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37431, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37432, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37433, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37434, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37435, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37436, training loss 0.000006, validation loss 0.000081\n",
      "Iter 37437, training loss 0.000006, validation loss 0.000085\n",
      "Iter 37438, training loss 0.000006, validation loss 0.000081\n",
      "Iter 37439, training loss 0.000006, validation loss 0.000084\n",
      "Iter 37440, training loss 0.000006, validation loss 0.000081\n",
      "Iter 37441, training loss 0.000006, validation loss 0.000085\n",
      "Iter 37442, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37443, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37444, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37445, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37446, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37447, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37448, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37449, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37450, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37451, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37452, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37453, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37454, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37455, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37456, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37457, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37458, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37459, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37460, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37461, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37462, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37463, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37464, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37465, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37466, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37467, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37468, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37469, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37470, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37471, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37472, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37473, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37474, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37475, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37476, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37477, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37478, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37479, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37480, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37481, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37482, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37483, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37484, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37485, training loss 0.000005, validation loss 0.000084\n",
      "Iter 37486, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37487, training loss 0.000006, validation loss 0.000086\n",
      "Iter 37488, training loss 0.000008, validation loss 0.000083\n",
      "Iter 37489, training loss 0.000011, validation loss 0.000092\n",
      "Iter 37490, training loss 0.000017, validation loss 0.000090\n",
      "Iter 37491, training loss 0.000029, validation loss 0.000114\n",
      "Iter 37492, training loss 0.000051, validation loss 0.000119\n",
      "Iter 37493, training loss 0.000078, validation loss 0.000176\n",
      "Iter 37494, training loss 0.000097, validation loss 0.000157\n",
      "Iter 37495, training loss 0.000078, validation loss 0.000178\n",
      "Iter 37496, training loss 0.000034, validation loss 0.000097\n",
      "Iter 37497, training loss 0.000008, validation loss 0.000079\n",
      "Iter 37498, training loss 0.000025, validation loss 0.000100\n",
      "Iter 37499, training loss 0.000054, validation loss 0.000120\n",
      "Iter 37500, training loss 0.000064, validation loss 0.000160\n",
      "Iter 37501, training loss 0.000029, validation loss 0.000099\n",
      "Iter 37502, training loss 0.000008, validation loss 0.000079\n",
      "Iter 37503, training loss 0.000034, validation loss 0.000115\n",
      "Iter 37504, training loss 0.000059, validation loss 0.000125\n",
      "Iter 37505, training loss 0.000033, validation loss 0.000123\n",
      "Iter 37506, training loss 0.000006, validation loss 0.000082\n",
      "Iter 37507, training loss 0.000030, validation loss 0.000098\n",
      "Iter 37508, training loss 0.000042, validation loss 0.000128\n",
      "Iter 37509, training loss 0.000032, validation loss 0.000121\n",
      "Iter 37510, training loss 0.000011, validation loss 0.000092\n",
      "Iter 37511, training loss 0.000019, validation loss 0.000095\n",
      "Iter 37512, training loss 0.000026, validation loss 0.000088\n",
      "Iter 37513, training loss 0.000013, validation loss 0.000091\n",
      "Iter 37514, training loss 0.000009, validation loss 0.000089\n",
      "Iter 37515, training loss 0.000016, validation loss 0.000089\n",
      "Iter 37516, training loss 0.000017, validation loss 0.000099\n",
      "Iter 37517, training loss 0.000009, validation loss 0.000082\n",
      "Iter 37518, training loss 0.000009, validation loss 0.000084\n",
      "Iter 37519, training loss 0.000014, validation loss 0.000099\n",
      "Iter 37520, training loss 0.000010, validation loss 0.000085\n",
      "Iter 37521, training loss 0.000006, validation loss 0.000081\n",
      "Iter 37522, training loss 0.000009, validation loss 0.000086\n",
      "Iter 37523, training loss 0.000011, validation loss 0.000084\n",
      "Iter 37524, training loss 0.000007, validation loss 0.000089\n",
      "Iter 37525, training loss 0.000006, validation loss 0.000084\n",
      "Iter 37526, training loss 0.000008, validation loss 0.000082\n",
      "Iter 37527, training loss 0.000008, validation loss 0.000088\n",
      "Iter 37528, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37529, training loss 0.000006, validation loss 0.000080\n",
      "Iter 37530, training loss 0.000007, validation loss 0.000085\n",
      "Iter 37531, training loss 0.000006, validation loss 0.000079\n",
      "Iter 37532, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37533, training loss 0.000006, validation loss 0.000085\n",
      "Iter 37534, training loss 0.000006, validation loss 0.000081\n",
      "Iter 37535, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37536, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37537, training loss 0.000005, validation loss 0.000079\n",
      "Iter 37538, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37539, training loss 0.000005, validation loss 0.000079\n",
      "Iter 37540, training loss 0.000004, validation loss 0.000080\n",
      "Iter 37541, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37542, training loss 0.000005, validation loss 0.000080\n",
      "Iter 37543, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37544, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37545, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37546, training loss 0.000005, validation loss 0.000083\n",
      "Iter 37547, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37548, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37549, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37550, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37551, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37552, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37553, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37554, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37555, training loss 0.000004, validation loss 0.000083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 37556, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37557, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37558, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37559, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37560, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37561, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37562, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37563, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37564, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37565, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37566, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37567, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37568, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37569, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37570, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37571, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37572, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37573, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37574, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37575, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37576, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37577, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37578, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37579, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37580, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37581, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37582, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37583, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37584, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37585, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37586, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37587, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37588, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37589, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37590, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37591, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37592, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37593, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37594, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37595, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37596, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37597, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37598, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37599, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37600, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37601, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37602, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37603, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37604, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37605, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37606, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37607, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37608, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37609, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37610, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37611, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37612, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37613, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37614, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37615, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37616, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37617, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37618, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37619, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37620, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37621, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37622, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37623, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37624, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37625, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37626, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37627, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37628, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37629, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37630, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37631, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37632, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37633, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37634, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37635, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37636, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37637, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37638, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37639, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37640, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37641, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37642, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37643, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37644, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37645, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37646, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37647, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37648, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37649, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37650, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37651, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37652, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37653, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37654, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37655, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37656, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37657, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37658, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37659, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37660, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37661, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37662, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37663, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37664, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37665, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37666, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37667, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37668, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37669, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37670, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37671, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37672, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37673, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37674, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37675, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37676, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37677, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37678, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37679, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37680, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37681, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37682, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37683, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37684, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37685, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37686, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37687, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37688, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37689, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37690, training loss 0.000004, validation loss 0.000083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 37691, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37692, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37693, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37694, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37695, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37696, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37697, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37698, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37699, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37700, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37701, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37702, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37703, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37704, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37705, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37706, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37707, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37708, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37709, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37710, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37711, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37712, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37713, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37714, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37715, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37716, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37717, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37718, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37719, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37720, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37721, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37722, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37723, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37724, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37725, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37726, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37727, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37728, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37729, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37730, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37731, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37732, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37733, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37734, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37735, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37736, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37737, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37738, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37739, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37740, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37741, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37742, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37743, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37744, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37745, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37746, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37747, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37748, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37749, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37750, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37751, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37752, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37753, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37754, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37755, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37756, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37757, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37758, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37759, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37760, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37761, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37762, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37763, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37764, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37765, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37766, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37767, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37768, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37769, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37770, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37771, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37772, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37773, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37774, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37775, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37776, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37777, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37778, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37779, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37780, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37781, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37782, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37783, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37784, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37785, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37786, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37787, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37788, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37789, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37790, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37791, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37792, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37793, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37794, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37795, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37796, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37797, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37798, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37799, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37800, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37801, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37802, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37803, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37804, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37805, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37806, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37807, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37808, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37809, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37810, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37811, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37812, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37813, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37814, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37815, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37816, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37817, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37818, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37819, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37820, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37821, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37822, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37823, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37824, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37825, training loss 0.000004, validation loss 0.000082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 37826, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37827, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37828, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37829, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37830, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37831, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37832, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37833, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37834, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37835, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37836, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37837, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37838, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37839, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37840, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37841, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37842, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37843, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37844, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37845, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37846, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37847, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37848, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37849, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37850, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37851, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37852, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37853, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37854, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37855, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37856, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37857, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37858, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37859, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37860, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37861, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37862, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37863, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37864, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37865, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37866, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37867, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37868, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37869, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37870, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37871, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37872, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37873, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37874, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37875, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37876, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37877, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37878, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37879, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37880, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37881, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37882, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37883, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37884, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37885, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37886, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37887, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37888, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37889, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37890, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37891, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37892, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37893, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37894, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37895, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37896, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37897, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37898, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37899, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37900, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37901, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37902, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37903, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37904, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37905, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37906, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37907, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37908, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37909, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37910, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37911, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37912, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37913, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37914, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37915, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37916, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37917, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37918, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37919, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37920, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37921, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37922, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37923, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37924, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37925, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37926, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37927, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37928, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37929, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37930, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37931, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37932, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37933, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37934, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37935, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37936, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37937, training loss 0.000006, validation loss 0.000087\n",
      "Iter 37938, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37939, training loss 0.000006, validation loss 0.000088\n",
      "Iter 37940, training loss 0.000006, validation loss 0.000082\n",
      "Iter 37941, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37942, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37943, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37944, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37945, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37946, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37947, training loss 0.000003, validation loss 0.000083\n",
      "Iter 37948, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37949, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37950, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37951, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37952, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37953, training loss 0.000005, validation loss 0.000081\n",
      "Iter 37954, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37955, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37956, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37957, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37958, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37959, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37960, training loss 0.000004, validation loss 0.000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 37961, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37962, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37963, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37964, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37965, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37966, training loss 0.000003, validation loss 0.000083\n",
      "Iter 37967, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37968, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37969, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37970, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37971, training loss 0.000003, validation loss 0.000083\n",
      "Iter 37972, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37973, training loss 0.000003, validation loss 0.000083\n",
      "Iter 37974, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37975, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37976, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37977, training loss 0.000004, validation loss 0.000084\n",
      "Iter 37978, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37979, training loss 0.000004, validation loss 0.000085\n",
      "Iter 37980, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37981, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37982, training loss 0.000005, validation loss 0.000082\n",
      "Iter 37983, training loss 0.000005, validation loss 0.000086\n",
      "Iter 37984, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37985, training loss 0.000006, validation loss 0.000086\n",
      "Iter 37986, training loss 0.000006, validation loss 0.000082\n",
      "Iter 37987, training loss 0.000006, validation loss 0.000087\n",
      "Iter 37988, training loss 0.000006, validation loss 0.000083\n",
      "Iter 37989, training loss 0.000006, validation loss 0.000087\n",
      "Iter 37990, training loss 0.000006, validation loss 0.000082\n",
      "Iter 37991, training loss 0.000005, validation loss 0.000085\n",
      "Iter 37992, training loss 0.000004, validation loss 0.000082\n",
      "Iter 37993, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37994, training loss 0.000004, validation loss 0.000081\n",
      "Iter 37995, training loss 0.000004, validation loss 0.000083\n",
      "Iter 37996, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37997, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37998, training loss 0.000003, validation loss 0.000082\n",
      "Iter 37999, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38000, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38001, training loss 0.000003, validation loss 0.000083\n",
      "Iter 38002, training loss 0.000003, validation loss 0.000083\n",
      "Iter 38003, training loss 0.000003, validation loss 0.000083\n",
      "Iter 38004, training loss 0.000003, validation loss 0.000083\n",
      "Iter 38005, training loss 0.000003, validation loss 0.000083\n",
      "Iter 38006, training loss 0.000003, validation loss 0.000083\n",
      "Iter 38007, training loss 0.000003, validation loss 0.000083\n",
      "Iter 38008, training loss 0.000003, validation loss 0.000084\n",
      "Iter 38009, training loss 0.000003, validation loss 0.000083\n",
      "Iter 38010, training loss 0.000004, validation loss 0.000084\n",
      "Iter 38011, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38012, training loss 0.000004, validation loss 0.000086\n",
      "Iter 38013, training loss 0.000005, validation loss 0.000083\n",
      "Iter 38014, training loss 0.000005, validation loss 0.000087\n",
      "Iter 38015, training loss 0.000006, validation loss 0.000084\n",
      "Iter 38016, training loss 0.000007, validation loss 0.000089\n",
      "Iter 38017, training loss 0.000007, validation loss 0.000085\n",
      "Iter 38018, training loss 0.000008, validation loss 0.000090\n",
      "Iter 38019, training loss 0.000009, validation loss 0.000086\n",
      "Iter 38020, training loss 0.000010, validation loss 0.000093\n",
      "Iter 38021, training loss 0.000010, validation loss 0.000087\n",
      "Iter 38022, training loss 0.000011, validation loss 0.000093\n",
      "Iter 38023, training loss 0.000012, validation loss 0.000088\n",
      "Iter 38024, training loss 0.000013, validation loss 0.000097\n",
      "Iter 38025, training loss 0.000012, validation loss 0.000088\n",
      "Iter 38026, training loss 0.000011, validation loss 0.000092\n",
      "Iter 38027, training loss 0.000009, validation loss 0.000086\n",
      "Iter 38028, training loss 0.000007, validation loss 0.000088\n",
      "Iter 38029, training loss 0.000005, validation loss 0.000083\n",
      "Iter 38030, training loss 0.000004, validation loss 0.000084\n",
      "Iter 38031, training loss 0.000004, validation loss 0.000084\n",
      "Iter 38032, training loss 0.000004, validation loss 0.000084\n",
      "Iter 38033, training loss 0.000004, validation loss 0.000086\n",
      "Iter 38034, training loss 0.000005, validation loss 0.000083\n",
      "Iter 38035, training loss 0.000006, validation loss 0.000086\n",
      "Iter 38036, training loss 0.000008, validation loss 0.000084\n",
      "Iter 38037, training loss 0.000011, validation loss 0.000093\n",
      "Iter 38038, training loss 0.000014, validation loss 0.000090\n",
      "Iter 38039, training loss 0.000017, validation loss 0.000100\n",
      "Iter 38040, training loss 0.000020, validation loss 0.000094\n",
      "Iter 38041, training loss 0.000019, validation loss 0.000105\n",
      "Iter 38042, training loss 0.000015, validation loss 0.000090\n",
      "Iter 38043, training loss 0.000008, validation loss 0.000089\n",
      "Iter 38044, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38045, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38046, training loss 0.000007, validation loss 0.000088\n",
      "Iter 38047, training loss 0.000010, validation loss 0.000086\n",
      "Iter 38048, training loss 0.000012, validation loss 0.000095\n",
      "Iter 38049, training loss 0.000011, validation loss 0.000086\n",
      "Iter 38050, training loss 0.000008, validation loss 0.000089\n",
      "Iter 38051, training loss 0.000005, validation loss 0.000081\n",
      "Iter 38052, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38053, training loss 0.000005, validation loss 0.000084\n",
      "Iter 38054, training loss 0.000007, validation loss 0.000083\n",
      "Iter 38055, training loss 0.000009, validation loss 0.000090\n",
      "Iter 38056, training loss 0.000010, validation loss 0.000084\n",
      "Iter 38057, training loss 0.000008, validation loss 0.000089\n",
      "Iter 38058, training loss 0.000005, validation loss 0.000081\n",
      "Iter 38059, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38060, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38061, training loss 0.000006, validation loss 0.000081\n",
      "Iter 38062, training loss 0.000007, validation loss 0.000087\n",
      "Iter 38063, training loss 0.000007, validation loss 0.000082\n",
      "Iter 38064, training loss 0.000005, validation loss 0.000085\n",
      "Iter 38065, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38066, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38067, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38068, training loss 0.000005, validation loss 0.000082\n",
      "Iter 38069, training loss 0.000006, validation loss 0.000085\n",
      "Iter 38070, training loss 0.000005, validation loss 0.000081\n",
      "Iter 38071, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38072, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38073, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38074, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38075, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38076, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38077, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38078, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38079, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38080, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38081, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38082, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38083, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38084, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38085, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38086, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38087, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38088, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38089, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38090, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38091, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38092, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38093, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38094, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38095, training loss 0.000003, validation loss 0.000081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 38096, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38097, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38098, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38099, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38100, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38101, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38102, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38103, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38104, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38105, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38106, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38107, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38108, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38109, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38110, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38111, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38112, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38113, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38114, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38115, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38116, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38117, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38118, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38119, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38120, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38121, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38122, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38123, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38124, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38125, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38126, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38127, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38128, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38129, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38130, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38131, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38132, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38133, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38134, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38135, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38136, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38137, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38138, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38139, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38140, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38141, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38142, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38143, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38144, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38145, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38146, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38147, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38148, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38149, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38150, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38151, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38152, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38153, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38154, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38155, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38156, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38157, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38158, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38159, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38160, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38161, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38162, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38163, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38164, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38165, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38166, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38167, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38168, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38169, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38170, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38171, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38172, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38173, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38174, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38175, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38176, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38177, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38178, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38179, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38180, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38181, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38182, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38183, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38184, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38185, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38186, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38187, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38188, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38189, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38190, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38191, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38192, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38193, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38194, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38195, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38196, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38197, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38198, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38199, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38200, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38201, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38202, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38203, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38204, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38205, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38206, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38207, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38208, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38209, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38210, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38211, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38212, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38213, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38214, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38215, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38216, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38217, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38218, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38219, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38220, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38221, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38222, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38223, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38224, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38225, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38226, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38227, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38228, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38229, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38230, training loss 0.000004, validation loss 0.000080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 38231, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38232, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38233, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38234, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38235, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38236, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38237, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38238, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38239, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38240, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38241, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38242, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38243, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38244, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38245, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38246, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38247, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38248, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38249, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38250, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38251, training loss 0.000005, validation loss 0.000081\n",
      "Iter 38252, training loss 0.000005, validation loss 0.000085\n",
      "Iter 38253, training loss 0.000006, validation loss 0.000081\n",
      "Iter 38254, training loss 0.000007, validation loss 0.000087\n",
      "Iter 38255, training loss 0.000009, validation loss 0.000084\n",
      "Iter 38256, training loss 0.000013, validation loss 0.000095\n",
      "Iter 38257, training loss 0.000019, validation loss 0.000093\n",
      "Iter 38258, training loss 0.000032, validation loss 0.000116\n",
      "Iter 38259, training loss 0.000052, validation loss 0.000121\n",
      "Iter 38260, training loss 0.000063, validation loss 0.000156\n",
      "Iter 38261, training loss 0.000058, validation loss 0.000126\n",
      "Iter 38262, training loss 0.000029, validation loss 0.000116\n",
      "Iter 38263, training loss 0.000006, validation loss 0.000080\n",
      "Iter 38264, training loss 0.000010, validation loss 0.000083\n",
      "Iter 38265, training loss 0.000030, validation loss 0.000113\n",
      "Iter 38266, training loss 0.000042, validation loss 0.000114\n",
      "Iter 38267, training loss 0.000028, validation loss 0.000114\n",
      "Iter 38268, training loss 0.000009, validation loss 0.000080\n",
      "Iter 38269, training loss 0.000008, validation loss 0.000079\n",
      "Iter 38270, training loss 0.000021, validation loss 0.000101\n",
      "Iter 38271, training loss 0.000031, validation loss 0.000106\n",
      "Iter 38272, training loss 0.000019, validation loss 0.000103\n",
      "Iter 38273, training loss 0.000006, validation loss 0.000081\n",
      "Iter 38274, training loss 0.000007, validation loss 0.000080\n",
      "Iter 38275, training loss 0.000014, validation loss 0.000096\n",
      "Iter 38276, training loss 0.000016, validation loss 0.000091\n",
      "Iter 38277, training loss 0.000008, validation loss 0.000088\n",
      "Iter 38278, training loss 0.000005, validation loss 0.000081\n",
      "Iter 38279, training loss 0.000009, validation loss 0.000083\n",
      "Iter 38280, training loss 0.000012, validation loss 0.000094\n",
      "Iter 38281, training loss 0.000008, validation loss 0.000086\n",
      "Iter 38282, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38283, training loss 0.000006, validation loss 0.000084\n",
      "Iter 38284, training loss 0.000009, validation loss 0.000084\n",
      "Iter 38285, training loss 0.000009, validation loss 0.000091\n",
      "Iter 38286, training loss 0.000005, validation loss 0.000082\n",
      "Iter 38287, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38288, training loss 0.000007, validation loss 0.000081\n",
      "Iter 38289, training loss 0.000009, validation loss 0.000082\n",
      "Iter 38290, training loss 0.000007, validation loss 0.000087\n",
      "Iter 38291, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38292, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38293, training loss 0.000005, validation loss 0.000082\n",
      "Iter 38294, training loss 0.000006, validation loss 0.000081\n",
      "Iter 38295, training loss 0.000005, validation loss 0.000084\n",
      "Iter 38296, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38297, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38298, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38299, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38300, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38301, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38302, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38303, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38304, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38305, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38306, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38307, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38308, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38309, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38310, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38311, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38312, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38313, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38314, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38315, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38316, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38317, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38318, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38319, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38320, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38321, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38322, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38323, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38324, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38325, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38326, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38327, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38328, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38329, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38330, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38331, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38332, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38333, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38334, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38335, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38336, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38337, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38338, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38339, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38340, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38341, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38342, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38343, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38344, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38345, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38346, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38347, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38348, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38349, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38350, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38351, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38352, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38353, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38354, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38355, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38356, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38357, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38358, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38359, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38360, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38361, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38362, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38363, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38364, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38365, training loss 0.000003, validation loss 0.000080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 38366, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38367, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38368, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38369, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38370, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38371, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38372, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38373, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38374, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38375, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38376, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38377, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38378, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38379, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38380, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38381, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38382, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38383, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38384, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38385, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38386, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38387, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38388, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38389, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38390, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38391, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38392, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38393, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38394, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38395, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38396, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38397, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38398, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38399, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38400, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38401, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38402, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38403, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38404, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38405, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38406, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38407, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38408, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38409, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38410, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38411, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38412, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38413, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38414, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38415, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38416, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38417, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38418, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38419, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38420, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38421, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38422, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38423, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38424, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38425, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38426, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38427, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38428, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38429, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38430, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38431, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38432, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38433, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38434, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38435, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38436, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38437, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38438, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38439, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38440, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38441, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38442, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38443, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38444, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38445, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38446, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38447, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38448, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38449, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38450, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38451, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38452, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38453, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38454, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38455, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38456, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38457, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38458, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38459, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38460, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38461, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38462, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38463, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38464, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38465, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38466, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38467, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38468, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38469, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38470, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38471, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38472, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38473, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38474, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38475, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38476, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38477, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38478, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38479, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38480, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38481, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38482, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38483, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38484, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38485, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38486, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38487, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38488, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38489, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38490, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38491, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38492, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38493, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38494, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38495, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38496, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38497, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38498, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38499, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38500, training loss 0.000003, validation loss 0.000081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 38501, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38502, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38503, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38504, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38505, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38506, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38507, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38508, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38509, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38510, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38511, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38512, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38513, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38514, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38515, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38516, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38517, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38518, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38519, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38520, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38521, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38522, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38523, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38524, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38525, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38526, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38527, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38528, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38529, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38530, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38531, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38532, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38533, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38534, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38535, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38536, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38537, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38538, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38539, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38540, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38541, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38542, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38543, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38544, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38545, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38546, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38547, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38548, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38549, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38550, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38551, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38552, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38553, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38554, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38555, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38556, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38557, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38558, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38559, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38560, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38561, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38562, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38563, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38564, training loss 0.000003, validation loss 0.000082\n",
      "Iter 38565, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38566, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38567, training loss 0.000005, validation loss 0.000081\n",
      "Iter 38568, training loss 0.000006, validation loss 0.000086\n",
      "Iter 38569, training loss 0.000007, validation loss 0.000083\n",
      "Iter 38570, training loss 0.000009, validation loss 0.000089\n",
      "Iter 38571, training loss 0.000012, validation loss 0.000086\n",
      "Iter 38572, training loss 0.000014, validation loss 0.000095\n",
      "Iter 38573, training loss 0.000017, validation loss 0.000089\n",
      "Iter 38574, training loss 0.000019, validation loss 0.000102\n",
      "Iter 38575, training loss 0.000021, validation loss 0.000092\n",
      "Iter 38576, training loss 0.000018, validation loss 0.000100\n",
      "Iter 38577, training loss 0.000011, validation loss 0.000084\n",
      "Iter 38578, training loss 0.000005, validation loss 0.000083\n",
      "Iter 38579, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38580, training loss 0.000005, validation loss 0.000079\n",
      "Iter 38581, training loss 0.000008, validation loss 0.000087\n",
      "Iter 38582, training loss 0.000010, validation loss 0.000083\n",
      "Iter 38583, training loss 0.000010, validation loss 0.000089\n",
      "Iter 38584, training loss 0.000008, validation loss 0.000082\n",
      "Iter 38585, training loss 0.000005, validation loss 0.000083\n",
      "Iter 38586, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38587, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38588, training loss 0.000005, validation loss 0.000082\n",
      "Iter 38589, training loss 0.000006, validation loss 0.000080\n",
      "Iter 38590, training loss 0.000006, validation loss 0.000084\n",
      "Iter 38591, training loss 0.000005, validation loss 0.000079\n",
      "Iter 38592, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38593, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38594, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38595, training loss 0.000005, validation loss 0.000083\n",
      "Iter 38596, training loss 0.000005, validation loss 0.000080\n",
      "Iter 38597, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38598, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38599, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38600, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38601, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38602, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38603, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38604, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38605, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38606, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38607, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38608, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38609, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38610, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38611, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38612, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38613, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38614, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38615, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38616, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38617, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38618, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38619, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38620, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38621, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38622, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38623, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38624, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38625, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38626, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38627, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38628, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38629, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38630, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38631, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38632, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38633, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38634, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38635, training loss 0.000003, validation loss 0.000079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 38636, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38637, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38638, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38639, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38640, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38641, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38642, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38643, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38644, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38645, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38646, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38647, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38648, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38649, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38650, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38651, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38652, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38653, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38654, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38655, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38656, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38657, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38658, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38659, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38660, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38661, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38662, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38663, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38664, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38665, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38666, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38667, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38668, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38669, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38670, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38671, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38672, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38673, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38674, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38675, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38676, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38677, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38678, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38679, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38680, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38681, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38682, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38683, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38684, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38685, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38686, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38687, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38688, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38689, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38690, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38691, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38692, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38693, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38694, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38695, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38696, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38697, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38698, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38699, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38700, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38701, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38702, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38703, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38704, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38705, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38706, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38707, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38708, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38709, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38710, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38711, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38712, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38713, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38714, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38715, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38716, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38717, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38718, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38719, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38720, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38721, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38722, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38723, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38724, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38725, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38726, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38727, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38728, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38729, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38730, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38731, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38732, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38733, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38734, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38735, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38736, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38737, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38738, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38739, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38740, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38741, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38742, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38743, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38744, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38745, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38746, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38747, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38748, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38749, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38750, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38751, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38752, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38753, training loss 0.000004, validation loss 0.000082\n",
      "Iter 38754, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38755, training loss 0.000004, validation loss 0.000083\n",
      "Iter 38756, training loss 0.000005, validation loss 0.000080\n",
      "Iter 38757, training loss 0.000006, validation loss 0.000085\n",
      "Iter 38758, training loss 0.000008, validation loss 0.000082\n",
      "Iter 38759, training loss 0.000010, validation loss 0.000090\n",
      "Iter 38760, training loss 0.000015, validation loss 0.000088\n",
      "Iter 38761, training loss 0.000021, validation loss 0.000103\n",
      "Iter 38762, training loss 0.000031, validation loss 0.000102\n",
      "Iter 38763, training loss 0.000042, validation loss 0.000128\n",
      "Iter 38764, training loss 0.000052, validation loss 0.000120\n",
      "Iter 38765, training loss 0.000044, validation loss 0.000132\n",
      "Iter 38766, training loss 0.000021, validation loss 0.000091\n",
      "Iter 38767, training loss 0.000005, validation loss 0.000080\n",
      "Iter 38768, training loss 0.000010, validation loss 0.000087\n",
      "Iter 38769, training loss 0.000027, validation loss 0.000100\n",
      "Iter 38770, training loss 0.000036, validation loss 0.000122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 38771, training loss 0.000025, validation loss 0.000095\n",
      "Iter 38772, training loss 0.000007, validation loss 0.000086\n",
      "Iter 38773, training loss 0.000006, validation loss 0.000083\n",
      "Iter 38774, training loss 0.000018, validation loss 0.000090\n",
      "Iter 38775, training loss 0.000024, validation loss 0.000105\n",
      "Iter 38776, training loss 0.000016, validation loss 0.000091\n",
      "Iter 38777, training loss 0.000005, validation loss 0.000084\n",
      "Iter 38778, training loss 0.000006, validation loss 0.000084\n",
      "Iter 38779, training loss 0.000014, validation loss 0.000088\n",
      "Iter 38780, training loss 0.000017, validation loss 0.000099\n",
      "Iter 38781, training loss 0.000009, validation loss 0.000084\n",
      "Iter 38782, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38783, training loss 0.000008, validation loss 0.000086\n",
      "Iter 38784, training loss 0.000013, validation loss 0.000086\n",
      "Iter 38785, training loss 0.000009, validation loss 0.000090\n",
      "Iter 38786, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38787, training loss 0.000004, validation loss 0.000078\n",
      "Iter 38788, training loss 0.000008, validation loss 0.000085\n",
      "Iter 38789, training loss 0.000010, validation loss 0.000084\n",
      "Iter 38790, training loss 0.000006, validation loss 0.000084\n",
      "Iter 38791, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38792, training loss 0.000005, validation loss 0.000079\n",
      "Iter 38793, training loss 0.000007, validation loss 0.000085\n",
      "Iter 38794, training loss 0.000006, validation loss 0.000081\n",
      "Iter 38795, training loss 0.000003, validation loss 0.000081\n",
      "Iter 38796, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38797, training loss 0.000006, validation loss 0.000080\n",
      "Iter 38798, training loss 0.000006, validation loss 0.000084\n",
      "Iter 38799, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38800, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38801, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38802, training loss 0.000005, validation loss 0.000079\n",
      "Iter 38803, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38804, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38805, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38806, training loss 0.000004, validation loss 0.000081\n",
      "Iter 38807, training loss 0.000004, validation loss 0.000079\n",
      "Iter 38808, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38809, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38810, training loss 0.000004, validation loss 0.000078\n",
      "Iter 38811, training loss 0.000004, validation loss 0.000080\n",
      "Iter 38812, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38813, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38814, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38815, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38816, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38817, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38818, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38819, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38820, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38821, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38822, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38823, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38824, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38825, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38826, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38827, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38828, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38829, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38830, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38831, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38832, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38833, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38834, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38835, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38836, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38837, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38838, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38839, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38840, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38841, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38842, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38843, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38844, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38845, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38846, training loss 0.000003, validation loss 0.000078\n",
      "Iter 38847, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38848, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38849, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38850, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38851, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38852, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38853, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38854, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38855, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38856, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38857, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38858, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38859, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38860, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38861, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38862, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38863, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38864, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38865, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38866, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38867, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38868, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38869, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38870, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38871, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38872, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38873, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38874, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38875, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38876, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38877, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38878, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38879, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38880, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38881, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38882, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38883, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38884, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38885, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38886, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38887, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38888, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38889, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38890, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38891, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38892, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38893, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38894, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38895, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38896, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38897, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38898, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38899, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38900, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38901, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38902, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38903, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38904, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38905, training loss 0.000003, validation loss 0.000079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 38906, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38907, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38908, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38909, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38910, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38911, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38912, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38913, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38914, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38915, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38916, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38917, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38918, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38919, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38920, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38921, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38922, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38923, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38924, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38925, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38926, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38927, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38928, training loss 0.000003, validation loss 0.000079\n",
      "Iter 38929, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38930, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38931, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38932, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38933, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38934, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38935, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38936, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38937, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38938, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38939, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38940, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38941, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38942, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38943, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38944, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38945, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38946, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38947, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38948, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38949, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38950, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38951, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38952, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38953, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38954, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38955, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38956, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38957, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38958, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38959, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38960, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38961, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38962, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38963, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38964, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38965, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38966, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38967, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38968, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38969, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38970, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38971, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38972, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38973, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38974, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38975, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38976, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38977, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38978, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38979, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38980, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38981, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38982, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38983, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38984, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38985, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38986, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38987, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38988, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38989, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38990, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38991, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38992, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38993, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38994, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38995, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38996, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38997, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38998, training loss 0.000003, validation loss 0.000080\n",
      "Iter 38999, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39000, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39001, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39002, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39003, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39004, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39005, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39006, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39007, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39008, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39009, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39010, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39011, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39012, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39013, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39014, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39015, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39016, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39017, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39018, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39019, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39020, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39021, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39022, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39023, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39024, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39025, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39026, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39027, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39028, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39029, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39030, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39031, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39032, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39033, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39034, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39035, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39036, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39037, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39038, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39039, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39040, training loss 0.000003, validation loss 0.000080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39041, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39042, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39043, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39044, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39045, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39046, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39047, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39048, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39049, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39050, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39051, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39052, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39053, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39054, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39055, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39056, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39057, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39058, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39059, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39060, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39061, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39062, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39063, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39064, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39065, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39066, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39067, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39068, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39069, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39070, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39071, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39072, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39073, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39074, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39075, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39076, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39077, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39078, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39079, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39080, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39081, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39082, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39083, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39084, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39085, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39086, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39087, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39088, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39089, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39090, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39091, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39092, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39093, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39094, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39095, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39096, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39097, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39098, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39099, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39100, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39101, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39102, training loss 0.000003, validation loss 0.000082\n",
      "Iter 39103, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39104, training loss 0.000004, validation loss 0.000082\n",
      "Iter 39105, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39106, training loss 0.000004, validation loss 0.000083\n",
      "Iter 39107, training loss 0.000004, validation loss 0.000081\n",
      "Iter 39108, training loss 0.000005, validation loss 0.000084\n",
      "Iter 39109, training loss 0.000005, validation loss 0.000081\n",
      "Iter 39110, training loss 0.000005, validation loss 0.000085\n",
      "Iter 39111, training loss 0.000005, validation loss 0.000081\n",
      "Iter 39112, training loss 0.000005, validation loss 0.000085\n",
      "Iter 39113, training loss 0.000005, validation loss 0.000082\n",
      "Iter 39114, training loss 0.000005, validation loss 0.000085\n",
      "Iter 39115, training loss 0.000005, validation loss 0.000081\n",
      "Iter 39116, training loss 0.000005, validation loss 0.000084\n",
      "Iter 39117, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39118, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39119, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39120, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39121, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39122, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39123, training loss 0.000003, validation loss 0.000082\n",
      "Iter 39124, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39125, training loss 0.000005, validation loss 0.000084\n",
      "Iter 39126, training loss 0.000005, validation loss 0.000081\n",
      "Iter 39127, training loss 0.000005, validation loss 0.000085\n",
      "Iter 39128, training loss 0.000006, validation loss 0.000081\n",
      "Iter 39129, training loss 0.000005, validation loss 0.000085\n",
      "Iter 39130, training loss 0.000005, validation loss 0.000081\n",
      "Iter 39131, training loss 0.000004, validation loss 0.000083\n",
      "Iter 39132, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39133, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39134, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39135, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39136, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39137, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39138, training loss 0.000005, validation loss 0.000083\n",
      "Iter 39139, training loss 0.000005, validation loss 0.000080\n",
      "Iter 39140, training loss 0.000005, validation loss 0.000084\n",
      "Iter 39141, training loss 0.000005, validation loss 0.000081\n",
      "Iter 39142, training loss 0.000005, validation loss 0.000084\n",
      "Iter 39143, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39144, training loss 0.000004, validation loss 0.000082\n",
      "Iter 39145, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39146, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39147, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39148, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39149, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39150, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39151, training loss 0.000003, validation loss 0.000082\n",
      "Iter 39152, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39153, training loss 0.000003, validation loss 0.000082\n",
      "Iter 39154, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39155, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39156, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39157, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39158, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39159, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39160, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39161, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39162, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39163, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39164, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39165, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39166, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39167, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39168, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39169, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39170, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39171, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39172, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39173, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39174, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39175, training loss 0.000003, validation loss 0.000081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39176, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39177, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39178, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39179, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39180, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39181, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39182, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39183, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39184, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39185, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39186, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39187, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39188, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39189, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39190, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39191, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39192, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39193, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39194, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39195, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39196, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39197, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39198, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39199, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39200, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39201, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39202, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39203, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39204, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39205, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39206, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39207, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39208, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39209, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39210, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39211, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39212, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39213, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39214, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39215, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39216, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39217, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39218, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39219, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39220, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39221, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39222, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39223, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39224, training loss 0.000003, validation loss 0.000082\n",
      "Iter 39225, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39226, training loss 0.000004, validation loss 0.000082\n",
      "Iter 39227, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39228, training loss 0.000004, validation loss 0.000083\n",
      "Iter 39229, training loss 0.000005, validation loss 0.000081\n",
      "Iter 39230, training loss 0.000006, validation loss 0.000086\n",
      "Iter 39231, training loss 0.000008, validation loss 0.000083\n",
      "Iter 39232, training loss 0.000012, validation loss 0.000093\n",
      "Iter 39233, training loss 0.000021, validation loss 0.000094\n",
      "Iter 39234, training loss 0.000037, validation loss 0.000124\n",
      "Iter 39235, training loss 0.000065, validation loss 0.000134\n",
      "Iter 39236, training loss 0.000085, validation loss 0.000181\n",
      "Iter 39237, training loss 0.000085, validation loss 0.000150\n",
      "Iter 39238, training loss 0.000040, validation loss 0.000130\n",
      "Iter 39239, training loss 0.000005, validation loss 0.000079\n",
      "Iter 39240, training loss 0.000025, validation loss 0.000096\n",
      "Iter 39241, training loss 0.000058, validation loss 0.000140\n",
      "Iter 39242, training loss 0.000053, validation loss 0.000126\n",
      "Iter 39243, training loss 0.000014, validation loss 0.000098\n",
      "Iter 39244, training loss 0.000014, validation loss 0.000091\n",
      "Iter 39245, training loss 0.000045, validation loss 0.000111\n",
      "Iter 39246, training loss 0.000045, validation loss 0.000130\n",
      "Iter 39247, training loss 0.000018, validation loss 0.000090\n",
      "Iter 39248, training loss 0.000010, validation loss 0.000082\n",
      "Iter 39249, training loss 0.000035, validation loss 0.000112\n",
      "Iter 39250, training loss 0.000035, validation loss 0.000101\n",
      "Iter 39251, training loss 0.000014, validation loss 0.000099\n",
      "Iter 39252, training loss 0.000010, validation loss 0.000093\n",
      "Iter 39253, training loss 0.000023, validation loss 0.000089\n",
      "Iter 39254, training loss 0.000020, validation loss 0.000091\n",
      "Iter 39255, training loss 0.000006, validation loss 0.000079\n",
      "Iter 39256, training loss 0.000010, validation loss 0.000086\n",
      "Iter 39257, training loss 0.000015, validation loss 0.000097\n",
      "Iter 39258, training loss 0.000012, validation loss 0.000080\n",
      "Iter 39259, training loss 0.000006, validation loss 0.000077\n",
      "Iter 39260, training loss 0.000009, validation loss 0.000084\n",
      "Iter 39261, training loss 0.000009, validation loss 0.000083\n",
      "Iter 39262, training loss 0.000005, validation loss 0.000081\n",
      "Iter 39263, training loss 0.000006, validation loss 0.000080\n",
      "Iter 39264, training loss 0.000008, validation loss 0.000079\n",
      "Iter 39265, training loss 0.000005, validation loss 0.000082\n",
      "Iter 39266, training loss 0.000004, validation loss 0.000081\n",
      "Iter 39267, training loss 0.000006, validation loss 0.000078\n",
      "Iter 39268, training loss 0.000006, validation loss 0.000080\n",
      "Iter 39269, training loss 0.000004, validation loss 0.000077\n",
      "Iter 39270, training loss 0.000004, validation loss 0.000078\n",
      "Iter 39271, training loss 0.000005, validation loss 0.000082\n",
      "Iter 39272, training loss 0.000004, validation loss 0.000077\n",
      "Iter 39273, training loss 0.000003, validation loss 0.000076\n",
      "Iter 39274, training loss 0.000004, validation loss 0.000079\n",
      "Iter 39275, training loss 0.000004, validation loss 0.000078\n",
      "Iter 39276, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39277, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39278, training loss 0.000004, validation loss 0.000077\n",
      "Iter 39279, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39280, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39281, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39282, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39283, training loss 0.000003, validation loss 0.000077\n",
      "Iter 39284, training loss 0.000003, validation loss 0.000077\n",
      "Iter 39285, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39286, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39287, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39288, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39289, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39290, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39291, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39292, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39293, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39294, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39295, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39296, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39297, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39298, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39299, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39300, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39301, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39302, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39303, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39304, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39305, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39306, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39307, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39308, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39309, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39310, training loss 0.000002, validation loss 0.000078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39311, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39312, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39313, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39314, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39315, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39316, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39317, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39318, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39319, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39320, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39321, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39322, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39323, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39324, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39325, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39326, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39327, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39328, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39329, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39330, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39331, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39332, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39333, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39334, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39335, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39336, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39337, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39338, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39339, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39340, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39341, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39342, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39343, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39344, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39345, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39346, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39347, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39348, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39349, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39350, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39351, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39352, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39353, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39354, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39355, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39356, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39357, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39358, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39359, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39360, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39361, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39362, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39363, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39364, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39365, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39366, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39367, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39368, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39369, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39370, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39371, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39372, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39373, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39374, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39375, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39376, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39377, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39378, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39379, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39380, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39381, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39382, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39383, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39384, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39385, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39386, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39387, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39388, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39389, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39390, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39391, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39392, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39393, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39394, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39395, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39396, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39397, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39398, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39399, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39400, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39401, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39402, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39403, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39404, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39405, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39406, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39407, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39408, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39409, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39410, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39411, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39412, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39413, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39414, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39415, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39416, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39417, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39418, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39419, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39420, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39421, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39422, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39423, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39424, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39425, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39426, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39427, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39428, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39429, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39430, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39431, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39432, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39433, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39434, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39435, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39436, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39437, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39438, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39439, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39440, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39441, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39442, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39443, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39444, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39445, training loss 0.000002, validation loss 0.000078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39446, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39447, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39448, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39449, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39450, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39451, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39452, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39453, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39454, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39455, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39456, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39457, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39458, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39459, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39460, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39461, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39462, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39463, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39464, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39465, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39466, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39467, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39468, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39469, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39470, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39471, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39472, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39473, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39474, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39475, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39476, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39477, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39478, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39479, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39480, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39481, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39482, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39483, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39484, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39485, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39486, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39487, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39488, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39489, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39490, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39491, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39492, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39493, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39494, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39495, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39496, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39497, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39498, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39499, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39500, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39501, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39502, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39503, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39504, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39505, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39506, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39507, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39508, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39509, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39510, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39511, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39512, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39513, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39514, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39515, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39516, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39517, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39518, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39519, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39520, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39521, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39522, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39523, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39524, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39525, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39526, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39527, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39528, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39529, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39530, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39531, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39532, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39533, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39534, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39535, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39536, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39537, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39538, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39539, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39540, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39541, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39542, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39543, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39544, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39545, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39546, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39547, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39548, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39549, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39550, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39551, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39552, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39553, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39554, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39555, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39556, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39557, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39558, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39559, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39560, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39561, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39562, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39563, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39564, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39565, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39566, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39567, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39568, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39569, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39570, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39571, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39572, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39573, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39574, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39575, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39576, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39577, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39578, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39579, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39580, training loss 0.000002, validation loss 0.000078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39581, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39582, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39583, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39584, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39585, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39586, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39587, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39588, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39589, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39590, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39591, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39592, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39593, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39594, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39595, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39596, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39597, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39598, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39599, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39600, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39601, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39602, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39603, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39604, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39605, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39606, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39607, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39608, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39609, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39610, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39611, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39612, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39613, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39614, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39615, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39616, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39617, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39618, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39619, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39620, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39621, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39622, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39623, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39624, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39625, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39626, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39627, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39628, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39629, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39630, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39631, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39632, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39633, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39634, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39635, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39636, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39637, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39638, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39639, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39640, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39641, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39642, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39643, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39644, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39645, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39646, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39647, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39648, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39649, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39650, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39651, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39652, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39653, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39654, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39655, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39656, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39657, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39658, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39659, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39660, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39661, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39662, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39663, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39664, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39665, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39666, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39667, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39668, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39669, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39670, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39671, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39672, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39673, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39674, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39675, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39676, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39677, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39678, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39679, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39680, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39681, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39682, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39683, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39684, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39685, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39686, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39687, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39688, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39689, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39690, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39691, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39692, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39693, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39694, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39695, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39696, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39697, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39698, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39699, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39700, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39701, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39702, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39703, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39704, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39705, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39706, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39707, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39708, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39709, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39710, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39711, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39712, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39713, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39714, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39715, training loss 0.000003, validation loss 0.000081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39716, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39717, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39718, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39719, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39720, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39721, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39722, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39723, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39724, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39725, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39726, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39727, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39728, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39729, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39730, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39731, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39732, training loss 0.000003, validation loss 0.000081\n",
      "Iter 39733, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39734, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39735, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39736, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39737, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39738, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39739, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39740, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39741, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39742, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39743, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39744, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39745, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39746, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39747, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39748, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39749, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39750, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39751, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39752, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39753, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39754, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39755, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39756, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39757, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39758, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39759, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39760, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39761, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39762, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39763, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39764, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39765, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39766, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39767, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39768, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39769, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39770, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39771, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39772, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39773, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39774, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39775, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39776, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39777, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39778, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39779, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39780, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39781, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39782, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39783, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39784, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39785, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39786, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39787, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39788, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39789, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39790, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39791, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39792, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39793, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39794, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39795, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39796, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39797, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39798, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39799, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39800, training loss 0.000003, validation loss 0.000082\n",
      "Iter 39801, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39802, training loss 0.000006, validation loss 0.000086\n",
      "Iter 39803, training loss 0.000011, validation loss 0.000084\n",
      "Iter 39804, training loss 0.000019, validation loss 0.000102\n",
      "Iter 39805, training loss 0.000035, validation loss 0.000106\n",
      "Iter 39806, training loss 0.000058, validation loss 0.000150\n",
      "Iter 39807, training loss 0.000086, validation loss 0.000152\n",
      "Iter 39808, training loss 0.000083, validation loss 0.000188\n",
      "Iter 39809, training loss 0.000035, validation loss 0.000101\n",
      "Iter 39810, training loss 0.000006, validation loss 0.000077\n",
      "Iter 39811, training loss 0.000025, validation loss 0.000100\n",
      "Iter 39812, training loss 0.000060, validation loss 0.000132\n",
      "Iter 39813, training loss 0.000050, validation loss 0.000139\n",
      "Iter 39814, training loss 0.000011, validation loss 0.000083\n",
      "Iter 39815, training loss 0.000012, validation loss 0.000082\n",
      "Iter 39816, training loss 0.000041, validation loss 0.000125\n",
      "Iter 39817, training loss 0.000040, validation loss 0.000110\n",
      "Iter 39818, training loss 0.000011, validation loss 0.000093\n",
      "Iter 39819, training loss 0.000010, validation loss 0.000088\n",
      "Iter 39820, training loss 0.000033, validation loss 0.000098\n",
      "Iter 39821, training loss 0.000025, validation loss 0.000108\n",
      "Iter 39822, training loss 0.000007, validation loss 0.000083\n",
      "Iter 39823, training loss 0.000011, validation loss 0.000085\n",
      "Iter 39824, training loss 0.000025, validation loss 0.000103\n",
      "Iter 39825, training loss 0.000017, validation loss 0.000088\n",
      "Iter 39826, training loss 0.000004, validation loss 0.000083\n",
      "Iter 39827, training loss 0.000011, validation loss 0.000095\n",
      "Iter 39828, training loss 0.000016, validation loss 0.000086\n",
      "Iter 39829, training loss 0.000008, validation loss 0.000082\n",
      "Iter 39830, training loss 0.000004, validation loss 0.000078\n",
      "Iter 39831, training loss 0.000010, validation loss 0.000082\n",
      "Iter 39832, training loss 0.000009, validation loss 0.000087\n",
      "Iter 39833, training loss 0.000003, validation loss 0.000077\n",
      "Iter 39834, training loss 0.000006, validation loss 0.000079\n",
      "Iter 39835, training loss 0.000008, validation loss 0.000088\n",
      "Iter 39836, training loss 0.000005, validation loss 0.000082\n",
      "Iter 39837, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39838, training loss 0.000006, validation loss 0.000084\n",
      "Iter 39839, training loss 0.000006, validation loss 0.000079\n",
      "Iter 39840, training loss 0.000003, validation loss 0.000080\n",
      "Iter 39841, training loss 0.000004, validation loss 0.000082\n",
      "Iter 39842, training loss 0.000005, validation loss 0.000079\n",
      "Iter 39843, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39844, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39845, training loss 0.000004, validation loss 0.000079\n",
      "Iter 39846, training loss 0.000004, validation loss 0.000082\n",
      "Iter 39847, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39848, training loss 0.000003, validation loss 0.000077\n",
      "Iter 39849, training loss 0.000004, validation loss 0.000080\n",
      "Iter 39850, training loss 0.000003, validation loss 0.000078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39851, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39852, training loss 0.000003, validation loss 0.000079\n",
      "Iter 39853, training loss 0.000003, validation loss 0.000077\n",
      "Iter 39854, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39855, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39856, training loss 0.000003, validation loss 0.000078\n",
      "Iter 39857, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39858, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39859, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39860, training loss 0.000002, validation loss 0.000080\n",
      "Iter 39861, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39862, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39863, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39864, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39865, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39866, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39867, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39868, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39869, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39870, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39871, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39872, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39873, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39874, training loss 0.000002, validation loss 0.000079\n",
      "Iter 39875, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39876, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39877, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39878, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39879, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39880, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39881, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39882, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39883, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39884, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39885, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39886, training loss 0.000002, validation loss 0.000077\n",
      "Iter 39887, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39888, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39889, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39890, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39891, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39892, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39893, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39894, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39895, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39896, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39897, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39898, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39899, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39900, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39901, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39902, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39903, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39904, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39905, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39906, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39907, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39908, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39909, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39910, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39911, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39912, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39913, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39914, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39915, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39916, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39917, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39918, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39919, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39920, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39921, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39922, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39923, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39924, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39925, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39926, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39927, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39928, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39929, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39930, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39931, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39932, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39933, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39934, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39935, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39936, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39937, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39938, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39939, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39940, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39941, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39942, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39943, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39944, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39945, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39946, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39947, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39948, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39949, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39950, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39951, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39952, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39953, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39954, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39955, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39956, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39957, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39958, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39959, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39960, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39961, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39962, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39963, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39964, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39965, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39966, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39967, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39968, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39969, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39970, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39971, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39972, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39973, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39974, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39975, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39976, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39977, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39978, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39979, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39980, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39981, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39982, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39983, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39984, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39985, training loss 0.000002, validation loss 0.000078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39986, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39987, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39988, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39989, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39990, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39991, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39992, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39993, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39994, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39995, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39996, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39997, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39998, training loss 0.000002, validation loss 0.000078\n",
      "Iter 39999, training loss 0.000002, validation loss 0.000078\n",
      "Size of the model predictions for training is:  (6716, 1)\n",
      "Size of the model predictions for validation is:  (2238, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Ventricles_Norm\")\n",
    "ventricles_train_pred, ventricles_val_pred, tr_losses, te_losses=ventricles_pred=NN(x_train,y_train_ventricles,x_val,y_val_ventricles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEFCAYAAAABjYvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwP8HIEuRO8WVyOgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25a698c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch number is:  39273\n",
      "The lowest validation loss is:  0.008743904418121578\n",
      "At the same epoch, the training loss is:  0.001833263631911086\n"
     ]
    }
   ],
   "source": [
    "epochs=40000\n",
    "tr_loss, te_loss=plot_NN(epochs, tr_losses, te_losses)\n",
    "print(\"Best epoch number is: \", te_loss.index(min(te_loss)))\n",
    "print(\"The lowest validation loss is: \", min(te_loss))\n",
    "print(\"At the same epoch, the training loss is: \", tr_loss[te_loss.index(min(te_loss))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMSE\n",
      "Iter 0, training loss 175.320511, validation loss 176.891129\n",
      "Iter 1, training loss 285.139954, validation loss 298.083710\n",
      "Iter 2, training loss 215.111801, validation loss 229.713730\n",
      "Iter 3, training loss 108.159332, validation loss 118.080765\n",
      "Iter 4, training loss 66.770805, validation loss 71.175941\n",
      "Iter 5, training loss 70.530914, validation loss 71.704109\n",
      "Iter 6, training loss 72.033630, validation loss 72.671242\n",
      "Iter 7, training loss 56.986130, validation loss 59.093918\n",
      "Iter 8, training loss 46.243073, validation loss 51.244072\n",
      "Iter 9, training loss 58.215569, validation loss 66.837791\n",
      "Iter 10, training loss 61.303082, validation loss 72.167580\n",
      "Iter 11, training loss 45.125553, validation loss 56.467430\n",
      "Iter 12, training loss 33.192337, validation loss 44.210888\n",
      "Iter 13, training loss 33.076748, validation loss 43.940140\n",
      "Iter 14, training loss 34.134068, validation loss 45.513126\n",
      "Iter 15, training loss 29.001177, validation loss 41.639866\n",
      "Iter 16, training loss 21.333023, validation loss 35.856693\n",
      "Iter 17, training loss 18.389452, validation loss 35.067280\n",
      "Iter 18, training loss 21.191702, validation loss 39.676174\n",
      "Iter 19, training loss 23.007376, validation loss 42.343578\n",
      "Iter 20, training loss 19.937435, validation loss 39.047630\n",
      "Iter 21, training loss 15.932611, validation loss 34.174362\n",
      "Iter 22, training loss 15.202950, validation loss 32.538342\n",
      "Iter 23, training loss 16.594885, validation loss 33.415493\n",
      "Iter 24, training loss 16.471436, validation loss 33.313477\n",
      "Iter 25, training loss 13.866813, validation loss 31.182871\n",
      "Iter 26, training loss 11.252849, validation loss 29.278580\n",
      "Iter 27, training loss 10.992156, validation loss 29.678829\n",
      "Iter 28, training loss 12.009706, validation loss 31.037695\n",
      "Iter 29, training loss 11.524653, validation loss 30.474771\n",
      "Iter 30, training loss 9.608472, validation loss 28.202700\n",
      "Iter 31, training loss 8.654985, validation loss 26.873585\n",
      "Iter 32, training loss 9.263443, validation loss 27.274590\n",
      "Iter 33, training loss 9.690298, validation loss 27.716375\n",
      "Iter 34, training loss 8.843746, validation loss 27.059282\n",
      "Iter 35, training loss 7.729248, validation loss 26.214912\n",
      "Iter 36, training loss 7.621849, validation loss 26.367487\n",
      "Iter 37, training loss 8.010090, validation loss 26.943308\n",
      "Iter 38, training loss 7.612429, validation loss 26.641468\n",
      "Iter 39, training loss 6.662490, validation loss 25.719694\n",
      "Iter 40, training loss 6.283997, validation loss 25.347372\n",
      "Iter 41, training loss 6.499743, validation loss 25.585947\n",
      "Iter 42, training loss 6.447665, validation loss 25.589542\n",
      "Iter 43, training loss 5.919568, validation loss 25.144918\n",
      "Iter 44, training loss 5.560378, validation loss 24.879204\n",
      "Iter 45, training loss 5.678788, validation loss 25.073923\n",
      "Iter 46, training loss 5.753170, validation loss 25.189402\n",
      "Iter 47, training loss 5.423936, validation loss 24.864641\n",
      "Iter 48, training loss 5.083055, validation loss 24.506548\n",
      "Iter 49, training loss 5.063478, validation loss 24.471048\n",
      "Iter 50, training loss 5.087225, validation loss 24.497168\n",
      "Iter 51, training loss 4.857740, validation loss 24.293751\n",
      "Iter 52, training loss 4.575037, validation loss 24.052992\n",
      "Iter 53, training loss 4.520637, validation loss 24.040052\n",
      "Iter 54, training loss 4.548045, validation loss 24.094873\n",
      "Iter 55, training loss 4.403629, validation loss 23.959488\n",
      "Iter 56, training loss 4.207879, validation loss 23.761631\n",
      "Iter 57, training loss 4.167386, validation loss 23.721682\n",
      "Iter 58, training loss 4.173759, validation loss 23.743519\n",
      "Iter 59, training loss 4.054319, validation loss 23.657579\n",
      "Iter 60, training loss 3.898808, validation loss 23.547148\n",
      "Iter 61, training loss 3.849346, validation loss 23.542271\n",
      "Iter 62, training loss 3.823533, validation loss 23.550776\n",
      "Iter 63, training loss 3.709513, validation loss 23.459000\n",
      "Iter 64, training loss 3.592351, validation loss 23.358854\n",
      "Iter 65, training loss 3.557444, validation loss 23.341892\n",
      "Iter 66, training loss 3.525650, validation loss 23.328897\n",
      "Iter 67, training loss 3.436617, validation loss 23.253944\n",
      "Iter 68, training loss 3.361876, validation loss 23.184273\n",
      "Iter 69, training loss 3.338626, validation loss 23.159155\n",
      "Iter 70, training loss 3.297034, validation loss 23.118040\n",
      "Iter 71, training loss 3.217257, validation loss 23.049089\n",
      "Iter 72, training loss 3.161345, validation loss 23.013197\n",
      "Iter 73, training loss 3.132148, validation loss 23.003107\n",
      "Iter 74, training loss 3.079041, validation loss 22.955833\n",
      "Iter 75, training loss 3.013225, validation loss 22.878839\n",
      "Iter 76, training loss 2.975238, validation loss 22.822685\n",
      "Iter 77, training loss 2.944951, validation loss 22.784397\n",
      "Iter 78, training loss 2.893689, validation loss 22.746618\n",
      "Iter 79, training loss 2.846691, validation loss 22.728956\n",
      "Iter 80, training loss 2.818319, validation loss 22.727503\n",
      "Iter 81, training loss 2.781766, validation loss 22.698544\n",
      "Iter 82, training loss 2.733880, validation loss 22.636116\n",
      "Iter 83, training loss 2.697951, validation loss 22.577257\n",
      "Iter 84, training loss 2.667901, validation loss 22.536564\n",
      "Iter 85, training loss 2.627090, validation loss 22.508684\n",
      "Iter 86, training loss 2.587960, validation loss 22.499004\n",
      "Iter 87, training loss 2.559320, validation loss 22.497023\n",
      "Iter 88, training loss 2.527140, validation loss 22.472750\n",
      "Iter 89, training loss 2.490359, validation loss 22.424490\n",
      "Iter 90, training loss 2.460624, validation loss 22.378626\n",
      "Iter 91, training loss 2.432925, validation loss 22.349089\n",
      "Iter 92, training loss 2.399534, validation loss 22.336149\n",
      "Iter 93, training loss 2.368303, validation loss 22.338018\n",
      "Iter 94, training loss 2.341589, validation loss 22.338215\n",
      "Iter 95, training loss 2.311880, validation loss 22.316229\n",
      "Iter 96, training loss 2.281506, validation loss 22.277151\n",
      "Iter 97, training loss 2.255499, validation loss 22.241581\n",
      "Iter 98, training loss 2.229090, validation loss 22.220026\n",
      "Iter 99, training loss 2.200686, validation loss 22.213018\n",
      "Iter 100, training loss 2.175055, validation loss 22.212570\n",
      "Iter 101, training loss 2.150534, validation loss 22.200985\n",
      "Iter 102, training loss 2.124165, validation loss 22.169226\n",
      "Iter 103, training loss 2.099154, validation loss 22.129128\n",
      "Iter 104, training loss 2.075964, validation loss 22.096975\n",
      "Iter 105, training loss 2.051759, validation loss 22.079506\n",
      "Iter 106, training loss 2.028024, validation loss 22.073902\n",
      "Iter 107, training loss 2.006115, validation loss 22.067701\n",
      "Iter 108, training loss 1.983740, validation loss 22.048250\n",
      "Iter 109, training loss 1.961253, validation loss 22.017950\n",
      "Iter 110, training loss 1.940205, validation loss 21.990644\n",
      "Iter 111, training loss 1.919158, validation loss 21.976587\n",
      "Iter 112, training loss 1.897833, validation loss 21.975967\n",
      "Iter 113, training loss 1.877603, validation loss 21.979597\n",
      "Iter 114, training loss 1.857659, validation loss 21.974993\n",
      "Iter 115, training loss 1.837524, validation loss 21.959209\n",
      "Iter 116, training loss 1.818247, validation loss 21.941441\n",
      "Iter 117, training loss 1.799392, validation loss 21.931913\n",
      "Iter 118, training loss 1.780412, validation loss 21.933058\n",
      "Iter 119, training loss 1.762044, validation loss 21.938320\n",
      "Iter 120, training loss 1.744080, validation loss 21.937256\n",
      "Iter 121, training loss 1.726044, validation loss 21.925676\n",
      "Iter 122, training loss 1.708494, validation loss 21.910118\n",
      "Iter 123, training loss 1.691356, validation loss 21.899805\n",
      "Iter 124, training loss 1.674216, validation loss 21.898232\n",
      "Iter 125, training loss 1.657514, validation loss 21.900894\n",
      "Iter 126, training loss 1.641193, validation loss 21.899511\n",
      "Iter 127, training loss 1.624949, validation loss 21.890553\n",
      "Iter 128, training loss 1.609129, validation loss 21.879208\n",
      "Iter 129, training loss 1.593604, validation loss 21.873276\n",
      "Iter 130, training loss 1.578138, validation loss 21.875154\n",
      "Iter 131, training loss 1.562973, validation loss 21.880501\n",
      "Iter 132, training loss 1.548081, validation loss 21.882299\n",
      "Iter 133, training loss 1.533316, validation loss 21.877981\n",
      "Iter 134, training loss 1.518857, validation loss 21.871986\n",
      "Iter 135, training loss 1.504673, validation loss 21.870327\n",
      "Iter 136, training loss 1.490626, validation loss 21.874420\n",
      "Iter 137, training loss 1.476853, validation loss 21.880154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 138, training loss 1.463292, validation loss 21.882154\n",
      "Iter 139, training loss 1.449919, validation loss 21.879507\n",
      "Iter 140, training loss 1.436768, validation loss 21.875799\n",
      "Iter 141, training loss 1.423779, validation loss 21.874777\n",
      "Iter 142, training loss 1.410998, validation loss 21.876429\n",
      "Iter 143, training loss 1.398463, validation loss 21.877626\n",
      "Iter 144, training loss 1.386054, validation loss 21.876339\n",
      "Iter 145, training loss 1.373851, validation loss 21.874142\n",
      "Iter 146, training loss 1.361885, validation loss 21.874044\n",
      "Iter 147, training loss 1.350071, validation loss 21.876657\n",
      "Iter 148, training loss 1.338431, validation loss 21.879370\n",
      "Iter 149, training loss 1.326978, validation loss 21.879446\n",
      "Iter 150, training loss 1.315694, validation loss 21.877722\n",
      "Iter 151, training loss 1.304590, validation loss 21.877541\n",
      "Iter 152, training loss 1.293651, validation loss 21.880878\n",
      "Iter 153, training loss 1.282897, validation loss 21.886219\n",
      "Iter 154, training loss 1.272315, validation loss 21.889956\n",
      "Iter 155, training loss 1.261875, validation loss 21.890411\n",
      "Iter 156, training loss 1.251595, validation loss 21.889488\n",
      "Iter 157, training loss 1.241469, validation loss 21.890663\n",
      "Iter 158, training loss 1.231479, validation loss 21.894985\n",
      "Iter 159, training loss 1.221644, validation loss 21.900072\n",
      "Iter 160, training loss 1.211939, validation loss 21.903275\n",
      "Iter 161, training loss 1.202384, validation loss 21.904757\n",
      "Iter 162, training loss 1.192945, validation loss 21.906971\n",
      "Iter 163, training loss 1.183640, validation loss 21.911127\n",
      "Iter 164, training loss 1.174461, validation loss 21.916218\n",
      "Iter 165, training loss 1.165421, validation loss 21.920935\n",
      "Iter 166, training loss 1.156516, validation loss 21.925169\n",
      "Iter 167, training loss 1.147749, validation loss 21.929102\n",
      "Iter 168, training loss 1.139119, validation loss 21.933521\n",
      "Iter 169, training loss 1.130618, validation loss 21.938765\n",
      "Iter 170, training loss 1.122236, validation loss 21.944700\n",
      "Iter 171, training loss 1.113970, validation loss 21.950188\n",
      "Iter 172, training loss 1.105809, validation loss 21.954845\n",
      "Iter 173, training loss 1.097780, validation loss 21.959927\n",
      "Iter 174, training loss 1.089849, validation loss 21.966026\n",
      "Iter 175, training loss 1.082028, validation loss 21.972240\n",
      "Iter 176, training loss 1.074311, validation loss 21.977535\n",
      "Iter 177, training loss 1.066702, validation loss 21.982401\n",
      "Iter 178, training loss 1.059192, validation loss 21.988220\n",
      "Iter 179, training loss 1.051776, validation loss 21.994734\n",
      "Iter 180, training loss 1.044462, validation loss 22.000767\n",
      "Iter 181, training loss 1.037243, validation loss 22.006102\n",
      "Iter 182, training loss 1.030116, validation loss 22.011618\n",
      "Iter 183, training loss 1.023065, validation loss 22.017488\n",
      "Iter 184, training loss 1.016109, validation loss 22.022560\n",
      "Iter 185, training loss 1.009249, validation loss 22.026594\n",
      "Iter 186, training loss 1.002474, validation loss 22.030844\n",
      "Iter 187, training loss 0.995791, validation loss 22.036463\n",
      "Iter 188, training loss 0.989186, validation loss 22.042723\n",
      "Iter 189, training loss 0.982668, validation loss 22.047806\n",
      "Iter 190, training loss 0.976215, validation loss 22.051441\n",
      "Iter 191, training loss 0.969851, validation loss 22.055723\n",
      "Iter 192, training loss 0.963559, validation loss 22.061728\n",
      "Iter 193, training loss 0.957352, validation loss 22.067879\n",
      "Iter 194, training loss 0.951218, validation loss 22.072481\n",
      "Iter 195, training loss 0.945159, validation loss 22.076059\n",
      "Iter 196, training loss 0.939174, validation loss 22.080240\n",
      "Iter 197, training loss 0.933261, validation loss 22.085478\n",
      "Iter 198, training loss 0.927428, validation loss 22.091173\n",
      "Iter 199, training loss 0.921673, validation loss 22.097034\n",
      "Iter 200, training loss 0.915993, validation loss 22.103189\n",
      "Iter 201, training loss 0.910377, validation loss 22.108768\n",
      "Iter 202, training loss 0.904836, validation loss 22.113106\n",
      "Iter 203, training loss 0.899347, validation loss 22.117655\n",
      "Iter 204, training loss 0.893942, validation loss 22.124411\n",
      "Iter 205, training loss 0.888591, validation loss 22.132296\n",
      "Iter 206, training loss 0.883319, validation loss 22.138271\n",
      "Iter 207, training loss 0.878105, validation loss 22.142824\n",
      "Iter 208, training loss 0.872965, validation loss 22.148664\n",
      "Iter 209, training loss 0.867885, validation loss 22.156012\n",
      "Iter 210, training loss 0.862857, validation loss 22.162426\n",
      "Iter 211, training loss 0.857911, validation loss 22.167501\n",
      "Iter 212, training loss 0.853007, validation loss 22.172968\n",
      "Iter 213, training loss 0.848178, validation loss 22.178999\n",
      "Iter 214, training loss 0.843379, validation loss 22.184284\n",
      "Iter 215, training loss 0.838658, validation loss 22.189297\n",
      "Iter 216, training loss 0.834002, validation loss 22.196154\n",
      "Iter 217, training loss 0.829376, validation loss 22.203186\n",
      "Iter 218, training loss 0.824830, validation loss 22.208113\n",
      "Iter 219, training loss 0.820309, validation loss 22.212673\n",
      "Iter 220, training loss 0.815854, validation loss 22.219736\n",
      "Iter 221, training loss 0.811450, validation loss 22.228271\n",
      "Iter 222, training loss 0.807071, validation loss 22.234583\n",
      "Iter 223, training loss 0.802777, validation loss 22.239231\n",
      "Iter 224, training loss 0.798502, validation loss 22.245523\n",
      "Iter 225, training loss 0.794298, validation loss 22.252802\n",
      "Iter 226, training loss 0.790072, validation loss 22.257898\n",
      "Iter 227, training loss 0.785922, validation loss 22.262327\n",
      "Iter 228, training loss 0.781810, validation loss 22.268938\n",
      "Iter 229, training loss 0.777724, validation loss 22.274904\n",
      "Iter 230, training loss 0.773684, validation loss 22.278152\n",
      "Iter 231, training loss 0.769697, validation loss 22.282850\n",
      "Iter 232, training loss 0.765737, validation loss 22.290577\n",
      "Iter 233, training loss 0.761822, validation loss 22.297230\n",
      "Iter 234, training loss 0.757987, validation loss 22.300446\n",
      "Iter 235, training loss 0.754167, validation loss 22.303444\n",
      "Iter 236, training loss 0.750342, validation loss 22.310495\n",
      "Iter 237, training loss 0.746578, validation loss 22.319189\n",
      "Iter 238, training loss 0.742847, validation loss 22.324394\n",
      "Iter 239, training loss 0.739164, validation loss 22.326906\n",
      "Iter 240, training loss 0.735516, validation loss 22.331198\n",
      "Iter 241, training loss 0.731901, validation loss 22.338539\n",
      "Iter 242, training loss 0.728330, validation loss 22.346159\n",
      "Iter 243, training loss 0.724777, validation loss 22.351242\n",
      "Iter 244, training loss 0.721259, validation loss 22.353903\n",
      "Iter 245, training loss 0.717763, validation loss 22.357180\n",
      "Iter 246, training loss 0.714326, validation loss 22.363251\n",
      "Iter 247, training loss 0.710899, validation loss 22.369459\n",
      "Iter 248, training loss 0.707532, validation loss 22.373180\n",
      "Iter 249, training loss 0.704168, validation loss 22.377127\n",
      "Iter 250, training loss 0.700868, validation loss 22.383228\n",
      "Iter 251, training loss 0.697628, validation loss 22.390354\n",
      "Iter 252, training loss 0.694316, validation loss 22.396893\n",
      "Iter 253, training loss 0.691129, validation loss 22.401171\n",
      "Iter 254, training loss 0.687949, validation loss 22.405828\n",
      "Iter 255, training loss 0.684760, validation loss 22.411320\n",
      "Iter 256, training loss 0.681659, validation loss 22.420069\n",
      "Iter 257, training loss 0.678540, validation loss 22.430252\n",
      "Iter 258, training loss 0.675481, validation loss 22.433950\n",
      "Iter 259, training loss 0.672427, validation loss 22.433874\n",
      "Iter 260, training loss 0.669407, validation loss 22.441561\n",
      "Iter 261, training loss 0.666443, validation loss 22.453764\n",
      "Iter 262, training loss 0.663479, validation loss 22.458662\n",
      "Iter 263, training loss 0.660556, validation loss 22.459761\n",
      "Iter 264, training loss 0.657681, validation loss 22.465115\n",
      "Iter 265, training loss 0.654791, validation loss 22.472939\n",
      "Iter 266, training loss 0.651979, validation loss 22.480417\n",
      "Iter 267, training loss 0.649122, validation loss 22.487644\n",
      "Iter 268, training loss 0.646316, validation loss 22.492601\n",
      "Iter 269, training loss 0.643556, validation loss 22.495020\n",
      "Iter 270, training loss 0.640756, validation loss 22.500027\n",
      "Iter 271, training loss 0.638036, validation loss 22.510122\n",
      "Iter 272, training loss 0.635300, validation loss 22.517565\n",
      "Iter 273, training loss 0.632643, validation loss 22.518385\n",
      "Iter 274, training loss 0.629952, validation loss 22.521721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 275, training loss 0.627235, validation loss 22.532082\n",
      "Iter 276, training loss 0.624578, validation loss 22.539341\n",
      "Iter 277, training loss 0.621936, validation loss 22.541309\n",
      "Iter 278, training loss 0.619333, validation loss 22.545361\n",
      "Iter 279, training loss 0.616745, validation loss 22.551531\n",
      "Iter 280, training loss 0.614260, validation loss 22.559759\n",
      "Iter 281, training loss 0.611658, validation loss 22.566704\n",
      "Iter 282, training loss 0.609176, validation loss 22.569717\n",
      "Iter 283, training loss 0.606657, validation loss 22.573320\n",
      "Iter 284, training loss 0.604257, validation loss 22.580378\n",
      "Iter 285, training loss 0.601750, validation loss 22.589014\n",
      "Iter 286, training loss 0.599370, validation loss 22.590858\n",
      "Iter 287, training loss 0.597029, validation loss 22.591961\n",
      "Iter 288, training loss 0.594557, validation loss 22.599260\n",
      "Iter 289, training loss 0.592077, validation loss 22.603737\n",
      "Iter 290, training loss 0.589786, validation loss 22.608349\n",
      "Iter 291, training loss 0.587411, validation loss 22.615505\n",
      "Iter 292, training loss 0.585119, validation loss 22.619326\n",
      "Iter 293, training loss 0.582808, validation loss 22.623653\n",
      "Iter 294, training loss 0.580491, validation loss 22.630291\n",
      "Iter 295, training loss 0.578352, validation loss 22.632504\n",
      "Iter 296, training loss 0.575988, validation loss 22.638538\n",
      "Iter 297, training loss 0.573785, validation loss 22.645805\n",
      "Iter 298, training loss 0.571625, validation loss 22.646015\n",
      "Iter 299, training loss 0.569399, validation loss 22.650461\n",
      "Iter 300, training loss 0.567082, validation loss 22.663004\n",
      "Iter 301, training loss 0.565047, validation loss 22.668049\n",
      "Iter 302, training loss 0.562878, validation loss 22.672165\n",
      "Iter 303, training loss 0.560579, validation loss 22.678879\n",
      "Iter 304, training loss 0.558564, validation loss 22.679440\n",
      "Iter 305, training loss 0.556539, validation loss 22.682791\n",
      "Iter 306, training loss 0.554331, validation loss 22.694635\n",
      "Iter 307, training loss 0.552148, validation loss 22.699366\n",
      "Iter 308, training loss 0.550198, validation loss 22.702211\n",
      "Iter 309, training loss 0.548104, validation loss 22.710861\n",
      "Iter 310, training loss 0.546073, validation loss 22.715321\n",
      "Iter 311, training loss 0.544102, validation loss 22.718500\n",
      "Iter 312, training loss 0.542101, validation loss 22.722675\n",
      "Iter 313, training loss 0.540078, validation loss 22.726265\n",
      "Iter 314, training loss 0.538136, validation loss 22.736118\n",
      "Iter 315, training loss 0.536166, validation loss 22.740612\n",
      "Iter 316, training loss 0.534182, validation loss 22.744759\n",
      "Iter 317, training loss 0.532305, validation loss 22.749496\n",
      "Iter 318, training loss 0.530403, validation loss 22.754364\n",
      "Iter 319, training loss 0.528558, validation loss 22.762455\n",
      "Iter 320, training loss 0.526644, validation loss 22.763121\n",
      "Iter 321, training loss 0.524763, validation loss 22.771179\n",
      "Iter 322, training loss 0.523020, validation loss 22.782206\n",
      "Iter 323, training loss 0.521109, validation loss 22.779272\n",
      "Iter 324, training loss 0.519289, validation loss 22.785778\n",
      "Iter 325, training loss 0.517435, validation loss 22.794849\n",
      "Iter 326, training loss 0.515660, validation loss 22.799543\n",
      "Iter 327, training loss 0.513846, validation loss 22.805252\n",
      "Iter 328, training loss 0.512106, validation loss 22.808842\n",
      "Iter 329, training loss 0.510323, validation loss 22.817236\n",
      "Iter 330, training loss 0.508637, validation loss 22.824558\n",
      "Iter 331, training loss 0.506916, validation loss 22.823650\n",
      "Iter 332, training loss 0.505161, validation loss 22.831783\n",
      "Iter 333, training loss 0.503434, validation loss 22.838066\n",
      "Iter 334, training loss 0.501764, validation loss 22.844854\n",
      "Iter 335, training loss 0.500123, validation loss 22.848694\n",
      "Iter 336, training loss 0.498512, validation loss 22.848867\n",
      "Iter 337, training loss 0.496797, validation loss 22.863291\n",
      "Iter 338, training loss 0.495086, validation loss 22.865047\n",
      "Iter 339, training loss 0.493482, validation loss 22.867075\n",
      "Iter 340, training loss 0.491896, validation loss 22.877832\n",
      "Iter 341, training loss 0.490282, validation loss 22.880510\n",
      "Iter 342, training loss 0.488662, validation loss 22.887136\n",
      "Iter 343, training loss 0.487129, validation loss 22.894196\n",
      "Iter 344, training loss 0.485504, validation loss 22.900036\n",
      "Iter 345, training loss 0.483958, validation loss 22.906384\n",
      "Iter 346, training loss 0.482459, validation loss 22.910227\n",
      "Iter 347, training loss 0.480948, validation loss 22.917160\n",
      "Iter 348, training loss 0.479513, validation loss 22.918966\n",
      "Iter 349, training loss 0.477994, validation loss 22.928797\n",
      "Iter 350, training loss 0.476392, validation loss 22.928322\n",
      "Iter 351, training loss 0.474900, validation loss 22.935930\n",
      "Iter 352, training loss 0.473425, validation loss 22.945568\n",
      "Iter 353, training loss 0.471958, validation loss 22.948608\n",
      "Iter 354, training loss 0.470496, validation loss 22.950699\n",
      "Iter 355, training loss 0.469046, validation loss 22.956867\n",
      "Iter 356, training loss 0.467645, validation loss 22.968983\n",
      "Iter 357, training loss 0.466273, validation loss 22.969994\n",
      "Iter 358, training loss 0.464827, validation loss 22.979589\n",
      "Iter 359, training loss 0.463334, validation loss 22.980337\n",
      "Iter 360, training loss 0.461934, validation loss 22.988859\n",
      "Iter 361, training loss 0.460528, validation loss 22.998539\n",
      "Iter 362, training loss 0.459207, validation loss 23.000872\n",
      "Iter 363, training loss 0.457874, validation loss 23.003746\n",
      "Iter 364, training loss 0.456436, validation loss 23.006069\n",
      "Iter 365, training loss 0.455253, validation loss 23.023066\n",
      "Iter 366, training loss 0.453706, validation loss 23.025520\n",
      "Iter 367, training loss 0.452512, validation loss 23.028736\n",
      "Iter 368, training loss 0.451243, validation loss 23.030695\n",
      "Iter 369, training loss 0.449846, validation loss 23.036806\n",
      "Iter 370, training loss 0.448469, validation loss 23.049469\n",
      "Iter 371, training loss 0.447301, validation loss 23.047863\n",
      "Iter 372, training loss 0.445917, validation loss 23.058205\n",
      "Iter 373, training loss 0.444743, validation loss 23.071243\n",
      "Iter 374, training loss 0.443500, validation loss 23.055386\n",
      "Iter 375, training loss 0.442068, validation loss 23.067642\n",
      "Iter 376, training loss 0.440671, validation loss 23.082464\n",
      "Iter 377, training loss 0.439491, validation loss 23.081522\n",
      "Iter 378, training loss 0.438187, validation loss 23.094633\n",
      "Iter 379, training loss 0.436827, validation loss 23.096159\n",
      "Iter 380, training loss 0.435621, validation loss 23.095047\n",
      "Iter 381, training loss 0.434399, validation loss 23.112549\n",
      "Iter 382, training loss 0.433088, validation loss 23.114836\n",
      "Iter 383, training loss 0.431797, validation loss 23.117146\n",
      "Iter 384, training loss 0.430674, validation loss 23.124846\n",
      "Iter 385, training loss 0.429492, validation loss 23.125832\n",
      "Iter 386, training loss 0.428260, validation loss 23.140495\n",
      "Iter 387, training loss 0.427014, validation loss 23.136656\n",
      "Iter 388, training loss 0.425754, validation loss 23.143805\n",
      "Iter 389, training loss 0.424580, validation loss 23.153114\n",
      "Iter 390, training loss 0.423435, validation loss 23.157911\n",
      "Iter 391, training loss 0.422252, validation loss 23.166620\n",
      "Iter 392, training loss 0.421061, validation loss 23.164474\n",
      "Iter 393, training loss 0.419915, validation loss 23.171890\n",
      "Iter 394, training loss 0.418778, validation loss 23.187298\n",
      "Iter 395, training loss 0.417655, validation loss 23.185516\n",
      "Iter 396, training loss 0.416492, validation loss 23.192202\n",
      "Iter 397, training loss 0.415371, validation loss 23.197485\n",
      "Iter 398, training loss 0.414292, validation loss 23.208183\n",
      "Iter 399, training loss 0.413183, validation loss 23.203962\n",
      "Iter 400, training loss 0.411995, validation loss 23.219421\n",
      "Iter 401, training loss 0.410893, validation loss 23.224974\n",
      "Iter 402, training loss 0.409779, validation loss 23.222298\n",
      "Iter 403, training loss 0.408694, validation loss 23.233801\n",
      "Iter 404, training loss 0.407595, validation loss 23.231833\n",
      "Iter 405, training loss 0.406403, validation loss 23.235680\n",
      "Iter 406, training loss 0.405349, validation loss 23.253967\n",
      "Iter 407, training loss 0.404268, validation loss 23.254648\n",
      "Iter 408, training loss 0.403123, validation loss 23.255335\n",
      "Iter 409, training loss 0.402149, validation loss 23.258812\n",
      "Iter 410, training loss 0.401044, validation loss 23.268396\n",
      "Iter 411, training loss 0.399938, validation loss 23.275131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 412, training loss 0.399028, validation loss 23.275801\n",
      "Iter 413, training loss 0.397934, validation loss 23.286186\n",
      "Iter 414, training loss 0.397028, validation loss 23.286261\n",
      "Iter 415, training loss 0.395863, validation loss 23.301901\n",
      "Iter 416, training loss 0.394751, validation loss 23.296646\n",
      "Iter 417, training loss 0.393746, validation loss 23.302393\n",
      "Iter 418, training loss 0.392707, validation loss 23.315136\n",
      "Iter 419, training loss 0.391664, validation loss 23.322250\n",
      "Iter 420, training loss 0.390664, validation loss 23.321268\n",
      "Iter 421, training loss 0.389637, validation loss 23.324308\n",
      "Iter 422, training loss 0.388643, validation loss 23.338356\n",
      "Iter 423, training loss 0.387650, validation loss 23.344290\n",
      "Iter 424, training loss 0.386657, validation loss 23.341948\n",
      "Iter 425, training loss 0.385708, validation loss 23.348026\n",
      "Iter 426, training loss 0.384811, validation loss 23.362396\n",
      "Iter 427, training loss 0.384120, validation loss 23.357662\n",
      "Iter 428, training loss 0.382878, validation loss 23.378721\n",
      "Iter 429, training loss 0.381826, validation loss 23.368479\n",
      "Iter 430, training loss 0.380957, validation loss 23.365826\n",
      "Iter 431, training loss 0.380080, validation loss 23.391531\n",
      "Iter 432, training loss 0.379149, validation loss 23.386381\n",
      "Iter 433, training loss 0.378065, validation loss 23.393833\n",
      "Iter 434, training loss 0.377274, validation loss 23.397757\n",
      "Iter 435, training loss 0.376445, validation loss 23.395105\n",
      "Iter 436, training loss 0.375430, validation loss 23.422667\n",
      "Iter 437, training loss 0.374416, validation loss 23.412378\n",
      "Iter 438, training loss 0.373697, validation loss 23.398499\n",
      "Iter 439, training loss 0.372730, validation loss 23.432106\n",
      "Iter 440, training loss 0.371828, validation loss 23.431852\n",
      "Iter 441, training loss 0.370833, validation loss 23.426451\n",
      "Iter 442, training loss 0.370174, validation loss 23.431652\n",
      "Iter 443, training loss 0.369209, validation loss 23.427898\n",
      "Iter 444, training loss 0.368236, validation loss 23.454525\n",
      "Iter 445, training loss 0.367401, validation loss 23.453327\n",
      "Iter 446, training loss 0.366644, validation loss 23.433277\n",
      "Iter 447, training loss 0.365617, validation loss 23.460783\n",
      "Iter 448, training loss 0.364784, validation loss 23.468597\n",
      "Iter 449, training loss 0.364026, validation loss 23.449802\n",
      "Iter 450, training loss 0.363276, validation loss 23.466871\n",
      "Iter 451, training loss 0.362182, validation loss 23.467081\n",
      "Iter 452, training loss 0.361448, validation loss 23.475208\n",
      "Iter 453, training loss 0.360719, validation loss 23.486525\n",
      "Iter 454, training loss 0.359839, validation loss 23.464890\n",
      "Iter 455, training loss 0.358850, validation loss 23.488752\n",
      "Iter 456, training loss 0.358098, validation loss 23.501177\n",
      "Iter 457, training loss 0.357390, validation loss 23.481464\n",
      "Iter 458, training loss 0.356450, validation loss 23.501888\n",
      "Iter 459, training loss 0.355557, validation loss 23.503782\n",
      "Iter 460, training loss 0.354763, validation loss 23.501013\n",
      "Iter 461, training loss 0.354067, validation loss 23.513044\n",
      "Iter 462, training loss 0.353267, validation loss 23.509117\n",
      "Iter 463, training loss 0.352404, validation loss 23.528072\n",
      "Iter 464, training loss 0.351563, validation loss 23.521912\n",
      "Iter 465, training loss 0.350837, validation loss 23.514360\n",
      "Iter 466, training loss 0.350029, validation loss 23.538574\n",
      "Iter 467, training loss 0.349256, validation loss 23.538918\n",
      "Iter 468, training loss 0.348418, validation loss 23.537462\n",
      "Iter 469, training loss 0.347661, validation loss 23.539610\n",
      "Iter 470, training loss 0.347013, validation loss 23.546045\n",
      "Iter 471, training loss 0.346189, validation loss 23.564127\n",
      "Iter 472, training loss 0.345451, validation loss 23.548580\n",
      "Iter 473, training loss 0.344606, validation loss 23.561085\n",
      "Iter 474, training loss 0.343882, validation loss 23.565016\n",
      "Iter 475, training loss 0.343065, validation loss 23.569960\n",
      "Iter 476, training loss 0.342378, validation loss 23.567465\n",
      "Iter 477, training loss 0.341687, validation loss 23.567177\n",
      "Iter 478, training loss 0.340936, validation loss 23.596577\n",
      "Iter 479, training loss 0.340067, validation loss 23.579412\n",
      "Iter 480, training loss 0.339464, validation loss 23.572035\n",
      "Iter 481, training loss 0.338753, validation loss 23.608826\n",
      "Iter 482, training loss 0.338036, validation loss 23.597490\n",
      "Iter 483, training loss 0.337136, validation loss 23.596403\n",
      "Iter 484, training loss 0.336499, validation loss 23.606630\n",
      "Iter 485, training loss 0.335860, validation loss 23.609657\n",
      "Iter 486, training loss 0.335087, validation loss 23.625654\n",
      "Iter 487, training loss 0.334397, validation loss 23.608149\n",
      "Iter 488, training loss 0.333779, validation loss 23.613550\n",
      "Iter 489, training loss 0.333207, validation loss 23.657654\n",
      "Iter 490, training loss 0.332205, validation loss 23.628939\n",
      "Iter 491, training loss 0.331678, validation loss 23.616867\n",
      "Iter 492, training loss 0.331169, validation loss 23.661188\n",
      "Iter 493, training loss 0.330480, validation loss 23.644049\n",
      "Iter 494, training loss 0.329430, validation loss 23.657860\n",
      "Iter 495, training loss 0.329125, validation loss 23.660479\n",
      "Iter 496, training loss 0.328363, validation loss 23.641352\n",
      "Iter 497, training loss 0.327603, validation loss 23.685740\n",
      "Iter 498, training loss 0.326770, validation loss 23.677723\n",
      "Iter 499, training loss 0.326387, validation loss 23.647543\n",
      "Iter 500, training loss 0.325623, validation loss 23.693756\n",
      "Iter 501, training loss 0.324937, validation loss 23.693171\n",
      "Iter 502, training loss 0.324150, validation loss 23.687456\n",
      "Iter 503, training loss 0.323808, validation loss 23.692875\n",
      "Iter 504, training loss 0.322885, validation loss 23.678629\n",
      "Iter 505, training loss 0.322245, validation loss 23.714882\n",
      "Iter 506, training loss 0.321530, validation loss 23.719934\n",
      "Iter 507, training loss 0.321028, validation loss 23.689726\n",
      "Iter 508, training loss 0.320268, validation loss 23.721128\n",
      "Iter 509, training loss 0.319676, validation loss 23.729034\n",
      "Iter 510, training loss 0.318943, validation loss 23.725883\n",
      "Iter 511, training loss 0.318456, validation loss 23.728041\n",
      "Iter 512, training loss 0.317754, validation loss 23.718321\n",
      "Iter 513, training loss 0.317074, validation loss 23.751778\n",
      "Iter 514, training loss 0.316401, validation loss 23.750475\n",
      "Iter 515, training loss 0.315914, validation loss 23.728666\n",
      "Iter 516, training loss 0.315297, validation loss 23.760742\n",
      "Iter 517, training loss 0.314658, validation loss 23.753506\n",
      "Iter 518, training loss 0.313904, validation loss 23.761322\n",
      "Iter 519, training loss 0.313390, validation loss 23.766413\n",
      "Iter 520, training loss 0.312851, validation loss 23.757803\n",
      "Iter 521, training loss 0.312278, validation loss 23.788179\n",
      "Iter 522, training loss 0.311561, validation loss 23.763021\n",
      "Iter 523, training loss 0.310923, validation loss 23.773472\n",
      "Iter 524, training loss 0.310325, validation loss 23.794334\n",
      "Iter 525, training loss 0.309774, validation loss 23.783766\n",
      "Iter 526, training loss 0.309112, validation loss 23.788008\n",
      "Iter 527, training loss 0.308507, validation loss 23.791426\n",
      "Iter 528, training loss 0.307935, validation loss 23.801805\n",
      "Iter 529, training loss 0.307344, validation loss 23.804863\n",
      "Iter 530, training loss 0.306786, validation loss 23.800463\n",
      "Iter 531, training loss 0.306250, validation loss 23.816053\n",
      "Iter 532, training loss 0.305762, validation loss 23.805069\n",
      "Iter 533, training loss 0.305107, validation loss 23.829317\n",
      "Iter 534, training loss 0.304490, validation loss 23.816000\n",
      "Iter 535, training loss 0.303955, validation loss 23.813990\n",
      "Iter 536, training loss 0.303518, validation loss 23.842913\n",
      "Iter 537, training loss 0.303056, validation loss 23.818871\n",
      "Iter 538, training loss 0.302294, validation loss 23.847622\n",
      "Iter 539, training loss 0.301678, validation loss 23.835579\n",
      "Iter 540, training loss 0.301123, validation loss 23.835497\n",
      "Iter 541, training loss 0.300661, validation loss 23.854549\n",
      "Iter 542, training loss 0.300263, validation loss 23.838284\n",
      "Iter 543, training loss 0.299650, validation loss 23.872467\n",
      "Iter 544, training loss 0.299024, validation loss 23.849539\n",
      "Iter 545, training loss 0.298429, validation loss 23.863823\n",
      "Iter 546, training loss 0.297890, validation loss 23.868059\n",
      "Iter 547, training loss 0.297373, validation loss 23.863670\n",
      "Iter 548, training loss 0.296835, validation loss 23.883673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 549, training loss 0.296330, validation loss 23.876272\n",
      "Iter 550, training loss 0.295739, validation loss 23.880482\n",
      "Iter 551, training loss 0.295263, validation loss 23.883850\n",
      "Iter 552, training loss 0.294693, validation loss 23.891596\n",
      "Iter 553, training loss 0.294199, validation loss 23.891354\n",
      "Iter 554, training loss 0.293657, validation loss 23.895426\n",
      "Iter 555, training loss 0.293284, validation loss 23.896524\n",
      "Iter 556, training loss 0.292804, validation loss 23.915440\n",
      "Iter 557, training loss 0.292326, validation loss 23.893698\n",
      "Iter 558, training loss 0.291823, validation loss 23.929037\n",
      "Iter 559, training loss 0.291197, validation loss 23.910049\n",
      "Iter 560, training loss 0.290634, validation loss 23.918596\n",
      "Iter 561, training loss 0.290227, validation loss 23.930313\n",
      "Iter 562, training loss 0.289955, validation loss 23.916494\n",
      "Iter 563, training loss 0.289499, validation loss 23.960171\n",
      "Iter 564, training loss 0.288771, validation loss 23.916082\n",
      "Iter 565, training loss 0.288176, validation loss 23.929129\n",
      "Iter 566, training loss 0.287996, validation loss 23.970373\n",
      "Iter 567, training loss 0.287542, validation loss 23.924730\n",
      "Iter 568, training loss 0.286951, validation loss 23.960785\n",
      "Iter 569, training loss 0.286258, validation loss 23.945515\n",
      "Iter 570, training loss 0.285713, validation loss 23.949879\n",
      "Iter 571, training loss 0.285455, validation loss 23.968769\n",
      "Iter 572, training loss 0.284999, validation loss 23.945742\n",
      "Iter 573, training loss 0.284549, validation loss 23.984720\n",
      "Iter 574, training loss 0.283928, validation loss 23.951698\n",
      "Iter 575, training loss 0.283288, validation loss 23.962914\n",
      "Iter 576, training loss 0.282906, validation loss 23.987097\n",
      "Iter 577, training loss 0.282463, validation loss 23.973499\n",
      "Iter 578, training loss 0.281983, validation loss 23.985878\n",
      "Iter 579, training loss 0.281494, validation loss 23.967598\n",
      "Iter 580, training loss 0.280999, validation loss 23.995262\n",
      "Iter 581, training loss 0.280468, validation loss 23.989515\n",
      "Iter 582, training loss 0.280044, validation loss 23.985302\n",
      "Iter 583, training loss 0.279553, validation loss 24.000456\n",
      "Iter 584, training loss 0.279134, validation loss 23.998962\n",
      "Iter 585, training loss 0.278692, validation loss 24.002321\n",
      "Iter 586, training loss 0.278241, validation loss 23.992292\n",
      "Iter 587, training loss 0.277795, validation loss 24.019690\n",
      "Iter 588, training loss 0.277297, validation loss 24.002390\n",
      "Iter 589, training loss 0.276847, validation loss 24.012943\n",
      "Iter 590, training loss 0.276401, validation loss 24.011503\n",
      "Iter 591, training loss 0.275931, validation loss 24.022646\n",
      "Iter 592, training loss 0.275491, validation loss 24.014181\n",
      "Iter 593, training loss 0.275034, validation loss 24.027937\n",
      "Iter 594, training loss 0.274612, validation loss 24.026382\n",
      "Iter 595, training loss 0.274169, validation loss 24.026348\n",
      "Iter 596, training loss 0.273783, validation loss 24.025333\n",
      "Iter 597, training loss 0.273411, validation loss 24.050819\n",
      "Iter 598, training loss 0.273115, validation loss 24.023085\n",
      "Iter 599, training loss 0.272680, validation loss 24.058250\n",
      "Iter 600, training loss 0.272251, validation loss 24.021727\n",
      "Iter 601, training loss 0.271644, validation loss 24.058435\n",
      "Iter 602, training loss 0.271169, validation loss 24.044451\n",
      "Iter 603, training loss 0.270710, validation loss 24.047375\n",
      "Iter 604, training loss 0.270318, validation loss 24.060387\n",
      "Iter 605, training loss 0.269917, validation loss 24.048923\n",
      "Iter 606, training loss 0.269556, validation loss 24.072048\n",
      "Iter 607, training loss 0.269158, validation loss 24.049540\n",
      "Iter 608, training loss 0.268725, validation loss 24.080578\n",
      "Iter 609, training loss 0.268319, validation loss 24.053917\n",
      "Iter 610, training loss 0.267791, validation loss 24.079350\n",
      "Iter 611, training loss 0.267392, validation loss 24.078175\n",
      "Iter 612, training loss 0.266939, validation loss 24.081726\n",
      "Iter 613, training loss 0.266552, validation loss 24.083849\n",
      "Iter 614, training loss 0.266165, validation loss 24.079830\n",
      "Iter 615, training loss 0.265837, validation loss 24.104652\n",
      "Iter 616, training loss 0.265518, validation loss 24.073471\n",
      "Iter 617, training loss 0.265171, validation loss 24.117041\n",
      "Iter 618, training loss 0.264929, validation loss 24.079624\n",
      "Iter 619, training loss 0.264499, validation loss 24.131203\n",
      "Iter 620, training loss 0.264018, validation loss 24.079788\n",
      "Iter 621, training loss 0.263444, validation loss 24.126329\n",
      "Iter 622, training loss 0.262972, validation loss 24.107996\n",
      "Iter 623, training loss 0.262489, validation loss 24.123976\n",
      "Iter 624, training loss 0.262129, validation loss 24.119694\n",
      "Iter 625, training loss 0.261802, validation loss 24.106754\n",
      "Iter 626, training loss 0.261568, validation loss 24.148335\n",
      "Iter 627, training loss 0.261276, validation loss 24.102282\n",
      "Iter 628, training loss 0.260901, validation loss 24.152649\n",
      "Iter 629, training loss 0.260582, validation loss 24.112614\n",
      "Iter 630, training loss 0.260137, validation loss 24.166313\n",
      "Iter 631, training loss 0.259643, validation loss 24.113255\n",
      "Iter 632, training loss 0.259074, validation loss 24.156672\n",
      "Iter 633, training loss 0.258661, validation loss 24.145517\n",
      "Iter 634, training loss 0.258210, validation loss 24.147041\n",
      "Iter 635, training loss 0.257943, validation loss 24.158606\n",
      "Iter 636, training loss 0.257713, validation loss 24.140026\n",
      "Iter 637, training loss 0.257580, validation loss 24.193823\n",
      "Iter 638, training loss 0.257318, validation loss 24.124134\n",
      "Iter 639, training loss 0.256898, validation loss 24.190737\n",
      "Iter 640, training loss 0.256509, validation loss 24.144728\n",
      "Iter 641, training loss 0.255945, validation loss 24.202379\n",
      "Iter 642, training loss 0.255423, validation loss 24.151381\n",
      "Iter 643, training loss 0.254902, validation loss 24.183290\n",
      "Iter 644, training loss 0.254524, validation loss 24.186890\n",
      "Iter 645, training loss 0.254102, validation loss 24.181177\n",
      "Iter 646, training loss 0.253844, validation loss 24.189474\n",
      "Iter 647, training loss 0.253444, validation loss 24.178350\n",
      "Iter 648, training loss 0.253220, validation loss 24.222719\n",
      "Iter 649, training loss 0.252808, validation loss 24.178526\n",
      "Iter 650, training loss 0.252517, validation loss 24.205048\n",
      "Iter 651, training loss 0.252035, validation loss 24.186008\n",
      "Iter 652, training loss 0.251716, validation loss 24.231447\n",
      "Iter 653, training loss 0.251258, validation loss 24.197123\n",
      "Iter 654, training loss 0.250844, validation loss 24.213345\n",
      "Iter 655, training loss 0.250433, validation loss 24.216473\n",
      "Iter 656, training loss 0.250093, validation loss 24.220268\n",
      "Iter 657, training loss 0.249775, validation loss 24.227242\n",
      "Iter 658, training loss 0.249461, validation loss 24.208433\n",
      "Iter 659, training loss 0.249218, validation loss 24.252457\n",
      "Iter 660, training loss 0.248941, validation loss 24.212830\n",
      "Iter 661, training loss 0.248820, validation loss 24.259407\n",
      "Iter 662, training loss 0.248578, validation loss 24.202291\n",
      "Iter 663, training loss 0.248448, validation loss 24.285055\n",
      "Iter 664, training loss 0.248195, validation loss 24.200735\n",
      "Iter 665, training loss 0.247891, validation loss 24.285097\n",
      "Iter 666, training loss 0.247438, validation loss 24.212393\n",
      "Iter 667, training loss 0.247001, validation loss 24.301577\n",
      "Iter 668, training loss 0.246398, validation loss 24.228401\n",
      "Iter 669, training loss 0.245800, validation loss 24.276882\n",
      "Iter 670, training loss 0.245296, validation loss 24.246063\n",
      "Iter 671, training loss 0.244855, validation loss 24.280876\n",
      "Iter 672, training loss 0.244469, validation loss 24.279123\n",
      "Iter 673, training loss 0.244177, validation loss 24.260918\n",
      "Iter 674, training loss 0.243914, validation loss 24.287930\n",
      "Iter 675, training loss 0.243785, validation loss 24.256504\n",
      "Iter 676, training loss 0.243568, validation loss 24.314274\n",
      "Iter 677, training loss 0.243517, validation loss 24.246334\n",
      "Iter 678, training loss 0.243562, validation loss 24.336632\n",
      "Iter 679, training loss 0.243622, validation loss 24.238098\n",
      "Iter 680, training loss 0.243626, validation loss 24.351183\n",
      "Iter 681, training loss 0.243525, validation loss 24.224808\n",
      "Iter 682, training loss 0.243229, validation loss 24.362356\n",
      "Iter 683, training loss 0.242626, validation loss 24.247601\n",
      "Iter 684, training loss 0.241906, validation loss 24.365347\n",
      "Iter 685, training loss 0.241037, validation loss 24.262272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 686, training loss 0.240207, validation loss 24.336168\n",
      "Iter 687, training loss 0.239533, validation loss 24.293617\n",
      "Iter 688, training loss 0.239106, validation loss 24.312931\n",
      "Iter 689, training loss 0.238855, validation loss 24.332472\n",
      "Iter 690, training loss 0.238798, validation loss 24.293768\n",
      "Iter 691, training loss 0.238837, validation loss 24.364388\n",
      "Iter 692, training loss 0.238916, validation loss 24.276876\n",
      "Iter 693, training loss 0.239029, validation loss 24.382032\n",
      "Iter 694, training loss 0.238971, validation loss 24.263889\n",
      "Iter 695, training loss 0.238810, validation loss 24.397066\n",
      "Iter 696, training loss 0.238317, validation loss 24.272200\n",
      "Iter 697, training loss 0.237734, validation loss 24.393204\n",
      "Iter 698, training loss 0.236981, validation loss 24.287893\n",
      "Iter 699, training loss 0.236252, validation loss 24.383606\n",
      "Iter 700, training loss 0.235568, validation loss 24.316841\n",
      "Iter 701, training loss 0.235004, validation loss 24.363665\n",
      "Iter 702, training loss 0.234554, validation loss 24.337072\n",
      "Iter 703, training loss 0.234194, validation loss 24.349941\n",
      "Iter 704, training loss 0.233931, validation loss 24.365829\n",
      "Iter 705, training loss 0.233724, validation loss 24.338278\n",
      "Iter 706, training loss 0.233667, validation loss 24.383286\n",
      "Iter 707, training loss 0.233706, validation loss 24.318020\n",
      "Iter 708, training loss 0.234047, validation loss 24.414299\n",
      "Iter 709, training loss 0.234473, validation loss 24.297750\n",
      "Iter 710, training loss 0.235201, validation loss 24.448019\n",
      "Iter 711, training loss 0.235765, validation loss 24.279795\n",
      "Iter 712, training loss 0.236338, validation loss 24.477354\n",
      "Iter 713, training loss 0.236091, validation loss 24.271996\n",
      "Iter 714, training loss 0.235535, validation loss 24.478062\n",
      "Iter 715, training loss 0.234202, validation loss 24.291592\n",
      "Iter 716, training loss 0.232780, validation loss 24.458197\n",
      "Iter 717, training loss 0.231360, validation loss 24.331905\n",
      "Iter 718, training loss 0.230223, validation loss 24.420740\n",
      "Iter 719, training loss 0.229488, validation loss 24.371454\n",
      "Iter 720, training loss 0.229096, validation loss 24.385853\n",
      "Iter 721, training loss 0.229007, validation loss 24.420244\n",
      "Iter 722, training loss 0.229129, validation loss 24.362547\n",
      "Iter 723, training loss 0.229401, validation loss 24.449270\n",
      "Iter 724, training loss 0.229679, validation loss 24.335474\n",
      "Iter 725, training loss 0.230016, validation loss 24.474823\n",
      "Iter 726, training loss 0.230211, validation loss 24.327040\n",
      "Iter 727, training loss 0.230351, validation loss 24.496742\n",
      "Iter 728, training loss 0.230269, validation loss 24.327862\n",
      "Iter 729, training loss 0.230034, validation loss 24.507679\n",
      "Iter 730, training loss 0.229427, validation loss 24.332989\n",
      "Iter 731, training loss 0.228877, validation loss 24.497986\n",
      "Iter 732, training loss 0.228217, validation loss 24.344002\n",
      "Iter 733, training loss 0.227534, validation loss 24.495661\n",
      "Iter 734, training loss 0.226704, validation loss 24.366642\n",
      "Iter 735, training loss 0.225956, validation loss 24.481865\n",
      "Iter 736, training loss 0.225216, validation loss 24.390503\n",
      "Iter 737, training loss 0.224591, validation loss 24.463211\n",
      "Iter 738, training loss 0.224076, validation loss 24.413378\n",
      "Iter 739, training loss 0.223655, validation loss 24.445528\n",
      "Iter 740, training loss 0.223328, validation loss 24.437597\n",
      "Iter 741, training loss 0.223061, validation loss 24.435335\n",
      "Iter 742, training loss 0.222854, validation loss 24.457027\n",
      "Iter 743, training loss 0.222734, validation loss 24.422247\n",
      "Iter 744, training loss 0.222753, validation loss 24.479206\n",
      "Iter 745, training loss 0.222974, validation loss 24.398676\n",
      "Iter 746, training loss 0.223638, validation loss 24.514952\n",
      "Iter 747, training loss 0.224728, validation loss 24.367283\n",
      "Iter 748, training loss 0.226673, validation loss 24.567381\n",
      "Iter 749, training loss 0.229216, validation loss 24.325811\n",
      "Iter 750, training loss 0.232955, validation loss 24.634396\n",
      "Iter 751, training loss 0.235852, validation loss 24.283026\n",
      "Iter 752, training loss 0.238999, validation loss 24.687107\n",
      "Iter 753, training loss 0.238812, validation loss 24.271479\n",
      "Iter 754, training loss 0.237727, validation loss 24.688929\n",
      "Iter 755, training loss 0.233210, validation loss 24.300426\n",
      "Iter 756, training loss 0.228437, validation loss 24.624853\n",
      "Iter 757, training loss 0.223420, validation loss 24.370722\n",
      "Iter 758, training loss 0.220052, validation loss 24.534077\n",
      "Iter 759, training loss 0.218392, validation loss 24.462225\n",
      "Iter 760, training loss 0.218286, validation loss 24.457193\n",
      "Iter 761, training loss 0.219335, validation loss 24.548344\n",
      "Iter 762, training loss 0.221045, validation loss 24.399456\n",
      "Iter 763, training loss 0.223190, validation loss 24.610384\n",
      "Iter 764, training loss 0.224927, validation loss 24.357212\n",
      "Iter 765, training loss 0.226533, validation loss 24.651182\n",
      "Iter 766, training loss 0.226999, validation loss 24.343971\n",
      "Iter 767, training loss 0.227208, validation loss 24.665653\n",
      "Iter 768, training loss 0.226118, validation loss 24.347616\n",
      "Iter 769, training loss 0.224888, validation loss 24.648945\n",
      "Iter 770, training loss 0.222816, validation loss 24.367451\n",
      "Iter 771, training loss 0.220805, validation loss 24.617899\n",
      "Iter 772, training loss 0.218597, validation loss 24.409689\n",
      "Iter 773, training loss 0.216730, validation loss 24.578402\n",
      "Iter 774, training loss 0.215295, validation loss 24.462862\n",
      "Iter 775, training loss 0.214352, validation loss 24.536894\n",
      "Iter 776, training loss 0.213835, validation loss 24.509518\n",
      "Iter 777, training loss 0.213661, validation loss 24.500368\n",
      "Iter 778, training loss 0.213720, validation loss 24.546864\n",
      "Iter 779, training loss 0.214001, validation loss 24.472464\n",
      "Iter 780, training loss 0.214541, validation loss 24.585625\n",
      "Iter 781, training loss 0.215398, validation loss 24.441734\n",
      "Iter 782, training loss 0.216735, validation loss 24.628662\n",
      "Iter 783, training loss 0.218394, validation loss 24.405853\n",
      "Iter 784, training loss 0.221024, validation loss 24.680265\n",
      "Iter 785, training loss 0.223973, validation loss 24.365623\n",
      "Iter 786, training loss 0.228390, validation loss 24.745148\n",
      "Iter 787, training loss 0.232388, validation loss 24.323030\n",
      "Iter 788, training loss 0.237779, validation loss 24.810976\n",
      "Iter 789, training loss 0.240718, validation loss 24.290308\n",
      "Iter 790, training loss 0.244157, validation loss 24.851620\n",
      "Iter 791, training loss 0.242945, validation loss 24.283390\n",
      "Iter 792, training loss 0.241151, validation loss 24.842909\n",
      "Iter 793, training loss 0.234864, validation loss 24.315277\n",
      "Iter 794, training loss 0.228556, validation loss 24.773579\n",
      "Iter 795, training loss 0.221140, validation loss 24.381758\n",
      "Iter 796, training loss 0.215235, validation loss 24.673256\n",
      "Iter 797, training loss 0.210999, validation loss 24.477968\n",
      "Iter 798, training loss 0.208835, validation loss 24.579725\n",
      "Iter 799, training loss 0.208434, validation loss 24.575399\n",
      "Iter 800, training loss 0.209350, validation loss 24.506561\n",
      "Iter 801, training loss 0.211220, validation loss 24.657257\n",
      "Iter 802, training loss 0.213645, validation loss 24.448969\n",
      "Iter 803, training loss 0.216638, validation loss 24.725666\n",
      "Iter 804, training loss 0.219551, validation loss 24.404947\n",
      "Iter 805, training loss 0.223181, validation loss 24.782988\n",
      "Iter 806, training loss 0.226047, validation loss 24.368660\n",
      "Iter 807, training loss 0.229599, validation loss 24.830647\n",
      "Iter 808, training loss 0.231507, validation loss 24.345490\n",
      "Iter 809, training loss 0.233994, validation loss 24.862299\n",
      "Iter 810, training loss 0.233840, validation loss 24.337450\n",
      "Iter 811, training loss 0.233855, validation loss 24.867384\n",
      "Iter 812, training loss 0.230997, validation loss 24.350044\n",
      "Iter 813, training loss 0.228239, validation loss 24.843569\n",
      "Iter 814, training loss 0.223613, validation loss 24.387335\n",
      "Iter 815, training loss 0.219433, validation loss 24.794170\n",
      "Iter 816, training loss 0.214840, validation loss 24.439390\n",
      "Iter 817, training loss 0.211072, validation loss 24.728750\n",
      "Iter 818, training loss 0.207898, validation loss 24.498270\n",
      "Iter 819, training loss 0.205621, validation loss 24.667425\n",
      "Iter 820, training loss 0.204122, validation loss 24.560360\n",
      "Iter 821, training loss 0.203318, validation loss 24.617922\n",
      "Iter 822, training loss 0.203008, validation loss 24.613762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 823, training loss 0.203088, validation loss 24.578966\n",
      "Iter 824, training loss 0.203505, validation loss 24.659985\n",
      "Iter 825, training loss 0.204307, validation loss 24.544376\n",
      "Iter 826, training loss 0.205606, validation loss 24.709888\n",
      "Iter 827, training loss 0.207383, validation loss 24.501150\n",
      "Iter 828, training loss 0.210235, validation loss 24.765039\n",
      "Iter 829, training loss 0.213969, validation loss 24.449501\n",
      "Iter 830, training loss 0.219980, validation loss 24.848619\n",
      "Iter 831, training loss 0.227406, validation loss 24.386997\n",
      "Iter 832, training loss 0.239232, validation loss 24.963896\n",
      "Iter 833, training loss 0.252041, validation loss 24.310139\n",
      "Iter 834, training loss 0.271355, validation loss 25.103708\n",
      "Iter 835, training loss 0.286934, validation loss 24.229374\n",
      "Iter 836, training loss 0.307367, validation loss 25.234306\n",
      "Iter 837, training loss 0.312802, validation loss 24.186163\n",
      "Iter 838, training loss 0.315525, validation loss 25.262468\n",
      "Iter 839, training loss 0.296077, validation loss 24.207359\n",
      "Iter 840, training loss 0.271799, validation loss 25.105822\n",
      "Iter 841, training loss 0.239439, validation loss 24.330458\n",
      "Iter 842, training loss 0.214316, validation loss 24.823734\n",
      "Iter 843, training loss 0.200670, validation loss 24.558552\n",
      "Iter 844, training loss 0.200327, validation loss 24.565336\n",
      "Iter 845, training loss 0.209864, validation loss 24.807903\n",
      "Iter 846, training loss 0.223414, validation loss 24.404461\n",
      "Iter 847, training loss 0.236904, validation loss 24.982849\n",
      "Iter 848, training loss 0.243914, validation loss 24.338711\n",
      "Iter 849, training loss 0.246096, validation loss 25.033108\n",
      "Iter 850, training loss 0.239044, validation loss 24.355034\n",
      "Iter 851, training loss 0.229112, validation loss 24.955458\n",
      "Iter 852, training loss 0.216254, validation loss 24.440401\n",
      "Iter 853, training loss 0.205774, validation loss 24.802057\n",
      "Iter 854, training loss 0.198795, validation loss 24.569262\n",
      "Iter 855, training loss 0.196280, validation loss 24.646965\n",
      "Iter 856, training loss 0.197545, validation loss 24.707754\n",
      "Iter 857, training loss 0.201252, validation loss 24.534866\n",
      "Iter 858, training loss 0.206173, validation loss 24.822439\n",
      "Iter 859, training loss 0.210723, validation loss 24.470583\n",
      "Iter 860, training loss 0.214955, validation loss 24.897335\n",
      "Iter 861, training loss 0.217205, validation loss 24.441450\n",
      "Iter 862, training loss 0.218731, validation loss 24.928377\n",
      "Iter 863, training loss 0.217821, validation loss 24.441769\n",
      "Iter 864, training loss 0.216429, validation loss 24.922405\n",
      "Iter 865, training loss 0.213204, validation loss 24.466778\n",
      "Iter 866, training loss 0.210088, validation loss 24.887323\n",
      "Iter 867, training loss 0.206216, validation loss 24.505051\n",
      "Iter 868, training loss 0.202824, validation loss 24.834530\n",
      "Iter 869, training loss 0.199614, validation loss 24.553297\n",
      "Iter 870, training loss 0.197091, validation loss 24.781164\n",
      "Iter 871, training loss 0.195127, validation loss 24.605688\n",
      "Iter 872, training loss 0.193755, validation loss 24.736820\n",
      "Iter 873, training loss 0.192827, validation loss 24.652868\n",
      "Iter 874, training loss 0.192278, validation loss 24.702631\n",
      "Iter 875, training loss 0.191981, validation loss 24.692701\n",
      "Iter 876, training loss 0.191857, validation loss 24.678701\n",
      "Iter 877, training loss 0.191888, validation loss 24.724127\n",
      "Iter 878, training loss 0.192082, validation loss 24.652712\n",
      "Iter 879, training loss 0.192542, validation loss 24.756315\n",
      "Iter 880, training loss 0.193351, validation loss 24.622820\n",
      "Iter 881, training loss 0.194807, validation loss 24.805685\n",
      "Iter 882, training loss 0.197170, validation loss 24.581692\n",
      "Iter 883, training loss 0.201189, validation loss 24.880487\n",
      "Iter 884, training loss 0.207155, validation loss 24.516581\n",
      "Iter 885, training loss 0.217103, validation loss 24.993561\n",
      "Iter 886, training loss 0.230574, validation loss 24.425768\n",
      "Iter 887, training loss 0.252390, validation loss 25.166216\n",
      "Iter 888, training loss 0.278292, validation loss 24.319933\n",
      "Iter 889, training loss 0.319083, validation loss 25.405151\n",
      "Iter 890, training loss 0.357067, validation loss 24.211622\n",
      "Iter 891, training loss 0.409543, validation loss 25.659435\n",
      "Iter 892, training loss 0.431206, validation loss 24.140375\n",
      "Iter 893, training loss 0.447253, validation loss 25.749136\n",
      "Iter 894, training loss 0.402067, validation loss 24.149855\n",
      "Iter 895, training loss 0.340093, validation loss 25.448652\n",
      "Iter 896, training loss 0.258383, validation loss 24.328526\n",
      "Iter 897, training loss 0.203570, validation loss 24.887316\n",
      "Iter 898, training loss 0.190342, validation loss 24.738686\n",
      "Iter 899, training loss 0.214282, validation loss 24.466772\n",
      "Iter 900, training loss 0.253717, validation loss 25.167055\n",
      "Iter 901, training loss 0.280058, validation loss 24.303347\n",
      "Iter 902, training loss 0.286024, validation loss 25.286423\n",
      "Iter 903, training loss 0.261440, validation loss 24.332619\n",
      "Iter 904, training loss 0.228178, validation loss 25.052731\n",
      "Iter 905, training loss 0.198778, validation loss 24.542473\n",
      "Iter 906, training loss 0.187410, validation loss 24.714100\n",
      "Iter 907, training loss 0.194506, validation loss 24.855579\n",
      "Iter 908, training loss 0.211242, validation loss 24.488846\n",
      "Iter 909, training loss 0.227218, validation loss 25.073368\n",
      "Iter 910, training loss 0.232220, validation loss 24.414175\n",
      "Iter 911, training loss 0.227868, validation loss 25.073166\n",
      "Iter 912, training loss 0.214046, validation loss 24.466644\n",
      "Iter 913, training loss 0.199636, validation loss 24.909264\n",
      "Iter 914, training loss 0.188909, validation loss 24.621372\n",
      "Iter 915, training loss 0.185222, validation loss 24.714912\n",
      "Iter 916, training loss 0.187938, validation loss 24.811071\n",
      "Iter 917, training loss 0.194257, validation loss 24.582134\n",
      "Iter 918, training loss 0.201057, validation loss 24.954275\n",
      "Iter 919, training loss 0.205244, validation loss 24.528458\n",
      "Iter 920, training loss 0.206445, validation loss 25.000000\n",
      "Iter 921, training loss 0.203651, validation loss 24.537420\n",
      "Iter 922, training loss 0.199048, validation loss 24.954226\n",
      "Iter 923, training loss 0.193317, validation loss 24.596371\n",
      "Iter 924, training loss 0.188348, validation loss 24.864422\n",
      "Iter 925, training loss 0.184797, validation loss 24.687977\n",
      "Iter 926, training loss 0.183072, validation loss 24.773808\n",
      "Iter 927, training loss 0.182991, validation loss 24.785040\n",
      "Iter 928, training loss 0.184104, validation loss 24.701946\n",
      "Iter 929, training loss 0.185960, validation loss 24.864923\n",
      "Iter 930, training loss 0.188068, validation loss 24.651979\n",
      "Iter 931, training loss 0.190293, validation loss 24.922808\n",
      "Iter 932, training loss 0.192085, validation loss 24.620377\n",
      "Iter 933, training loss 0.193692, validation loss 24.960684\n",
      "Iter 934, training loss 0.194605, validation loss 24.607450\n",
      "Iter 935, training loss 0.195502, validation loss 24.983913\n",
      "Iter 936, training loss 0.195690, validation loss 24.603916\n",
      "Iter 937, training loss 0.195991, validation loss 24.994131\n",
      "Iter 938, training loss 0.195690, validation loss 24.605118\n",
      "Iter 939, training loss 0.195657, validation loss 24.997946\n",
      "Iter 940, training loss 0.195218, validation loss 24.610189\n",
      "Iter 941, training loss 0.195197, validation loss 25.002003\n",
      "Iter 942, training loss 0.194888, validation loss 24.614645\n",
      "Iter 943, training loss 0.195197, validation loss 25.009066\n",
      "Iter 944, training loss 0.195352, validation loss 24.614540\n",
      "Iter 945, training loss 0.196309, validation loss 25.024199\n",
      "Iter 946, training loss 0.197197, validation loss 24.608418\n",
      "Iter 947, training loss 0.199170, validation loss 25.052240\n",
      "Iter 948, training loss 0.201213, validation loss 24.594885\n",
      "Iter 949, training loss 0.204832, validation loss 25.096640\n",
      "Iter 950, training loss 0.208678, validation loss 24.569773\n",
      "Iter 951, training loss 0.215044, validation loss 25.158562\n",
      "Iter 952, training loss 0.221552, validation loss 24.529209\n",
      "Iter 953, training loss 0.231951, validation loss 25.240450\n",
      "Iter 954, training loss 0.241586, validation loss 24.479031\n",
      "Iter 955, training loss 0.256322, validation loss 25.343231\n",
      "Iter 956, training loss 0.267497, validation loss 24.430532\n",
      "Iter 957, training loss 0.283453, validation loss 25.443546\n",
      "Iter 958, training loss 0.290693, validation loss 24.393255\n",
      "Iter 959, training loss 0.300323, validation loss 25.498653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 960, training loss 0.295515, validation loss 24.382671\n",
      "Iter 961, training loss 0.289222, validation loss 25.459457\n",
      "Iter 962, training loss 0.268674, validation loss 24.421251\n",
      "Iter 963, training loss 0.246947, validation loss 25.306658\n",
      "Iter 964, training loss 0.220714, validation loss 24.527020\n",
      "Iter 965, training loss 0.199234, validation loss 25.081915\n",
      "Iter 966, training loss 0.183942, validation loss 24.696260\n",
      "Iter 967, training loss 0.176988, validation loss 24.865692\n",
      "Iter 968, training loss 0.177457, validation loss 24.888571\n",
      "Iter 969, training loss 0.183315, validation loss 24.705982\n",
      "Iter 970, training loss 0.192316, validation loss 25.055914\n",
      "Iter 971, training loss 0.201915, validation loss 24.605499\n",
      "Iter 972, training loss 0.211604, validation loss 25.177513\n",
      "Iter 973, training loss 0.218599, validation loss 24.554161\n",
      "Iter 974, training loss 0.224898, validation loss 25.250280\n",
      "Iter 975, training loss 0.226738, validation loss 24.537289\n",
      "Iter 976, training loss 0.228254, validation loss 25.272213\n",
      "Iter 977, training loss 0.225142, validation loss 24.542704\n",
      "Iter 978, training loss 0.221986, validation loss 25.246653\n",
      "Iter 979, training loss 0.215374, validation loss 24.568249\n",
      "Iter 980, training loss 0.209222, validation loss 25.186626\n",
      "Iter 981, training loss 0.201420, validation loss 24.616632\n",
      "Iter 982, training loss 0.194606, validation loss 25.109079\n",
      "Iter 983, training loss 0.187963, validation loss 24.685257\n",
      "Iter 984, training loss 0.182689, validation loss 25.030153\n",
      "Iter 985, training loss 0.178528, validation loss 24.761337\n",
      "Iter 986, training loss 0.175699, validation loss 24.957840\n",
      "Iter 987, training loss 0.173905, validation loss 24.829300\n",
      "Iter 988, training loss 0.172952, validation loss 24.901936\n",
      "Iter 989, training loss 0.172599, validation loss 24.888161\n",
      "Iter 990, training loss 0.172677, validation loss 24.863590\n",
      "Iter 991, training loss 0.173095, validation loss 24.940199\n",
      "Iter 992, training loss 0.173884, validation loss 24.829863\n",
      "Iter 993, training loss 0.175118, validation loss 24.991064\n",
      "Iter 994, training loss 0.176939, validation loss 24.790249\n",
      "Iter 995, training loss 0.179786, validation loss 25.054268\n",
      "Iter 996, training loss 0.183826, validation loss 24.738798\n",
      "Iter 997, training loss 0.190224, validation loss 25.144640\n",
      "Iter 998, training loss 0.199183, validation loss 24.668266\n",
      "Iter 999, training loss 0.213797, validation loss 25.281872\n",
      "Size of the model predictions for training is:  (6716, 1)\n",
      "Size of the model predictions for validation is:  (2238, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"MMSE\")\n",
    "mmse_train_pred, mmse_val_pred, tr_losses, te_losses=NN(x_train,y_train_mmse,x_val,y_val_mmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEFCAYAAAABjYvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERFBYSAiIigMREQEhYGIiKAwEBER4P8D0MQaPg+kdc8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x222eb8668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch number is:  135\n",
      "The lowest validation loss is:  4.676572141627841\n",
      "At the same epoch, the training loss is:  1.2266509394743201\n"
     ]
    }
   ],
   "source": [
    "tr_loss, te_loss=plot_NN(epochs, tr_losses, te_losses)\n",
    "print(\"Best epoch number is: \", te_loss.index(min(te_loss)))\n",
    "print(\"The lowest validation loss is: \", min(te_loss))\n",
    "print(\"At the same epoch, the training loss is: \", tr_loss[te_loss.index(min(te_loss))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKWH24CLYFuJ"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zi4wyUA6uzez"
   },
   "outputs": [],
   "source": [
    "x_train_diag=x_train\n",
    "x_val_diag=x_val\n",
    "\n",
    "# Append the predicted numerical variables (ADAS13, Ventricles_Norm, MMSE) as new features for diagnosis predictions\n",
    "x_train_diag['ADAS13']=adas_train_pred\n",
    "x_train_diag['Ventricles_Norm']=ventricles_train_pred\n",
    "x_train_diag['MMSE']=mmse_train_pred\n",
    "\n",
    "y_train_diag = y_train_diag1.values.reshape(-1,1)\n",
    "\n",
    "# Same for validation data\n",
    "x_val_diag['ADAS13']=adas_val_pred\n",
    "x_val_diag['Ventricles_Norm']=ventricles_val_pred\n",
    "x_val_diag['MMSE']=mmse_val_pred\n",
    "\n",
    "y_val_diag = y_val_diag1.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WnyWGWatgBVA"
   },
   "outputs": [],
   "source": [
    "def transform(y_val,predictions):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    ohe = OneHotEncoder()\n",
    "    y_val_cls = ohe.fit_transform(y_val.reshape(-1,1)).toarray()\n",
    "\n",
    "    predictions_cls = ohe.fit_transform(predictions.reshape(-1,1)).toarray()\n",
    "    cn_cls = y_val_cls[:,0]\n",
    "    mci_cls = y_val_cls[:,1]\n",
    "    ad_cls = y_val_cls[:,2]\n",
    "\n",
    "    cn_pred = predictions_cls[:,0]\n",
    "    mci_pred = predictions_cls[:,1]\n",
    "    ad_pred = predictions_cls[:,2]\n",
    "    return cn_cls,mci_cls,ad_cls,cn_pred,mci_pred,ad_pred\n",
    "\n",
    "def metrics(DX,model,X_test,y_test,predictions):\n",
    "    from sklearn.metrics import accuracy_score,precision_score,recall_score,roc_auc_score\n",
    "    pred_prob = model.predict_proba(X_test)[:,1]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision=precision_score(y_test, predictions)\n",
    "    recall=recall_score(y_test, predictions)\n",
    "    roc=roc_auc_score(y_test,pred_prob)\n",
    "    print(\"%s Accuracy: %.2f%% \" % (DX,accuracy *100))\n",
    "    print(\"%s Precision: %.2f%% \" % (DX,precision *100))\n",
    "    print(\"%s Recall: %.2f%% \" % (DX,recall * 100))\n",
    "    print(\"%s AUC: %.2f%% \" % (DX,roc *100))\n",
    "    return  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Zu1C-mCYFuP"
   },
   "source": [
    "* **Diag**\n",
    " * **SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "p1kkR7OrYFuP",
    "outputId": "0f7de62b-2c5c-43a9-c095-ca5bb264a6f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.77 (+/- 0.05)\n",
      "Validation Accuracy: 0.56 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(probability=True)\n",
    "svc.fit(x_train_diag,y_train_diag)\n",
    "scores = cross_val_score(svc, x_train_diag, y_train_diag, cv=5)\n",
    "print('Training Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))\n",
    "y_validation_pred = svc.predict(x_val_diag)\n",
    "# print(x_train_diag.shape,x_val_diag.shape,y_validation_pred.shape,y_val_diag.shape)\n",
    "acc = accuracy_score(y_validation_pred,y_val_diag)\n",
    "print('Validation Accuracy: %0.2f (+/- %0.2f)' % (acc.mean(), acc.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "UdSzXSYqg7VW",
    "outputId": "b1bb4878-68e0-4f19-a55f-262ed7f30410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN_Diag Accuracy: 69.93% \n",
      "CN_Diag Precision: 66.19% \n",
      "CN_Diag Recall: 39.06% \n",
      "CN_Diag AUC: 40.02% \n",
      "******************************\n",
      "MCI_Diag Accuracy: 61.39% \n",
      "MCI_Diag Precision: 53.94% \n",
      "MCI_Diag Recall: 76.77% \n",
      "MCI_Diag AUC: 69.63% \n",
      "******************************\n",
      "AD_Diag Accuracy: 81.19% \n",
      "AD_Diag Precision: 51.66% \n",
      "AD_Diag Recall: 43.19% \n",
      "AD_Diag AUC: 34.03% \n"
     ]
    }
   ],
   "source": [
    "cn_cls,mci_cls,ad_cls,cn_pred,mci_pred,ad_pred = transform(y_val_diag,y_validation_pred)\n",
    "metrics('CN_Diag',svc,x_val_diag,cn_cls,cn_pred)\n",
    "print('*'*30)\n",
    "metrics('MCI_Diag',svc,x_val_diag,mci_cls,mci_pred)\n",
    "print('*'*30)\n",
    "metrics('AD_Diag',svc,x_val_diag,ad_cls,ad_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5RpeSYiYFuj"
   },
   "source": [
    "* **Diag**\n",
    " * **NEURAL NETWORKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kTxWfFkG0ssJ"
   },
   "outputs": [],
   "source": [
    "# features = ['AGE','DX_bl','Month','ADAS13','Ventricles_Norm','MMSE']\n",
    "\n",
    "# X_train = train_diag_2[features].values\n",
    "# y_train = y_train_diag_2[['CN_Diag','MCI_Diag','AD_Diag']].values.reshape(-1,3)\n",
    "\n",
    "# X_val = validation_diag_2[features].values\n",
    "# y_val = y_validation_diag_2[['CN_Diag','MCI_Diag','AD_Diag']].values.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_HbGf1D24WW"
   },
   "outputs": [],
   "source": [
    "x_train = x_train_diag\n",
    "y_train = y_train_diag2.values.reshape(-1,3)\n",
    "\n",
    "x_val = x_val_diag\n",
    "y_val = y_val_diag2.values.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6vy2ixswYFuj"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ohe = OneHotEncoder()\n",
    "# y_train_cls = ohe.fit_transform(y_train_diag1.values.reshape(-1,1)).toarray()\n",
    "\n",
    "# y_validation_cls = ohe.fit_transform(y_validation_diag1.values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "W3276U3TYFuk",
    "outputId": "b1d6b63b-aeef-48ce-8e7b-d1b0914cbf69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, training loss 8.108478, training accuracy 0.467093\n",
      "Iter 0, validation loss 7.110089, validation accuracy 0.440125\n",
      "Iter 1, training loss 15.050492, training accuracy 0.326534\n",
      "Iter 1, validation loss 13.828911, validation accuracy 0.371760\n",
      "Iter 2, training loss 10.025228, training accuracy 0.326534\n",
      "Iter 2, validation loss 9.596361, validation accuracy 0.371760\n",
      "Iter 3, training loss 8.529650, training accuracy 0.469923\n",
      "Iter 3, validation loss 8.974334, validation accuracy 0.436104\n",
      "Iter 4, training loss 8.187607, training accuracy 0.469029\n",
      "Iter 4, validation loss 9.410601, validation accuracy 0.434763\n",
      "Iter 5, training loss 5.239175, training accuracy 0.486599\n",
      "Iter 5, validation loss 7.046713, validation accuracy 0.438785\n",
      "Iter 6, training loss 1.709167, training accuracy 0.595295\n",
      "Iter 6, validation loss 3.330801, validation accuracy 0.467382\n",
      "Iter 7, training loss 1.442896, training accuracy 0.586808\n",
      "Iter 7, validation loss 2.895856, validation accuracy 0.375782\n",
      "Iter 8, training loss 3.458262, training accuracy 0.496873\n",
      "Iter 8, validation loss 5.553473, validation accuracy 0.324397\n",
      "Iter 9, training loss 4.149286, training accuracy 0.480494\n",
      "Iter 9, validation loss 6.703623, validation accuracy 0.301162\n",
      "Iter 10, training loss 3.420575, training accuracy 0.488386\n",
      "Iter 10, validation loss 6.088332, validation accuracy 0.305183\n",
      "Iter 11, training loss 1.860939, training accuracy 0.583532\n",
      "Iter 11, validation loss 4.330034, validation accuracy 0.341823\n",
      "Iter 12, training loss 0.838514, training accuracy 0.733770\n",
      "Iter 12, validation loss 2.875505, validation accuracy 0.449062\n",
      "Iter 13, training loss 0.996993, training accuracy 0.702650\n",
      "Iter 13, validation loss 3.008136, validation accuracy 0.484808\n",
      "Iter 14, training loss 1.307196, training accuracy 0.659321\n",
      "Iter 14, validation loss 3.575550, validation accuracy 0.486148\n",
      "Iter 15, training loss 1.312679, training accuracy 0.673764\n",
      "Iter 15, validation loss 3.715616, validation accuracy 0.493298\n",
      "Iter 16, training loss 1.106889, training accuracy 0.726325\n",
      "Iter 16, validation loss 3.407182, validation accuracy 0.509830\n",
      "Iter 17, training loss 1.054665, training accuracy 0.763103\n",
      "Iter 17, validation loss 3.112994, validation accuracy 0.508043\n",
      "Iter 18, training loss 1.236199, training accuracy 0.683443\n",
      "Iter 18, validation loss 3.145752, validation accuracy 0.470063\n",
      "Iter 19, training loss 1.262485, training accuracy 0.659619\n",
      "Iter 19, validation loss 3.122410, validation accuracy 0.471403\n",
      "Iter 20, training loss 0.976704, training accuracy 0.711882\n",
      "Iter 20, validation loss 2.835414, validation accuracy 0.479446\n",
      "Iter 21, training loss 0.629842, training accuracy 0.809857\n",
      "Iter 21, validation loss 2.525203, validation accuracy 0.508043\n",
      "Iter 22, training loss 0.484642, training accuracy 0.847082\n",
      "Iter 22, validation loss 2.458641, validation accuracy 0.530384\n",
      "Iter 23, training loss 0.507327, training accuracy 0.806283\n",
      "Iter 23, validation loss 2.570149, validation accuracy 0.520107\n",
      "Iter 24, training loss 0.544241, training accuracy 0.791840\n",
      "Iter 24, validation loss 2.651957, validation accuracy 0.491957\n",
      "Iter 25, training loss 0.519062, training accuracy 0.800030\n",
      "Iter 25, validation loss 2.650678, validation accuracy 0.489276\n",
      "Iter 26, training loss 0.460865, training accuracy 0.831447\n",
      "Iter 26, validation loss 2.605546, validation accuracy 0.491063\n",
      "Iter 27, training loss 0.439019, training accuracy 0.862567\n",
      "Iter 27, validation loss 2.577204, validation accuracy 0.484808\n",
      "Iter 28, training loss 0.465768, training accuracy 0.868374\n",
      "Iter 28, validation loss 2.575765, validation accuracy 0.480786\n",
      "Iter 29, training loss 0.494351, training accuracy 0.855122\n",
      "Iter 29, validation loss 2.551908, validation accuracy 0.480786\n",
      "Iter 30, training loss 0.475866, training accuracy 0.852144\n",
      "Iter 30, validation loss 2.460607, validation accuracy 0.475871\n",
      "Iter 31, training loss 0.405016, training accuracy 0.873288\n",
      "Iter 31, validation loss 2.310117, validation accuracy 0.487042\n",
      "Iter 32, training loss 0.320357, training accuracy 0.897111\n",
      "Iter 32, validation loss 2.160027, validation accuracy 0.496872\n",
      "Iter 33, training loss 0.264968, training accuracy 0.913639\n",
      "Iter 33, validation loss 2.082935, validation accuracy 0.513852\n",
      "Iter 34, training loss 0.252163, training accuracy 0.916319\n",
      "Iter 34, validation loss 2.109518, validation accuracy 0.516979\n",
      "Iter 35, training loss 0.262842, training accuracy 0.906343\n",
      "Iter 35, validation loss 2.188036, validation accuracy 0.521448\n",
      "Iter 36, training loss 0.266102, training accuracy 0.903365\n",
      "Iter 36, validation loss 2.231569, validation accuracy 0.526363\n",
      "Iter 37, training loss 0.252548, training accuracy 0.909321\n",
      "Iter 37, validation loss 2.205652, validation accuracy 0.526363\n",
      "Iter 38, training loss 0.234078, training accuracy 0.920339\n",
      "Iter 38, validation loss 2.132562, validation accuracy 0.533065\n",
      "Iter 39, training loss 0.222811, training accuracy 0.926593\n",
      "Iter 39, validation loss 2.053147, validation accuracy 0.537980\n",
      "Iter 40, training loss 0.221932, training accuracy 0.924658\n",
      "Iter 40, validation loss 1.991395, validation accuracy 0.534406\n",
      "Iter 41, training loss 0.223304, training accuracy 0.920637\n",
      "Iter 41, validation loss 1.946238, validation accuracy 0.538874\n",
      "Iter 42, training loss 0.214499, training accuracy 0.922275\n",
      "Iter 42, validation loss 1.908541, validation accuracy 0.536640\n",
      "Iter 43, training loss 0.193916, training accuracy 0.932847\n",
      "Iter 43, validation loss 1.879072, validation accuracy 0.538427\n",
      "Iter 44, training loss 0.172989, training accuracy 0.939547\n",
      "Iter 44, validation loss 1.868087, validation accuracy 0.542449\n",
      "Iter 45, training loss 0.161624, training accuracy 0.942525\n",
      "Iter 45, validation loss 1.881665, validation accuracy 0.545130\n",
      "Iter 46, training loss 0.160326, training accuracy 0.941483\n",
      "Iter 46, validation loss 1.909741, validation accuracy 0.531725\n",
      "Iter 47, training loss 0.161228, training accuracy 0.940292\n",
      "Iter 47, validation loss 1.927988, validation accuracy 0.524576\n",
      "Iter 48, training loss 0.156405, training accuracy 0.940590\n",
      "Iter 48, validation loss 1.917394, validation accuracy 0.521448\n",
      "Iter 49, training loss 0.146094, training accuracy 0.947141\n",
      "Iter 49, validation loss 1.879578, validation accuracy 0.524129\n",
      "Iter 50, training loss 0.135509, training accuracy 0.951161\n",
      "Iter 50, validation loss 1.830809, validation accuracy 0.533512\n",
      "Iter 51, training loss 0.128041, training accuracy 0.954288\n",
      "Iter 51, validation loss 1.787511, validation accuracy 0.539321\n",
      "Iter 52, training loss 0.122663, training accuracy 0.955182\n",
      "Iter 52, validation loss 1.759066, validation accuracy 0.546470\n",
      "Iter 53, training loss 0.117109, training accuracy 0.956968\n",
      "Iter 53, validation loss 1.746904, validation accuracy 0.542895\n",
      "Iter 54, training loss 0.111917, training accuracy 0.959202\n",
      "Iter 54, validation loss 1.750992, validation accuracy 0.547364\n",
      "Iter 55, training loss 0.108893, training accuracy 0.959649\n",
      "Iter 55, validation loss 1.770053, validation accuracy 0.551385\n",
      "Iter 56, training loss 0.107641, training accuracy 0.960542\n",
      "Iter 56, validation loss 1.789660, validation accuracy 0.554066\n",
      "Iter 57, training loss 0.104903, training accuracy 0.960989\n",
      "Iter 57, validation loss 1.787899, validation accuracy 0.554513\n",
      "Iter 58, training loss 0.099437, training accuracy 0.962329\n",
      "Iter 58, validation loss 1.759238, validation accuracy 0.550938\n",
      "Iter 59, training loss 0.093838, training accuracy 0.964413\n",
      "Iter 59, validation loss 1.721259, validation accuracy 0.547364\n",
      "Iter 60, training loss 0.090559, training accuracy 0.965456\n",
      "Iter 60, validation loss 1.694307, validation accuracy 0.548704\n",
      "Iter 61, training loss 0.088911, training accuracy 0.965753\n",
      "Iter 61, validation loss 1.688608, validation accuracy 0.541555\n",
      "Iter 62, training loss 0.086822, training accuracy 0.965902\n",
      "Iter 62, validation loss 1.702712, validation accuracy 0.538874\n",
      "Iter 63, training loss 0.084757, training accuracy 0.966051\n",
      "Iter 63, validation loss 1.730152, validation accuracy 0.534406\n",
      "Iter 64, training loss 0.082303, training accuracy 0.967391\n",
      "Iter 64, validation loss 1.746497, validation accuracy 0.536640\n",
      "Iter 65, training loss 0.078308, training accuracy 0.968434\n",
      "Iter 65, validation loss 1.733220, validation accuracy 0.537980\n",
      "Iter 66, training loss 0.075669, training accuracy 0.969029\n",
      "Iter 66, validation loss 1.706599, validation accuracy 0.541555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 67, training loss 0.075471, training accuracy 0.970071\n",
      "Iter 67, validation loss 1.690875, validation accuracy 0.542895\n",
      "Iter 68, training loss 0.073677, training accuracy 0.970220\n",
      "Iter 68, validation loss 1.692679, validation accuracy 0.544236\n",
      "Iter 69, training loss 0.070221, training accuracy 0.969923\n",
      "Iter 69, validation loss 1.708448, validation accuracy 0.541108\n",
      "Iter 70, training loss 0.068238, training accuracy 0.970220\n",
      "Iter 70, validation loss 1.729617, validation accuracy 0.541108\n",
      "Iter 71, training loss 0.067199, training accuracy 0.970518\n",
      "Iter 71, validation loss 1.738574, validation accuracy 0.539768\n",
      "Iter 72, training loss 0.066098, training accuracy 0.971560\n",
      "Iter 72, validation loss 1.734138, validation accuracy 0.542895\n",
      "Iter 73, training loss 0.064022, training accuracy 0.972603\n",
      "Iter 73, validation loss 1.729907, validation accuracy 0.542449\n",
      "Iter 74, training loss 0.061581, training accuracy 0.972752\n",
      "Iter 74, validation loss 1.737788, validation accuracy 0.547811\n",
      "Iter 75, training loss 0.060931, training accuracy 0.972454\n",
      "Iter 75, validation loss 1.753235, validation accuracy 0.550492\n",
      "Iter 76, training loss 0.059645, training accuracy 0.972603\n",
      "Iter 76, validation loss 1.750801, validation accuracy 0.551385\n",
      "Iter 77, training loss 0.058018, training accuracy 0.974538\n",
      "Iter 77, validation loss 1.741992, validation accuracy 0.545576\n",
      "Iter 78, training loss 0.056758, training accuracy 0.974241\n",
      "Iter 78, validation loss 1.749859, validation accuracy 0.547811\n",
      "Iter 79, training loss 0.056006, training accuracy 0.973645\n",
      "Iter 79, validation loss 1.762718, validation accuracy 0.549151\n",
      "Iter 80, training loss 0.053862, training accuracy 0.974687\n",
      "Iter 80, validation loss 1.746261, validation accuracy 0.553172\n",
      "Iter 81, training loss 0.053380, training accuracy 0.974687\n",
      "Iter 81, validation loss 1.743943, validation accuracy 0.553619\n",
      "Iter 82, training loss 0.052307, training accuracy 0.974241\n",
      "Iter 82, validation loss 1.767027, validation accuracy 0.562109\n",
      "Iter 83, training loss 0.050338, training accuracy 0.975878\n",
      "Iter 83, validation loss 1.766560, validation accuracy 0.560322\n",
      "Iter 84, training loss 0.050085, training accuracy 0.975134\n",
      "Iter 84, validation loss 1.779642, validation accuracy 0.563003\n",
      "Iter 85, training loss 0.048589, training accuracy 0.975581\n",
      "Iter 85, validation loss 1.796353, validation accuracy 0.563896\n",
      "Iter 86, training loss 0.050445, training accuracy 0.975730\n",
      "Iter 86, validation loss 1.767640, validation accuracy 0.571492\n",
      "Iter 87, training loss 0.050143, training accuracy 0.972752\n",
      "Iter 87, validation loss 1.825781, validation accuracy 0.575067\n",
      "Iter 88, training loss 0.058981, training accuracy 0.971709\n",
      "Iter 88, validation loss 1.795128, validation accuracy 0.552279\n",
      "Iter 89, training loss 0.073453, training accuracy 0.958904\n",
      "Iter 89, validation loss 1.993202, validation accuracy 0.575067\n",
      "Iter 90, training loss 0.091302, training accuracy 0.960542\n",
      "Iter 90, validation loss 1.863998, validation accuracy 0.555407\n",
      "Iter 91, training loss 0.059162, training accuracy 0.964860\n",
      "Iter 91, validation loss 1.941148, validation accuracy 0.579088\n",
      "Iter 92, training loss 0.048367, training accuracy 0.972752\n",
      "Iter 92, validation loss 1.906036, validation accuracy 0.572386\n",
      "Iter 93, training loss 0.077001, training accuracy 0.965605\n",
      "Iter 93, validation loss 1.902386, validation accuracy 0.542449\n",
      "Iter 94, training loss 0.064306, training accuracy 0.963818\n",
      "Iter 94, validation loss 2.034465, validation accuracy 0.586238\n",
      "Iter 95, training loss 0.046817, training accuracy 0.974687\n",
      "Iter 95, validation loss 1.891318, validation accuracy 0.582663\n",
      "Iter 96, training loss 0.063470, training accuracy 0.970071\n",
      "Iter 96, validation loss 1.923399, validation accuracy 0.551385\n",
      "Iter 97, training loss 0.087495, training accuracy 0.954586\n",
      "Iter 97, validation loss 2.202512, validation accuracy 0.565684\n",
      "Iter 98, training loss 0.161326, training accuracy 0.934783\n",
      "Iter 98, validation loss 2.183211, validation accuracy 0.530384\n",
      "Iter 99, training loss 0.330337, training accuracy 0.857058\n",
      "Iter 99, validation loss 3.024381, validation accuracy 0.516533\n",
      "Iter 100, training loss 0.597659, training accuracy 0.797647\n",
      "Iter 100, validation loss 3.112160, validation accuracy 0.446828\n",
      "Iter 101, training loss 0.194518, training accuracy 0.919893\n",
      "Iter 101, validation loss 2.810322, validation accuracy 0.542449\n",
      "Iter 102, training loss 0.336071, training accuracy 0.872543\n",
      "Iter 102, validation loss 3.392702, validation accuracy 0.513852\n",
      "Iter 103, training loss 0.351325, training accuracy 0.889220\n",
      "Iter 103, validation loss 2.941409, validation accuracy 0.482127\n",
      "Iter 104, training loss 0.200715, training accuracy 0.935229\n",
      "Iter 104, validation loss 2.689356, validation accuracy 0.504915\n",
      "Iter 105, training loss 0.174125, training accuracy 0.934336\n",
      "Iter 105, validation loss 3.108854, validation accuracy 0.538427\n",
      "Iter 106, training loss 0.271745, training accuracy 0.910810\n",
      "Iter 106, validation loss 3.578574, validation accuracy 0.527256\n",
      "Iter 107, training loss 0.160176, training accuracy 0.955777\n",
      "Iter 107, validation loss 2.775681, validation accuracy 0.542002\n",
      "Iter 108, training loss 0.371529, training accuracy 0.904258\n",
      "Iter 108, validation loss 3.454805, validation accuracy 0.492404\n",
      "Iter 109, training loss 0.172012, training accuracy 0.939994\n",
      "Iter 109, validation loss 3.417428, validation accuracy 0.534406\n",
      "Iter 110, training loss 0.412454, training accuracy 0.885944\n",
      "Iter 110, validation loss 4.486697, validation accuracy 0.509830\n",
      "Iter 111, training loss 0.541336, training accuracy 0.869416\n",
      "Iter 111, validation loss 4.014193, validation accuracy 0.497766\n",
      "Iter 112, training loss 0.223485, training accuracy 0.938356\n",
      "Iter 112, validation loss 3.667427, validation accuracy 0.498213\n",
      "Iter 113, training loss 1.143079, training accuracy 0.757147\n",
      "Iter 113, validation loss 6.327199, validation accuracy 0.480786\n",
      "Iter 114, training loss 2.571212, training accuracy 0.602293\n",
      "Iter 114, validation loss 7.100628, validation accuracy 0.446381\n",
      "Iter 115, training loss 0.605378, training accuracy 0.859142\n",
      "Iter 115, validation loss 5.677504, validation accuracy 0.484808\n",
      "Iter 116, training loss 3.028214, training accuracy 0.689845\n",
      "Iter 116, validation loss 9.741286, validation accuracy 0.459786\n",
      "Iter 117, training loss 0.662056, training accuracy 0.873734\n",
      "Iter 117, validation loss 5.741352, validation accuracy 0.449955\n",
      "Iter 118, training loss 2.589753, training accuracy 0.663788\n",
      "Iter 118, validation loss 8.412580, validation accuracy 0.485702\n",
      "Iter 119, training loss 2.182404, training accuracy 0.801668\n",
      "Iter 119, validation loss 8.272065, validation accuracy 0.521001\n",
      "Iter 120, training loss 0.827848, training accuracy 0.876266\n",
      "Iter 120, validation loss 8.156199, validation accuracy 0.473190\n",
      "Iter 121, training loss 4.827864, training accuracy 0.525015\n",
      "Iter 121, validation loss 16.325399, validation accuracy 0.290885\n",
      "Iter 122, training loss 1.037463, training accuracy 0.880286\n",
      "Iter 122, validation loss 8.929987, validation accuracy 0.511618\n",
      "Iter 123, training loss 3.887956, training accuracy 0.792734\n",
      "Iter 123, validation loss 11.274015, validation accuracy 0.546023\n",
      "Iter 124, training loss 5.454983, training accuracy 0.596486\n",
      "Iter 124, validation loss 15.215361, validation accuracy 0.468722\n",
      "Iter 125, training loss 0.397910, training accuracy 0.946397\n",
      "Iter 125, validation loss 7.060115, validation accuracy 0.558088\n",
      "Iter 126, training loss 1.959202, training accuracy 0.821024\n",
      "Iter 126, validation loss 13.526649, validation accuracy 0.459339\n",
      "Iter 127, training loss 4.278793, training accuracy 0.709202\n",
      "Iter 127, validation loss 19.277140, validation accuracy 0.410634\n",
      "Iter 128, training loss 2.070956, training accuracy 0.848868\n",
      "Iter 128, validation loss 14.991584, validation accuracy 0.439678\n",
      "Iter 129, training loss 0.506084, training accuracy 0.948630\n",
      "Iter 129, validation loss 8.725442, validation accuracy 0.537087\n",
      "Iter 130, training loss 1.097169, training accuracy 0.920339\n",
      "Iter 130, validation loss 10.275653, validation accuracy 0.582216\n",
      "Iter 131, training loss 2.723126, training accuracy 0.851251\n",
      "Iter 131, validation loss 14.579662, validation accuracy 0.538427\n",
      "Iter 132, training loss 2.091115, training accuracy 0.881477\n",
      "Iter 132, validation loss 13.470716, validation accuracy 0.558088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 133, training loss 0.698323, training accuracy 0.940887\n",
      "Iter 133, validation loss 9.942912, validation accuracy 0.581769\n",
      "Iter 134, training loss 0.683826, training accuracy 0.943270\n",
      "Iter 134, validation loss 10.473213, validation accuracy 0.528597\n",
      "Iter 135, training loss 1.407145, training accuracy 0.901281\n",
      "Iter 135, validation loss 14.820379, validation accuracy 0.479446\n",
      "Iter 136, training loss 1.860698, training accuracy 0.870161\n",
      "Iter 136, validation loss 16.970526, validation accuracy 0.463360\n",
      "Iter 137, training loss 0.858701, training accuracy 0.932400\n",
      "Iter 137, validation loss 13.321865, validation accuracy 0.495979\n",
      "Iter 138, training loss 0.494684, training accuracy 0.956224\n",
      "Iter 138, validation loss 10.016380, validation accuracy 0.558534\n",
      "Iter 139, training loss 1.109766, training accuracy 0.917361\n",
      "Iter 139, validation loss 11.751773, validation accuracy 0.553619\n",
      "Iter 140, training loss 1.481112, training accuracy 0.902918\n",
      "Iter 140, validation loss 12.760850, validation accuracy 0.554066\n",
      "Iter 141, training loss 0.644921, training accuracy 0.947886\n",
      "Iter 141, validation loss 10.579826, validation accuracy 0.577301\n",
      "Iter 142, training loss 0.647488, training accuracy 0.946843\n",
      "Iter 142, validation loss 11.590642, validation accuracy 0.530831\n",
      "Iter 143, training loss 1.062998, training accuracy 0.916170\n",
      "Iter 143, validation loss 15.359480, validation accuracy 0.486595\n",
      "Iter 144, training loss 0.736076, training accuracy 0.934932\n",
      "Iter 144, validation loss 13.774235, validation accuracy 0.503575\n",
      "Iter 145, training loss 0.529201, training accuracy 0.955331\n",
      "Iter 145, validation loss 10.421305, validation accuracy 0.543342\n",
      "Iter 146, training loss 1.309927, training accuracy 0.906641\n",
      "Iter 146, validation loss 12.556831, validation accuracy 0.553172\n",
      "Iter 147, training loss 0.625824, training accuracy 0.945652\n",
      "Iter 147, validation loss 10.734957, validation accuracy 0.574620\n",
      "Iter 148, training loss 2.445821, training accuracy 0.864354\n",
      "Iter 148, validation loss 16.844614, validation accuracy 0.474084\n",
      "Iter 149, training loss 18.313936, training accuracy 0.541394\n",
      "Iter 149, validation loss 41.188316, validation accuracy 0.445487\n",
      "Iter 150, training loss 38.653706, training accuracy 0.516230\n",
      "Iter 150, validation loss 50.641457, validation accuracy 0.414656\n",
      "Iter 151, training loss 29.025330, training accuracy 0.446546\n",
      "Iter 151, validation loss 55.670532, validation accuracy 0.268097\n",
      "Iter 152, training loss 33.241089, training accuracy 0.493448\n",
      "Iter 152, validation loss 59.241798, validation accuracy 0.440125\n",
      "Iter 153, training loss 20.974859, training accuracy 0.724985\n",
      "Iter 153, validation loss 40.500824, validation accuracy 0.504021\n",
      "Iter 154, training loss 38.573105, training accuracy 0.393240\n",
      "Iter 154, validation loss 60.340279, validation accuracy 0.374441\n",
      "Iter 155, training loss 6.038431, training accuracy 0.803901\n",
      "Iter 155, validation loss 28.723257, validation accuracy 0.424933\n",
      "Iter 156, training loss 23.520704, training accuracy 0.635944\n",
      "Iter 156, validation loss 56.033817, validation accuracy 0.404379\n",
      "Iter 157, training loss 30.016895, training accuracy 0.666170\n",
      "Iter 157, validation loss 61.693958, validation accuracy 0.452636\n",
      "Iter 158, training loss 6.297005, training accuracy 0.791245\n",
      "Iter 158, validation loss 35.588535, validation accuracy 0.457551\n",
      "Iter 159, training loss 6.660230, training accuracy 0.765783\n",
      "Iter 159, validation loss 28.635773, validation accuracy 0.498213\n",
      "Iter 160, training loss 19.474100, training accuracy 0.564920\n",
      "Iter 160, validation loss 47.641247, validation accuracy 0.449955\n",
      "Iter 161, training loss 11.504905, training accuracy 0.808815\n",
      "Iter 161, validation loss 40.947163, validation accuracy 0.504915\n",
      "Iter 162, training loss 17.257671, training accuracy 0.681805\n",
      "Iter 162, validation loss 57.757133, validation accuracy 0.463807\n",
      "Iter 163, training loss 6.981402, training accuracy 0.814175\n",
      "Iter 163, validation loss 44.241817, validation accuracy 0.437891\n",
      "Iter 164, training loss 18.095583, training accuracy 0.667361\n",
      "Iter 164, validation loss 62.422852, validation accuracy 0.346291\n",
      "Iter 165, training loss 1.660980, training accuracy 0.941632\n",
      "Iter 165, validation loss 23.269257, validation accuracy 0.540214\n",
      "Iter 166, training loss 9.912342, training accuracy 0.848273\n",
      "Iter 166, validation loss 40.023800, validation accuracy 0.537534\n",
      "Iter 167, training loss 15.162692, training accuracy 0.789309\n",
      "Iter 167, validation loss 49.710846, validation accuracy 0.526363\n",
      "Iter 168, training loss 8.698903, training accuracy 0.819535\n",
      "Iter 168, validation loss 42.497570, validation accuracy 0.527703\n",
      "Iter 169, training loss 2.586075, training accuracy 0.928231\n",
      "Iter 169, validation loss 27.242218, validation accuracy 0.529491\n",
      "Iter 170, training loss 5.347056, training accuracy 0.877308\n",
      "Iter 170, validation loss 42.131046, validation accuracy 0.463360\n",
      "Iter 171, training loss 8.681327, training accuracy 0.819833\n",
      "Iter 171, validation loss 53.020485, validation accuracy 0.477212\n",
      "Iter 172, training loss 7.670582, training accuracy 0.837403\n",
      "Iter 172, validation loss 49.066498, validation accuracy 0.503575\n",
      "Iter 173, training loss 1.997757, training accuracy 0.938505\n",
      "Iter 173, validation loss 29.685385, validation accuracy 0.533959\n",
      "Iter 174, training loss 4.954035, training accuracy 0.880286\n",
      "Iter 174, validation loss 36.265217, validation accuracy 0.538427\n",
      "Iter 175, training loss 6.729266, training accuracy 0.852889\n",
      "Iter 175, validation loss 41.957237, validation accuracy 0.538874\n",
      "Iter 176, training loss 5.576893, training accuracy 0.910363\n",
      "Iter 176, validation loss 37.542629, validation accuracy 0.555853\n",
      "Iter 177, training loss 4.039310, training accuracy 0.914384\n",
      "Iter 177, validation loss 38.003262, validation accuracy 0.518767\n",
      "Iter 178, training loss 5.335822, training accuracy 0.884902\n",
      "Iter 178, validation loss 46.486240, validation accuracy 0.469169\n",
      "Iter 179, training loss 5.794171, training accuracy 0.876266\n",
      "Iter 179, validation loss 48.432709, validation accuracy 0.461573\n",
      "Iter 180, training loss 3.913489, training accuracy 0.912597\n",
      "Iter 180, validation loss 39.625061, validation accuracy 0.518320\n",
      "Iter 181, training loss 3.905043, training accuracy 0.924062\n",
      "Iter 181, validation loss 35.197571, validation accuracy 0.557194\n",
      "Iter 182, training loss 6.330129, training accuracy 0.854527\n",
      "Iter 182, validation loss 42.502056, validation accuracy 0.526810\n",
      "Iter 183, training loss 1.848073, training accuracy 0.948332\n",
      "Iter 183, validation loss 30.704433, validation accuracy 0.558534\n",
      "Iter 184, training loss 4.561266, training accuracy 0.887880\n",
      "Iter 184, validation loss 45.290165, validation accuracy 0.521001\n",
      "Iter 185, training loss 4.683303, training accuracy 0.896367\n",
      "Iter 185, validation loss 45.051609, validation accuracy 0.472297\n",
      "Iter 186, training loss 1.764872, training accuracy 0.949970\n",
      "Iter 186, validation loss 32.055672, validation accuracy 0.541108\n",
      "Iter 187, training loss 4.896839, training accuracy 0.917659\n",
      "Iter 187, validation loss 39.196331, validation accuracy 0.556747\n",
      "Iter 188, training loss 4.002925, training accuracy 0.908725\n",
      "Iter 188, validation loss 37.902256, validation accuracy 0.529491\n",
      "Iter 189, training loss 1.732631, training accuracy 0.952353\n",
      "Iter 189, validation loss 31.882803, validation accuracy 0.545130\n",
      "Iter 190, training loss 5.674114, training accuracy 0.866438\n",
      "Iter 190, validation loss 49.373981, validation accuracy 0.519214\n",
      "Iter 191, training loss 13.287141, training accuracy 0.790351\n",
      "Iter 191, validation loss 55.304371, validation accuracy 0.442359\n",
      "Iter 192, training loss 35.422157, training accuracy 0.759678\n",
      "Iter 192, validation loss 83.876068, validation accuracy 0.506256\n",
      "Iter 193, training loss 25.765633, training accuracy 0.627606\n",
      "Iter 193, validation loss 69.059555, validation accuracy 0.477212\n",
      "Iter 194, training loss 84.526337, training accuracy 0.660661\n",
      "Iter 194, validation loss 150.590652, validation accuracy 0.438338\n",
      "Iter 195, training loss 105.420578, training accuracy 0.468880\n",
      "Iter 195, validation loss 194.946152, validation accuracy 0.306971\n",
      "Iter 196, training loss 102.461853, training accuracy 0.533800\n",
      "Iter 196, validation loss 165.197235, validation accuracy 0.458445\n",
      "Iter 197, training loss 387.576202, training accuracy 0.326534\n",
      "Iter 197, validation loss 417.418427, validation accuracy 0.371760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 198, training loss 132.419998, training accuracy 0.353782\n",
      "Iter 198, validation loss 191.686478, validation accuracy 0.376676\n",
      "Iter 199, training loss 158.132294, training accuracy 0.617778\n",
      "Iter 199, validation loss 247.115555, validation accuracy 0.446381\n",
      "Iter 200, training loss 375.931946, training accuracy 0.318344\n",
      "Iter 200, validation loss 517.301208, validation accuracy 0.226988\n",
      "Iter 201, training loss 436.479309, training accuracy 0.530226\n",
      "Iter 201, validation loss 574.635376, validation accuracy 0.436997\n",
      "Iter 202, training loss 395.582794, training accuracy 0.561644\n",
      "Iter 202, validation loss 531.741455, validation accuracy 0.437444\n",
      "Iter 203, training loss 275.487030, training accuracy 0.405896\n",
      "Iter 203, validation loss 417.119751, validation accuracy 0.269437\n",
      "Iter 204, training loss 66.898849, training accuracy 0.678827\n",
      "Iter 204, validation loss 159.955566, validation accuracy 0.438785\n",
      "Iter 205, training loss 81.848465, training accuracy 0.480048\n",
      "Iter 205, validation loss 160.738647, validation accuracy 0.415550\n",
      "Iter 206, training loss 160.379089, training accuracy 0.371799\n",
      "Iter 206, validation loss 240.974472, validation accuracy 0.390974\n",
      "Iter 207, training loss 117.861229, training accuracy 0.668404\n",
      "Iter 207, validation loss 200.625809, validation accuracy 0.493744\n",
      "Iter 208, training loss 110.732063, training accuracy 0.549434\n",
      "Iter 208, validation loss 227.063400, validation accuracy 0.453083\n",
      "Iter 209, training loss 48.670685, training accuracy 0.635795\n",
      "Iter 209, validation loss 172.261612, validation accuracy 0.356568\n",
      "Iter 210, training loss 100.469131, training accuracy 0.490917\n",
      "Iter 210, validation loss 236.659607, validation accuracy 0.285523\n",
      "Iter 211, training loss 4.789889, training accuracy 0.934187\n",
      "Iter 211, validation loss 62.538288, validation accuracy 0.533512\n",
      "Iter 212, training loss 49.814392, training accuracy 0.788118\n",
      "Iter 212, validation loss 131.312256, validation accuracy 0.524576\n",
      "Iter 213, training loss 70.564957, training accuracy 0.725730\n",
      "Iter 213, validation loss 163.390625, validation accuracy 0.494638\n",
      "Iter 214, training loss 56.525616, training accuracy 0.653514\n",
      "Iter 214, validation loss 163.062408, validation accuracy 0.484808\n",
      "Iter 215, training loss 8.117418, training accuracy 0.916617\n",
      "Iter 215, validation loss 74.001259, validation accuracy 0.538874\n",
      "Iter 216, training loss 18.299620, training accuracy 0.846188\n",
      "Iter 216, validation loss 119.887764, validation accuracy 0.455764\n",
      "Iter 217, training loss 48.235325, training accuracy 0.701012\n",
      "Iter 217, validation loss 189.850220, validation accuracy 0.421358\n",
      "Iter 218, training loss 46.190975, training accuracy 0.751936\n",
      "Iter 218, validation loss 169.890732, validation accuracy 0.469616\n",
      "Iter 219, training loss 18.396763, training accuracy 0.856760\n",
      "Iter 219, validation loss 122.270638, validation accuracy 0.503128\n",
      "Iter 220, training loss 7.778306, training accuracy 0.923615\n",
      "Iter 220, validation loss 78.275810, validation accuracy 0.526810\n",
      "Iter 221, training loss 46.923271, training accuracy 0.692228\n",
      "Iter 221, validation loss 149.547897, validation accuracy 0.491063\n",
      "Iter 222, training loss 16.213182, training accuracy 0.886837\n",
      "Iter 222, validation loss 103.032097, validation accuracy 0.521448\n",
      "Iter 223, training loss 18.879908, training accuracy 0.892942\n",
      "Iter 223, validation loss 113.840508, validation accuracy 0.522788\n",
      "Iter 224, training loss 13.891269, training accuracy 0.891602\n",
      "Iter 224, validation loss 114.110390, validation accuracy 0.512511\n",
      "Iter 225, training loss 17.654449, training accuracy 0.870161\n",
      "Iter 225, validation loss 128.184708, validation accuracy 0.469616\n",
      "Iter 226, training loss 26.474243, training accuracy 0.821769\n",
      "Iter 226, validation loss 148.496567, validation accuracy 0.445934\n",
      "Iter 227, training loss 9.316234, training accuracy 0.917064\n",
      "Iter 227, validation loss 105.933067, validation accuracy 0.516086\n",
      "Iter 228, training loss 17.916044, training accuracy 0.897558\n",
      "Iter 228, validation loss 120.226791, validation accuracy 0.520107\n",
      "Iter 229, training loss 13.878389, training accuracy 0.907534\n",
      "Iter 229, validation loss 102.675781, validation accuracy 0.532172\n",
      "Iter 230, training loss 23.528500, training accuracy 0.825640\n",
      "Iter 230, validation loss 124.273735, validation accuracy 0.506256\n",
      "Iter 231, training loss 10.515466, training accuracy 0.915277\n",
      "Iter 231, validation loss 92.970604, validation accuracy 0.517426\n",
      "Iter 232, training loss 9.482777, training accuracy 0.920637\n",
      "Iter 232, validation loss 105.643906, validation accuracy 0.513405\n",
      "Iter 233, training loss 13.599692, training accuracy 0.891007\n",
      "Iter 233, validation loss 126.134247, validation accuracy 0.516086\n",
      "Iter 234, training loss 14.480448, training accuracy 0.891602\n",
      "Iter 234, validation loss 124.128738, validation accuracy 0.477659\n",
      "Iter 235, training loss 10.283647, training accuracy 0.917361\n",
      "Iter 235, validation loss 103.197853, validation accuracy 0.507596\n",
      "Iter 236, training loss 9.506145, training accuracy 0.921977\n",
      "Iter 236, validation loss 101.006500, validation accuracy 0.531278\n",
      "Iter 237, training loss 10.617499, training accuracy 0.919297\n",
      "Iter 237, validation loss 99.869957, validation accuracy 0.534853\n",
      "Iter 238, training loss 14.980632, training accuracy 0.888326\n",
      "Iter 238, validation loss 107.824089, validation accuracy 0.504021\n",
      "Iter 239, training loss 7.075681, training accuracy 0.936867\n",
      "Iter 239, validation loss 89.705666, validation accuracy 0.531725\n",
      "Iter 240, training loss 12.862493, training accuracy 0.900983\n",
      "Iter 240, validation loss 120.709030, validation accuracy 0.505809\n",
      "Iter 241, training loss 8.643609, training accuracy 0.926295\n",
      "Iter 241, validation loss 103.882599, validation accuracy 0.511171\n",
      "Iter 242, training loss 10.034789, training accuracy 0.918702\n",
      "Iter 242, validation loss 104.544617, validation accuracy 0.504021\n",
      "Iter 243, training loss 10.150739, training accuracy 0.916319\n",
      "Iter 243, validation loss 111.120308, validation accuracy 0.514298\n",
      "Iter 244, training loss 6.717360, training accuracy 0.939845\n",
      "Iter 244, validation loss 89.817772, validation accuracy 0.530384\n",
      "Iter 245, training loss 8.647441, training accuracy 0.929422\n",
      "Iter 245, validation loss 93.914764, validation accuracy 0.519214\n",
      "Iter 246, training loss 9.942533, training accuracy 0.921680\n",
      "Iter 246, validation loss 103.171692, validation accuracy 0.541555\n",
      "Iter 247, training loss 7.963847, training accuracy 0.933294\n",
      "Iter 247, validation loss 94.540375, validation accuracy 0.511171\n",
      "Iter 248, training loss 6.588549, training accuracy 0.932996\n",
      "Iter 248, validation loss 103.758514, validation accuracy 0.505809\n",
      "Iter 249, training loss 8.124129, training accuracy 0.920935\n",
      "Iter 249, validation loss 109.199524, validation accuracy 0.509830\n",
      "Iter 250, training loss 24.238758, training accuracy 0.830405\n",
      "Iter 250, validation loss 124.815681, validation accuracy 0.473637\n",
      "Iter 251, training loss 62.232372, training accuracy 0.749107\n",
      "Iter 251, validation loss 209.440765, validation accuracy 0.501341\n",
      "Iter 252, training loss 148.315598, training accuracy 0.557921\n",
      "Iter 252, validation loss 262.008240, validation accuracy 0.406613\n",
      "Iter 253, training loss 166.855042, training accuracy 0.583830\n",
      "Iter 253, validation loss 354.119232, validation accuracy 0.445040\n",
      "Iter 254, training loss 88.054108, training accuracy 0.643538\n",
      "Iter 254, validation loss 249.153870, validation accuracy 0.344057\n",
      "Iter 255, training loss 152.208817, training accuracy 0.745384\n",
      "Iter 255, validation loss 270.204895, validation accuracy 0.501787\n",
      "Iter 256, training loss 151.694107, training accuracy 0.636242\n",
      "Iter 256, validation loss 295.789764, validation accuracy 0.476765\n",
      "Iter 257, training loss 166.112076, training accuracy 0.674062\n",
      "Iter 257, validation loss 336.705139, validation accuracy 0.440572\n",
      "Iter 258, training loss 738.520386, training accuracy 0.230941\n",
      "Iter 258, validation loss 998.554504, validation accuracy 0.204200\n",
      "Iter 259, training loss 119.280937, training accuracy 0.756998\n",
      "Iter 259, validation loss 260.661316, validation accuracy 0.500000\n",
      "Iter 260, training loss 712.864502, training accuracy 0.334276\n",
      "Iter 260, validation loss 816.033752, validation accuracy 0.374441\n",
      "Iter 261, training loss 735.512146, training accuracy 0.479005\n",
      "Iter 261, validation loss 899.141174, validation accuracy 0.437444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 262, training loss 323.401489, training accuracy 0.516230\n",
      "Iter 262, validation loss 550.935852, validation accuracy 0.439231\n",
      "Iter 263, training loss 1058.408691, training accuracy 0.246575\n",
      "Iter 263, validation loss 1311.784302, validation accuracy 0.218945\n",
      "Iter 264, training loss 942.309998, training accuracy 0.512805\n",
      "Iter 264, validation loss 1046.109985, validation accuracy 0.422252\n",
      "Iter 265, training loss 560.074646, training accuracy 0.460989\n",
      "Iter 265, validation loss 706.951599, validation accuracy 0.399911\n",
      "Iter 266, training loss 458.293335, training accuracy 0.497618\n",
      "Iter 266, validation loss 716.985352, validation accuracy 0.438338\n",
      "Iter 267, training loss 679.406799, training accuracy 0.514443\n",
      "Iter 267, validation loss 982.291321, validation accuracy 0.438338\n",
      "Iter 268, training loss 723.154846, training accuracy 0.278886\n",
      "Iter 268, validation loss 1079.909668, validation accuracy 0.212690\n",
      "Iter 269, training loss 122.736412, training accuracy 0.697588\n",
      "Iter 269, validation loss 333.603119, validation accuracy 0.451296\n",
      "Iter 270, training loss 317.341827, training accuracy 0.420935\n",
      "Iter 270, validation loss 523.757568, validation accuracy 0.403932\n",
      "Iter 271, training loss 208.385269, training accuracy 0.761465\n",
      "Iter 271, validation loss 374.147003, validation accuracy 0.524576\n",
      "Iter 272, training loss 103.800514, training accuracy 0.753127\n",
      "Iter 272, validation loss 318.271515, validation accuracy 0.499106\n",
      "Iter 273, training loss 140.344452, training accuracy 0.651578\n",
      "Iter 273, validation loss 366.706055, validation accuracy 0.361037\n",
      "Iter 274, training loss 74.559288, training accuracy 0.755211\n",
      "Iter 274, validation loss 275.830170, validation accuracy 0.406166\n",
      "Iter 275, training loss 124.386688, training accuracy 0.694610\n",
      "Iter 275, validation loss 374.348328, validation accuracy 0.492851\n",
      "Iter 276, training loss 86.683517, training accuracy 0.818344\n",
      "Iter 276, validation loss 279.447723, validation accuracy 0.512511\n",
      "Iter 277, training loss 215.567307, training accuracy 0.547647\n",
      "Iter 277, validation loss 426.791107, validation accuracy 0.434316\n",
      "Iter 278, training loss 60.894394, training accuracy 0.770995\n",
      "Iter 278, validation loss 295.538879, validation accuracy 0.469169\n",
      "Iter 279, training loss 251.522217, training accuracy 0.651429\n",
      "Iter 279, validation loss 538.742493, validation accuracy 0.432529\n",
      "Iter 280, training loss 137.115036, training accuracy 0.675998\n",
      "Iter 280, validation loss 421.253021, validation accuracy 0.427167\n",
      "Iter 281, training loss 38.920242, training accuracy 0.845444\n",
      "Iter 281, validation loss 208.475906, validation accuracy 0.492404\n",
      "Iter 282, training loss 269.316315, training accuracy 0.495235\n",
      "Iter 282, validation loss 528.173218, validation accuracy 0.428954\n",
      "Iter 283, training loss 391.659912, training accuracy 0.530673\n",
      "Iter 283, validation loss 716.499390, validation accuracy 0.448168\n",
      "Iter 284, training loss 141.168365, training accuracy 0.716945\n",
      "Iter 284, validation loss 413.403900, validation accuracy 0.463360\n",
      "Iter 285, training loss 668.697876, training accuracy 0.360334\n",
      "Iter 285, validation loss 1057.508545, validation accuracy 0.264522\n",
      "Iter 286, training loss 361.914581, training accuracy 0.520697\n",
      "Iter 286, validation loss 592.046936, validation accuracy 0.424933\n",
      "Iter 287, training loss 291.283142, training accuracy 0.697439\n",
      "Iter 287, validation loss 561.226807, validation accuracy 0.495979\n",
      "Iter 288, training loss 416.204834, training accuracy 0.573556\n",
      "Iter 288, validation loss 734.191101, validation accuracy 0.471850\n",
      "Iter 289, training loss 54.655113, training accuracy 0.815664\n",
      "Iter 289, validation loss 260.468445, validation accuracy 0.498213\n",
      "Iter 290, training loss 369.918030, training accuracy 0.550179\n",
      "Iter 290, validation loss 648.994324, validation accuracy 0.351653\n",
      "Iter 291, training loss 22.598980, training accuracy 0.915277\n",
      "Iter 291, validation loss 204.246979, validation accuracy 0.510724\n",
      "Iter 292, training loss 301.462463, training accuracy 0.589637\n",
      "Iter 292, validation loss 667.769897, validation accuracy 0.466488\n",
      "Iter 293, training loss 52.206242, training accuracy 0.883562\n",
      "Iter 293, validation loss 275.061829, validation accuracy 0.525916\n",
      "Iter 294, training loss 306.972870, training accuracy 0.567451\n",
      "Iter 294, validation loss 558.490295, validation accuracy 0.446828\n",
      "Iter 295, training loss 121.804779, training accuracy 0.722752\n",
      "Iter 295, validation loss 449.828430, validation accuracy 0.379357\n",
      "Iter 296, training loss 496.629730, training accuracy 0.550179\n",
      "Iter 296, validation loss 929.229370, validation accuracy 0.439678\n",
      "Iter 297, training loss 188.228821, training accuracy 0.685378\n",
      "Iter 297, validation loss 553.927307, validation accuracy 0.469169\n",
      "Iter 298, training loss 386.148682, training accuracy 0.556135\n",
      "Iter 298, validation loss 646.477966, validation accuracy 0.416443\n",
      "Iter 299, training loss 354.536713, training accuracy 0.565962\n",
      "Iter 299, validation loss 620.898499, validation accuracy 0.431189\n",
      "Iter 300, training loss 180.938400, training accuracy 0.693568\n",
      "Iter 300, validation loss 560.072937, validation accuracy 0.473190\n",
      "Iter 301, training loss 317.511017, training accuracy 0.619714\n",
      "Iter 301, validation loss 741.639893, validation accuracy 0.448615\n",
      "Iter 302, training loss 131.319305, training accuracy 0.742555\n",
      "Iter 302, validation loss 430.419281, validation accuracy 0.407954\n",
      "Iter 303, training loss 231.648849, training accuracy 0.621650\n",
      "Iter 303, validation loss 527.724304, validation accuracy 0.475424\n",
      "Iter 304, training loss 137.490173, training accuracy 0.755807\n",
      "Iter 304, validation loss 508.557983, validation accuracy 0.495532\n",
      "Iter 305, training loss 155.487106, training accuracy 0.723198\n",
      "Iter 305, validation loss 538.602234, validation accuracy 0.478552\n",
      "Iter 306, training loss 243.890869, training accuracy 0.643240\n",
      "Iter 306, validation loss 612.453125, validation accuracy 0.357462\n",
      "Iter 307, training loss 111.092941, training accuracy 0.766677\n",
      "Iter 307, validation loss 415.771332, validation accuracy 0.485702\n",
      "Iter 308, training loss 285.903656, training accuracy 0.705628\n",
      "Iter 308, validation loss 665.534729, validation accuracy 0.496425\n",
      "Iter 309, training loss 64.291023, training accuracy 0.867332\n",
      "Iter 309, validation loss 371.614563, validation accuracy 0.500000\n",
      "Iter 310, training loss 303.625366, training accuracy 0.608547\n",
      "Iter 310, validation loss 697.344543, validation accuracy 0.343610\n",
      "Iter 311, training loss 35.207447, training accuracy 0.910214\n",
      "Iter 311, validation loss 280.491425, validation accuracy 0.507596\n",
      "Iter 312, training loss 201.232986, training accuracy 0.781864\n",
      "Iter 312, validation loss 540.495728, validation accuracy 0.512958\n",
      "Iter 313, training loss 46.563057, training accuracy 0.898600\n",
      "Iter 313, validation loss 312.892120, validation accuracy 0.503575\n",
      "Iter 314, training loss 182.409058, training accuracy 0.716051\n",
      "Iter 314, validation loss 523.473389, validation accuracy 0.399464\n",
      "Iter 315, training loss 49.647736, training accuracy 0.878499\n",
      "Iter 315, validation loss 358.939209, validation accuracy 0.497766\n",
      "Iter 316, training loss 95.388550, training accuracy 0.830703\n",
      "Iter 316, validation loss 459.692627, validation accuracy 0.498660\n",
      "Iter 317, training loss 66.192825, training accuracy 0.854080\n",
      "Iter 317, validation loss 336.357239, validation accuracy 0.482574\n",
      "Iter 318, training loss 66.527405, training accuracy 0.854229\n",
      "Iter 318, validation loss 333.162659, validation accuracy 0.476318\n",
      "Iter 319, training loss 79.727654, training accuracy 0.843806\n",
      "Iter 319, validation loss 435.508148, validation accuracy 0.500000\n",
      "Iter 320, training loss 31.018375, training accuracy 0.910810\n",
      "Iter 320, validation loss 310.468842, validation accuracy 0.503575\n",
      "Iter 321, training loss 102.020195, training accuracy 0.810304\n",
      "Iter 321, validation loss 403.680023, validation accuracy 0.437444\n",
      "Iter 322, training loss 55.731033, training accuracy 0.895473\n",
      "Iter 322, validation loss 351.856201, validation accuracy 0.523682\n",
      "Iter 323, training loss 72.548195, training accuracy 0.879988\n",
      "Iter 323, validation loss 387.781555, validation accuracy 0.521448\n",
      "Iter 324, training loss 68.536018, training accuracy 0.857504\n",
      "Iter 324, validation loss 339.682648, validation accuracy 0.471850\n",
      "Iter 325, training loss 29.108728, training accuracy 0.918999\n",
      "Iter 325, validation loss 289.775482, validation accuracy 0.514745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 326, training loss 113.880264, training accuracy 0.779482\n",
      "Iter 326, validation loss 517.028564, validation accuracy 0.492404\n",
      "Iter 327, training loss 227.202560, training accuracy 0.663043\n",
      "Iter 327, validation loss 560.898865, validation accuracy 0.473190\n",
      "Iter 328, training loss 188.384979, training accuracy 0.710393\n",
      "Iter 328, validation loss 615.177612, validation accuracy 0.473637\n",
      "Iter 329, training loss 113.824280, training accuracy 0.795265\n",
      "Iter 329, validation loss 448.190338, validation accuracy 0.429401\n",
      "Iter 330, training loss 164.649460, training accuracy 0.796158\n",
      "Iter 330, validation loss 527.938538, validation accuracy 0.482574\n",
      "Iter 331, training loss 80.636475, training accuracy 0.864652\n",
      "Iter 331, validation loss 430.254456, validation accuracy 0.509830\n",
      "Iter 332, training loss 406.748962, training accuracy 0.531864\n",
      "Iter 332, validation loss 938.959900, validation accuracy 0.298481\n",
      "Iter 333, training loss 767.860352, training accuracy 0.690590\n",
      "Iter 333, validation loss 1119.592896, validation accuracy 0.497319\n",
      "Iter 334, training loss 1477.517212, training accuracy 0.374330\n",
      "Iter 334, validation loss 1796.264282, validation accuracy 0.403038\n",
      "Iter 335, training loss 1962.580078, training accuracy 0.472901\n",
      "Iter 335, validation loss 2550.438965, validation accuracy 0.437444\n",
      "Iter 336, training loss 2654.670654, training accuracy 0.235706\n",
      "Iter 336, validation loss 3498.660400, validation accuracy 0.201072\n",
      "Iter 337, training loss 1422.980835, training accuracy 0.557326\n",
      "Iter 337, validation loss 2090.754150, validation accuracy 0.439678\n",
      "Iter 338, training loss 48.153004, training accuracy 0.905003\n",
      "Iter 338, validation loss 349.731354, validation accuracy 0.496872\n",
      "Iter 339, training loss 2486.733154, training accuracy 0.333383\n",
      "Iter 339, validation loss 2837.413574, validation accuracy 0.372654\n",
      "Iter 340, training loss 344.078827, training accuracy 0.571024\n",
      "Iter 340, validation loss 922.603882, validation accuracy 0.313673\n",
      "Iter 341, training loss 2426.010498, training accuracy 0.471709\n",
      "Iter 341, validation loss 3049.179688, validation accuracy 0.436104\n",
      "Iter 342, training loss 1874.233521, training accuracy 0.474985\n",
      "Iter 342, validation loss 2502.138916, validation accuracy 0.437444\n",
      "Iter 343, training loss 910.852722, training accuracy 0.518166\n",
      "Iter 343, validation loss 1357.868652, validation accuracy 0.345845\n",
      "Iter 344, training loss 2360.238037, training accuracy 0.365694\n",
      "Iter 344, validation loss 2773.579590, validation accuracy 0.373101\n",
      "Iter 345, training loss 129.889450, training accuracy 0.776802\n",
      "Iter 345, validation loss 601.987732, validation accuracy 0.408847\n",
      "Iter 346, training loss 2216.330078, training accuracy 0.473943\n",
      "Iter 346, validation loss 2929.704590, validation accuracy 0.436550\n",
      "Iter 347, training loss 1415.182373, training accuracy 0.540798\n",
      "Iter 347, validation loss 2160.659180, validation accuracy 0.439678\n",
      "Iter 348, training loss 2315.024658, training accuracy 0.248511\n",
      "Iter 348, validation loss 3073.976807, validation accuracy 0.221626\n",
      "Iter 349, training loss 3259.456299, training accuracy 0.331894\n",
      "Iter 349, validation loss 3642.635498, validation accuracy 0.372654\n",
      "Iter 350, training loss 1096.595825, training accuracy 0.402918\n",
      "Iter 350, validation loss 1601.933472, validation accuracy 0.411528\n",
      "Iter 351, training loss 3668.277832, training accuracy 0.470071\n",
      "Iter 351, validation loss 4447.376465, validation accuracy 0.434763\n",
      "Iter 352, training loss 4373.713867, training accuracy 0.473943\n",
      "Iter 352, validation loss 5440.181152, validation accuracy 0.435210\n",
      "Iter 353, training loss 5956.319336, training accuracy 0.214413\n",
      "Iter 353, validation loss 7264.158691, validation accuracy 0.197498\n",
      "Iter 354, training loss 4131.681152, training accuracy 0.284098\n",
      "Iter 354, validation loss 5370.629395, validation accuracy 0.212243\n",
      "Iter 355, training loss 4336.186035, training accuracy 0.469774\n",
      "Iter 355, validation loss 5170.600586, validation accuracy 0.434763\n",
      "Iter 356, training loss 3253.046387, training accuracy 0.479750\n",
      "Iter 356, validation loss 3724.510986, validation accuracy 0.437444\n",
      "Iter 357, training loss 6033.650879, training accuracy 0.326683\n",
      "Iter 357, validation loss 6068.155762, validation accuracy 0.371760\n",
      "Iter 358, training loss 6858.675293, training accuracy 0.326534\n",
      "Iter 358, validation loss 6868.734375, validation accuracy 0.371760\n",
      "Iter 359, training loss 1896.397705, training accuracy 0.413192\n",
      "Iter 359, validation loss 2376.326904, validation accuracy 0.421805\n",
      "Iter 360, training loss 3443.342529, training accuracy 0.472603\n",
      "Iter 360, validation loss 4431.397461, validation accuracy 0.434763\n",
      "Iter 361, training loss 4504.751953, training accuracy 0.545116\n",
      "Iter 361, validation loss 5799.804688, validation accuracy 0.360590\n",
      "Iter 362, training loss 7046.556641, training accuracy 0.251638\n",
      "Iter 362, validation loss 8731.026367, validation accuracy 0.201519\n",
      "Iter 363, training loss 6662.217773, training accuracy 0.529035\n",
      "Iter 363, validation loss 8238.708984, validation accuracy 0.436104\n",
      "Iter 364, training loss 6289.801270, training accuracy 0.495682\n",
      "Iter 364, validation loss 7812.050781, validation accuracy 0.433423\n",
      "Iter 365, training loss 4070.058350, training accuracy 0.526206\n",
      "Iter 365, validation loss 5388.776855, validation accuracy 0.348525\n",
      "Iter 366, training loss 2058.857178, training accuracy 0.433443\n",
      "Iter 366, validation loss 3212.562988, validation accuracy 0.276586\n",
      "Iter 367, training loss 449.093658, training accuracy 0.734812\n",
      "Iter 367, validation loss 1066.096191, validation accuracy 0.472297\n",
      "Iter 368, training loss 4727.198730, training accuracy 0.329512\n",
      "Iter 368, validation loss 5070.473633, validation accuracy 0.373101\n",
      "Iter 369, training loss 3560.611328, training accuracy 0.356164\n",
      "Iter 369, validation loss 3954.147949, validation accuracy 0.392761\n",
      "Iter 370, training loss 3729.748047, training accuracy 0.483026\n",
      "Iter 370, validation loss 4399.079102, validation accuracy 0.437444\n",
      "Iter 371, training loss 4069.574463, training accuracy 0.473049\n",
      "Iter 371, validation loss 4999.265625, validation accuracy 0.436997\n",
      "Iter 372, training loss 1559.632690, training accuracy 0.616587\n",
      "Iter 372, validation loss 2588.764893, validation accuracy 0.438785\n",
      "Iter 373, training loss 4949.175781, training accuracy 0.221560\n",
      "Iter 373, validation loss 6231.595703, validation accuracy 0.198391\n",
      "Iter 374, training loss 4023.345947, training accuracy 0.281566\n",
      "Iter 374, validation loss 5126.997559, validation accuracy 0.228329\n",
      "Iter 375, training loss 3234.074951, training accuracy 0.363163\n",
      "Iter 375, validation loss 3933.377686, validation accuracy 0.374441\n",
      "Iter 376, training loss 1997.008789, training accuracy 0.429273\n",
      "Iter 376, validation loss 2678.027100, validation accuracy 0.426720\n",
      "Iter 377, training loss 4394.536133, training accuracy 0.478856\n",
      "Iter 377, validation loss 5190.036621, validation accuracy 0.437891\n",
      "Iter 378, training loss 5878.039551, training accuracy 0.472305\n",
      "Iter 378, validation loss 6794.985352, validation accuracy 0.436550\n",
      "Iter 379, training loss 4246.232910, training accuracy 0.474092\n",
      "Iter 379, validation loss 5320.223633, validation accuracy 0.437891\n",
      "Iter 380, training loss 406.913574, training accuracy 0.737790\n",
      "Iter 380, validation loss 1274.647095, validation accuracy 0.453977\n",
      "Iter 381, training loss 3575.546387, training accuracy 0.378648\n",
      "Iter 381, validation loss 4647.869629, validation accuracy 0.267650\n",
      "Iter 382, training loss 4584.558594, training accuracy 0.514741\n",
      "Iter 382, validation loss 5136.052246, validation accuracy 0.405273\n",
      "Iter 383, training loss 4804.623047, training accuracy 0.450566\n",
      "Iter 383, validation loss 5475.043945, validation accuracy 0.398570\n",
      "Iter 384, training loss 1772.479736, training accuracy 0.533353\n",
      "Iter 384, validation loss 2448.862549, validation accuracy 0.406166\n",
      "Iter 385, training loss 982.309570, training accuracy 0.616587\n",
      "Iter 385, validation loss 2145.435791, validation accuracy 0.444147\n",
      "Iter 386, training loss 3099.884521, training accuracy 0.488088\n",
      "Iter 386, validation loss 4497.486328, validation accuracy 0.436997\n",
      "Iter 387, training loss 2351.059082, training accuracy 0.595444\n",
      "Iter 387, validation loss 3721.500977, validation accuracy 0.437891\n",
      "Iter 388, training loss 2410.008545, training accuracy 0.332490\n",
      "Iter 388, validation loss 3966.446045, validation accuracy 0.220286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 389, training loss 368.742035, training accuracy 0.719923\n",
      "Iter 389, validation loss 1303.827148, validation accuracy 0.390080\n",
      "Iter 390, training loss 832.689697, training accuracy 0.637284\n",
      "Iter 390, validation loss 1745.856689, validation accuracy 0.465594\n",
      "Iter 391, training loss 1681.879150, training accuracy 0.677189\n",
      "Iter 391, validation loss 2407.608154, validation accuracy 0.464254\n",
      "Iter 392, training loss 1860.438110, training accuracy 0.736301\n",
      "Iter 392, validation loss 2503.041992, validation accuracy 0.504915\n",
      "Iter 393, training loss 1126.730103, training accuracy 0.742108\n",
      "Iter 393, validation loss 1906.872192, validation accuracy 0.505809\n",
      "Iter 394, training loss 145.818146, training accuracy 0.866438\n",
      "Iter 394, validation loss 764.941345, validation accuracy 0.483467\n",
      "Iter 395, training loss 1533.693604, training accuracy 0.472454\n",
      "Iter 395, validation loss 2771.076904, validation accuracy 0.279714\n",
      "Iter 396, training loss 396.507080, training accuracy 0.732430\n",
      "Iter 396, validation loss 1308.686157, validation accuracy 0.392761\n",
      "Iter 397, training loss 842.778564, training accuracy 0.642049\n",
      "Iter 397, validation loss 1986.283936, validation accuracy 0.486595\n",
      "Iter 398, training loss 944.677917, training accuracy 0.731537\n",
      "Iter 398, validation loss 1839.668457, validation accuracy 0.491063\n",
      "Iter 399, training loss 823.572388, training accuracy 0.603335\n",
      "Iter 399, validation loss 1826.811279, validation accuracy 0.460232\n",
      "Iter 400, training loss 147.800537, training accuracy 0.868374\n",
      "Iter 400, validation loss 777.255798, validation accuracy 0.483021\n",
      "Iter 401, training loss 391.841644, training accuracy 0.722156\n",
      "Iter 401, validation loss 1470.096313, validation accuracy 0.415996\n",
      "Iter 402, training loss 628.488403, training accuracy 0.715456\n",
      "Iter 402, validation loss 1743.790405, validation accuracy 0.443700\n",
      "Iter 403, training loss 284.844666, training accuracy 0.786033\n",
      "Iter 403, validation loss 1264.898193, validation accuracy 0.473190\n",
      "Iter 404, training loss 218.574081, training accuracy 0.833979\n",
      "Iter 404, validation loss 911.763977, validation accuracy 0.471850\n",
      "Iter 405, training loss 765.030762, training accuracy 0.619565\n",
      "Iter 405, validation loss 1677.095825, validation accuracy 0.466488\n",
      "Iter 406, training loss 240.676849, training accuracy 0.839637\n",
      "Iter 406, validation loss 1152.462280, validation accuracy 0.495085\n",
      "Iter 407, training loss 618.139221, training accuracy 0.704288\n",
      "Iter 407, validation loss 1825.291138, validation accuracy 0.461573\n",
      "Iter 408, training loss 421.789337, training accuracy 0.716498\n",
      "Iter 408, validation loss 1540.722656, validation accuracy 0.399911\n",
      "Iter 409, training loss 181.943176, training accuracy 0.857951\n",
      "Iter 409, validation loss 876.554443, validation accuracy 0.472744\n",
      "Iter 410, training loss 221.763947, training accuracy 0.857653\n",
      "Iter 410, validation loss 1037.801025, validation accuracy 0.481233\n",
      "Iter 411, training loss 422.497650, training accuracy 0.801817\n",
      "Iter 411, validation loss 1338.276123, validation accuracy 0.490170\n",
      "Iter 412, training loss 186.587296, training accuracy 0.874926\n",
      "Iter 412, validation loss 972.785950, validation accuracy 0.489276\n",
      "Iter 413, training loss 182.902374, training accuracy 0.858547\n",
      "Iter 413, validation loss 890.704102, validation accuracy 0.473190\n",
      "Iter 414, training loss 243.209198, training accuracy 0.808070\n",
      "Iter 414, validation loss 1197.128296, validation accuracy 0.453083\n",
      "Iter 415, training loss 344.253510, training accuracy 0.771292\n",
      "Iter 415, validation loss 1424.745850, validation accuracy 0.486595\n",
      "Iter 416, training loss 99.959503, training accuracy 0.910959\n",
      "Iter 416, validation loss 795.870667, validation accuracy 0.496872\n",
      "Iter 417, training loss 441.040588, training accuracy 0.721412\n",
      "Iter 417, validation loss 1293.645142, validation accuracy 0.480786\n",
      "Iter 418, training loss 109.593971, training accuracy 0.891155\n",
      "Iter 418, validation loss 895.235840, validation accuracy 0.490170\n",
      "Iter 419, training loss 394.524445, training accuracy 0.755956\n",
      "Iter 419, validation loss 1493.002686, validation accuracy 0.467828\n",
      "Iter 420, training loss 392.646667, training accuracy 0.742704\n",
      "Iter 420, validation loss 1425.918457, validation accuracy 0.393655\n",
      "Iter 421, training loss 137.346725, training accuracy 0.891751\n",
      "Iter 421, validation loss 886.723511, validation accuracy 0.496425\n",
      "Iter 422, training loss 413.984802, training accuracy 0.794818\n",
      "Iter 422, validation loss 1375.621948, validation accuracy 0.478105\n",
      "Iter 423, training loss 196.253906, training accuracy 0.875521\n",
      "Iter 423, validation loss 1018.859009, validation accuracy 0.508043\n",
      "Iter 424, training loss 195.016190, training accuracy 0.849911\n",
      "Iter 424, validation loss 973.321045, validation accuracy 0.455317\n",
      "Iter 425, training loss 229.086182, training accuracy 0.811197\n",
      "Iter 425, validation loss 1216.471680, validation accuracy 0.481680\n",
      "Iter 426, training loss 227.152344, training accuracy 0.824151\n",
      "Iter 426, validation loss 1228.708984, validation accuracy 0.489723\n",
      "Iter 427, training loss 213.204544, training accuracy 0.835914\n",
      "Iter 427, validation loss 993.808960, validation accuracy 0.478999\n",
      "Iter 428, training loss 152.924606, training accuracy 0.872245\n",
      "Iter 428, validation loss 887.488464, validation accuracy 0.486148\n",
      "Iter 429, training loss 181.088043, training accuracy 0.846784\n",
      "Iter 429, validation loss 1125.637451, validation accuracy 0.489276\n",
      "Iter 430, training loss 186.669891, training accuracy 0.841721\n",
      "Iter 430, validation loss 1082.928711, validation accuracy 0.480786\n",
      "Iter 431, training loss 102.878723, training accuracy 0.903216\n",
      "Iter 431, validation loss 823.033020, validation accuracy 0.504915\n",
      "Iter 432, training loss 167.082001, training accuracy 0.889666\n",
      "Iter 432, validation loss 994.235229, validation accuracy 0.503128\n",
      "Iter 433, training loss 160.464996, training accuracy 0.872841\n",
      "Iter 433, validation loss 929.276978, validation accuracy 0.478105\n",
      "Iter 434, training loss 101.222717, training accuracy 0.910810\n",
      "Iter 434, validation loss 780.990906, validation accuracy 0.509383\n",
      "Iter 435, training loss 185.442490, training accuracy 0.833234\n",
      "Iter 435, validation loss 1137.844482, validation accuracy 0.481233\n",
      "Iter 436, training loss 142.710953, training accuracy 0.882370\n",
      "Iter 436, validation loss 847.786194, validation accuracy 0.482574\n",
      "Iter 437, training loss 132.489014, training accuracy 0.892793\n",
      "Iter 437, validation loss 891.404175, validation accuracy 0.483467\n",
      "Iter 438, training loss 135.610519, training accuracy 0.897111\n",
      "Iter 438, validation loss 906.422913, validation accuracy 0.510277\n",
      "Iter 439, training loss 140.539612, training accuracy 0.882966\n",
      "Iter 439, validation loss 836.690125, validation accuracy 0.486148\n",
      "Iter 440, training loss 122.891380, training accuracy 0.877159\n",
      "Iter 440, validation loss 977.090698, validation accuracy 0.476318\n",
      "Iter 441, training loss 86.673111, training accuracy 0.915426\n",
      "Iter 441, validation loss 806.974243, validation accuracy 0.507596\n",
      "Iter 442, training loss 102.986359, training accuracy 0.907683\n",
      "Iter 442, validation loss 790.594299, validation accuracy 0.489276\n",
      "Iter 443, training loss 107.135872, training accuracy 0.906492\n",
      "Iter 443, validation loss 845.474731, validation accuracy 0.507149\n",
      "Iter 444, training loss 110.849312, training accuracy 0.900238\n",
      "Iter 444, validation loss 787.074280, validation accuracy 0.501787\n",
      "Iter 445, training loss 136.435867, training accuracy 0.869714\n",
      "Iter 445, validation loss 1025.444580, validation accuracy 0.478999\n",
      "Iter 446, training loss 252.880585, training accuracy 0.814026\n",
      "Iter 446, validation loss 1013.911499, validation accuracy 0.452189\n",
      "Iter 447, training loss 531.245483, training accuracy 0.759529\n",
      "Iter 447, validation loss 1607.594360, validation accuracy 0.486595\n",
      "Iter 448, training loss 790.842407, training accuracy 0.623585\n",
      "Iter 448, validation loss 1641.016235, validation accuracy 0.443700\n",
      "Iter 449, training loss 1634.643066, training accuracy 0.575640\n",
      "Iter 449, validation loss 3123.732422, validation accuracy 0.438785\n",
      "Iter 450, training loss 2153.401855, training accuracy 0.374330\n",
      "Iter 450, validation loss 3704.645264, validation accuracy 0.252458\n",
      "Iter 451, training loss 3695.427734, training accuracy 0.461435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 451, validation loss 4463.287598, validation accuracy 0.446381\n",
      "Iter 452, training loss 10958.856445, training accuracy 0.470518\n",
      "Iter 452, validation loss 11751.850586, validation accuracy 0.436550\n",
      "Iter 453, training loss 9100.291992, training accuracy 0.472007\n",
      "Iter 453, validation loss 10016.734375, validation accuracy 0.436997\n",
      "Iter 454, training loss 3645.994385, training accuracy 0.359887\n",
      "Iter 454, validation loss 4649.336914, validation accuracy 0.383825\n",
      "Iter 455, training loss 9904.578125, training accuracy 0.212775\n",
      "Iter 455, validation loss 11793.573242, validation accuracy 0.194817\n",
      "Iter 456, training loss 2784.220947, training accuracy 0.453097\n",
      "Iter 456, validation loss 4578.799805, validation accuracy 0.292672\n",
      "Iter 457, training loss 9971.616211, training accuracy 0.471263\n",
      "Iter 457, validation loss 10932.019531, validation accuracy 0.436997\n",
      "Iter 458, training loss 8406.513672, training accuracy 0.717987\n",
      "Iter 458, validation loss 8264.289062, validation accuracy 0.490170\n",
      "Iter 459, training loss 18791.453125, training accuracy 0.326831\n",
      "Iter 459, validation loss 18006.722656, validation accuracy 0.371760\n",
      "Iter 460, training loss 11461.791992, training accuracy 0.333383\n",
      "Iter 460, validation loss 11381.428711, validation accuracy 0.375335\n",
      "Iter 461, training loss 12289.548828, training accuracy 0.469625\n",
      "Iter 461, validation loss 13602.249023, validation accuracy 0.434763\n",
      "Iter 462, training loss 11105.791992, training accuracy 0.473943\n",
      "Iter 462, validation loss 13638.812500, validation accuracy 0.434763\n",
      "Iter 463, training loss 25429.658203, training accuracy 0.205926\n",
      "Iter 463, validation loss 28590.681641, validation accuracy 0.193029\n",
      "Iter 464, training loss 25761.201172, training accuracy 0.205628\n",
      "Iter 464, validation loss 28835.021484, validation accuracy 0.192583\n",
      "Iter 465, training loss 7100.723633, training accuracy 0.410959\n",
      "Iter 465, validation loss 9572.043945, validation accuracy 0.263181\n",
      "Iter 466, training loss 13790.665039, training accuracy 0.470071\n",
      "Iter 466, validation loss 14688.495117, validation accuracy 0.435210\n",
      "Iter 467, training loss 13500.678711, training accuracy 0.744044\n",
      "Iter 467, validation loss 12534.333984, validation accuracy 0.500894\n",
      "Iter 468, training loss 29715.091797, training accuracy 0.326831\n",
      "Iter 468, validation loss 27505.587891, validation accuracy 0.371760\n",
      "Iter 469, training loss 28268.316406, training accuracy 0.327278\n",
      "Iter 469, validation loss 26038.535156, validation accuracy 0.371760\n",
      "Iter 470, training loss 21882.912109, training accuracy 0.479750\n",
      "Iter 470, validation loss 20658.613281, validation accuracy 0.437444\n",
      "Iter 471, training loss 22474.589844, training accuracy 0.470667\n",
      "Iter 471, validation loss 21909.382812, validation accuracy 0.436550\n",
      "Iter 472, training loss 13752.704102, training accuracy 0.480941\n",
      "Iter 472, validation loss 13883.083008, validation accuracy 0.437891\n",
      "Iter 473, training loss 10000.084961, training accuracy 0.331149\n",
      "Iter 473, validation loss 10581.057617, validation accuracy 0.373101\n",
      "Iter 474, training loss 8151.306641, training accuracy 0.333383\n",
      "Iter 474, validation loss 9860.807617, validation accuracy 0.247989\n",
      "Iter 475, training loss 12597.095703, training accuracy 0.219923\n",
      "Iter 475, validation loss 14723.460938, validation accuracy 0.195710\n",
      "Iter 476, training loss 562.016174, training accuracy 0.716945\n",
      "Iter 476, validation loss 1902.819092, validation accuracy 0.375782\n",
      "Iter 477, training loss 7986.775879, training accuracy 0.482579\n",
      "Iter 477, validation loss 9168.108398, validation accuracy 0.437891\n",
      "Iter 478, training loss 8987.501953, training accuracy 0.657236\n",
      "Iter 478, validation loss 9257.699219, validation accuracy 0.491510\n",
      "Iter 479, training loss 15320.194336, training accuracy 0.334127\n",
      "Iter 479, validation loss 15119.395508, validation accuracy 0.374888\n",
      "Iter 480, training loss 9501.281250, training accuracy 0.480643\n",
      "Iter 480, validation loss 9779.010742, validation accuracy 0.447721\n",
      "Iter 481, training loss 12703.823242, training accuracy 0.472156\n",
      "Iter 481, validation loss 13927.026367, validation accuracy 0.436997\n",
      "Iter 482, training loss 10195.218750, training accuracy 0.470071\n",
      "Iter 482, validation loss 12284.879883, validation accuracy 0.435210\n",
      "Iter 483, training loss 8806.778320, training accuracy 0.230643\n",
      "Iter 483, validation loss 11624.260742, validation accuracy 0.199285\n",
      "Iter 484, training loss 8547.968750, training accuracy 0.224985\n",
      "Iter 484, validation loss 11166.426758, validation accuracy 0.199285\n",
      "Iter 485, training loss 1010.013428, training accuracy 0.760125\n",
      "Iter 485, validation loss 2383.461914, validation accuracy 0.504021\n",
      "Iter 486, training loss 6824.256348, training accuracy 0.371948\n",
      "Iter 486, validation loss 7832.307617, validation accuracy 0.399911\n",
      "Iter 487, training loss 6615.383789, training accuracy 0.584574\n",
      "Iter 487, validation loss 7668.514648, validation accuracy 0.474084\n",
      "Iter 488, training loss 5454.588379, training accuracy 0.603186\n",
      "Iter 488, validation loss 6639.182129, validation accuracy 0.480340\n",
      "Iter 489, training loss 3763.633789, training accuracy 0.403663\n",
      "Iter 489, validation loss 5256.237305, validation accuracy 0.407954\n",
      "Iter 490, training loss 2426.949463, training accuracy 0.413043\n",
      "Iter 490, validation loss 4731.085938, validation accuracy 0.252011\n",
      "Iter 491, training loss 3037.019775, training accuracy 0.641007\n",
      "Iter 491, validation loss 5102.440918, validation accuracy 0.431189\n",
      "Iter 492, training loss 2551.059814, training accuracy 0.639815\n",
      "Iter 492, validation loss 4588.308105, validation accuracy 0.434763\n",
      "Iter 493, training loss 1298.108154, training accuracy 0.621203\n",
      "Iter 493, validation loss 2937.703369, validation accuracy 0.328418\n",
      "Iter 494, training loss 3424.411865, training accuracy 0.418702\n",
      "Iter 494, validation loss 5026.968750, validation accuracy 0.401698\n",
      "Iter 495, training loss 3180.922852, training accuracy 0.556432\n",
      "Iter 495, validation loss 5086.878418, validation accuracy 0.459339\n",
      "Iter 496, training loss 1772.977661, training accuracy 0.593806\n",
      "Iter 496, validation loss 3840.877197, validation accuracy 0.461573\n",
      "Iter 497, training loss 3384.746582, training accuracy 0.513996\n",
      "Iter 497, validation loss 4991.443359, validation accuracy 0.329312\n",
      "Iter 498, training loss 3641.524902, training accuracy 0.535289\n",
      "Iter 498, validation loss 4927.398926, validation accuracy 0.394996\n",
      "Iter 499, training loss 276.921021, training accuracy 0.880733\n",
      "Iter 499, validation loss 1601.613892, validation accuracy 0.502681\n",
      "Iter 500, training loss 3361.142334, training accuracy 0.509678\n",
      "Iter 500, validation loss 5623.723145, validation accuracy 0.440125\n",
      "Iter 501, training loss 170.357910, training accuracy 0.897111\n",
      "Iter 501, validation loss 1407.475464, validation accuracy 0.497319\n",
      "Iter 502, training loss 2517.338867, training accuracy 0.558517\n",
      "Iter 502, validation loss 3956.467773, validation accuracy 0.373101\n",
      "Iter 503, training loss 618.200867, training accuracy 0.761316\n",
      "Iter 503, validation loss 1895.229004, validation accuracy 0.461573\n",
      "Iter 504, training loss 3067.116943, training accuracy 0.518761\n",
      "Iter 504, validation loss 5374.671387, validation accuracy 0.440572\n",
      "Iter 505, training loss 606.881653, training accuracy 0.761912\n",
      "Iter 505, validation loss 2451.891846, validation accuracy 0.491957\n",
      "Iter 506, training loss 3232.631104, training accuracy 0.542436\n",
      "Iter 506, validation loss 4689.964355, validation accuracy 0.372207\n",
      "Iter 507, training loss 2361.197021, training accuracy 0.566409\n",
      "Iter 507, validation loss 3762.504395, validation accuracy 0.416443\n",
      "Iter 508, training loss 2527.536133, training accuracy 0.538267\n",
      "Iter 508, validation loss 4935.055176, validation accuracy 0.441466\n",
      "Iter 509, training loss 1565.809082, training accuracy 0.635200\n",
      "Iter 509, validation loss 3818.037598, validation accuracy 0.446828\n",
      "Iter 510, training loss 3220.994141, training accuracy 0.530226\n",
      "Iter 510, validation loss 4920.117188, validation accuracy 0.340929\n",
      "Iter 511, training loss 4220.323242, training accuracy 0.458160\n",
      "Iter 511, validation loss 5880.523926, validation accuracy 0.407060\n",
      "Iter 512, training loss 2232.451416, training accuracy 0.553752\n",
      "Iter 512, validation loss 4635.616699, validation accuracy 0.447274\n",
      "Iter 513, training loss 1677.883057, training accuracy 0.666319\n",
      "Iter 513, validation loss 3836.767578, validation accuracy 0.435210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 514, training loss 4277.898926, training accuracy 0.379690\n",
      "Iter 514, validation loss 6846.833496, validation accuracy 0.257373\n",
      "Iter 515, training loss 4516.656738, training accuracy 0.402323\n",
      "Iter 515, validation loss 6332.871094, validation accuracy 0.400804\n",
      "Iter 516, training loss 5551.274414, training accuracy 0.503722\n",
      "Iter 516, validation loss 7677.995605, validation accuracy 0.441019\n",
      "Iter 517, training loss 4304.542480, training accuracy 0.511316\n",
      "Iter 517, validation loss 6596.462891, validation accuracy 0.442806\n",
      "Iter 518, training loss 2838.741455, training accuracy 0.557921\n",
      "Iter 518, validation loss 4353.428223, validation accuracy 0.410188\n",
      "Iter 519, training loss 6027.148438, training accuracy 0.389964\n",
      "Iter 519, validation loss 8490.172852, validation accuracy 0.269884\n",
      "Iter 520, training loss 441.984711, training accuracy 0.854973\n",
      "Iter 520, validation loss 1963.179565, validation accuracy 0.487489\n",
      "Iter 521, training loss 4656.165039, training accuracy 0.547052\n",
      "Iter 521, validation loss 6778.741211, validation accuracy 0.455764\n",
      "Iter 522, training loss 2529.413330, training accuracy 0.704884\n",
      "Iter 522, validation loss 4169.775879, validation accuracy 0.461126\n",
      "Iter 523, training loss 1748.407593, training accuracy 0.608696\n",
      "Iter 523, validation loss 3440.745850, validation accuracy 0.455317\n",
      "Iter 524, training loss 4912.598145, training accuracy 0.339488\n",
      "Iter 524, validation loss 8122.391602, validation accuracy 0.222520\n",
      "Iter 525, training loss 8781.985352, training accuracy 0.475283\n",
      "Iter 525, validation loss 11783.154297, validation accuracy 0.436997\n",
      "Iter 526, training loss 6477.794922, training accuracy 0.479750\n",
      "Iter 526, validation loss 9220.984375, validation accuracy 0.437444\n",
      "Iter 527, training loss 3434.645020, training accuracy 0.527099\n",
      "Iter 527, validation loss 5143.969727, validation accuracy 0.422252\n",
      "Iter 528, training loss 5604.016113, training accuracy 0.493895\n",
      "Iter 528, validation loss 7592.572754, validation accuracy 0.315013\n",
      "Iter 529, training loss 296.624664, training accuracy 0.876563\n",
      "Iter 529, validation loss 1592.364380, validation accuracy 0.479893\n",
      "Iter 530, training loss 5468.368164, training accuracy 0.496129\n",
      "Iter 530, validation loss 7980.502930, validation accuracy 0.439231\n",
      "Iter 531, training loss 2111.084717, training accuracy 0.729154\n",
      "Iter 531, validation loss 3982.228760, validation accuracy 0.500894\n",
      "Iter 532, training loss 5076.212402, training accuracy 0.486748\n",
      "Iter 532, validation loss 6870.519043, validation accuracy 0.415103\n",
      "Iter 533, training loss 8421.223633, training accuracy 0.253127\n",
      "Iter 533, validation loss 11680.208984, validation accuracy 0.214924\n",
      "Iter 534, training loss 3258.504395, training accuracy 0.533055\n",
      "Iter 534, validation loss 6146.391602, validation accuracy 0.439231\n",
      "Iter 535, training loss 2153.530273, training accuracy 0.700119\n",
      "Iter 535, validation loss 4218.922852, validation accuracy 0.488829\n",
      "Iter 536, training loss 6732.386719, training accuracy 0.379244\n",
      "Iter 536, validation loss 8740.983398, validation accuracy 0.383825\n",
      "Iter 537, training loss 2755.927979, training accuracy 0.462329\n",
      "Iter 537, validation loss 5669.619141, validation accuracy 0.274352\n",
      "Iter 538, training loss 6361.832031, training accuracy 0.485706\n",
      "Iter 538, validation loss 9475.902344, validation accuracy 0.436997\n",
      "Iter 539, training loss 3478.272705, training accuracy 0.524419\n",
      "Iter 539, validation loss 6432.204590, validation accuracy 0.439231\n",
      "Iter 540, training loss 5761.120117, training accuracy 0.526653\n",
      "Iter 540, validation loss 7448.494141, validation accuracy 0.394996\n",
      "Iter 541, training loss 7921.825684, training accuracy 0.513699\n",
      "Iter 541, validation loss 9685.622070, validation accuracy 0.356122\n",
      "Iter 542, training loss 1007.790100, training accuracy 0.735557\n",
      "Iter 542, validation loss 2692.971924, validation accuracy 0.456658\n",
      "Iter 543, training loss 9442.197266, training accuracy 0.474687\n",
      "Iter 543, validation loss 12370.425781, validation accuracy 0.437444\n",
      "Iter 544, training loss 9086.320312, training accuracy 0.475134\n",
      "Iter 544, validation loss 12081.498047, validation accuracy 0.437444\n",
      "Iter 545, training loss 833.136719, training accuracy 0.771888\n",
      "Iter 545, validation loss 2477.267090, validation accuracy 0.432529\n",
      "Iter 546, training loss 6294.508789, training accuracy 0.519952\n",
      "Iter 546, validation loss 8043.418945, validation accuracy 0.408400\n",
      "Iter 547, training loss 2675.595947, training accuracy 0.547052\n",
      "Iter 547, validation loss 5236.555664, validation accuracy 0.292672\n",
      "Iter 548, training loss 8179.006348, training accuracy 0.477963\n",
      "Iter 548, validation loss 11038.230469, validation accuracy 0.437444\n",
      "Iter 549, training loss 7368.186035, training accuracy 0.509232\n",
      "Iter 549, validation loss 9803.062500, validation accuracy 0.443700\n",
      "Iter 550, training loss 10779.298828, training accuracy 0.344401\n",
      "Iter 550, validation loss 12635.836914, validation accuracy 0.379803\n",
      "Iter 551, training loss 2767.612061, training accuracy 0.588297\n",
      "Iter 551, validation loss 4799.676270, validation accuracy 0.367739\n",
      "Iter 552, training loss 3565.455566, training accuracy 0.625819\n",
      "Iter 552, validation loss 6454.042969, validation accuracy 0.445040\n",
      "Iter 553, training loss 6044.206543, training accuracy 0.557921\n",
      "Iter 553, validation loss 9579.347656, validation accuracy 0.438785\n",
      "Iter 554, training loss 1744.232788, training accuracy 0.590381\n",
      "Iter 554, validation loss 4652.428711, validation accuracy 0.349419\n",
      "Iter 555, training loss 6125.767578, training accuracy 0.396962\n",
      "Iter 555, validation loss 8413.847656, validation accuracy 0.402592\n",
      "Iter 556, training loss 4525.509277, training accuracy 0.574151\n",
      "Iter 556, validation loss 7193.513672, validation accuracy 0.466041\n",
      "Iter 557, training loss 1021.849182, training accuracy 0.738237\n",
      "Iter 557, validation loss 3641.775635, validation accuracy 0.489276\n",
      "Iter 558, training loss 7272.411133, training accuracy 0.394729\n",
      "Iter 558, validation loss 10374.814453, validation accuracy 0.268990\n",
      "Iter 559, training loss 6601.827637, training accuracy 0.419893\n",
      "Iter 559, validation loss 8900.047852, validation accuracy 0.391868\n",
      "Iter 560, training loss 4714.647461, training accuracy 0.505211\n",
      "Iter 560, validation loss 7973.272461, validation accuracy 0.438785\n",
      "Iter 561, training loss 3234.441895, training accuracy 0.620161\n",
      "Iter 561, validation loss 6424.507812, validation accuracy 0.439231\n",
      "Iter 562, training loss 8203.130859, training accuracy 0.313728\n",
      "Iter 562, validation loss 11809.236328, validation accuracy 0.234584\n",
      "Iter 563, training loss 10688.009766, training accuracy 0.351697\n",
      "Iter 563, validation loss 12867.953125, validation accuracy 0.378463\n",
      "Iter 564, training loss 4837.991211, training accuracy 0.640560\n",
      "Iter 564, validation loss 7257.737305, validation accuracy 0.485702\n",
      "Iter 565, training loss 5724.720215, training accuracy 0.503574\n",
      "Iter 565, validation loss 8884.202148, validation accuracy 0.441466\n",
      "Iter 566, training loss 3299.349609, training accuracy 0.560304\n",
      "Iter 566, validation loss 5822.257812, validation accuracy 0.328418\n",
      "Iter 567, training loss 3746.417480, training accuracy 0.561048\n",
      "Iter 567, validation loss 5848.708496, validation accuracy 0.424486\n",
      "Iter 568, training loss 2458.352051, training accuracy 0.601251\n",
      "Iter 568, validation loss 5790.688965, validation accuracy 0.449508\n",
      "Iter 569, training loss 1253.960693, training accuracy 0.728708\n",
      "Iter 569, validation loss 4013.461182, validation accuracy 0.447274\n",
      "Iter 570, training loss 3011.662354, training accuracy 0.591870\n",
      "Iter 570, validation loss 5307.521973, validation accuracy 0.369080\n",
      "Iter 571, training loss 2919.770508, training accuracy 0.552859\n",
      "Iter 571, validation loss 5538.923340, validation accuracy 0.449955\n",
      "Iter 572, training loss 9091.725586, training accuracy 0.477219\n",
      "Iter 572, validation loss 12662.435547, validation accuracy 0.437444\n",
      "Iter 573, training loss 5009.149414, training accuracy 0.604824\n",
      "Iter 573, validation loss 8643.563477, validation accuracy 0.438338\n",
      "Iter 574, training loss 13632.113281, training accuracy 0.240471\n",
      "Iter 574, validation loss 17723.480469, validation accuracy 0.203753\n",
      "Iter 575, training loss 13027.646484, training accuracy 0.354973\n",
      "Iter 575, validation loss 15347.771484, validation accuracy 0.376676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 576, training loss 2876.039551, training accuracy 0.719029\n",
      "Iter 576, validation loss 5192.017090, validation accuracy 0.466935\n",
      "Iter 577, training loss 9715.522461, training accuracy 0.476921\n",
      "Iter 577, validation loss 13467.765625, validation accuracy 0.437444\n",
      "Iter 578, training loss 5029.554688, training accuracy 0.532013\n",
      "Iter 578, validation loss 8860.804688, validation accuracy 0.350760\n",
      "Iter 579, training loss 1536.917114, training accuracy 0.648303\n",
      "Iter 579, validation loss 4620.962402, validation accuracy 0.361930\n",
      "Iter 580, training loss 6237.442383, training accuracy 0.490322\n",
      "Iter 580, validation loss 8729.466797, validation accuracy 0.448615\n",
      "Iter 581, training loss 10720.423828, training accuracy 0.504914\n",
      "Iter 581, validation loss 13438.355469, validation accuracy 0.441912\n",
      "Iter 582, training loss 5067.633301, training accuracy 0.681358\n",
      "Iter 582, validation loss 7497.950195, validation accuracy 0.487042\n",
      "Iter 583, training loss 6596.879883, training accuracy 0.511912\n",
      "Iter 583, validation loss 8919.358398, validation accuracy 0.407507\n",
      "Iter 584, training loss 18049.074219, training accuracy 0.222156\n",
      "Iter 584, validation loss 22540.289062, validation accuracy 0.197945\n",
      "Iter 585, training loss 2502.391357, training accuracy 0.660214\n",
      "Iter 585, validation loss 5870.127930, validation accuracy 0.434763\n",
      "Iter 586, training loss 4243.347168, training accuracy 0.668255\n",
      "Iter 586, validation loss 6990.320312, validation accuracy 0.482127\n",
      "Iter 587, training loss 11863.553711, training accuracy 0.353782\n",
      "Iter 587, validation loss 14195.483398, validation accuracy 0.386506\n",
      "Iter 588, training loss 742.773804, training accuracy 0.806879\n",
      "Iter 588, validation loss 3458.766113, validation accuracy 0.489723\n",
      "Iter 589, training loss 8230.674805, training accuracy 0.337403\n",
      "Iter 589, validation loss 13027.607422, validation accuracy 0.225201\n",
      "Iter 590, training loss 10274.999023, training accuracy 0.477219\n",
      "Iter 590, validation loss 14446.616211, validation accuracy 0.437444\n",
      "Iter 591, training loss 2543.668945, training accuracy 0.722305\n",
      "Iter 591, validation loss 5334.585449, validation accuracy 0.496425\n",
      "Iter 592, training loss 14316.168945, training accuracy 0.367778\n",
      "Iter 592, validation loss 16874.783203, validation accuracy 0.377569\n",
      "Iter 593, training loss 14607.350586, training accuracy 0.275759\n",
      "Iter 593, validation loss 18825.197266, validation accuracy 0.223861\n",
      "Iter 594, training loss 741.230652, training accuracy 0.798839\n",
      "Iter 594, validation loss 3569.890625, validation accuracy 0.476318\n",
      "Iter 595, training loss 6670.602539, training accuracy 0.540500\n",
      "Iter 595, validation loss 10053.211914, validation accuracy 0.452189\n",
      "Iter 596, training loss 7814.250977, training accuracy 0.407981\n",
      "Iter 596, validation loss 10611.384766, validation accuracy 0.412422\n",
      "Iter 597, training loss 1121.852173, training accuracy 0.750893\n",
      "Iter 597, validation loss 4263.612305, validation accuracy 0.462020\n",
      "Iter 598, training loss 10599.452148, training accuracy 0.265783\n",
      "Iter 598, validation loss 15686.139648, validation accuracy 0.203753\n",
      "Iter 599, training loss 6376.791016, training accuracy 0.499702\n",
      "Iter 599, validation loss 10379.933594, validation accuracy 0.439678\n",
      "Iter 600, training loss 8119.715332, training accuracy 0.427487\n",
      "Iter 600, validation loss 10881.741211, validation accuracy 0.420465\n",
      "Iter 601, training loss 3251.931396, training accuracy 0.680911\n",
      "Iter 601, validation loss 6372.103516, validation accuracy 0.483021\n",
      "Iter 602, training loss 2919.347900, training accuracy 0.527546\n",
      "Iter 602, validation loss 6975.164551, validation accuracy 0.305630\n",
      "Iter 603, training loss 705.310181, training accuracy 0.817749\n",
      "Iter 603, validation loss 3571.812988, validation accuracy 0.483467\n",
      "Iter 604, training loss 1388.092041, training accuracy 0.761763\n",
      "Iter 604, validation loss 4149.913086, validation accuracy 0.452636\n",
      "Iter 605, training loss 439.920197, training accuracy 0.883562\n",
      "Iter 605, validation loss 2801.729492, validation accuracy 0.497766\n",
      "Iter 606, training loss 1237.088867, training accuracy 0.712924\n",
      "Iter 606, validation loss 4495.722168, validation accuracy 0.413315\n",
      "Iter 607, training loss 1218.357666, training accuracy 0.753127\n",
      "Iter 607, validation loss 4595.753418, validation accuracy 0.484808\n",
      "Iter 608, training loss 2205.009521, training accuracy 0.657832\n",
      "Iter 608, validation loss 4981.858887, validation accuracy 0.464254\n",
      "Iter 609, training loss 1074.930786, training accuracy 0.764145\n",
      "Iter 609, validation loss 4224.607910, validation accuracy 0.463360\n",
      "Iter 610, training loss 1534.253174, training accuracy 0.695503\n",
      "Iter 610, validation loss 5015.486328, validation accuracy 0.438338\n",
      "Iter 611, training loss 704.068481, training accuracy 0.856462\n",
      "Iter 611, validation loss 3172.763672, validation accuracy 0.477212\n",
      "Iter 612, training loss 1936.050903, training accuracy 0.700268\n",
      "Iter 612, validation loss 4941.780273, validation accuracy 0.449955\n",
      "Iter 613, training loss 2663.359619, training accuracy 0.645622\n",
      "Iter 613, validation loss 6630.507812, validation accuracy 0.448615\n",
      "Iter 614, training loss 8149.715820, training accuracy 0.308964\n",
      "Iter 614, validation loss 13065.236328, validation accuracy 0.226542\n",
      "Iter 615, training loss 5223.979004, training accuracy 0.644580\n",
      "Iter 615, validation loss 8001.337891, validation accuracy 0.445934\n",
      "Iter 616, training loss 11206.988281, training accuracy 0.618970\n",
      "Iter 616, validation loss 13701.711914, validation accuracy 0.483021\n",
      "Iter 617, training loss 8794.568359, training accuracy 0.557624\n",
      "Iter 617, validation loss 11321.950195, validation accuracy 0.453083\n",
      "Iter 618, training loss 2745.989990, training accuracy 0.614949\n",
      "Iter 618, validation loss 6864.889648, validation accuracy 0.460679\n",
      "Iter 619, training loss 23941.644531, training accuracy 0.217242\n",
      "Iter 619, validation loss 29189.521484, validation accuracy 0.196604\n",
      "Iter 620, training loss 5890.360352, training accuracy 0.534693\n",
      "Iter 620, validation loss 8884.740234, validation accuracy 0.340483\n",
      "Iter 621, training loss 11227.606445, training accuracy 0.726772\n",
      "Iter 621, validation loss 12939.495117, validation accuracy 0.498660\n",
      "Iter 622, training loss 19694.044922, training accuracy 0.712775\n",
      "Iter 622, validation loss 20216.064453, validation accuracy 0.494191\n",
      "Iter 623, training loss 23132.345703, training accuracy 0.454288\n",
      "Iter 623, validation loss 23764.248047, validation accuracy 0.435657\n",
      "Iter 624, training loss 24700.421875, training accuracy 0.474241\n",
      "Iter 624, validation loss 27139.699219, validation accuracy 0.437444\n",
      "Iter 625, training loss 8732.930664, training accuracy 0.498213\n",
      "Iter 625, validation loss 12748.076172, validation accuracy 0.440572\n",
      "Iter 626, training loss 27175.443359, training accuracy 0.272633\n",
      "Iter 626, validation loss 31490.148438, validation accuracy 0.221626\n",
      "Iter 627, training loss 36712.578125, training accuracy 0.431954\n",
      "Iter 627, validation loss 38519.347656, validation accuracy 0.398123\n",
      "Iter 628, training loss 25954.103516, training accuracy 0.509678\n",
      "Iter 628, validation loss 28028.984375, validation accuracy 0.357462\n",
      "Iter 629, training loss 1398.869019, training accuracy 0.754020\n",
      "Iter 629, validation loss 4047.050049, validation accuracy 0.405719\n",
      "Iter 630, training loss 32353.595703, training accuracy 0.469327\n",
      "Iter 630, validation loss 36338.851562, validation accuracy 0.434763\n",
      "Iter 631, training loss 40293.316406, training accuracy 0.469178\n",
      "Iter 631, validation loss 43693.546875, validation accuracy 0.434763\n",
      "Iter 632, training loss 19268.667969, training accuracy 0.476921\n",
      "Iter 632, validation loss 22362.193359, validation accuracy 0.436997\n",
      "Iter 633, training loss 36473.824219, training accuracy 0.327874\n",
      "Iter 633, validation loss 37889.230469, validation accuracy 0.371760\n",
      "Iter 634, training loss 30704.849609, training accuracy 0.514741\n",
      "Iter 634, validation loss 32492.853516, validation accuracy 0.384272\n",
      "Iter 635, training loss 46256.039062, training accuracy 0.208011\n",
      "Iter 635, validation loss 51320.914062, validation accuracy 0.194817\n",
      "Iter 636, training loss 7975.801758, training accuracy 0.326236\n",
      "Iter 636, validation loss 13261.455078, validation accuracy 0.228776\n",
      "Iter 637, training loss 35601.324219, training accuracy 0.470667\n",
      "Iter 637, validation loss 37523.093750, validation accuracy 0.436997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 638, training loss 48348.140625, training accuracy 0.477814\n",
      "Iter 638, validation loss 47051.042969, validation accuracy 0.437444\n",
      "Iter 639, training loss 69546.726562, training accuracy 0.331298\n",
      "Iter 639, validation loss 64620.539062, validation accuracy 0.373548\n",
      "Iter 640, training loss 62738.996094, training accuracy 0.361376\n",
      "Iter 640, validation loss 58043.324219, validation accuracy 0.394549\n",
      "Iter 641, training loss 74081.937500, training accuracy 0.469476\n",
      "Iter 641, validation loss 71840.789062, validation accuracy 0.434763\n",
      "Iter 642, training loss 75166.640625, training accuracy 0.469029\n",
      "Iter 642, validation loss 74934.359375, validation accuracy 0.434763\n",
      "Iter 643, training loss 48203.511719, training accuracy 0.469476\n",
      "Iter 643, validation loss 50020.066406, validation accuracy 0.434763\n",
      "Iter 644, training loss 6324.522949, training accuracy 0.520697\n",
      "Iter 644, validation loss 9830.260742, validation accuracy 0.453530\n",
      "Iter 645, training loss 48726.582031, training accuracy 0.207564\n",
      "Iter 645, validation loss 54349.140625, validation accuracy 0.194370\n",
      "Iter 646, training loss 57969.328125, training accuracy 0.206671\n",
      "Iter 646, validation loss 63496.449219, validation accuracy 0.193923\n",
      "Iter 647, training loss 16574.000000, training accuracy 0.518166\n",
      "Iter 647, validation loss 19200.699219, validation accuracy 0.388740\n",
      "Iter 648, training loss 11368.457031, training accuracy 0.722007\n",
      "Iter 648, validation loss 13424.484375, validation accuracy 0.485255\n",
      "Iter 649, training loss 33183.421875, training accuracy 0.474092\n",
      "Iter 649, validation loss 35228.722656, validation accuracy 0.437444\n",
      "Iter 650, training loss 28586.091797, training accuracy 0.546010\n",
      "Iter 650, validation loss 29729.648438, validation accuracy 0.455764\n",
      "Iter 651, training loss 43186.949219, training accuracy 0.330405\n",
      "Iter 651, validation loss 43022.710938, validation accuracy 0.372207\n",
      "Iter 652, training loss 21560.976562, training accuracy 0.351995\n",
      "Iter 652, validation loss 23752.529297, validation accuracy 0.388293\n",
      "Iter 653, training loss 18434.316406, training accuracy 0.489726\n",
      "Iter 653, validation loss 24951.312500, validation accuracy 0.433870\n",
      "Iter 654, training loss 53318.500000, training accuracy 0.210840\n",
      "Iter 654, validation loss 62712.128906, validation accuracy 0.193923\n",
      "Iter 655, training loss 48320.390625, training accuracy 0.239279\n",
      "Iter 655, validation loss 58523.582031, validation accuracy 0.197498\n",
      "Iter 656, training loss 59265.472656, training accuracy 0.469029\n",
      "Iter 656, validation loss 67361.507812, validation accuracy 0.434763\n",
      "Iter 657, training loss 61689.386719, training accuracy 0.469029\n",
      "Iter 657, validation loss 67755.992188, validation accuracy 0.434763\n",
      "Iter 658, training loss 35859.625000, training accuracy 0.469476\n",
      "Iter 658, validation loss 40268.898438, validation accuracy 0.435210\n",
      "Iter 659, training loss 31558.691406, training accuracy 0.331149\n",
      "Iter 659, validation loss 33520.437500, validation accuracy 0.372207\n",
      "Iter 660, training loss 39641.035156, training accuracy 0.331447\n",
      "Iter 660, validation loss 41860.554688, validation accuracy 0.371760\n",
      "Iter 661, training loss 32111.472656, training accuracy 0.229899\n",
      "Iter 661, validation loss 37934.921875, validation accuracy 0.198838\n",
      "Iter 662, training loss 13426.484375, training accuracy 0.280077\n",
      "Iter 662, validation loss 20215.656250, validation accuracy 0.205094\n",
      "Iter 663, training loss 37019.683594, training accuracy 0.469476\n",
      "Iter 663, validation loss 41418.460938, validation accuracy 0.435210\n",
      "Iter 664, training loss 47864.210938, training accuracy 0.469774\n",
      "Iter 664, validation loss 49991.601562, validation accuracy 0.436104\n",
      "Iter 665, training loss 33602.812500, training accuracy 0.727367\n",
      "Iter 665, validation loss 32676.328125, validation accuracy 0.488382\n",
      "Iter 666, training loss 61586.304688, training accuracy 0.328469\n",
      "Iter 666, validation loss 59679.425781, validation accuracy 0.371314\n",
      "Iter 667, training loss 40924.570312, training accuracy 0.339637\n",
      "Iter 667, validation loss 41052.617188, validation accuracy 0.377569\n",
      "Iter 668, training loss 32690.066406, training accuracy 0.469923\n",
      "Iter 668, validation loss 37078.207031, validation accuracy 0.436104\n",
      "Iter 669, training loss 27970.525391, training accuracy 0.476474\n",
      "Iter 669, validation loss 35669.203125, validation accuracy 0.434316\n",
      "Iter 670, training loss 64925.175781, training accuracy 0.207415\n",
      "Iter 670, validation loss 74458.437500, validation accuracy 0.191689\n",
      "Iter 671, training loss 69169.718750, training accuracy 0.206373\n",
      "Iter 671, validation loss 78426.750000, validation accuracy 0.191689\n",
      "Iter 672, training loss 16686.900391, training accuracy 0.364949\n",
      "Iter 672, validation loss 24278.419922, validation accuracy 0.238159\n",
      "Iter 673, training loss 31919.742188, training accuracy 0.475432\n",
      "Iter 673, validation loss 35101.328125, validation accuracy 0.437444\n",
      "Iter 674, training loss 48867.996094, training accuracy 0.396962\n",
      "Iter 674, validation loss 47259.832031, validation accuracy 0.411081\n",
      "Iter 675, training loss 56455.441406, training accuracy 0.678231\n",
      "Iter 675, validation loss 52546.582031, validation accuracy 0.460232\n",
      "Iter 676, training loss 67926.585938, training accuracy 0.511167\n",
      "Iter 676, validation loss 64513.765625, validation accuracy 0.444147\n",
      "Iter 677, training loss 60962.671875, training accuracy 0.726027\n",
      "Iter 677, validation loss 56223.480469, validation accuracy 0.490170\n",
      "Iter 678, training loss 61815.960938, training accuracy 0.373288\n",
      "Iter 678, validation loss 58569.175781, validation accuracy 0.399911\n",
      "Iter 679, training loss 45496.437500, training accuracy 0.511614\n",
      "Iter 679, validation loss 45610.894531, validation accuracy 0.444147\n",
      "Iter 680, training loss 24631.982422, training accuracy 0.555539\n",
      "Iter 680, validation loss 27419.111328, validation accuracy 0.458445\n",
      "Iter 681, training loss 10102.588867, training accuracy 0.502085\n",
      "Iter 681, validation loss 13960.144531, validation accuracy 0.413315\n",
      "Iter 682, training loss 62045.859375, training accuracy 0.207415\n",
      "Iter 682, validation loss 69720.320312, validation accuracy 0.193029\n",
      "Iter 683, training loss 64933.632812, training accuracy 0.206671\n",
      "Iter 683, validation loss 73317.062500, validation accuracy 0.192583\n",
      "Iter 684, training loss 10898.871094, training accuracy 0.456373\n",
      "Iter 684, validation loss 17918.859375, validation accuracy 0.281501\n",
      "Iter 685, training loss 37232.687500, training accuracy 0.472007\n",
      "Iter 685, validation loss 41037.968750, validation accuracy 0.436997\n",
      "Iter 686, training loss 42054.457031, training accuracy 0.648749\n",
      "Iter 686, validation loss 41451.257812, validation accuracy 0.483021\n",
      "Iter 687, training loss 78805.929688, training accuracy 0.330554\n",
      "Iter 687, validation loss 74825.039062, validation accuracy 0.372207\n",
      "Iter 688, training loss 72301.304688, training accuracy 0.340828\n",
      "Iter 688, validation loss 68459.289062, validation accuracy 0.379357\n",
      "Iter 689, training loss 70748.765625, training accuracy 0.472007\n",
      "Iter 689, validation loss 69754.367188, validation accuracy 0.436997\n",
      "Iter 690, training loss 73368.320312, training accuracy 0.469476\n",
      "Iter 690, validation loss 74298.726562, validation accuracy 0.435210\n",
      "Iter 691, training loss 46297.316406, training accuracy 0.471858\n",
      "Iter 691, validation loss 49133.488281, validation accuracy 0.436997\n",
      "Iter 692, training loss 15088.006836, training accuracy 0.393836\n",
      "Iter 692, validation loss 19208.478516, validation accuracy 0.408400\n",
      "Iter 693, training loss 35683.933594, training accuracy 0.226325\n",
      "Iter 693, validation loss 42936.484375, validation accuracy 0.197945\n",
      "Iter 694, training loss 43632.117188, training accuracy 0.213371\n",
      "Iter 694, validation loss 51322.085938, validation accuracy 0.195264\n",
      "Iter 695, training loss 1060.307617, training accuracy 0.839934\n",
      "Iter 695, validation loss 4195.287109, validation accuracy 0.467382\n",
      "Iter 696, training loss 19219.220703, training accuracy 0.552263\n",
      "Iter 696, validation loss 23433.826172, validation accuracy 0.457998\n",
      "Iter 697, training loss 24970.765625, training accuracy 0.677933\n",
      "Iter 697, validation loss 26443.691406, validation accuracy 0.462466\n",
      "Iter 698, training loss 29604.541016, training accuracy 0.507445\n",
      "Iter 698, validation loss 31238.519531, validation accuracy 0.451743\n",
      "Iter 699, training loss 30416.285156, training accuracy 0.493002\n",
      "Iter 699, validation loss 34021.832031, validation accuracy 0.440125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 700, training loss 14282.639648, training accuracy 0.577278\n",
      "Iter 700, validation loss 19071.169922, validation accuracy 0.466041\n",
      "Iter 701, training loss 12917.619141, training accuracy 0.525611\n",
      "Iter 701, validation loss 16726.011719, validation accuracy 0.391868\n",
      "Iter 702, training loss 40920.667969, training accuracy 0.226325\n",
      "Iter 702, validation loss 48308.015625, validation accuracy 0.197498\n",
      "Iter 703, training loss 15818.865234, training accuracy 0.378201\n",
      "Iter 703, validation loss 22272.074219, validation accuracy 0.262288\n",
      "Iter 704, training loss 13871.035156, training accuracy 0.717987\n",
      "Iter 704, validation loss 17133.230469, validation accuracy 0.495085\n",
      "Iter 705, training loss 30143.318359, training accuracy 0.666319\n",
      "Iter 705, validation loss 31675.757812, validation accuracy 0.485255\n",
      "Iter 706, training loss 37458.636719, training accuracy 0.543180\n",
      "Iter 706, validation loss 37998.371094, validation accuracy 0.454870\n",
      "Iter 707, training loss 36746.203125, training accuracy 0.631477\n",
      "Iter 707, validation loss 37808.757812, validation accuracy 0.481680\n",
      "Iter 708, training loss 26624.000000, training accuracy 0.729005\n",
      "Iter 708, validation loss 27934.654297, validation accuracy 0.489276\n",
      "Iter 709, training loss 16437.046875, training accuracy 0.453246\n",
      "Iter 709, validation loss 20516.416016, validation accuracy 0.435657\n",
      "Iter 710, training loss 7240.667480, training accuracy 0.547945\n",
      "Iter 710, validation loss 14026.157227, validation accuracy 0.361930\n",
      "Iter 711, training loss 26837.507812, training accuracy 0.272930\n",
      "Iter 711, validation loss 36574.164062, validation accuracy 0.203753\n",
      "Iter 712, training loss 28616.585938, training accuracy 0.475730\n",
      "Iter 712, validation loss 37224.312500, validation accuracy 0.435210\n",
      "Iter 713, training loss 18947.054688, training accuracy 0.477367\n",
      "Iter 713, validation loss 26337.126953, validation accuracy 0.437444\n",
      "Iter 714, training loss 14678.774414, training accuracy 0.428529\n",
      "Iter 714, validation loss 19416.556641, validation accuracy 0.392761\n",
      "Iter 715, training loss 16092.719727, training accuracy 0.519506\n",
      "Iter 715, validation loss 20243.722656, validation accuracy 0.368186\n",
      "Iter 716, training loss 5325.573730, training accuracy 0.533651\n",
      "Iter 716, validation loss 11785.776367, validation accuracy 0.296247\n",
      "Iter 717, training loss 24522.349609, training accuracy 0.473347\n",
      "Iter 717, validation loss 31066.031250, validation accuracy 0.437444\n",
      "Iter 718, training loss 24112.291016, training accuracy 0.489279\n",
      "Iter 718, validation loss 29385.992188, validation accuracy 0.439231\n",
      "Iter 719, training loss 30485.462891, training accuracy 0.342764\n",
      "Iter 719, validation loss 33807.402344, validation accuracy 0.380250\n",
      "Iter 720, training loss 9727.957031, training accuracy 0.461882\n",
      "Iter 720, validation loss 14957.426758, validation accuracy 0.427614\n",
      "Iter 721, training loss 18006.740234, training accuracy 0.620161\n",
      "Iter 721, validation loss 25577.839844, validation accuracy 0.445487\n",
      "Iter 722, training loss 41852.488281, training accuracy 0.316706\n",
      "Iter 722, validation loss 53615.222656, validation accuracy 0.213584\n",
      "Iter 723, training loss 50680.902344, training accuracy 0.476623\n",
      "Iter 723, validation loss 62058.550781, validation accuracy 0.434316\n",
      "Iter 724, training loss 41953.964844, training accuracy 0.482281\n",
      "Iter 724, validation loss 52679.363281, validation accuracy 0.433870\n",
      "Iter 725, training loss 25645.980469, training accuracy 0.267570\n",
      "Iter 725, validation loss 35593.273438, validation accuracy 0.203307\n",
      "Iter 726, training loss 7679.415039, training accuracy 0.508040\n",
      "Iter 726, validation loss 12889.892578, validation accuracy 0.429401\n",
      "Iter 727, training loss 12716.367188, training accuracy 0.563133\n",
      "Iter 727, validation loss 17326.304688, validation accuracy 0.452189\n",
      "Iter 728, training loss 26754.625000, training accuracy 0.476027\n",
      "Iter 728, validation loss 32866.746094, validation accuracy 0.437444\n",
      "Iter 729, training loss 15344.693359, training accuracy 0.493151\n",
      "Iter 729, validation loss 22287.398438, validation accuracy 0.440125\n",
      "Iter 730, training loss 14428.779297, training accuracy 0.524717\n",
      "Iter 730, validation loss 18879.199219, validation accuracy 0.372207\n",
      "Iter 731, training loss 25335.447266, training accuracy 0.493746\n",
      "Iter 731, validation loss 29976.386719, validation accuracy 0.336461\n",
      "Iter 732, training loss 11359.833984, training accuracy 0.515188\n",
      "Iter 732, validation loss 16040.918945, validation accuracy 0.413762\n",
      "Iter 733, training loss 23478.708984, training accuracy 0.474390\n",
      "Iter 733, validation loss 31387.570312, validation accuracy 0.436997\n",
      "Iter 734, training loss 29481.851562, training accuracy 0.472156\n",
      "Iter 734, validation loss 38100.437500, validation accuracy 0.435210\n",
      "Iter 735, training loss 9349.344727, training accuracy 0.494491\n",
      "Iter 735, validation loss 17348.628906, validation accuracy 0.304290\n",
      "Iter 736, training loss 6587.371582, training accuracy 0.585021\n",
      "Iter 736, validation loss 11104.215820, validation accuracy 0.435210\n",
      "Iter 737, training loss 3020.505127, training accuracy 0.742257\n",
      "Iter 737, validation loss 8109.566406, validation accuracy 0.455764\n",
      "Iter 738, training loss 10748.654297, training accuracy 0.511316\n",
      "Iter 738, validation loss 18445.105469, validation accuracy 0.440572\n",
      "Iter 739, training loss 3332.830322, training accuracy 0.656492\n",
      "Iter 739, validation loss 9466.226562, validation accuracy 0.357015\n",
      "Iter 740, training loss 2584.741699, training accuracy 0.744044\n",
      "Iter 740, validation loss 6807.451172, validation accuracy 0.453977\n",
      "Iter 741, training loss 2953.701416, training accuracy 0.740917\n",
      "Iter 741, validation loss 8812.585938, validation accuracy 0.493744\n",
      "Iter 742, training loss 825.637756, training accuracy 0.877755\n",
      "Iter 742, validation loss 5057.871582, validation accuracy 0.495085\n",
      "Iter 743, training loss 3703.971436, training accuracy 0.674509\n",
      "Iter 743, validation loss 8786.277344, validation accuracy 0.369080\n",
      "Iter 744, training loss 1923.959961, training accuracy 0.804050\n",
      "Iter 744, validation loss 7281.891602, validation accuracy 0.504915\n",
      "Iter 745, training loss 2055.343994, training accuracy 0.817600\n",
      "Iter 745, validation loss 7085.674316, validation accuracy 0.491957\n",
      "Iter 746, training loss 1829.398804, training accuracy 0.798094\n",
      "Iter 746, validation loss 5777.711914, validation accuracy 0.453977\n",
      "Iter 747, training loss 1344.835205, training accuracy 0.806432\n",
      "Iter 747, validation loss 6132.778809, validation accuracy 0.479893\n",
      "Iter 748, training loss 3446.587158, training accuracy 0.687016\n",
      "Iter 748, validation loss 10270.309570, validation accuracy 0.463360\n",
      "Iter 749, training loss 4405.271973, training accuracy 0.651727\n",
      "Iter 749, validation loss 8939.426758, validation accuracy 0.454424\n",
      "Iter 750, training loss 1070.003174, training accuracy 0.837552\n",
      "Iter 750, validation loss 5456.956055, validation accuracy 0.500447\n",
      "Iter 751, training loss 5943.761230, training accuracy 0.605718\n",
      "Iter 751, validation loss 13544.531250, validation accuracy 0.449508\n",
      "Iter 752, training loss 4837.202637, training accuracy 0.641155\n",
      "Iter 752, validation loss 9534.100586, validation accuracy 0.407507\n",
      "Iter 753, training loss 2672.832520, training accuracy 0.742853\n",
      "Iter 753, validation loss 7499.390625, validation accuracy 0.452636\n",
      "Iter 754, training loss 12577.686523, training accuracy 0.498809\n",
      "Iter 754, validation loss 20691.109375, validation accuracy 0.437444\n",
      "Iter 755, training loss 3981.857666, training accuracy 0.613758\n",
      "Iter 755, validation loss 10707.850586, validation accuracy 0.342270\n",
      "Iter 756, training loss 4864.439941, training accuracy 0.620607\n",
      "Iter 756, validation loss 10342.827148, validation accuracy 0.461126\n",
      "Iter 757, training loss 11642.399414, training accuracy 0.509232\n",
      "Iter 757, validation loss 19387.146484, validation accuracy 0.441019\n",
      "Iter 758, training loss 2222.722168, training accuracy 0.759381\n",
      "Iter 758, validation loss 6975.360840, validation accuracy 0.411528\n",
      "Iter 759, training loss 3183.563965, training accuracy 0.712478\n",
      "Iter 759, validation loss 7618.030273, validation accuracy 0.454870\n",
      "Iter 760, training loss 6014.285645, training accuracy 0.602591\n",
      "Iter 760, validation loss 13269.838867, validation accuracy 0.463360\n",
      "Iter 761, training loss 2276.526123, training accuracy 0.766528\n",
      "Iter 761, validation loss 6532.601562, validation accuracy 0.446828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 762, training loss 2607.083984, training accuracy 0.720518\n",
      "Iter 762, validation loss 8103.617676, validation accuracy 0.383378\n",
      "Iter 763, training loss 9250.066406, training accuracy 0.535736\n",
      "Iter 763, validation loss 16890.066406, validation accuracy 0.446828\n",
      "Iter 764, training loss 9800.157227, training accuracy 0.485259\n",
      "Iter 764, validation loss 15319.192383, validation accuracy 0.422252\n",
      "Iter 765, training loss 7979.709473, training accuracy 0.511465\n",
      "Iter 765, validation loss 16089.436523, validation accuracy 0.314567\n",
      "Iter 766, training loss 24323.406250, training accuracy 0.474241\n",
      "Iter 766, validation loss 32758.052734, validation accuracy 0.436997\n",
      "Iter 767, training loss 3398.593262, training accuracy 0.756998\n",
      "Iter 767, validation loss 9026.484375, validation accuracy 0.500447\n",
      "Iter 768, training loss 28941.818359, training accuracy 0.453544\n",
      "Iter 768, validation loss 33515.617188, validation accuracy 0.403038\n",
      "Iter 769, training loss 44822.648438, training accuracy 0.222454\n",
      "Iter 769, validation loss 53854.335938, validation accuracy 0.197051\n",
      "Iter 770, training loss 4910.876465, training accuracy 0.625670\n",
      "Iter 770, validation loss 12407.530273, validation accuracy 0.462466\n",
      "Iter 771, training loss 10638.589844, training accuracy 0.720965\n",
      "Iter 771, validation loss 15166.378906, validation accuracy 0.485702\n",
      "Iter 772, training loss 15442.208008, training accuracy 0.431954\n",
      "Iter 772, validation loss 20847.542969, validation accuracy 0.425827\n",
      "Iter 773, training loss 19793.060547, training accuracy 0.512805\n",
      "Iter 773, validation loss 29343.232422, validation accuracy 0.435657\n",
      "Iter 774, training loss 57015.242188, training accuracy 0.212180\n",
      "Iter 774, validation loss 69114.218750, validation accuracy 0.194370\n",
      "Iter 775, training loss 18546.332031, training accuracy 0.612120\n",
      "Iter 775, validation loss 27066.609375, validation accuracy 0.441912\n",
      "Iter 776, training loss 11464.319336, training accuracy 0.580405\n",
      "Iter 776, validation loss 18265.265625, validation accuracy 0.465147\n",
      "Iter 777, training loss 65792.812500, training accuracy 0.327129\n",
      "Iter 777, validation loss 67577.296875, validation accuracy 0.371760\n",
      "Iter 778, training loss 40490.886719, training accuracy 0.334276\n",
      "Iter 778, validation loss 44609.390625, validation accuracy 0.372654\n",
      "Iter 779, training loss 23167.013672, training accuracy 0.625372\n",
      "Iter 779, validation loss 32213.968750, validation accuracy 0.441019\n",
      "Iter 780, training loss 53702.832031, training accuracy 0.464562\n",
      "Iter 780, validation loss 67374.609375, validation accuracy 0.293119\n",
      "Iter 781, training loss 78121.671875, training accuracy 0.470667\n",
      "Iter 781, validation loss 92498.765625, validation accuracy 0.434763\n",
      "Iter 782, training loss 57295.789062, training accuracy 0.500149\n",
      "Iter 782, validation loss 71120.476562, validation accuracy 0.433423\n",
      "Iter 783, training loss 68792.070312, training accuracy 0.208457\n",
      "Iter 783, validation loss 81007.953125, validation accuracy 0.193476\n",
      "Iter 784, training loss 20571.369141, training accuracy 0.484663\n",
      "Iter 784, validation loss 25557.763672, validation accuracy 0.399911\n",
      "Iter 785, training loss 27266.865234, training accuracy 0.365843\n",
      "Iter 785, validation loss 31803.939453, validation accuracy 0.396783\n",
      "Iter 786, training loss 61341.070312, training accuracy 0.469327\n",
      "Iter 786, validation loss 68625.046875, validation accuracy 0.434763\n",
      "Iter 787, training loss 69807.718750, training accuracy 0.469029\n",
      "Iter 787, validation loss 78960.726562, validation accuracy 0.434763\n",
      "Iter 788, training loss 23292.876953, training accuracy 0.571471\n",
      "Iter 788, validation loss 33557.367188, validation accuracy 0.437444\n",
      "Iter 789, training loss 82621.742188, training accuracy 0.207862\n",
      "Iter 789, validation loss 92043.476562, validation accuracy 0.195264\n",
      "Iter 790, training loss 80047.265625, training accuracy 0.389666\n",
      "Iter 790, validation loss 83162.750000, validation accuracy 0.379803\n",
      "Iter 791, training loss 67119.726562, training accuracy 0.385200\n",
      "Iter 791, validation loss 70861.539062, validation accuracy 0.380250\n",
      "Iter 792, training loss 37881.078125, training accuracy 0.221114\n",
      "Iter 792, validation loss 48625.300781, validation accuracy 0.198838\n",
      "Iter 793, training loss 76545.523438, training accuracy 0.469029\n",
      "Iter 793, validation loss 86051.000000, validation accuracy 0.434763\n",
      "Iter 794, training loss 110536.531250, training accuracy 0.469029\n",
      "Iter 794, validation loss 118283.023438, validation accuracy 0.434763\n",
      "Iter 795, training loss 85518.578125, training accuracy 0.469029\n",
      "Iter 795, validation loss 91961.898438, validation accuracy 0.434763\n",
      "Iter 796, training loss 19817.835938, training accuracy 0.621650\n",
      "Iter 796, validation loss 24171.634766, validation accuracy 0.449062\n",
      "Iter 797, training loss 47782.382812, training accuracy 0.350804\n",
      "Iter 797, validation loss 52561.535156, validation accuracy 0.374888\n",
      "Iter 798, training loss 88894.226562, training accuracy 0.207117\n",
      "Iter 798, validation loss 99283.273438, validation accuracy 0.194370\n",
      "Iter 799, training loss 57810.109375, training accuracy 0.210840\n",
      "Iter 799, validation loss 69676.585938, validation accuracy 0.195264\n",
      "Iter 800, training loss 54535.882812, training accuracy 0.469625\n",
      "Iter 800, validation loss 62199.820312, validation accuracy 0.435210\n",
      "Iter 801, training loss 68141.757812, training accuracy 0.473347\n",
      "Iter 801, validation loss 70897.304688, validation accuracy 0.437444\n",
      "Iter 802, training loss 92867.515625, training accuracy 0.332787\n",
      "Iter 802, validation loss 89622.851562, validation accuracy 0.374888\n",
      "Iter 803, training loss 76404.515625, training accuracy 0.358100\n",
      "Iter 803, validation loss 74393.539062, validation accuracy 0.392761\n",
      "Iter 804, training loss 90097.726562, training accuracy 0.469327\n",
      "Iter 804, validation loss 94063.757812, validation accuracy 0.434763\n",
      "Iter 805, training loss 84161.179688, training accuracy 0.469029\n",
      "Iter 805, validation loss 92770.234375, validation accuracy 0.434763\n",
      "Iter 806, training loss 27211.974609, training accuracy 0.616885\n",
      "Iter 806, validation loss 37564.140625, validation accuracy 0.443700\n",
      "Iter 807, training loss 108005.250000, training accuracy 0.205926\n",
      "Iter 807, validation loss 119944.476562, validation accuracy 0.192583\n",
      "Iter 808, training loss 71466.109375, training accuracy 0.259083\n",
      "Iter 808, validation loss 79842.460938, validation accuracy 0.214477\n",
      "Iter 809, training loss 115872.085938, training accuracy 0.326534\n",
      "Iter 809, validation loss 114729.695312, validation accuracy 0.371760\n",
      "Iter 810, training loss 94546.429688, training accuracy 0.328767\n",
      "Iter 810, validation loss 92319.234375, validation accuracy 0.371760\n",
      "Iter 811, training loss 116389.296875, training accuracy 0.469327\n",
      "Iter 811, validation loss 117830.851562, validation accuracy 0.434763\n",
      "Iter 812, training loss 159494.687500, training accuracy 0.469029\n",
      "Iter 812, validation loss 162467.609375, validation accuracy 0.434763\n",
      "Iter 813, training loss 141199.500000, training accuracy 0.469029\n",
      "Iter 813, validation loss 145850.781250, validation accuracy 0.434763\n",
      "Iter 814, training loss 67644.570312, training accuracy 0.469774\n",
      "Iter 814, validation loss 74105.554688, validation accuracy 0.435210\n",
      "Iter 815, training loss 46215.304688, training accuracy 0.361078\n",
      "Iter 815, validation loss 51657.933594, validation accuracy 0.376676\n",
      "Iter 816, training loss 127489.335938, training accuracy 0.208457\n",
      "Iter 816, validation loss 136371.328125, validation accuracy 0.195264\n",
      "Iter 817, training loss 127848.031250, training accuracy 0.213371\n",
      "Iter 817, validation loss 136040.093750, validation accuracy 0.195264\n",
      "Iter 818, training loss 89149.218750, training accuracy 0.334723\n",
      "Iter 818, validation loss 92432.726562, validation accuracy 0.371760\n",
      "Iter 819, training loss 27276.576172, training accuracy 0.374628\n",
      "Iter 819, validation loss 32974.898438, validation accuracy 0.400357\n",
      "Iter 820, training loss 107636.757812, training accuracy 0.469029\n",
      "Iter 820, validation loss 117722.843750, validation accuracy 0.434763\n",
      "Iter 821, training loss 158535.546875, training accuracy 0.469029\n",
      "Iter 821, validation loss 171848.171875, validation accuracy 0.434763\n",
      "Iter 822, training loss 147174.171875, training accuracy 0.469029\n",
      "Iter 822, validation loss 163709.171875, validation accuracy 0.434763\n",
      "Iter 823, training loss 82791.203125, training accuracy 0.622096\n",
      "Iter 823, validation loss 100180.507812, validation accuracy 0.422699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 824, training loss 163174.953125, training accuracy 0.205479\n",
      "Iter 824, validation loss 181086.625000, validation accuracy 0.191689\n",
      "Iter 825, training loss 130506.367188, training accuracy 0.205479\n",
      "Iter 825, validation loss 144203.656250, validation accuracy 0.192136\n",
      "Iter 826, training loss 46370.882812, training accuracy 0.351102\n",
      "Iter 826, validation loss 52102.937500, validation accuracy 0.374441\n",
      "Iter 827, training loss 46948.246094, training accuracy 0.369267\n",
      "Iter 827, validation loss 50106.457031, validation accuracy 0.399911\n",
      "Iter 828, training loss 103231.929688, training accuracy 0.469327\n",
      "Iter 828, validation loss 108177.843750, validation accuracy 0.434763\n",
      "Iter 829, training loss 134330.609375, training accuracy 0.469029\n",
      "Iter 829, validation loss 140100.453125, validation accuracy 0.434763\n",
      "Iter 830, training loss 104765.179688, training accuracy 0.469029\n",
      "Iter 830, validation loss 111568.867188, validation accuracy 0.434763\n",
      "Iter 831, training loss 21478.576172, training accuracy 0.580703\n",
      "Iter 831, validation loss 28866.527344, validation accuracy 0.465594\n",
      "Iter 832, training loss 81025.835938, training accuracy 0.394133\n",
      "Iter 832, validation loss 85419.929688, validation accuracy 0.382484\n",
      "Iter 833, training loss 163466.078125, training accuracy 0.212180\n",
      "Iter 833, validation loss 171386.328125, validation accuracy 0.195264\n",
      "Iter 834, training loss 156558.390625, training accuracy 0.232877\n",
      "Iter 834, validation loss 163396.609375, validation accuracy 0.201072\n",
      "Iter 835, training loss 140039.390625, training accuracy 0.329363\n",
      "Iter 835, validation loss 141115.703125, validation accuracy 0.371760\n",
      "Iter 836, training loss 79858.296875, training accuracy 0.328023\n",
      "Iter 836, validation loss 83047.664062, validation accuracy 0.371760\n",
      "Iter 837, training loss 66835.179688, training accuracy 0.469476\n",
      "Iter 837, validation loss 76658.125000, validation accuracy 0.435210\n",
      "Iter 838, training loss 121179.398438, training accuracy 0.469029\n",
      "Iter 838, validation loss 134190.718750, validation accuracy 0.434763\n",
      "Iter 839, training loss 112091.171875, training accuracy 0.469029\n",
      "Iter 839, validation loss 128285.109375, validation accuracy 0.434763\n",
      "Iter 840, training loss 71894.007812, training accuracy 0.333234\n",
      "Iter 840, validation loss 90490.179688, validation accuracy 0.221626\n",
      "Iter 841, training loss 52134.503906, training accuracy 0.298094\n",
      "Iter 841, validation loss 68550.218750, validation accuracy 0.209115\n",
      "Iter 842, training loss 39054.316406, training accuracy 0.473794\n",
      "Iter 842, validation loss 48648.011719, validation accuracy 0.437444\n",
      "Iter 843, training loss 78028.039062, training accuracy 0.332936\n",
      "Iter 843, validation loss 79015.226562, validation accuracy 0.374888\n",
      "Iter 844, training loss 84276.937500, training accuracy 0.334425\n",
      "Iter 844, validation loss 84061.812500, validation accuracy 0.375335\n",
      "Iter 845, training loss 70431.382812, training accuracy 0.473794\n",
      "Iter 845, validation loss 75433.554688, validation accuracy 0.437444\n",
      "Iter 846, training loss 67440.523438, training accuracy 0.470071\n",
      "Iter 846, validation loss 75684.148438, validation accuracy 0.436104\n",
      "Iter 847, training loss 7226.175781, training accuracy 0.614949\n",
      "Iter 847, validation loss 17597.455078, validation accuracy 0.454424\n",
      "Iter 848, training loss 90300.273438, training accuracy 0.231983\n",
      "Iter 848, validation loss 100901.164062, validation accuracy 0.199732\n",
      "Iter 849, training loss 104820.421875, training accuracy 0.472603\n",
      "Iter 849, validation loss 108107.359375, validation accuracy 0.398570\n",
      "Iter 850, training loss 97012.328125, training accuracy 0.459351\n",
      "Iter 850, validation loss 100803.523438, validation accuracy 0.402145\n",
      "Iter 851, training loss 62127.871094, training accuracy 0.255658\n",
      "Iter 851, validation loss 73349.789062, validation accuracy 0.212690\n",
      "Iter 852, training loss 31694.212891, training accuracy 0.474985\n",
      "Iter 852, validation loss 42537.746094, validation accuracy 0.437444\n",
      "Iter 853, training loss 63341.703125, training accuracy 0.470667\n",
      "Iter 853, validation loss 71680.601562, validation accuracy 0.436550\n",
      "Iter 854, training loss 34821.441406, training accuracy 0.665426\n",
      "Iter 854, validation loss 39444.023438, validation accuracy 0.484361\n",
      "Iter 855, training loss 78348.304688, training accuracy 0.328767\n",
      "Iter 855, validation loss 81218.679688, validation accuracy 0.371760\n",
      "Iter 856, training loss 33308.765625, training accuracy 0.377308\n",
      "Iter 856, validation loss 40612.410156, validation accuracy 0.381591\n",
      "Iter 857, training loss 64801.500000, training accuracy 0.222454\n",
      "Iter 857, validation loss 81713.335938, validation accuracy 0.197498\n",
      "Iter 858, training loss 90180.156250, training accuracy 0.473496\n",
      "Iter 858, validation loss 109110.117188, validation accuracy 0.434763\n",
      "Iter 859, training loss 108707.812500, training accuracy 0.470965\n",
      "Iter 859, validation loss 129017.664062, validation accuracy 0.434763\n",
      "Iter 860, training loss 83485.921875, training accuracy 0.426742\n",
      "Iter 860, validation loss 103763.781250, validation accuracy 0.268543\n",
      "Iter 861, training loss 54351.960938, training accuracy 0.625074\n",
      "Iter 861, validation loss 69636.679688, validation accuracy 0.429401\n",
      "Iter 862, training loss 12056.445312, training accuracy 0.639220\n",
      "Iter 862, validation loss 22561.695312, validation accuracy 0.438785\n",
      "Iter 863, training loss 76227.148438, training accuracy 0.330107\n",
      "Iter 863, validation loss 80750.976562, validation accuracy 0.371760\n",
      "Iter 864, training loss 88296.054688, training accuracy 0.328023\n",
      "Iter 864, validation loss 91904.140625, validation accuracy 0.371760\n",
      "Iter 865, training loss 1947.721558, training accuracy 0.839488\n",
      "Iter 865, validation loss 7484.328125, validation accuracy 0.468722\n",
      "Iter 866, training loss 43119.046875, training accuracy 0.510721\n",
      "Iter 866, validation loss 58617.496094, validation accuracy 0.433423\n",
      "Iter 867, training loss 64626.125000, training accuracy 0.334276\n",
      "Iter 867, validation loss 83505.343750, validation accuracy 0.221626\n",
      "Iter 868, training loss 70042.445312, training accuracy 0.476325\n",
      "Iter 868, validation loss 87528.835938, validation accuracy 0.434316\n",
      "Iter 869, training loss 37653.851562, training accuracy 0.549136\n",
      "Iter 869, validation loss 52669.281250, validation accuracy 0.435657\n",
      "Iter 870, training loss 37499.578125, training accuracy 0.314324\n",
      "Iter 870, validation loss 49495.179688, validation accuracy 0.235478\n",
      "Iter 871, training loss 104428.460938, training accuracy 0.327427\n",
      "Iter 871, validation loss 107050.945312, validation accuracy 0.371760\n",
      "Iter 872, training loss 92497.804688, training accuracy 0.327874\n",
      "Iter 872, validation loss 94747.414062, validation accuracy 0.371760\n",
      "Iter 873, training loss 43938.582031, training accuracy 0.478856\n",
      "Iter 873, validation loss 53176.687500, validation accuracy 0.437891\n",
      "Iter 874, training loss 56148.496094, training accuracy 0.469625\n",
      "Iter 874, validation loss 69106.906250, validation accuracy 0.435210\n",
      "Iter 875, training loss 36807.050781, training accuracy 0.291096\n",
      "Iter 875, validation loss 52833.937500, validation accuracy 0.208668\n",
      "Iter 876, training loss 6525.194824, training accuracy 0.664532\n",
      "Iter 876, validation loss 16593.798828, validation accuracy 0.455317\n",
      "Iter 877, training loss 28064.630859, training accuracy 0.388177\n",
      "Iter 877, validation loss 35773.367188, validation accuracy 0.402145\n",
      "Iter 878, training loss 20034.929688, training accuracy 0.629244\n",
      "Iter 878, validation loss 28525.486328, validation accuracy 0.480340\n",
      "Iter 879, training loss 6961.481934, training accuracy 0.694461\n",
      "Iter 879, validation loss 16259.259766, validation accuracy 0.484808\n",
      "Iter 880, training loss 33803.796875, training accuracy 0.354824\n",
      "Iter 880, validation loss 45373.125000, validation accuracy 0.254692\n",
      "Iter 881, training loss 24689.787109, training accuracy 0.461435\n",
      "Iter 881, validation loss 32541.140625, validation accuracy 0.410634\n",
      "Iter 882, training loss 20542.806641, training accuracy 0.497618\n",
      "Iter 882, validation loss 32883.304688, validation accuracy 0.438785\n",
      "Iter 883, training loss 12258.875977, training accuracy 0.603782\n",
      "Iter 883, validation loss 24574.513672, validation accuracy 0.440572\n",
      "Iter 884, training loss 37549.546875, training accuracy 0.326980\n",
      "Iter 884, validation loss 49767.378906, validation accuracy 0.242627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 885, training loss 55089.761719, training accuracy 0.339934\n",
      "Iter 885, validation loss 61818.738281, validation accuracy 0.372654\n",
      "Iter 886, training loss 14340.905273, training accuracy 0.711584\n",
      "Iter 886, validation loss 21863.996094, validation accuracy 0.496872\n",
      "Iter 887, training loss 28495.947266, training accuracy 0.480197\n",
      "Iter 887, validation loss 41038.195312, validation accuracy 0.437444\n",
      "Iter 888, training loss 22552.279297, training accuracy 0.314026\n",
      "Iter 888, validation loss 36642.468750, validation accuracy 0.223414\n",
      "Iter 889, training loss 10181.315430, training accuracy 0.565068\n",
      "Iter 889, validation loss 19066.595703, validation accuracy 0.455764\n",
      "Iter 890, training loss 26680.207031, training accuracy 0.505807\n",
      "Iter 890, validation loss 37664.695312, validation accuracy 0.444593\n",
      "Iter 891, training loss 4756.786133, training accuracy 0.763550\n",
      "Iter 891, validation loss 13224.449219, validation accuracy 0.491063\n",
      "Iter 892, training loss 31158.982422, training accuracy 0.475730\n",
      "Iter 892, validation loss 39939.906250, validation accuracy 0.310098\n",
      "Iter 893, training loss 22083.425781, training accuracy 0.511465\n",
      "Iter 893, validation loss 29653.779297, validation accuracy 0.403932\n",
      "Iter 894, training loss 24965.537109, training accuracy 0.486004\n",
      "Iter 894, validation loss 38456.789062, validation accuracy 0.436997\n",
      "Iter 895, training loss 18143.759766, training accuracy 0.605122\n",
      "Iter 895, validation loss 31270.794922, validation accuracy 0.431635\n",
      "Iter 896, training loss 42786.472656, training accuracy 0.260274\n",
      "Iter 896, validation loss 56793.039062, validation accuracy 0.214477\n",
      "Iter 897, training loss 55751.480469, training accuracy 0.338148\n",
      "Iter 897, validation loss 62353.085938, validation accuracy 0.374888\n",
      "Iter 898, training loss 26922.972656, training accuracy 0.671084\n",
      "Iter 898, validation loss 33885.457031, validation accuracy 0.484808\n",
      "Iter 899, training loss 37862.289062, training accuracy 0.490917\n",
      "Iter 899, validation loss 48206.214844, validation accuracy 0.439678\n",
      "Iter 900, training loss 6986.695801, training accuracy 0.655301\n",
      "Iter 900, validation loss 15115.440430, validation accuracy 0.462020\n",
      "Iter 901, training loss 43936.234375, training accuracy 0.239130\n",
      "Iter 901, validation loss 59029.769531, validation accuracy 0.202413\n",
      "Iter 902, training loss 12022.187500, training accuracy 0.561197\n",
      "Iter 902, validation loss 24278.433594, validation accuracy 0.453083\n",
      "Iter 903, training loss 18680.976562, training accuracy 0.484961\n",
      "Iter 903, validation loss 27536.712891, validation accuracy 0.445487\n",
      "Iter 904, training loss 13100.612305, training accuracy 0.553454\n",
      "Iter 904, validation loss 25402.533203, validation accuracy 0.452636\n",
      "Iter 905, training loss 31080.736328, training accuracy 0.284098\n",
      "Iter 905, validation loss 45643.531250, validation accuracy 0.218945\n",
      "Iter 906, training loss 10322.019531, training accuracy 0.586659\n",
      "Iter 906, validation loss 19567.126953, validation accuracy 0.455764\n",
      "Iter 907, training loss 37075.898438, training accuracy 0.481537\n",
      "Iter 907, validation loss 48446.292969, validation accuracy 0.437891\n",
      "Iter 908, training loss 10335.544922, training accuracy 0.676147\n",
      "Iter 908, validation loss 20110.693359, validation accuracy 0.484361\n",
      "Iter 909, training loss 38608.726562, training accuracy 0.512656\n",
      "Iter 909, validation loss 45818.746094, validation accuracy 0.372654\n",
      "Iter 910, training loss 43606.609375, training accuracy 0.485557\n",
      "Iter 910, validation loss 51892.968750, validation accuracy 0.326184\n",
      "Iter 911, training loss 7486.227539, training accuracy 0.692823\n",
      "Iter 911, validation loss 16309.698242, validation accuracy 0.453977\n",
      "Iter 912, training loss 55186.171875, training accuracy 0.470816\n",
      "Iter 912, validation loss 67429.140625, validation accuracy 0.436997\n",
      "Iter 913, training loss 33408.949219, training accuracy 0.476623\n",
      "Iter 913, validation loss 46614.503906, validation accuracy 0.437444\n",
      "Iter 914, training loss 38905.789062, training accuracy 0.494491\n",
      "Iter 914, validation loss 47145.410156, validation accuracy 0.336461\n",
      "Iter 915, training loss 68775.781250, training accuracy 0.401132\n",
      "Iter 915, validation loss 75867.085938, validation accuracy 0.386059\n",
      "Iter 916, training loss 32064.267578, training accuracy 0.332043\n",
      "Iter 916, validation loss 45681.121094, validation accuracy 0.240393\n",
      "Iter 917, training loss 64017.675781, training accuracy 0.469923\n",
      "Iter 917, validation loss 75853.031250, validation accuracy 0.436550\n",
      "Iter 918, training loss 78838.968750, training accuracy 0.470965\n",
      "Iter 918, validation loss 87563.671875, validation accuracy 0.436997\n",
      "Iter 919, training loss 52732.917969, training accuracy 0.407981\n",
      "Iter 919, validation loss 57587.890625, validation accuracy 0.417784\n",
      "Iter 920, training loss 18948.875000, training accuracy 0.720667\n",
      "Iter 920, validation loss 26074.589844, validation accuracy 0.480786\n",
      "Iter 921, training loss 21166.039062, training accuracy 0.339637\n",
      "Iter 921, validation loss 36487.234375, validation accuracy 0.222967\n",
      "Iter 922, training loss 15475.092773, training accuracy 0.534991\n",
      "Iter 922, validation loss 29388.134766, validation accuracy 0.438338\n",
      "Iter 923, training loss 13536.353516, training accuracy 0.550179\n",
      "Iter 923, validation loss 22185.433594, validation accuracy 0.437444\n",
      "Iter 924, training loss 4031.983398, training accuracy 0.714860\n",
      "Iter 924, validation loss 13819.488281, validation accuracy 0.433870\n",
      "Iter 925, training loss 23341.513672, training accuracy 0.490917\n",
      "Iter 925, validation loss 37325.812500, validation accuracy 0.437444\n",
      "Iter 926, training loss 7255.620117, training accuracy 0.665575\n",
      "Iter 926, validation loss 14952.897461, validation accuracy 0.445487\n",
      "Iter 927, training loss 4645.843750, training accuracy 0.746724\n",
      "Iter 927, validation loss 11995.672852, validation accuracy 0.418677\n",
      "Iter 928, training loss 18821.134766, training accuracy 0.520697\n",
      "Iter 928, validation loss 31881.179688, validation accuracy 0.445934\n",
      "Iter 929, training loss 7340.572754, training accuracy 0.662001\n",
      "Iter 929, validation loss 15695.100586, validation accuracy 0.462466\n",
      "Iter 930, training loss 9855.728516, training accuracy 0.529780\n",
      "Iter 930, validation loss 22766.451172, validation accuracy 0.306077\n",
      "Iter 931, training loss 35454.660156, training accuracy 0.481239\n",
      "Iter 931, validation loss 47848.671875, validation accuracy 0.437891\n",
      "Iter 932, training loss 27879.851562, training accuracy 0.456671\n",
      "Iter 932, validation loss 36290.265625, validation accuracy 0.438338\n",
      "Iter 933, training loss 12797.543945, training accuracy 0.561048\n",
      "Iter 933, validation loss 25837.392578, validation accuracy 0.453977\n",
      "Iter 934, training loss 59988.917969, training accuracy 0.227219\n",
      "Iter 934, validation loss 75919.593750, validation accuracy 0.200179\n",
      "Iter 935, training loss 18035.937500, training accuracy 0.486748\n",
      "Iter 935, validation loss 27285.111328, validation accuracy 0.423146\n",
      "Iter 936, training loss 52459.750000, training accuracy 0.471412\n",
      "Iter 936, validation loss 65459.652344, validation accuracy 0.436997\n",
      "Iter 937, training loss 39307.855469, training accuracy 0.473794\n",
      "Iter 937, validation loss 53080.535156, validation accuracy 0.437444\n",
      "Iter 938, training loss 34576.523438, training accuracy 0.508189\n",
      "Iter 938, validation loss 42926.292969, validation accuracy 0.351653\n",
      "Iter 939, training loss 58146.023438, training accuracy 0.427487\n",
      "Iter 939, validation loss 65933.890625, validation accuracy 0.394549\n",
      "Iter 940, training loss 22159.296875, training accuracy 0.331596\n",
      "Iter 940, validation loss 37975.203125, validation accuracy 0.221626\n",
      "Iter 941, training loss 101898.789062, training accuracy 0.469327\n",
      "Iter 941, validation loss 113886.960938, validation accuracy 0.434763\n",
      "Iter 942, training loss 118155.921875, training accuracy 0.469625\n",
      "Iter 942, validation loss 126060.796875, validation accuracy 0.435210\n",
      "Iter 943, training loss 59393.468750, training accuracy 0.619714\n",
      "Iter 943, validation loss 61863.304688, validation accuracy 0.448168\n",
      "Iter 944, training loss 80889.937500, training accuracy 0.335468\n",
      "Iter 944, validation loss 84670.250000, validation accuracy 0.376676\n",
      "Iter 945, training loss 16152.291992, training accuracy 0.560006\n",
      "Iter 945, validation loss 30803.732422, validation accuracy 0.438338\n",
      "Iter 946, training loss 132782.781250, training accuracy 0.206820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 946, validation loss 153075.515625, validation accuracy 0.192136\n",
      "Iter 947, training loss 75577.679688, training accuracy 0.218136\n",
      "Iter 947, validation loss 95452.273438, validation accuracy 0.197051\n",
      "Iter 948, training loss 87404.937500, training accuracy 0.470071\n",
      "Iter 948, validation loss 96967.601562, validation accuracy 0.436550\n",
      "Iter 949, training loss 95364.914062, training accuracy 0.681805\n",
      "Iter 949, validation loss 91889.265625, validation accuracy 0.472744\n",
      "Iter 950, training loss 164117.156250, training accuracy 0.335914\n",
      "Iter 950, validation loss 155589.703125, validation accuracy 0.376676\n",
      "Iter 951, training loss 140227.109375, training accuracy 0.489279\n",
      "Iter 951, validation loss 137213.765625, validation accuracy 0.439678\n",
      "Iter 952, training loss 111249.093750, training accuracy 0.520548\n",
      "Iter 952, validation loss 111076.406250, validation accuracy 0.448615\n",
      "Iter 953, training loss 112905.593750, training accuracy 0.330852\n",
      "Iter 953, validation loss 113239.523438, validation accuracy 0.373101\n",
      "Iter 954, training loss 6158.216797, training accuracy 0.703544\n",
      "Iter 954, validation loss 14150.589844, validation accuracy 0.453977\n",
      "Iter 955, training loss 127252.968750, training accuracy 0.209946\n",
      "Iter 955, validation loss 150670.890625, validation accuracy 0.193923\n",
      "Iter 956, training loss 111649.007812, training accuracy 0.499256\n",
      "Iter 956, validation loss 137269.968750, validation accuracy 0.432976\n",
      "Iter 957, training loss 113826.507812, training accuracy 0.502978\n",
      "Iter 957, validation loss 139807.687500, validation accuracy 0.432976\n",
      "Iter 958, training loss 131742.343750, training accuracy 0.211882\n",
      "Iter 958, validation loss 156675.406250, validation accuracy 0.194370\n",
      "Iter 959, training loss 18270.009766, training accuracy 0.622394\n",
      "Iter 959, validation loss 32594.914062, validation accuracy 0.422252\n",
      "Iter 960, training loss 114617.921875, training accuracy 0.327576\n",
      "Iter 960, validation loss 117361.179688, validation accuracy 0.371760\n",
      "Iter 961, training loss 109319.117188, training accuracy 0.329363\n",
      "Iter 961, validation loss 111457.648438, validation accuracy 0.372207\n",
      "Iter 962, training loss 76826.132812, training accuracy 0.469923\n",
      "Iter 962, validation loss 89312.085938, validation accuracy 0.436550\n",
      "Iter 963, training loss 74913.570312, training accuracy 0.470965\n",
      "Iter 963, validation loss 93822.023438, validation accuracy 0.434763\n",
      "Iter 964, training loss 137089.796875, training accuracy 0.207415\n",
      "Iter 964, validation loss 159094.187500, validation accuracy 0.192583\n",
      "Iter 965, training loss 77811.921875, training accuracy 0.214562\n",
      "Iter 965, validation loss 96454.695312, validation accuracy 0.197945\n",
      "Iter 966, training loss 52083.792969, training accuracy 0.547647\n",
      "Iter 966, validation loss 57123.007812, validation accuracy 0.455764\n",
      "Iter 967, training loss 124330.921875, training accuracy 0.490024\n",
      "Iter 967, validation loss 124438.695312, validation accuracy 0.440572\n",
      "Iter 968, training loss 131880.375000, training accuracy 0.678678\n",
      "Iter 968, validation loss 123577.218750, validation accuracy 0.468722\n",
      "Iter 969, training loss 144647.828125, training accuracy 0.431507\n",
      "Iter 969, validation loss 137009.671875, validation accuracy 0.431635\n",
      "Iter 970, training loss 162624.031250, training accuracy 0.469923\n",
      "Iter 970, validation loss 163205.203125, validation accuracy 0.436104\n",
      "Iter 971, training loss 114693.250000, training accuracy 0.469923\n",
      "Iter 971, validation loss 121686.851562, validation accuracy 0.436550\n",
      "Iter 972, training loss 21064.488281, training accuracy 0.448928\n",
      "Iter 972, validation loss 31580.310547, validation accuracy 0.415550\n",
      "Iter 973, training loss 178784.140625, training accuracy 0.205628\n",
      "Iter 973, validation loss 198996.484375, validation accuracy 0.192136\n",
      "Iter 974, training loss 166892.890625, training accuracy 0.206075\n",
      "Iter 974, validation loss 189165.703125, validation accuracy 0.191689\n",
      "Iter 975, training loss 63740.199219, training accuracy 0.470071\n",
      "Iter 975, validation loss 81005.695312, validation accuracy 0.435210\n",
      "Iter 976, training loss 60583.777344, training accuracy 0.511018\n",
      "Iter 976, validation loss 69525.000000, validation accuracy 0.445934\n",
      "Iter 977, training loss 170899.765625, training accuracy 0.326534\n",
      "Iter 977, validation loss 167818.687500, validation accuracy 0.371760\n",
      "Iter 978, training loss 144555.718750, training accuracy 0.326980\n",
      "Iter 978, validation loss 144179.546875, validation accuracy 0.371760\n",
      "Iter 979, training loss 48701.687500, training accuracy 0.473347\n",
      "Iter 979, validation loss 63168.164062, validation accuracy 0.437444\n",
      "Iter 980, training loss 68923.500000, training accuracy 0.305092\n",
      "Iter 980, validation loss 91852.953125, validation accuracy 0.209562\n",
      "Iter 981, training loss 75685.296875, training accuracy 0.499107\n",
      "Iter 981, validation loss 98187.695312, validation accuracy 0.432976\n",
      "Iter 982, training loss 44904.871094, training accuracy 0.427933\n",
      "Iter 982, validation loss 64641.214844, validation accuracy 0.269884\n",
      "Iter 983, training loss 17568.248047, training accuracy 0.708160\n",
      "Iter 983, validation loss 27210.410156, validation accuracy 0.497319\n",
      "Iter 984, training loss 116755.703125, training accuracy 0.328172\n",
      "Iter 984, validation loss 119687.507812, validation accuracy 0.371760\n",
      "Iter 985, training loss 39473.390625, training accuracy 0.419297\n",
      "Iter 985, validation loss 48273.652344, validation accuracy 0.424933\n",
      "Iter 986, training loss 106823.984375, training accuracy 0.469029\n",
      "Iter 986, validation loss 127916.609375, validation accuracy 0.434763\n",
      "Iter 987, training loss 122798.164062, training accuracy 0.498958\n",
      "Iter 987, validation loss 150636.562500, validation accuracy 0.332440\n",
      "Iter 988, training loss 140012.375000, training accuracy 0.514145\n",
      "Iter 988, validation loss 169640.468750, validation accuracy 0.351206\n",
      "Iter 989, training loss 154680.250000, training accuracy 0.469774\n",
      "Iter 989, validation loss 182916.468750, validation accuracy 0.434763\n",
      "Iter 990, training loss 75855.320312, training accuracy 0.550923\n",
      "Iter 990, validation loss 98908.960938, validation accuracy 0.434316\n",
      "Iter 991, training loss 85919.312500, training accuracy 0.240768\n",
      "Iter 991, validation loss 102344.460938, validation accuracy 0.203753\n",
      "Iter 992, training loss 237305.250000, training accuracy 0.326534\n",
      "Iter 992, validation loss 233371.281250, validation accuracy 0.371760\n",
      "Iter 993, training loss 256290.890625, training accuracy 0.326534\n",
      "Iter 993, validation loss 248611.953125, validation accuracy 0.371760\n",
      "Iter 994, training loss 75308.929688, training accuracy 0.384008\n",
      "Iter 994, validation loss 79641.101562, validation accuracy 0.406166\n",
      "Iter 995, training loss 221403.062500, training accuracy 0.469029\n",
      "Iter 995, validation loss 240032.859375, validation accuracy 0.434763\n",
      "Iter 996, training loss 296962.500000, training accuracy 0.469029\n",
      "Iter 996, validation loss 327234.156250, validation accuracy 0.434763\n",
      "Iter 997, training loss 254248.296875, training accuracy 0.472901\n",
      "Iter 997, validation loss 295460.156250, validation accuracy 0.434763\n",
      "Iter 998, training loss 421034.156250, training accuracy 0.205033\n",
      "Iter 998, validation loss 466024.906250, validation accuracy 0.192136\n",
      "Iter 999, training loss 429108.250000, training accuracy 0.204884\n",
      "Iter 999, validation loss 471153.000000, validation accuracy 0.192136\n"
     ]
    }
   ],
   "source": [
    "n_input = x_train.shape[1]\n",
    "n_hidden1 = 128\n",
    "n_hidden2 = 512\n",
    "n_hidden3 = 1024\n",
    "n_output = 3\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,n_input])\n",
    "y_gt = tf.placeholder(tf.float32,[None,n_output])\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False)\n",
    "W1 = tf.Variable(initializer([n_input,n_hidden1]))\n",
    "b1 = tf.Variable(tf.constant(0.1,shape=[n_hidden1]))\n",
    "H1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(initializer([n_hidden1,n_hidden2]))\n",
    "b2 = tf.Variable(tf.constant(0.1,shape=[n_hidden2]))\n",
    "H2 = tf.nn.relu(tf.matmul(H1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(initializer([n_hidden2,n_hidden3]))\n",
    "b3 = tf.Variable(tf.constant(0.1,shape=[n_hidden3]))\n",
    "H3 = tf.nn.relu(tf.matmul(H2,W3)+b3)\n",
    "\n",
    "W_out = tf.Variable(initializer([n_hidden3,n_output]))\n",
    "b_out = tf.Variable(tf.constant(0.1,shape=[n_output]))\n",
    "y_pred = tf.matmul(H3,W_out)+b_out\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_gt,logits=y_pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_gt,axis=1),tf.argmax(y_pred,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "tr_losses, te_losses = [], []\n",
    "for iter in range(epochs):\n",
    "#     batch_x,batch_y = random_batch(x_train_cls.values,y_train_cls,batch_size)\n",
    "    sess.run(train_step, feed_dict={x:x_train, y_gt:y_train})\n",
    "    if iter%1 == 0:\n",
    "        \n",
    "        train_loss = sess.run(loss, feed_dict={x:x_train, y_gt:y_train})\n",
    "        train_acc = sess.run(accuracy, feed_dict={x:x_train, y_gt:y_train})\n",
    "        tr_losses.append(train_acc)\n",
    "        print(\"Iter %d, training loss %f, training accuracy %f\" % (iter, train_loss, train_acc))\n",
    "        \n",
    "        y_validation_pred = sess.run(y_pred,feed_dict={x:x_val})\n",
    "        val_loss = sess.run(loss,feed_dict={x:x_val, y_gt:y_val})\n",
    "        val_acc = sess.run(accuracy,feed_dict={x:x_val, y_gt:y_val})\n",
    "        te_losses.append(val_acc)\n",
    "        print(\"Iter %d, validation loss %f, validation accuracy %f\" % (iter, val_loss, val_acc))\n",
    "\n",
    "y_validation_pred = np.argmax(sess.run(y_pred,feed_dict={x:x_val}),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA7s/6gTBKvUFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223325208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch number is:  94\n",
      "The hightest validation accuracy is:  0.5862377\n",
      "At the same epoch, the training accuracy is:  0.9638178\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch=list(range(1,epochs+1))\n",
    "plt.figure()\n",
    "plt.plot(epoch,tr_losses,label='training accuracy')\n",
    "plt.plot(epoch,te_losses,label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best epoch number is: \", te_losses.index(max(te_losses)))\n",
    "print(\"The hightest validation accuracy is: \", max(te_losses))\n",
    "print(\"At the same epoch, the training accuracy is: \", tr_losses[te_losses.index(max(te_losses))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Icf8yWaBYFuV"
   },
   "source": [
    "* **Diag**\n",
    " * **GBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "I-7JscpBYFuV",
    "outputId": "7a974328-188e-4c14-f5d0-cac1d0ce6946"
   },
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingClassifier(n_estimators=3000,learning_rate=0.05,max_depth=3,max_features='sqrt',\n",
    "                                          min_samples_leaf=15,min_samples_split=10)\n",
    "GBoost.fit(x_train_diag,y_train_diag)\n",
    "scores = cross_val_score(GBoost, x_train_diag, y_train_diag, cv=5)\n",
    "print('Training Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))\n",
    "y_validation_pred = GBoost.predict(x_val_diag)\n",
    "acc = accuracy_score(y_validation_pred,y_val_diag)\n",
    "print('Validation Accuracy: %0.2f (+/- %0.2f)' % (acc.mean(), acc.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "8adZIYmLh6cu",
    "outputId": "bbc03b23-3271-4703-a791-efc888ffac11"
   },
   "outputs": [],
   "source": [
    "cn_cls,mci_cls,ad_cls,cn_pred,mci_pred,ad_pred = transform(y_val_diag,y_validation_pred)\n",
    "metrics('CN_Diag',GBoost,X_val,cn_cls,cn_pred)\n",
    "print('*'*30)\n",
    "metrics('MCI_Diag',GBoost,X_val,mci_cls,mci_pred)\n",
    "print('*'*30)\n",
    "metrics('AD_Diag',GBoost,X_val,ad_cls,ad_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
